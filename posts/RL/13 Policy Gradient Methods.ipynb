{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Approximation and its Advantages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### short intro"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In Policy gradient methods, the policy can be parameterized in any way, as long as ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. differentiable with respect to its parameters\n",
    "i.e. $\\nabla \\pi(a|s,\\boldsymbol{\\theta})$ exists\n",
    "2. finite for all $s\\in S$ , $a \\in A(s)$ and $\\boldsymbol{\\theta} \\in \\mathbb R^{d'}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In practice, to ensure exploration we generally require that the policy never becomes deterministic i.e. $\\pi(a|s,\\boldsymbol{\\theta}) \\in (0,1)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Policy based methods offer useful ways of dealing with continuous action spaces, as we describe later in Section 13.7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### natural and common kind of parameterization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the action space is **discrete and not too large**, then a natural and common kind of parameterization is to form **parameterized numerical preferences** $h(s,a,\\boldsymbol{\\theta}) \\in \\mathbb{R}$ for each state-action pair\n",
    "i.e. **action space가 이산적이고 그렇게 크지 않은 경우**, action에 대한 선호도를 parameterization한다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actions with the highest preferences $\\to$ highest probabilities of being selected."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\pi(a|s;\\boldsymbol{\\theta}) \\overset{.}{=} \\frac{e^{h({s,a,\\boldsymbol{\\theta}})}}{\\sum_{b \\in A(s)} e^{h(s,b,\\boldsymbol{\\theta})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "comment : sutton교수님의 교재와는 다르게 $b$의 대상을 적어줬음."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We call this kind of policy parameterization **soft-max in action preferences**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The action preferences themselves can be parameterized arbitariliy. For example, they might be computed by a deep artificial neural network (ANN), where $\\boldsymbol{\\theta}$ is the vector of all the connection weights of network. Or the preferences could simply be linear in features, using feature vectors $\\bold{x}(s,a) \\in \\mathbb{R}^{d'}$ constricted by any of the methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "& h(s,a,\\boldsymbol{\\theta}) = \\boldsymbol{\\theta}^T\\bold{x}(s,a) \\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
