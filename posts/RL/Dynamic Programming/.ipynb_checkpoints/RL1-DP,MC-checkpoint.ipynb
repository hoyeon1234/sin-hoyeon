{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbfb86b6",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[Reinforcement Learning] 1. Dynamic Programming \"\n",
    "categories: Reinforcement Learning\n",
    "format: html\n",
    "author: 신호연\n",
    "date: 2023-07-04\n",
    "---\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee646c69",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "IPython.notebook.set_autosave_interval(60000)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Autosaving every 60 seconds\n"
     ]
    }
   ],
   "source": [
    "%autosave 60\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2e71367",
   "metadata": {},
   "source": [
    "# Dinamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f43a861",
   "metadata": {},
   "source": [
    "- definition : DP refers to a collection of **algorithms** that can be used to **compute optimal policies given a perfect model of the environment** as a Markov decision process(by suttion)\n",
    "    - MDP의 envirionment를 알고있을때 optimal policy를 구할 수 있는 알고리즘들\n",
    "- How? : prediction과 improvement를 번갈아가면서 수행\n",
    "- 한계\n",
    "    1. environment의 model에 대해서 완벽히 알아야함\n",
    "    2. computational expense가 너무 크다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc5b951a",
   "metadata": {},
   "source": [
    "# DP Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956a27c9",
   "metadata": {},
   "source": [
    "- definition : prediction $:=$ compute the state value function $v_\\pi$ for an arbitarary policy $\\pi$ = policy evaluation in the DP literature.\n",
    "- How? : $v_\\pi$에 대한 Bellman equation을 update rule로 바꿈 = iterative Policy evaluation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695c4115",
   "metadata": {},
   "source": [
    "**(Bellman equation for $v_\\pi$)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352f8514",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "v_\\pi(s) &:= E_\\pi[G_t|S_t = s] \\\\\n",
    "&= E_\\pi[R_{t+1} + \\gamma G_{t+1}|S_t = s]\\\\ \n",
    "&= E_\\pi[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t = s] \\\\\n",
    "& = \\sum_a\\pi(a|s)\\sum_{r,s'}p(r,s'|s,a)[r + \\gamma v_\\pi(s')]\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9114b13a",
   "metadata": {},
   "source": [
    "- Bellman eq + updaterule $\\to$ policy evaluation algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df7b7fce",
   "metadata": {},
   "source": [
    "Q. 2에서 3으로 넘어가는거 모르겠음."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90a23cd",
   "metadata": {},
   "source": [
    "**(Iterative policy evaluation)**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0161ca",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "&\\text{For all s :}\\\\\n",
    "&v_{k+1}(s) = \\sum_a\\pi(a|s)\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_k(s')]\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80b50a95",
   "metadata": {},
   "source": [
    "- $k \\to \\infty$ : $v_k \\to v_\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a17971",
   "metadata": {},
   "source": [
    "- 지금상태에서 (행동을 했을때) 모든 가능한 다음 state들의 value + 보상r의 기댓값으로 값을 현재의 value를 update함.\n",
    "    - DP를 expected update라 하는 이유.\n",
    "- 이와반대인 sample update도 있음.\n",
    "    - 지금상태에서 가능한 모든 상태와 모든 보상들을 고려하지 않음.\n",
    "    - 하나의 샘플을 기반하여 value function을 구하는 방식임(나중에 나옴)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "989b1b4b",
   "metadata": {},
   "source": [
    "## Iterative Policy Evaluation, for estimating $ V \\approx v_\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6ddd2f",
   "metadata": {},
   "source": [
    "![Example 4.1](./ex4-1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "12cc0043",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -14., -20., -22.],\n",
       "       [-14., -18., -20., -20.],\n",
       "       [-20., -20., -18., -14.],\n",
       "       [-22., -20., -14.,   0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DP_prediction():\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        self.grid_size = grid_size\n",
    "        self.value_func = np.zeros(grid_size)\n",
    "        self.possible_states = np.array([[i,j] for i in range(grid_size[0]) for j in range(grid_size[1])])\n",
    "        self.possible_rewards = np.array([0,1])\n",
    "        self.action_mapper = {\"n\" : np.array([-1,0]),\"s\":np.array([1,0]) , \"e\":np.array([0,1]), \"w\":np.array([0,-1])}\n",
    "    def move(self,current_state,a):\n",
    "        bound = self.grid_size[0]\n",
    "        next_move = current_state + self.action_mapper[a]\n",
    "        for coord in next_move:\n",
    "            if coord >= bound or coord < 0:\n",
    "                return current_state\n",
    "        return next_move\n",
    "    def π(self,s,a):\n",
    "        return 0.25\n",
    "    def dynamics(self,s,a,s_next,r):\n",
    "        terminal_states = [[0,0],[self.grid_size[0]-1,self.grid_size[1]-1]]\n",
    "        deterministic_pos = self.move(s,a).tolist()\n",
    "        #print(deterministic_pos,s_next)\n",
    "        if deterministic_pos[0] != s_next[0] or deterministic_pos[1] != s_next[1] :\n",
    "            p = 0\n",
    "        elif deterministic_pos[0] == s_next[0] and deterministic_pos[1] == s_next[1]:   \n",
    "            p = 1\n",
    "        return p\n",
    "    def updt_value(self,s,a,s_next,r):\n",
    "        gamma = 1\n",
    "        x = s_next[0]\n",
    "        y = s_next[1]\n",
    "        return self.dynamics(s,a,s_next,r)*(r + gamma * self.value_func[x][y])\n",
    "    def policy_eval(self,iter_nums = 50):\n",
    "        r = -1\n",
    "        gamma = 1\n",
    "        for k in range(iter_nums):\n",
    "            for i in range(self.grid_size[0]):\n",
    "                for j in range(self.grid_size[1]):\n",
    "                    s = np.array([i,j])\n",
    "                    if (i == 0 and j == 0) or (i==self.grid_size[0]-1 and j==self.grid_size[1]-1):\n",
    "                        continue\n",
    "                    v_update = 0\n",
    "                    for a in self.action_mapper.keys():\n",
    "                        update_value = 0\n",
    "                        for next_s in self.possible_states:\n",
    "                            r=-1\n",
    "                            add = self.updt_value(s,a,next_s,r)\n",
    "                            update_value += add\n",
    "                        v_update += update_value\n",
    "                    self.value_func[i][j] = self.π(s,a) * v_update\n",
    "        \n",
    "\n",
    "pred = DP_prediction()\n",
    "pred.policy_eval(iter_nums =1000)\n",
    "pred.value_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41f7b792",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-1. -2. -2.  0.]]\n",
      "5\n",
      "[[ 0. -5. -7. -8.]\n",
      " [-5. -7. -8. -8.]\n",
      " [-7. -8. -8. -7.]\n",
      " [-8. -8. -7.  0.]]\n",
      "10\n",
      "[[  0.  -8. -12. -13.]\n",
      " [ -8. -11. -12. -13.]\n",
      " [-12. -12. -12.  -9.]\n",
      " [-13. -13.  -9.   0.]]\n",
      "15\n",
      "[[  0. -10. -15. -16.]\n",
      " [-10. -14. -15. -15.]\n",
      " [-15. -15. -14. -11.]\n",
      " [-16. -15. -11.   0.]]\n",
      "20\n",
      "[[  0. -12. -17. -18.]\n",
      " [-12. -15. -17. -17.]\n",
      " [-17. -17. -15. -12.]\n",
      " [-18. -17. -12.   0.]]\n",
      "25\n",
      "[[  0. -12. -18. -20.]\n",
      " [-12. -16. -18. -18.]\n",
      " [-18. -18. -16. -13.]\n",
      " [-20. -18. -13.   0.]]\n",
      "30\n",
      "[[  0. -13. -19. -20.]\n",
      " [-13. -17. -19. -19.]\n",
      " [-19. -19. -17. -13.]\n",
      " [-20. -19. -13.   0.]]\n",
      "35\n",
      "[[  0. -13. -19. -21.]\n",
      " [-13. -17. -19. -19.]\n",
      " [-19. -19. -17. -13.]\n",
      " [-21. -19. -13.   0.]]\n",
      "40\n",
      "[[  0. -14. -19. -21.]\n",
      " [-14. -17. -19. -19.]\n",
      " [-19. -19. -18. -14.]\n",
      " [-21. -19. -14.   0.]]\n",
      "45\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "50\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "55\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "60\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "65\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "70\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "75\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "80\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "85\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "90\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n",
      "95\n",
      "[[  0. -14. -20. -22.]\n",
      " [-14. -18. -20. -20.]\n",
      " [-20. -20. -18. -14.]\n",
      " [-22. -20. -14.   0.]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[  0.        , -13.99765839, -19.99663362, -21.99629468],\n",
       "       [-13.99765839, -17.99712654, -19.99688008, -19.99691576],\n",
       "       [-19.99663362, -19.99688008, -17.99736736, -13.99803444],\n",
       "       [-21.99629468, -19.99691576, -13.99803444,   0.        ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DP_prediction():\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        self.grid_size = grid_size\n",
    "        self.value_func = np.zeros(grid_size)\n",
    "        self.possible_states = np.array([[i,j] for i in range(grid_size[0]) for j in range(grid_size[1])])\n",
    "        self.possible_rewards = np.array([0,1])\n",
    "        self.action_mapper = {\"n\" : np.array([-1,0]),\"s\":np.array([1,0]) , \"e\":np.array([0,1]), \"w\":np.array([0,-1])}\n",
    "    def move(self,current_state,a):\n",
    "        bound = self.grid_size[0]\n",
    "        next_move = current_state + self.action_mapper[a]\n",
    "        for coord in next_move:\n",
    "            if coord >= bound or coord < 0:\n",
    "                return current_state\n",
    "        return next_move\n",
    "    def π(self,s,a):\n",
    "        return 0.25\n",
    "    def dynamics(self,s,a,s_next,r):\n",
    "        terminal_states = [[0,0],[self.grid_size[0]-1,self.grid_size[1]-1]]\n",
    "        deterministic_pos = self.move(s,a).tolist()\n",
    "        #print(deterministic_pos,s_next)\n",
    "        if deterministic_pos[0] != s_next[0] or deterministic_pos[1] != s_next[1] :\n",
    "            p = 0\n",
    "        elif deterministic_pos[0] == s_next[0] and deterministic_pos[1] == s_next[1]:   \n",
    "            p = 1\n",
    "        return p\n",
    "    def updt_value(self,s,a,s_next,r):\n",
    "        gamma = 1\n",
    "        x = s_next[0]\n",
    "        y = s_next[1]\n",
    "        return self.dynamics(s,a,s_next,r)*(r + gamma * self.value_func[x][y])\n",
    "    def policy_eval(self,iter_nums = 50):\n",
    "        r = -1\n",
    "        gamma = 1\n",
    "        for k in range(iter_nums):\n",
    "            for i in range(self.grid_size[0]):\n",
    "                for j in range(self.grid_size[1]):\n",
    "                    s = np.array([i,j])\n",
    "                    if (i == 0 and j == 0) or (i==self.grid_size[0]-1 and j==self.grid_size[1]-1):\n",
    "                        continue\n",
    "                    v_update = 0\n",
    "                    for a in self.action_mapper.keys():\n",
    "                        update_value = 0\n",
    "                        for next_s in self.possible_states:\n",
    "                            r=-1\n",
    "                            add = self.updt_value(s,a,next_s,r)\n",
    "                            update_value += add\n",
    "                        v_update += update_value\n",
    "                    self.value_func[i][j] = self.π(s,a) * v_update\n",
    "            if k%5 ==0:\n",
    "                print(k)\n",
    "                print(np.round(self.value_func))\n",
    "        \n",
    "\n",
    "pred = DP_prediction()\n",
    "pred.policy_eval(iter_nums =100)\n",
    "pred.value_func"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34bdfbf4",
   "metadata": {},
   "source": [
    "## DP - Policy Improvement & Value Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51a06b08",
   "metadata": {},
   "source": [
    "- DP는 Prediction + Improvement를 통해서 optimal policy를 구하는 control을 함.\n",
    "- Prediction이 policy $\\pi$에 대한 value function을 구하는 것이였다면\n",
    "- Improvement는 구해진 value function을 기반으로 더 나은 policy를 향상(개선)시키는 과정임."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be60a49",
   "metadata": {},
   "source": [
    "# Policy Iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bae219ec",
   "metadata": {},
   "source": [
    "- value_func $\\to$ np.array\n",
    "    - value_func[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6a1f2dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP():\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        self.grid_size = np.array(grid_size)\n",
    "        self.S = [np.array([i,j]) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "        self.A = [\"W\",\"E\",\"N\",\"S\"]\n",
    "        self.A_to_coord = {\"W\" : [0,-1],\"E\" : [0,1], \"N\" : [-1,0], \"S\" : [1,0]}\n",
    "        for k,v in self.A_to_coord.items():\n",
    "            self.A_to_coord[k] = np.array(v)\n",
    "        self.gamma = 1\n",
    "        self.R = -1\n",
    "        self.V = np.zeros(grid_size)\n",
    "        self.π = np.array([np.random.choice(self.A) for i in range(grid_size[0]) for j in range(grid_size[1])]).reshape(grid_size)\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : two-dimensional np.array \n",
    "        Output\n",
    "        a : action \n",
    "        ex)\n",
    "        Input : [0,1]\n",
    "        output : \"W\"\n",
    "        \"\"\"\n",
    "        \n",
    "    def move(self,s,a):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        output\n",
    "        s_next : state after one_step transition\n",
    "        ex)\n",
    "        Input : s = [0,1],a = \"W\"\n",
    "        Output : s_next = [0,0]\n",
    "        \"\"\"\n",
    "        s_next = s + self.A_to_coord[a]\n",
    "        if s_next[0] < 0 or s_next[1] <0 or s_next[0] >= self.grid_size[0] or s_next[1] >= self.grid_size[1]:\n",
    "            return s # |S|를 넘어갈 경우, 원래 상태를 반환\n",
    "        else:\n",
    "            return s_next\n",
    "        \n",
    "    def move_test(self):\n",
    "        S = [np.array([i,j]) for i in [0,1,2,3] for j in [0,1,2,3]]\n",
    "        for s in S:\n",
    "            for a in self.A:\n",
    "                print(s,a,self.move(s,a))\n",
    "    def dynamics(self,s_prime,r,s,a):\n",
    "        r=-1\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        s_prime : all of the possible states after one-step transition\n",
    "        r : immediate reward(-1)\n",
    "        output\n",
    "        0 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 다를때\n",
    "        1 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 같을때\n",
    "        즉, a방향으로 움직이면 반드시 a방향으로 감(deterministic)\n",
    "        \"\"\"\n",
    "        s_next = self.move(s,a)\n",
    "        if np.sum(s_next != s_prime)>=1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    def dynamics_test(self,check_p = 1):\n",
    "        r=-1\n",
    "        for s in self.S:\n",
    "            for a in self.A:\n",
    "                for s_prime in self.S: #가능한 모든 next_state\n",
    "                    if self.dynamics(s_prime,r,s,a) == check_p:\n",
    "                        print(f\"state : {s} action : {a} s_prime : {s_prime} dynamics : {self.dynamics(s_prime,r,s,a)}\")\n",
    "    def Policy_Evaluation(self):\n",
    "        \"\"\"\n",
    "        estimating optimal policy\n",
    "        Input : X\n",
    "        Output : V ~= V_* \\pi ~= pi_*\n",
    "        \"\"\"\n",
    "        Δ = 0 ; θ = 5\n",
    "        terminal_states = [[0,0],[self.grid_size[0]-1,self.grid_size[1]-1]]\n",
    "        while True:\n",
    "            for s in self.S:\n",
    "                v = self.V[s[0]][s[1]] #여기 틀렸던 부분\n",
    "                a = self.π[s[0]][s[1]]\n",
    "                total = 0\n",
    "                if list(s) in terminal_states:\n",
    "                        continue\n",
    "                for s_prime in self.S:\n",
    "                    r = -1\n",
    "                    v_dynamics = self.dynamics(s_prime,r,s,a)\n",
    "                    total += v_dynamics * (r + self.gamma * self.V[s_prime[0]][s_prime[1]])\n",
    "                self.V[s[0]][s[1]] = total\n",
    "                Δ = max(Δ,abs(v-self.V[s[0]][s[1]]))\n",
    "            if Δ < θ:\n",
    "                break\n",
    "\n",
    "    def Policy_Improvement(self):\n",
    "        terminal_states = [[0,0],[self.grid_size[0]-1,self.grid_size[1]-1]]\n",
    "        policy_stable = True\n",
    "        for s in self.S:\n",
    "            old_action = self.π[s[0]][s[1]]\n",
    "            if list(s) in terminal_states:\n",
    "                continue\n",
    "            new_a = \"S\"\n",
    "            new_a_val = -5000\n",
    "            for a in self.A:\n",
    "                \"\"\"\n",
    "                1. 먼저 move해서 s_next 얻고\n",
    "                2. s_next얻으면 그값의 value를 얻고\n",
    "                3. 이전 a에 의한 value보다 크면 바꾸고 아니면 그대로\n",
    "                \"\"\"\n",
    "                s_next = self.move(s,a)\n",
    "                if self.V[s_next[0]][s_next[1]] > new_a_val:\n",
    "                    new_a = a\n",
    "                    new_a_val = self.V[s_next[0]][s_next[1]]\n",
    "                else:\n",
    "                    pass\n",
    "            self.π[s[0]][s[1]] = new_a\n",
    "            if old_action != self.π[s[0]][s[1]]:\n",
    "                policy_stable = False\n",
    "        return policy_stable\n",
    "    def Policy_Iteration(self):\n",
    "        policy_stable = False\n",
    "        count = 0 \n",
    "        while policy_stable != True:\n",
    "            self.Policy_Evaluation()\n",
    "            policy_stable = self.Policy_Improvement()\n",
    "            count +=1\n",
    "            print(\"==========================\")\n",
    "            print(f'iteration : {count}')\n",
    "            print(self.V)\n",
    "            self.π[0][0] = \"X\"\n",
    "            self.π[self.grid_size[0]-1][self.grid_size[1]-1] = \"X\"\n",
    "            print(self.π)\n",
    "            \n",
    "        \n",
    "    def Value_Iteration(self):\n",
    "        θ = 0.5; r = -1\n",
    "        terminal_states = [[0,0],[self.grid_size[0]-1,self.grid_size[1]-1]]\n",
    "        count = 0\n",
    "        \n",
    "        while True:\n",
    "            count+=1\n",
    "            print(f\"iteration : {count}\")\n",
    "            Δ = 0  \n",
    "            for s in self.S:\n",
    "                v = self.V[s[0]][s[1]]\n",
    "                #print(\"v\",v)\n",
    "                \"\"\"\n",
    "                1. 가능한 모든 succesor state에서 dynamics * (r + gamma * V(s'))을 계산한 뒤\n",
    "                2. \n",
    "                \"\"\"\n",
    "                update_val = -50000\n",
    "                if list(s) in terminal_states:\n",
    "                    continue\n",
    "                for a in self.A:\n",
    "                    total = 0\n",
    "                    for s_prime in self.S:\n",
    "                        total += self.dynamics(s_prime,r,s,a) * (r + self.gamma * self.V[s_prime[0]][s_prime[1]])\n",
    "                    if update_val < total:\n",
    "                        update_val = total\n",
    "                self.V[s[0]][s[1]] = update_val\n",
    "                Δ = max(Δ,abs(v-self.V[s[0]][s[1]]))\n",
    "            print(self.V)\n",
    "            if Δ < θ:\n",
    "                self.Policy_Improvement()\n",
    "                self.π[0][0] = \"X\"\n",
    "                self.π[self.grid_size[0]-1][self.grid_size[1]-1] = \"X\"\n",
    "                print(\"========================\")\n",
    "                print(\"optimal policy π\")\n",
    "                print(self.π)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "cba1ce9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================\n",
      "iteration : 1\n",
      "[[ 0. -1. -2. -1.]\n",
      " [-1. -2. -1. -2.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-1. -2. -1.  0.]]\n",
      "[['X' 'W' 'W' 'E']\n",
      " ['N' 'W' 'W' 'W']\n",
      " ['W' 'W' 'N' 'S']\n",
      " ['W' 'W' 'E' 'X']]\n",
      "==========================\n",
      "iteration : 2\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -3. -4.]\n",
      " [-2. -3. -4. -1.]\n",
      " [-2. -3. -1.  0.]]\n",
      "[['X' 'W' 'W' 'W']\n",
      " ['N' 'W' 'W' 'S']\n",
      " ['N' 'W' 'E' 'S']\n",
      " ['W' 'E' 'E' 'X']]\n",
      "==========================\n",
      "iteration : 3\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "[['X' 'W' 'W' 'W']\n",
      " ['N' 'W' 'W' 'S']\n",
      " ['N' 'W' 'E' 'S']\n",
      " ['E' 'E' 'E' 'X']]\n",
      "==========================\n",
      "iteration : 4\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "[['X' 'W' 'W' 'W']\n",
      " ['N' 'W' 'W' 'S']\n",
      " ['N' 'W' 'E' 'S']\n",
      " ['E' 'E' 'E' 'X']]\n"
     ]
    }
   ],
   "source": [
    "dp = DP()\n",
    "dp.Policy_Iteration()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "fed4f794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['X', 'W', 'W', 'W'],\n",
       "       ['N', 'W', 'W', 'S'],\n",
       "       ['N', 'W', 'E', 'S'],\n",
       "       ['E', 'E', 'E', 'X']], dtype='<U1')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dp.π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "20f577f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration : 1\n",
      "[[ 0. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1. -1.]\n",
      " [-1. -1. -1.  0.]]\n",
      "iteration : 2\n",
      "[[ 0. -1. -2. -2.]\n",
      " [-1. -2. -2. -2.]\n",
      " [-2. -2. -2. -1.]\n",
      " [-2. -2. -1.  0.]]\n",
      "iteration : 3\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "iteration : 4\n",
      "[[ 0. -1. -2. -3.]\n",
      " [-1. -2. -3. -2.]\n",
      " [-2. -3. -2. -1.]\n",
      " [-3. -2. -1.  0.]]\n",
      "========================\n",
      "optimal policy π\n",
      "[['X' 'W' 'W' 'W']\n",
      " ['N' 'W' 'W' 'S']\n",
      " ['N' 'W' 'E' 'S']\n",
      " ['E' 'E' 'E' 'X']]\n"
     ]
    }
   ],
   "source": [
    "dp = DP()\n",
    "dp.Value_Iteration()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4347cf7",
   "metadata": {},
   "source": [
    "# MC Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0b8b8ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self,grid_size):\n",
    "        self.grid_size = np.array(grid_size)\n",
    "        self.S = [np.array([i,j]) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "        self.A = [\"W\",\"E\",\"N\",\"S\"]\n",
    "        self.A_to_coord = {\"W\" : [0,-1],\"E\" : [0,1], \"N\" : [-1,0], \"S\" : [1,0]}\n",
    "        for k,v in self.A_to_coord.items():\n",
    "            self.A_to_coord[k] = np.array(v)\n",
    "        self.gamma = 1\n",
    "        self.R = -1\n",
    "        self.V = np.zeros(grid_size)\n",
    "    \n",
    "    def move(self,s,a):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        output\n",
    "        s_next : state after one_step transition\n",
    "        ex)\n",
    "        Input : s = [0,1],a = \"W\"\n",
    "        Output : s_next = [0,0]\n",
    "        \"\"\"\n",
    "        s_next = s + self.A_to_coord[a]\n",
    "        if s_next[0] < 0 or s_next[1] <0 or s_next[0] >= self.grid_size[0] or s_next[1] >= self.grid_size[1]:\n",
    "            return s # |S|를 넘어갈 경우, 원래 상태를 반환\n",
    "        else:\n",
    "            return s_next\n",
    "        \n",
    "    def move_test(self):\n",
    "        S = [np.array([i,j]) for i in [0,1,2,3] for j in [0,1,2,3]]\n",
    "        for s in S:\n",
    "            for a in self.A:\n",
    "                print(s,a,self.move(s,a))\n",
    "    def dynamics(self,s_prime,r,s,a):\n",
    "        r=-1\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        s_prime : all of the possible states after one-step transition\n",
    "        r : immediate reward(-1)\n",
    "        output\n",
    "        0 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 다를때\n",
    "        1 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 같을때\n",
    "        즉, a방향으로 움직이면 반드시 a방향으로 감(deterministic)\n",
    "        \"\"\"\n",
    "        s_next = self.move(s,a)\n",
    "        if np.sum(s_next != s_prime)>=1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    def dynamics_test(self,check_p = 1):\n",
    "        r=-1\n",
    "        for s in self.S:\n",
    "            for a in self.A:\n",
    "                for s_prime in self.S: #가능한 모든 next_state\n",
    "                    if self.dynamics(s_prime,r,s,a) == check_p:\n",
    "                        print(f\"state : {s} action : {a} s_prime : {s_prime} dynamics : {self.dynamics(s_prime,r,s,a)}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a31c50c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0.        , -14.48957713, -20.86497403, -22.76839039],\n",
       "       [-14.09795918, -18.85109114, -20.67102397, -20.5477997 ],\n",
       "       [-20.28465063, -20.50443459, -18.2046    , -14.27503628],\n",
       "       [-22.47093023, -20.49948823, -14.08302446,   0.        ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MC_Prediction(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "        \n",
    "    def sampling_action(self,s_t,π):\n",
    "        \"\"\"\n",
    "        Input current state s_t : tuple\n",
    "        ouput action : str (a ~ π(a|s))\n",
    "        \"\"\"\n",
    "        actions = [];prob = []\n",
    "        for a,p in π[s_t].items():\n",
    "            actions.append(a)\n",
    "            prob.append(p)\n",
    "        return np.random.choice(a=actions,p=prob)\n",
    "        \n",
    "    def generate_episode(self,s_0,π):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        initial state s0 : tuple\n",
    "        output\n",
    "        sequence following policy π\n",
    "        \"\"\"\n",
    "        terminal_states = [(0,0),(grid_size[0]-1,grid_size[0]-1)]\n",
    "        sequence = [s_0];epi_num = 0;r=-1\n",
    "        while True:\n",
    "            a = self.sampling_action(sequence[-1],π)\n",
    "            sequence.append(a)\n",
    "            s_t = tuple(self.move(sequence[-2],a))\n",
    "            sequence.append(self.R)\n",
    "            sequence.append(s_t)\n",
    "            if s_t in terminal_states:\n",
    "                break\n",
    "        return sequence\n",
    "    def prediction(self,s_0,π,iter_nums = 10000):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        arbitrary policy π(a|s) : dict\n",
    "        output\n",
    "        value function : np.array\n",
    "        \"\"\"\n",
    "        Returns = {tuple(s) : [] for s in self.S}\n",
    "        count = 0\n",
    "        while True:\n",
    "            sequence = self.generate_episode(s_0,π)\n",
    "            G = 0\n",
    "            for i in range(len(sequence)-4,-1,-3):\n",
    "                s = sequence[i];a=sequence[i+1];r=sequence[i+2]\n",
    "                G = self.gamma * G + r\n",
    "                previous_state = [sequence[j] for j in range(i-3,-1,-3)] #first visit MC\n",
    "                if s not in previous_state:\n",
    "                    Returns[s].append(G)\n",
    "                    self.V[s[0]][s[1]] = np.mean(np.array(Returns[s])) #unbiased estimator of v_π(s)\n",
    "            count+=1    \n",
    "            if count == iter_nums:\n",
    "                break\n",
    "        \n",
    "grid_size = (4,4)\n",
    "π = {}\n",
    "for i in range(grid_size[0]):\n",
    "    for j in range(grid_size[1]):\n",
    "        state = (i,j)\n",
    "        action_prob = {}\n",
    "        for a in [\"W\",\"E\",\"S\",\"N\"]:\n",
    "            action_prob[a] = 0.25\n",
    "        π[state] = action_prob\n",
    "mc = MC_Prediction()\n",
    "s_0 = (2,2)\n",
    "mc.prediction(s_0,π,5000)\n",
    "mc.V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd69f84",
   "metadata": {},
   "source": [
    "**(why MC?)**\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "238975e3",
   "metadata": {},
   "source": [
    "- DP methods require dynamics $p(s',r|,s,a)$ but it is not easy to determine.\n",
    "    - All of the $p(s',r|s,a)$ must be computed before DP can be applied.\n",
    "    - However, such computations are often complex and error-prone.\n",
    "- MC is easy.\n",
    "    - Just sampling $G_t$ from episodes.\n",
    "    - It can be a significant advantage even when one has complete knowledge of the environment's dynamics.\n",
    "        - $p$를 알아도 $|\\mathbb{s}|$가 너무 크면 DP는 힘들다. 그러므로 $p$를 알 경우에도 sampling만으로 $v_\\pi$를 추정하는 MC가 쓰일 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "653eacea",
   "metadata": {},
   "source": [
    "**(MC vs DP)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c639d5",
   "metadata": {},
   "source": [
    "어느 시점? 어떤 값? 몇 개의 값?\n",
    "- DP는 **one-step transition 후** 가능한 **모든 value**로 update.\n",
    "- MC는 **episode가 끝난 후** sampling된 **하나의 Return**으로 update."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d18846",
   "metadata": {},
   "source": [
    "bootsrap?\n",
    "- DP는 **estimates로 estimates**를 update로 하는 bootstrap method.\n",
    "- MC는 sample로 update하는 method.(bootstrap method가 아님)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b73edf",
   "metadata": {},
   "source": [
    "# MC Control"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "454511fa",
   "metadata": {},
   "source": [
    "- environment의 model이 없는 상황에서 $\\pi_*,v_*$를 찾는 방법"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66123bc3",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\pi'(s) &\\overset{.}{=} \\underset{a}{\\text{argmax}}q_\\pi(s,a) \\\\\n",
    "&= \\underset{a}{\\text{argmax}}\\mathbb{E}[R_{t+1} + \\gamma v_\\pi(S_{t+1})|S_t = s,A_t  a ]\\\\\n",
    "&= \\underset{a}{\\text{argmax}}\\sum_{s',r}p(s',r|s,a)[r+\\gamma v_\\pi(s')] \n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3976401c",
   "metadata": {},
   "source": [
    "- How?\n",
    "    - 윗식과 같이 Greedy하게 policy를 improvement하면 됨.(policy improvement theorem에서 유도됨)\n",
    "    - DP에서는 $p(s',r|s,a)$를 알기에 (3)을 사용했었음.하지만 MC에서는 $p(s',r|s,a)$을 모르는 상황을 가정함.\n",
    "    - 그러므로 (1)식을 사용. \n",
    "        1. action value function을 estimate하고 \n",
    "        2. max q를 돌려주는 action으로 $\\pi(s)$를 update.(Greedy하게 update)\n",
    "    - 하지만 Greedy 하게 action만 하면 문제가 있음. $\\to$ 가보지 못한 (s,a)를 안가는 action을 계속 취함\n",
    "        - $Q \\approx q_\\pi$이려면 $\\forall s,\\forall a$에 대해 많이 여러번 방문해줘야함.\n",
    "        - Greedy하게 움직일 경우 $\\forall s,\\forall a$를 여러번 방문하지 못함.(Greedy한 action만 취하기 때문임)\n",
    "    - 그래서 exploring start를 가정을 사용함. 이렇게 하여 모든 $(s,a)$를 방문함을 여러번 방문함을 보장할 수 있음.\n",
    "        - episode가 시작하는 $(s,a)$를 모든 확률 > 0 이게 랜덤하게 시작.\n",
    "    - 하지만 exploring start도 문제가 있음. 특히, learning directly from actual interaction with an environment일때 문제라고 함.\n",
    "    - There are two approaches to ensuring this.\n",
    "        1. on-policy method\n",
    "            - 지속적으로 exploring하여 모든 (s,a)를 여러번 방문하게 하는 방법.(궁극적으로는, $\\to Q \\approx q_\\pi$)    \n",
    "            - How? $\\to$ using $\\epsilon$-soft policy\n",
    "                - $\\pi(a|s) >0$인 policy를 사용함.\n",
    "                - 즉,모든 $(s,a)$를 여러번 방문할 수 있도록 하는 policy를 사용.\n",
    "                - 대표적으로 $\\epsilon$ - greedy  policy가 있음.\n",
    "                     - 아주 작은 확률 $\\epsilon$으로 greedy하게 action하지 않고 균등한 확률로 action.\n",
    "        2. off-policy method\n",
    "            - ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c39f0ee1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa5c1c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment2():\n",
    "    def __init__(self,grid_size):\n",
    "        self.grid_size = np.array(grid_size)\n",
    "        self.S = [np.array([i,j]) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "        self.A = [\"W\",\"E\",\"N\",\"S\"]\n",
    "        self.A_to_coord = {\"W\" : [0,-1],\"E\" : [0,1], \"N\" : [-1,0], \"S\" : [1,0]}\n",
    "        for k,v in self.A_to_coord.items():\n",
    "            self.A_to_coord[k] = np.array(v)\n",
    "        self.gamma = 1\n",
    "        self.R = -1\n",
    "    \n",
    "    def move(self,s,a):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        output\n",
    "        s_next : state after one_step transition\n",
    "        ex)\n",
    "        Input : s = [0,1],a = \"W\"\n",
    "        Output : s_next = [0,0]\n",
    "        \"\"\"\n",
    "        s_next = s + self.A_to_coord[a]\n",
    "        if s_next[0] < 0 or s_next[1] <0 or s_next[0] >= self.grid_size[0] or s_next[1] >= self.grid_size[1]:\n",
    "            return s # |S|를 넘어갈 경우, 원래 상태를 반환\n",
    "        else:\n",
    "            return s_next\n",
    "        \n",
    "    def move_test(self):\n",
    "        S = [np.array([i,j]) for i in [0,1,2,3] for j in [0,1,2,3]]\n",
    "        for s in S:\n",
    "            for a in self.A:\n",
    "                print(s,a,self.move(s,a))\n",
    "    def dynamics(self,s_prime,r,s,a):\n",
    "        r=-1\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        s_prime : all of the possible states after one-step transition\n",
    "        r : immediate reward(-1)\n",
    "        output\n",
    "        0 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 다를때\n",
    "        1 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 같을때\n",
    "        즉, a방향으로 움직이면 반드시 a방향으로 감(deterministic)\n",
    "        \"\"\"\n",
    "        s_next = self.move(s,a)\n",
    "        if np.sum(s_next != s_prime)>=1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    def dynamics_test(self,check_p = 1):\n",
    "        r=-1\n",
    "        for s in self.S:\n",
    "            for a in self.A:\n",
    "                for s_prime in self.S: #가능한 모든 next_state\n",
    "                    if self.dynamics(s_prime,r,s,a) == check_p:\n",
    "                        print(f\"state : {s} action : {a} s_prime : {s_prime} dynamics : {self.dynamics(s_prime,r,s,a)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7024fd4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MC_Control(Environment2):\n",
    "    def __init__(self,grid_size=(4,4),uniform_dist=False):\n",
    "        super().__init__(grid_size)\n",
    "        \n",
    "    def sampling_action(self,s_t,π):\n",
    "        \"\"\"\n",
    "        Input current state s_t : tuple\n",
    "        ouput action : str (a ~ π(a|s))\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for a,p in π[s_t].items():\n",
    "            actions.append(a)\n",
    "            prob.append(p)\n",
    "        #print(prob)\n",
    "        return np.random.choice(a=actions,p=prob)\n",
    "    \n",
    "    def generate_episode(self,s_0,π):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        initial state s0 : tuple\n",
    "        output\n",
    "        sequence following policy π\n",
    "        \"\"\"\n",
    "        terminal_states = [(0,0),(self.grid_size[0]-1,self.grid_size[0]-1)]\n",
    "        sequence = [s_0];epi_num = 0;r=-1\n",
    "        while True:\n",
    "            a = self.sampling_action(sequence[-1],π)\n",
    "            sequence.append(a)\n",
    "            s_t = tuple(self.move(sequence[-2],a))\n",
    "            sequence.append(self.R)\n",
    "            sequence.append(s_t)\n",
    "            if s_t in terminal_states:\n",
    "                break\n",
    "        return sequence\n",
    "    \n",
    "    def generate_π(self,uniform=False):\n",
    "        \"\"\"\n",
    "        Input : NA\n",
    "        Output : {(x,y) : {\"W\" : pr1,\"E\" : pr2 ,...}}\n",
    "        ex)\n",
    "        {(0,0):{\"W\" : 0.1, \"E\" : 0.2, ...},(0,1):{\"W\":0.4,\"E\":0.5...}}\n",
    "        \"\"\"\n",
    "        π = {(i,j): {} for i in range(self.grid_size[0]) for j in range(self.grid_size[1])}\n",
    "        for t in π.values():\n",
    "            unnormalized_prob = np.random.rand(4)\n",
    "            if uniform == False:\n",
    "                prob = unnormalized_prob/np.sum(unnormalized_prob)\n",
    "            else:\n",
    "                prob = [0.25] * 4\n",
    "            for i in range(len(self.A)):\n",
    "                t[self.A[i]] = prob[i]\n",
    "        return π\n",
    "    \n",
    "    def generate_Q(self,random=False):\n",
    "        \"\"\"\n",
    "        Input : NA\n",
    "        Output : {s_0,a_0 : val1, s_1,a_2 : val2, ...}\n",
    "        where s_0 : (x,y) , val1 : k\n",
    "        ex)\n",
    "        {((0,0),\"W\") : 1.5 , ((0,0),\"E\") : 2.5, ...,((0,1)),\"W\")}\n",
    "        \"\"\"\n",
    "        if random == True:\n",
    "            return {(tuple(s),a) : np.random.randn() for s in self.S for a in self.A}\n",
    "        else :\n",
    "            return {(tuple(s),a) : 0 for s in self.S for a in self.A}\n",
    "    def generate_Returns(self):\n",
    "        \"\"\"\n",
    "        Input : NA\n",
    "        Output : {(s,a) : [] ...}\n",
    "        ex)\n",
    "        {((0,0),\"W\") : [], ((0,0),\"E\" : []), .... ,((3,2) : \"N\" : []) , ...}\n",
    "        \"\"\"\n",
    "        return {(tuple(s),a) : [] for s in self.S for a in self.A}\n",
    "    def argmax_a_Q(self,Q,s):\n",
    "        max_action = \"W\"\n",
    "        max_value = -5000\n",
    "        for visit,Q_val in Q.items():\n",
    "            if visit[0] == s:\n",
    "                if Q_val > max_value:\n",
    "                    max_action = visit[1]\n",
    "                    max_value = Q_val                    \n",
    "        return max_action\n",
    "\n",
    "    def argmax_test(self,s):\n",
    "        Q = self.generate_Q()\n",
    "        for visit,Q_val in Q.items():\n",
    "            Q[visit] = np.random.choice([0,1,2,3])\n",
    "            #print(visit,Q_val)\n",
    "        print(Q)\n",
    "        print(f\"state : {s} max_action {self.argmax_a_Q(Q,s)}\")\n",
    "    def control(self,s_0,iter_nums = 100000,epsilon = 0.99995,decaying = True):\n",
    "        π = self.generate_π(True)\n",
    "        Q = self.generate_Q()\n",
    "        Returns = self.generate_Returns()\n",
    "        terminal_states = [(0,0),(self.grid_size[0]-1,self.grid_size[0]-1)]\n",
    "        t = time.time()\n",
    "        count = 0\n",
    "        count = 0\n",
    "        while True:\n",
    "            count+=1\n",
    "            if decaying == True:\n",
    "                ϵ = epsilon ** count\n",
    "            if count % 10000 == 0:\n",
    "                print(f\"epsilon{count}: {ϵ}\")\n",
    "            sequence = self.generate_episode(s_0,π)\n",
    "            G = 0\n",
    "            for i in range(len(sequence)-4,-1,-3):\n",
    "                G = self.gamma * G + sequence[i+2]\n",
    "                previous_visit = [(sequence[j],sequence[j+1]) for j in range(i-3,-1,-3)]\n",
    "                current = (sequence[i],sequence[i+1])\n",
    "                if current not in previous_visit:\n",
    "                    Returns[current].append(G)\n",
    "                    Q[current] = np.mean(np.array(Returns[current]))\n",
    "                    s=current[0]\n",
    "                    max_action = self.argmax_a_Q(Q,s)\n",
    "                    for a in self.A:\n",
    "                        if a == max_action:\n",
    "                            π[s][a] = 1 - ϵ + ϵ/len(self.A)\n",
    "                        else:\n",
    "                            π[s][a] = ϵ/len(self.A)\n",
    "            if count >= iter_nums:\n",
    "                print(\"time :\",t - time.time())\n",
    "                return Q,Returns,π\n",
    "    def incremental_control(self,s_0,iter_nums = 100000,epsilon = 0.99995,decaying = True):\n",
    "        π = self.generate_π(True)\n",
    "        Q = self.generate_Q()\n",
    "        Returns = self.generate_Returns()\n",
    "        N = {(tuple(s),a) : 0 for s in self.S for a in self.A}\n",
    "        terminal_states = [(0,0),(self.grid_size[0]-1,self.grid_size[0]-1)]\n",
    "        t = time.time()\n",
    "        count = 0\n",
    "        while True:\n",
    "            count+=1\n",
    "            if decaying == True:\n",
    "                ϵ = epsilon ** count\n",
    "            if count % 10000 == 0:\n",
    "                print(f\"epsilon{count}: {ϵ}\")\n",
    "            sequence = self.generate_episode(s_0,π)\n",
    "            G = 0\n",
    "            for i in range(len(sequence)-4,-1,-3):\n",
    "                G = self.gamma * G + sequence[i+2]\n",
    "                previous_visit = [(sequence[j],sequence[j+1]) for j in range(i-3,-1,-3)]\n",
    "                current = (sequence[i],sequence[i+1])\n",
    "                if current not in previous_visit:\n",
    "                    Returns[current].append(G)\n",
    "                    N[sequence[i],sequence[i+1]] += 1\n",
    "                    Q[current] = Q[current] + (1/N[sequence[i],sequence[i+1]])*(G-Q[current])\n",
    "                    s=current[0]\n",
    "                    max_action = self.argmax_a_Q(Q,s)\n",
    "                    for a in self.A:\n",
    "                        if a == max_action:\n",
    "                            π[s][a] = 1 - ϵ + ϵ/len(self.A)\n",
    "                        else:\n",
    "                            π[s][a] = ϵ/len(self.A)\n",
    "            if count >= iter_nums:\n",
    "                print(\"time :\",t - time.time())\n",
    "                return Q,Returns,π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2f6937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon10000: 0.606523077874078\n",
      "epsilon20000: 0.3678702439938449\n",
      "epsilon30000: 0.22312179264543486\n",
      "epsilon40000: 0.13532851641609098\n",
      "epsilon50000: 0.08207986830082019\n",
      "epsilon60000: 0.049783334353312426\n",
      "epsilon70000: 0.030194741178805377\n",
      "epsilon80000: 0.018313807355380202\n",
      "epsilon90000: 0.01110774680477813\n",
      "epsilon100000: 0.006737104780279986\n",
      "time : -10.034584522247314\n"
     ]
    }
   ],
   "source": [
    "mc = MC_Control()\n",
    "s_0 = (2,2)\n",
    "#Q,R,π = mc.control(s_0)\n",
    "Q2,R2,π2 = mc.incremental_control(s_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "313b7d4e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 0), 'W'): 0,\n",
       " ((0, 0), 'E'): 0,\n",
       " ((0, 0), 'N'): 0,\n",
       " ((0, 0), 'S'): 0,\n",
       " ((0, 1), 'W'): -1.0,\n",
       " ((0, 1), 'E'): -10.013944223107552,\n",
       " ((0, 1), 'N'): -6.268623024830696,\n",
       " ((0, 1), 'S'): -9.052246603970728,\n",
       " ((0, 2), 'W'): -6.747187685020726,\n",
       " ((0, 2), 'E'): -14.498630136986284,\n",
       " ((0, 2), 'N'): -12.776056338028146,\n",
       " ((0, 2), 'S'): -13.368355995055618,\n",
       " ((0, 3), 'W'): -13.03755215577189,\n",
       " ((0, 3), 'E'): -15.35992217898833,\n",
       " ((0, 3), 'N'): -14.858585858585865,\n",
       " ((0, 3), 'S'): -10.554105909439766,\n",
       " ((1, 0), 'W'): -9.136170212765952,\n",
       " ((1, 0), 'E'): -11.186411149825785,\n",
       " ((1, 0), 'N'): -1.0,\n",
       " ((1, 0), 'S'): -12.917414721723514,\n",
       " ((1, 1), 'W'): -7.171156893819335,\n",
       " ((1, 1), 'E'): -11.358722358722364,\n",
       " ((1, 1), 'N'): -4.370344177274873,\n",
       " ((1, 1), 'S'): -11.373970345963762,\n",
       " ((1, 2), 'W'): -10.171052631578956,\n",
       " ((1, 2), 'E'): -11.063912201420285,\n",
       " ((1, 2), 'N'): -10.94296824368114,\n",
       " ((1, 2), 'S'): -6.765374467221436,\n",
       " ((1, 3), 'W'): -11.483893557422972,\n",
       " ((1, 3), 'E'): -10.893564356435634,\n",
       " ((1, 3), 'N'): -12.532346063912712,\n",
       " ((1, 3), 'S'): -4.618298488925688,\n",
       " ((2, 0), 'W'): -13.34355828220859,\n",
       " ((2, 0), 'E'): -13.707762557077613,\n",
       " ((2, 0), 'N'): -7.363119415109663,\n",
       " ((2, 0), 'S'): -14.433333333333337,\n",
       " ((2, 1), 'W'): -10.756267409470762,\n",
       " ((2, 1), 'E'): -10.363328822733449,\n",
       " ((2, 1), 'N'): -6.547498949138301,\n",
       " ((2, 1), 'S'): -11.027777777777764,\n",
       " ((2, 2), 'W'): -8.199448106699363,\n",
       " ((2, 2), 'E'): -2.5312929964093014,\n",
       " ((2, 2), 'N'): -8.5300193673338,\n",
       " ((2, 2), 'S'): -5.2740273719821555,\n",
       " ((2, 3), 'W'): -6.433562197092097,\n",
       " ((2, 3), 'E'): -4.328860536735588,\n",
       " ((2, 3), 'N'): -6.99127473930623,\n",
       " ((2, 3), 'S'): -1.0,\n",
       " ((3, 0), 'W'): -15.357476635514043,\n",
       " ((3, 0), 'E'): -11.664444444444454,\n",
       " ((3, 0), 'N'): -13.981378026070765,\n",
       " ((3, 0), 'S'): -15.982800982800986,\n",
       " ((3, 1), 'W'): -13.768028846153838,\n",
       " ((3, 1), 'E'): -6.739085772984088,\n",
       " ((3, 1), 'N'): -12.896698615548473,\n",
       " ((3, 1), 'S'): -12.837549933422094,\n",
       " ((3, 2), 'W'): -10.868327402135233,\n",
       " ((3, 2), 'E'): -1.0,\n",
       " ((3, 2), 'N'): -9.850946854001224,\n",
       " ((3, 2), 'S'): -6.852646638054353,\n",
       " ((3, 3), 'W'): 0,\n",
       " ((3, 3), 'E'): 0,\n",
       " ((3, 3), 'N'): 0,\n",
       " ((3, 3), 'S'): 0}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c3ab5230",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.4"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(np.array(R2[((0, 2), 'W')][-100:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36c2f43",
   "metadata": {},
   "source": [
    "- 벨만최적방정식과 뭔가 이상하게 다름.\n",
    "- 따라서 바꿈"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f102b156",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 0), 'W'): nan,\n",
       " ((0, 0), 'E'): nan,\n",
       " ((0, 0), 'N'): nan,\n",
       " ((0, 0), 'S'): nan,\n",
       " ((0, 1), 'W'): -1.0,\n",
       " ((0, 1), 'E'): -3.81,\n",
       " ((0, 1), 'N'): -2.49,\n",
       " ((0, 1), 'S'): -3.61,\n",
       " ((0, 2), 'W'): -2.4,\n",
       " ((0, 2), 'E'): -6.56,\n",
       " ((0, 2), 'N'): -6.41,\n",
       " ((0, 2), 'S'): -6.48,\n",
       " ((0, 3), 'W'): -5.19,\n",
       " ((0, 3), 'E'): -8.15,\n",
       " ((0, 3), 'N'): -6.6,\n",
       " ((0, 3), 'S'): -3.83,\n",
       " ((1, 0), 'W'): -4.14,\n",
       " ((1, 0), 'E'): -5.23,\n",
       " ((1, 0), 'N'): -1.0,\n",
       " ((1, 0), 'S'): -5.81,\n",
       " ((1, 1), 'W'): -2.45,\n",
       " ((1, 1), 'E'): -4.68,\n",
       " ((1, 1), 'N'): -2.01,\n",
       " ((1, 1), 'S'): -4.78,\n",
       " ((1, 2), 'W'): -3.59,\n",
       " ((1, 2), 'E'): -3.77,\n",
       " ((1, 2), 'N'): -3.9,\n",
       " ((1, 2), 'S'): -3.02,\n",
       " ((1, 3), 'W'): -4.6,\n",
       " ((1, 3), 'E'): -3.71,\n",
       " ((1, 3), 'N'): -5.0,\n",
       " ((1, 3), 'S'): -2.02,\n",
       " ((2, 0), 'W'): -6.41,\n",
       " ((2, 0), 'E'): -6.41,\n",
       " ((2, 0), 'N'): -2.38,\n",
       " ((2, 0), 'S'): -6.98,\n",
       " ((2, 1), 'W'): -3.68,\n",
       " ((2, 1), 'E'): -3.76,\n",
       " ((2, 1), 'N'): -3.01,\n",
       " ((2, 1), 'S'): -3.55,\n",
       " ((2, 2), 'W'): -4.0,\n",
       " ((2, 2), 'E'): -2.0,\n",
       " ((2, 2), 'N'): -4.02,\n",
       " ((2, 2), 'S'): -2.02,\n",
       " ((2, 3), 'W'): -3.01,\n",
       " ((2, 3), 'E'): -2.02,\n",
       " ((2, 3), 'N'): -3.06,\n",
       " ((2, 3), 'S'): -1.0,\n",
       " ((3, 0), 'W'): -9.21,\n",
       " ((3, 0), 'E'): -4.9,\n",
       " ((3, 0), 'N'): -7.65,\n",
       " ((3, 0), 'S'): -9.63,\n",
       " ((3, 1), 'W'): -6.64,\n",
       " ((3, 1), 'E'): -2.19,\n",
       " ((3, 1), 'N'): -6.02,\n",
       " ((3, 1), 'S'): -5.51,\n",
       " ((3, 2), 'W'): -3.96,\n",
       " ((3, 2), 'E'): -1.0,\n",
       " ((3, 2), 'N'): -3.5,\n",
       " ((3, 2), 'S'): -2.32,\n",
       " ((3, 3), 'W'): nan,\n",
       " ((3, 3), 'E'): nan,\n",
       " ((3, 3), 'N'): nan,\n",
       " ((3, 3), 'S'): nan}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R2_renew = {}\n",
    "for visit,value in R2.items():\n",
    "    R2_renew[visit]=np.mean(np.array(value[-100:]))\n",
    "R2_renew"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e04373b3",
   "metadata": {},
   "source": [
    "# epsilon이 중요함.\n",
    "- 처음엔 탐험하다가 마지막에 가서 0정도로 떨어져야함\n",
    "- 중간에 떨어져 버리면 ... 탐험을 하지못하고\n",
    "- 떨어지지 않으면 optimal과 전혀 가깝지 못한 stochastic policy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250eebb8",
   "metadata": {},
   "source": [
    "Q-learning 하면서 구현한 코드"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b03c40c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dab48891",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self,grid_size):\n",
    "        self.grid_size = np.array(grid_size)\n",
    "        self.S = [np.array([i,j]) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "        self.Terminal_states = [(0,0),(self.grid_size[0]-1,self.grid_size[1]-1)]\n",
    "        self.A = [\"W\",\"E\",\"N\",\"S\"]\n",
    "        self.A_to_coord = {\"W\" : [0,-1],\"E\" : [0,1], \"N\" : [-1,0], \"S\" : [1,0]}\n",
    "        for k,v in self.A_to_coord.items():\n",
    "            self.A_to_coord[k] = np.array(v)\n",
    "        self.gamma = 1\n",
    "        self.R = -1\n",
    "    \n",
    "    def move(self,s,a):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        output\n",
    "        s_next : state after one_step transition\n",
    "        ex)\n",
    "        Input : s = [0,1],a = \"W\"\n",
    "        Output : s_next = [0,0]\n",
    "        \"\"\"\n",
    "        s_next = s + self.A_to_coord[a]\n",
    "        if s_next[0] < 0 or s_next[1] <0 or s_next[0] >= self.grid_size[0] or s_next[1] >= self.grid_size[1]:\n",
    "            return s # |S|를 넘어갈 경우, 원래 상태를 반환\n",
    "        else:\n",
    "            return s_next\n",
    "        \n",
    "    def move_test(self):\n",
    "        S = [np.array([i,j]) for i in [0,1,2,3] for j in [0,1,2,3]]\n",
    "        for s in S:\n",
    "            for a in self.A:\n",
    "                print(s,a,self.move(s,a))\n",
    "    def dynamics(self,s_prime,r,s,a):\n",
    "        r=-1\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        s_prime : all of the possible states after one-step transition\n",
    "        r : immediate reward(-1)\n",
    "        output\n",
    "        0 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 다를때\n",
    "        1 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 같을때\n",
    "        즉, a방향으로 움직이면 반드시 a방향으로 감(deterministic)\n",
    "        \"\"\"\n",
    "        s_next = self.move(s,a)\n",
    "        if np.sum(s_next != s_prime)>=1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    def dynamics_test(self,check_p = 1):\n",
    "        r=-1\n",
    "        for s in self.S:\n",
    "            for a in self.A:\n",
    "                for s_prime in self.S: #가능한 모든 next_state\n",
    "                    if self.dynamics(s_prime,r,s,a) == check_p:\n",
    "                        print(f\"state : {s} action : {a} s_prime : {s_prime} dynamics : {self.dynamics(s_prime,r,s,a)}\")\n",
    "    def sampling_action(self,s_t,π):\n",
    "        \"\"\"\n",
    "        Input current state s_t : tuple\n",
    "        ouput action : str (a ~ π(a|s))\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for a,p in π[s_t].items():\n",
    "            actions.append(a)\n",
    "            prob.append(p)\n",
    "        #print(prob)\n",
    "        return np.random.choice(a=actions,p=prob)\n",
    "    def generate_π(self,uniform=False):\n",
    "        \"\"\"\n",
    "        Input : NA\n",
    "        Output : {(x,y) : {\"W\" : pr1,\"E\" : pr2 ,...}}\n",
    "        ex)\n",
    "        {(0,0):{\"W\" : 0.1, \"E\" : 0.2, ...},(0,1):{\"W\":0.4,\"E\":0.5...}}\n",
    "        \"\"\"\n",
    "        π = {(i,j): {} for i in range(self.grid_size[0]) for j in range(self.grid_size[1])}\n",
    "        for t in π.values():\n",
    "            unnormalized_prob = np.random.rand(4)\n",
    "            if uniform == False:\n",
    "                prob = unnormalized_prob/np.sum(unnormalized_prob)\n",
    "            else:\n",
    "                prob = [0.25] * 4\n",
    "            for i in range(len(self.A)):\n",
    "                t[self.A[i]] = prob[i]\n",
    "        return π\n",
    "    def argmax_a_Q(self,Q,s):\n",
    "        max_action = \"W\"\n",
    "        max_value = -5000\n",
    "        for visit,Q_val in Q.items():\n",
    "            if visit[0] == s:\n",
    "                if Q_val > max_value:\n",
    "                    max_action = visit[1]\n",
    "                    max_value = Q_val                    \n",
    "        return max_action\n",
    "class Q_learning(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    \n",
    "    def control(self,s_0,iter_num,alpha,epsilon):\n",
    "        Q = {(tuple(s),a) : 0 for s in self.S for a in self.A}\n",
    "        π = self.generate_π()\n",
    "        γ = 1\n",
    "        for ep_num in range(iter_num):\n",
    "            ϵ = epsilon ** (ep_num + 1)\n",
    "            α = alpha ** (ep_num + 1)\n",
    "            if ep_num % 1000 == True:\n",
    "                print(f\"epsilon : {ϵ} alpha : {α}\")\n",
    "            s_t = s_0\n",
    "            while s_t not in self.Terminal_states:\n",
    "                a_t = self.sampling_action(s_t,π)\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                Q[(s_t,a_t)] = Q[(s_t,a_t)] + α * (r + γ * Q[(s_prime,self.argmax_a_Q(Q,s_prime))] - Q[(s_t,a_t)])\n",
    "                \n",
    "                a_star = self.argmax_a_Q(Q,tuple(s_t))\n",
    "                for (state,action),value in Q.items():\n",
    "                    if action == a_star:\n",
    "                        π[state][action] = 1 - ϵ + ϵ /len(self.A)\n",
    "                    else:\n",
    "                        π[state][action] = ϵ/len(self.A)\n",
    "                s_t = s_prime\n",
    "        return π,Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6d0a343e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DP_prediction(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    def policy_eval(self,π,iter_nums = 10):\n",
    "        V = {tuple(s): 0 for s in self.S}\n",
    "        r = -1;gamma=1\n",
    "        for iter_num in range(iter_nums):\n",
    "            for s in self.S:\n",
    "                s_t = tuple(s) #for each s \\in S\n",
    "                if s_t in self.Terminal_states:\n",
    "                    continue\n",
    "                v = V[s_t]\n",
    "                updt_value = 0\n",
    "                for a in self.A:\n",
    "                    total = 0\n",
    "                    for s_prime in self.S:\n",
    "                        #print(s_prime,r,s_t,a)\n",
    "                        #print(self.dynamics(s_prime,r,s_t,a))\n",
    "                        total += self.dynamics(s_prime,r,s_t,a) * (r + gamma * V[tuple(s_prime)])\n",
    "                    updt_value += π[s_t][a] * total\n",
    "                V[s_t] = updt_value\n",
    "                \n",
    "        return V"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "296.328px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
