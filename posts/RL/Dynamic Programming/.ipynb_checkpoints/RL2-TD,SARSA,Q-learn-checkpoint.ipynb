{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f63952c5",
   "metadata": {},
   "source": [
    "# TD,SARSA,Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dc3024c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07ec753b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self,grid_size):\n",
    "        self.grid_size = np.array(grid_size)\n",
    "        self.S = [np.array([i,j]) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "        self.Terminal_states = [(0,0),(self.grid_size[0]-1,self.grid_size[1]-1)]\n",
    "        self.A = [\"W\",\"E\",\"N\",\"S\"]\n",
    "        self.A_to_coord = {\"W\" : [0,-1],\"E\" : [0,1], \"N\" : [-1,0], \"S\" : [1,0]}\n",
    "        for k,v in self.A_to_coord.items():\n",
    "            self.A_to_coord[k] = np.array(v)\n",
    "        self.gamma = 1\n",
    "        self.R = -1\n",
    "    \n",
    "    def move(self,s,a):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        output\n",
    "        s_next : state after one_step transition\n",
    "        ex)\n",
    "        Input : s = [0,1],a = \"W\"\n",
    "        Output : s_next = [0,0]\n",
    "        \"\"\"\n",
    "        s_next = s + self.A_to_coord[a]\n",
    "        if s_next[0] < 0 or s_next[1] <0 or s_next[0] >= self.grid_size[0] or s_next[1] >= self.grid_size[1]:\n",
    "            return s # |S|를 넘어갈 경우, 원래 상태를 반환\n",
    "        else:\n",
    "            return s_next\n",
    "        \n",
    "    def move_test(self):\n",
    "        S = [np.array([i,j]) for i in [0,1,2,3] for j in [0,1,2,3]]\n",
    "        for s in S:\n",
    "            for a in self.A:\n",
    "                print(s,a,self.move(s,a))\n",
    "    def dynamics(self,s_prime,r,s,a):\n",
    "        r=-1\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        s_prime : all of the possible states after one-step transition\n",
    "        r : immediate reward(-1)\n",
    "        output\n",
    "        0 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 다를때\n",
    "        1 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 같을때\n",
    "        즉, a방향으로 움직이면 반드시 a방향으로 감(deterministic)\n",
    "        \"\"\"\n",
    "        s_next = self.move(s,a)\n",
    "        if np.sum(s_next != s_prime)>=1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    def dynamics_test(self,check_p = 1):\n",
    "        r=-1\n",
    "        for s in self.S:\n",
    "            for a in self.A:\n",
    "                for s_prime in self.S: #가능한 모든 next_state\n",
    "                    if self.dynamics(s_prime,r,s,a) == check_p:\n",
    "                        print(f\"state : {s} action : {a} s_prime : {s_prime} dynamics : {self.dynamics(s_prime,r,s,a)}\")\n",
    "    def sampling_action(self,s_t,π):\n",
    "        \"\"\"\n",
    "        Input current state s_t : tuple\n",
    "        ouput action : str (a ~ π(a|s))\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for a,p in π[s_t].items():\n",
    "            actions.append(a)\n",
    "            prob.append(p)\n",
    "        #print(prob)\n",
    "        return np.random.choice(a=actions,p=prob)\n",
    "    def generate_π(self,uniform=False):\n",
    "        \"\"\"\n",
    "        Input : NA\n",
    "        Output : {(x,y) : {\"W\" : pr1,\"E\" : pr2 ,...}}\n",
    "        ex)\n",
    "        {(0,0):{\"W\" : 0.1, \"E\" : 0.2, ...},(0,1):{\"W\":0.4,\"E\":0.5...}}\n",
    "        \"\"\"\n",
    "        π = {(i,j): {} for i in range(self.grid_size[0]) for j in range(self.grid_size[1])}\n",
    "        for t in π.values():\n",
    "            unnormalized_prob = np.random.rand(4)\n",
    "            if uniform == False:\n",
    "                prob = unnormalized_prob/np.sum(unnormalized_prob)\n",
    "            else:\n",
    "                prob = [0.25] * 4\n",
    "            for i in range(len(self.A)):\n",
    "                t[self.A[i]] = prob[i]\n",
    "        return π\n",
    "    def argmax_a_Q(self,Q,s):\n",
    "        max_action = \"W\"\n",
    "        max_value = -5000\n",
    "        for visit,Q_val in Q.items():\n",
    "            if visit[0] == s:\n",
    "                if Q_val > max_value:\n",
    "                    max_action = visit[1]\n",
    "                    max_value = Q_val                    \n",
    "        return max_action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ad58d",
   "metadata": {},
   "source": [
    "# TD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f21789b",
   "metadata": {},
   "source": [
    "## strategy1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdb914",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\alpha(s) = \\frac{1}{N(s)}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "3bff4ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead time : 3.7524216175079346\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({(0, 0): 5023,\n",
       "  (0, 1): 8538,\n",
       "  (0, 2): 10765,\n",
       "  (0, 3): 10788,\n",
       "  (1, 0): 11414,\n",
       "  (1, 1): 14717,\n",
       "  (1, 2): 12761,\n",
       "  (1, 3): 10782,\n",
       "  (2, 0): 19311,\n",
       "  (2, 1): 26842,\n",
       "  (2, 2): 14842,\n",
       "  (2, 3): 8566,\n",
       "  (3, 0): 19240,\n",
       "  (3, 1): 19401,\n",
       "  (3, 2): 11260,\n",
       "  (3, 3): 4977},\n",
       " {(0, 0): 0,\n",
       "  (0, 1): -6.057028059816423,\n",
       "  (0, 2): -8.23025593693218,\n",
       "  (0, 3): -8.853066340068406,\n",
       "  (1, 0): -6.0761203632257015,\n",
       "  (1, 1): -7.667381252080251,\n",
       "  (1, 2): -8.359148841208096,\n",
       "  (1, 3): -8.301703652750412,\n",
       "  (2, 0): -8.256318312924831,\n",
       "  (2, 1): -8.343092685408902,\n",
       "  (2, 2): -7.68047765577927,\n",
       "  (2, 3): -6.103321633914351,\n",
       "  (3, 0): -8.838984661536804,\n",
       "  (3, 1): -8.232128867802299,\n",
       "  (3, 2): -6.042348547612324,\n",
       "  (3, 3): 0})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TD(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    def prediction(self,s_0,π,iter_nums = 50):\n",
    "        t = time.time()\n",
    "        V = {tuple(s) : 0 for s in self.S}\n",
    "        N = {tuple(s) : 0 for s in self.S}\n",
    "        for ep_num in range(iter_nums):\n",
    "            N[s_0] += 1\n",
    "            s_t = s_0\n",
    "            while s_t not in self.Terminal_states:\n",
    "                a_t = self.sampling_action(s_t,π)\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                N[s_prime] += 1\n",
    "                V[s_t] = V[s_t] + (1/N[s_t]) * (r + 1*V[s_prime] - V[s_t])\n",
    "                s_t = s_prime\n",
    "        print(f\"lead time : {time.time()-t}\")\n",
    "        return N,V\n",
    "\n",
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "N,V = td.prediction(s_0 = (2,1),π = π,iter_nums = 10000)\n",
    "N,V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc95a9",
   "metadata": {},
   "source": [
    "- True value에 수렴하지 않음\n",
    "- 혹시 iteration을 덜해서 수렴하지 않은걸까?\n",
    "    - 훨씬 많은 숫자를 부여함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "caf6c3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead time : 376.06808495521545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({(0, 0): 500675,\n",
       "  (0, 1): 857102,\n",
       "  (0, 2): 1071635,\n",
       "  (0, 3): 1068941,\n",
       "  (1, 0): 1141288,\n",
       "  (1, 1): 1497208,\n",
       "  (1, 2): 1284379,\n",
       "  (1, 3): 1069050,\n",
       "  (2, 0): 1930091,\n",
       "  (2, 1): 2714779,\n",
       "  (2, 2): 1498691,\n",
       "  (2, 3): 854888,\n",
       "  (3, 0): 1928620,\n",
       "  (3, 1): 1929364,\n",
       "  (3, 2): 1141312,\n",
       "  (3, 3): 499325},\n",
       " {(0, 0): 0,\n",
       "  (0, 1): -7.710187158147642,\n",
       "  (0, 2): -10.649747597920722,\n",
       "  (0, 3): -11.512033496895874,\n",
       "  (1, 0): -7.772392586046007,\n",
       "  (1, 1): -9.839184282402476,\n",
       "  (1, 2): -10.751129025128156,\n",
       "  (1, 3): -10.654188503693032,\n",
       "  (2, 0): -10.819929137588238,\n",
       "  (2, 1): -10.85383247342236,\n",
       "  (2, 2): -9.84822261146621,\n",
       "  (2, 3): -7.712727075121741,\n",
       "  (3, 0): -11.760013045162655,\n",
       "  (3, 1): -10.832300462530744,\n",
       "  (3, 2): -7.79454500536325,\n",
       "  (3, 3): 0})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TD(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    def prediction(self,s_0,π,iter_nums = 50):\n",
    "        t = time.time()\n",
    "        V = {tuple(s) : 0 for s in self.S}\n",
    "        N = {tuple(s) : 0 for s in self.S}\n",
    "        for ep_num in range(iter_nums):\n",
    "            N[s_0] += 1\n",
    "            s_t = s_0\n",
    "            while s_t not in self.Terminal_states:\n",
    "                a_t = self.sampling_action(s_t,π)\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                N[s_prime] += 1\n",
    "                V[s_t] = V[s_t] + (1/N[s_t]) * (r + 1*V[s_prime] - V[s_t])\n",
    "                s_t = s_prime\n",
    "        print(f\"lead time : {time.time()-t}\")\n",
    "        return N,V\n",
    "\n",
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "N,V = td.prediction(s_0 = (2,1),π = π,iter_nums = 1000000)\n",
    "N,V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a62ad9",
   "metadata": {},
   "source": [
    "- 늘렸음에도 불구하고 True value function에 한참 벗어남을 알 수 있음\n",
    "- MC처럼 return의 갯수로 $\\alpha$를 정하는 것은 잘못된 것 같음.\n",
    "- return의 갯수가 점점 늘어나서 분모가 커짐에도 불구하고 true value function에 수렴하지 않는 것을 보아하니 작은 값으로 $\\alpha$를 잡아야 할듯함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4a42c",
   "metadata": {},
   "source": [
    "## strategy2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112e1b9",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\alpha = k\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e47d213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    def prediction(self,s_0,π,iter_nums,α=0.1):\n",
    "        V = {tuple(s) : 0 for s in self.S}\n",
    "        for ep_num in range(iter_nums):\n",
    "            s_t = s_0\n",
    "            while s_t not in self.Terminal_states:\n",
    "                a_t = self.sampling_action(s_t,π)\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                V[s_t] = V[s_t] + α * (r + 1*V[s_prime] - V[s_t])\n",
    "                s_t = s_prime\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "313f9769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 1): -8.39849373243713,\n",
       " (0, 2): -21.498903370973046,\n",
       " (0, 3): -23.920539288117975,\n",
       " (1, 0): -18.9460598907016,\n",
       " (1, 1): -17.750880979198747,\n",
       " (1, 2): -20.324083266925584,\n",
       " (1, 3): -18.930470481543285,\n",
       " (2, 0): -21.57693034519246,\n",
       " (2, 1): -22.12603238307893,\n",
       " (2, 2): -20.463756775083247,\n",
       " (2, 3): -11.004329018730525,\n",
       " (3, 0): -23.981268316159696,\n",
       " (3, 1): -21.34414457966366,\n",
       " (3, 2): -10.81270597397484,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "V = td.prediction(s_0 = (2,1),π = π,iter_nums = 10000,α = 0.25)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a8b46",
   "metadata": {},
   "source": [
    "- $\\alpha = 0.25$로 두면 어느정도 True Value에 좀 가까워진듯 한 느낌이 듦\n",
    "- 더 반복수를 키워보면 수렴하지 않을까 싶음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07c4992f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 1): -10.892909639267481,\n",
       " (0, 2): -16.389984823596123,\n",
       " (0, 3): -20.578998512300483,\n",
       " (1, 0): -10.115580086229546,\n",
       " (1, 1): -16.7374227182455,\n",
       " (1, 2): -19.376736534483427,\n",
       " (1, 3): -17.403295865603702,\n",
       " (2, 0): -17.706264918772543,\n",
       " (2, 1): -18.118900013790565,\n",
       " (2, 2): -19.26210200598588,\n",
       " (2, 3): -6.951457310762545,\n",
       " (3, 0): -20.307685831910902,\n",
       " (3, 1): -21.122118838034453,\n",
       " (3, 2): -15.332188651767787,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "V = td.prediction(s_0 = (2,1),π = π,iter_nums = 100000,α = 0.25)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f5770",
   "metadata": {},
   "source": [
    "- 조금 더 True값과 가까워진 느낌은 있지만 그래도 좀 거리가 음.\n",
    "- $\\alpha$를 더 줄여봐야 할 것 같음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cc3bdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 1): -13.468481120602783,\n",
       " (0, 2): -18.04521926868933,\n",
       " (0, 3): -20.830446568464893,\n",
       " (1, 0): -14.162405826062944,\n",
       " (1, 1): -17.51880803992086,\n",
       " (1, 2): -18.99287999167345,\n",
       " (1, 3): -19.31588268251589,\n",
       " (2, 0): -19.68613223150354,\n",
       " (2, 1): -19.616839196046627,\n",
       " (2, 2): -17.409760812877337,\n",
       " (2, 3): -12.103952443141019,\n",
       " (3, 0): -21.66401720239512,\n",
       " (3, 1): -19.851963302190605,\n",
       " (3, 2): -14.369576703234577,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "V = td.prediction(s_0 = (2,1),π = π,iter_nums = 100000,α = 0.025)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd9eef9",
   "metadata": {},
   "source": [
    "- 더 가까워진 느낌이 드는데?\n",
    "- 더 줄여보자."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2710c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 1): -14.03275753892002,\n",
       " (0, 2): -20.120670066469458,\n",
       " (0, 3): -22.087632181402803,\n",
       " (1, 0): -14.356100397943504,\n",
       " (1, 1): -18.06656086981896,\n",
       " (1, 2): -20.057873409291272,\n",
       " (1, 3): -19.933245308012694,\n",
       " (2, 0): -20.11923861351503,\n",
       " (2, 1): -20.10721474631147,\n",
       " (2, 2): -18.017930371514616,\n",
       " (2, 3): -13.836185455422145,\n",
       " (3, 0): -22.0836078738287,\n",
       " (3, 1): -20.00180431090046,\n",
       " (3, 2): -14.000978315795132,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "V = td.prediction(s_0 = (2,1),π = π,iter_nums = 100000,α = 0.001)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f415a",
   "metadata": {},
   "source": [
    "- True값을 어느정도 잘 근사한다.\n",
    "- 혹시 반복수를 더 늘리면 값이 증가하는 거 아닐까? 이게 정말 수렴한걸까?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc7fd2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 1): -14.023947065291312,\n",
       " (0, 2): -19.88466015535378,\n",
       " (0, 3): -22.019204023526456,\n",
       " (1, 0): -13.997593366671204,\n",
       " (1, 1): -17.867270764828138,\n",
       " (1, 2): -19.89726566201253,\n",
       " (1, 3): -19.982618186721407,\n",
       " (2, 0): -19.907105325573927,\n",
       " (2, 1): -19.89049628015864,\n",
       " (2, 2): -17.926426396792216,\n",
       " (2, 3): -13.883623908772902,\n",
       " (3, 0): -21.86589991111874,\n",
       " (3, 1): -19.918957752488122,\n",
       " (3, 2): -14.041869061880227,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "V = td.prediction(s_0 = (2,1),π = π,iter_nums = 500000,α = 0.001)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a29e6e",
   "metadata": {},
   "source": [
    "- 수렴한 것이 맞는 것 같다.\n",
    "- Insight\n",
    "    1. $\\alpha$가 감소하면 반드시 True에 수렴한다고 나와 있었다. 하지만 $\\frac{1}{N}$으로 감소시켜도 True Value수렴하진 않았다. 아마 감소폭을 더 늘려야 할 듯 싶다.\n",
    "    2. $\\alpha$가 상수이면 평균적으로 True에 수렴한다고 나와있었다. 실험결과 $\\alpha$만 적절하면 100% 모두 True Value에 수렴했다.\n",
    "    3. TD에서 step size alpha는 기본적으로 작게 설정해줘야 한다. 최대 0.01이고 반드시 그 이하여야 할 것 같다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ad4b4",
   "metadata": {},
   "source": [
    "# SARSA(On-Policy TD Control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c50dd1",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\pi(a|s) = \\begin{cases}\n",
    "1-\\epsilon + \\frac{\\epsilon}{|A(s)|}\\quad(A = \\text{greedy action})\\\\\n",
    "\\epsilon /|A(s)| \\quad\\quad\\quad\\quad (\\text{otherwise}) \n",
    "\\end{cases}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fd54a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "class Environment():\n",
    "    def __init__(self,grid_size):\n",
    "        self.grid_size = np.array(grid_size)\n",
    "        self.S = [np.array([i,j]) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "        self.Terminal_states = [(0,0),(self.grid_size[0]-1,self.grid_size[1]-1)]\n",
    "        self.A = [\"W\",\"E\",\"N\",\"S\"]\n",
    "        self.A_to_coord = {\"W\" : [0,-1],\"E\" : [0,1], \"N\" : [-1,0], \"S\" : [1,0]}\n",
    "        for k,v in self.A_to_coord.items():\n",
    "            self.A_to_coord[k] = np.array(v)\n",
    "        self.gamma = 1\n",
    "        self.R = -1\n",
    "    \n",
    "    def move(self,s,a):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        output\n",
    "        s_next : state after one_step transition\n",
    "        ex)\n",
    "        Input : s = [0,1],a = \"W\"\n",
    "        Output : s_next = [0,0]\n",
    "        \"\"\"\n",
    "        s_next = s + self.A_to_coord[a]\n",
    "        if s_next[0] < 0 or s_next[1] <0 or s_next[0] >= self.grid_size[0] or s_next[1] >= self.grid_size[1]:\n",
    "            return s # |S|를 넘어갈 경우, 원래 상태를 반환\n",
    "        else:\n",
    "            return s_next\n",
    "        \n",
    "    def move_test(self):\n",
    "        S = [np.array([i,j]) for i in [0,1,2,3] for j in [0,1,2,3]]\n",
    "        for s in S:\n",
    "            for a in self.A:\n",
    "                print(s,a,self.move(s,a))\n",
    "    def dynamics(self,s_prime,r,s,a):\n",
    "        r=-1\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        s_prime : all of the possible states after one-step transition\n",
    "        r : immediate reward(-1)\n",
    "        output\n",
    "        0 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 다를때\n",
    "        1 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 같을때\n",
    "        즉, a방향으로 움직이면 반드시 a방향으로 감(deterministic)\n",
    "        \"\"\"\n",
    "        s_next = self.move(s,a)\n",
    "        if np.sum(s_next != s_prime)>=1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    def dynamics_test(self,check_p = 1):\n",
    "        r=-1\n",
    "        for s in self.S:\n",
    "            for a in self.A:\n",
    "                for s_prime in self.S: #가능한 모든 next_state\n",
    "                    if self.dynamics(s_prime,r,s,a) == check_p:\n",
    "                        print(f\"state : {s} action : {a} s_prime : {s_prime} dynamics : {self.dynamics(s_prime,r,s,a)}\")\n",
    "    def sampling_action(self,s_t,π):\n",
    "        \"\"\"\n",
    "        Input current state s_t : tuple\n",
    "        ouput action : str (a ~ π(a|s))\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for a,p in π[s_t].items():\n",
    "            actions.append(a)\n",
    "            prob.append(p)\n",
    "        #print(prob)\n",
    "        return np.random.choice(a=actions,p=prob)\n",
    "    def generate_π(self,uniform=False):\n",
    "        \"\"\"\n",
    "        Input : NA\n",
    "        Output : {(x,y) : {\"W\" : pr1,\"E\" : pr2 ,...}}\n",
    "        ex)\n",
    "        {(0,0):{\"W\" : 0.1, \"E\" : 0.2, ...},(0,1):{\"W\":0.4,\"E\":0.5...}}\n",
    "        \"\"\"\n",
    "        π = {(i,j): {} for i in range(self.grid_size[0]) for j in range(self.grid_size[1])}\n",
    "        for t in π.values():\n",
    "            unnormalized_prob = np.random.rand(4)\n",
    "            if uniform == False:\n",
    "                prob = unnormalized_prob/np.sum(unnormalized_prob)\n",
    "            else:\n",
    "                prob = [0.25] * 4\n",
    "            for i in range(len(self.A)):\n",
    "                t[self.A[i]] = prob[i]\n",
    "        return π\n",
    "    def argmax_a_Q(self,Q,s):\n",
    "        max_action = \"W\"\n",
    "        max_value = -5000\n",
    "        for visit,Q_val in Q.items():\n",
    "            if visit[0] == s:\n",
    "                if Q_val > max_value:\n",
    "                    max_action = visit[1]\n",
    "                    max_value = Q_val                    \n",
    "        return max_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "530f7298",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    def control(self,s_0,iter_nums,α=0.9,ϵ = 0.9995):\n",
    "        Q = {(tuple(s),a):0 for s in self.S for a in self.A}\n",
    "        π = self.generate_π()\n",
    "        action_space_size = len(self.A)\n",
    "        \n",
    "        count=0\n",
    "        for ep_num in range(1,iter_nums):\n",
    "            s_t = s_0;a = self.sampling_action(s_0,π)\n",
    "            a_t = self.sampling_action(s_t,π)\n",
    "            epsilon = ϵ ** (ep_num + 1) #실수한 부분\n",
    "            alpha = α**(ep_num+1) #GLIE?\n",
    "            \n",
    "            if ep_num % 1000 == 0:\n",
    "                print(f\"episode : {ep_num}, epsilon : {epsilon}, alpha : {alpha}\")\n",
    "            \n",
    "            while s_t not in self.Terminal_states:\n",
    "                count+=1\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                #sampling action ~ ϵ-greedy action\n",
    "                #1. make ϵ-greedy policy\n",
    "                a_star = self.argmax_a_Q(Q,s_prime)\n",
    "                for action,prob in π[s_prime].items():\n",
    "                    if action == a_star:\n",
    "                        π[s_prime][action] = 1 - epsilon + epsilon/action_space_size\n",
    "                    else:\n",
    "                        π[s_prime][action] = epsilon/action_space_size\n",
    "                #2. sampling action\n",
    "                a_prime = self.sampling_action(s_prime,π)\n",
    "                Q[(s_t,a_t)] = Q[(s_t,a_t)] + alpha*(r + 1*Q[(s_prime,a_prime)] - Q[(s_t,a_t)])\n",
    "                #print(alpha*(r + 1*Q[(s_prime,a_prime)] - Q[(s_t,a_t)]))\n",
    "                s_t = s_prime;a_t = a_prime\n",
    "                count+=1\n",
    "        return Q,π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0f706526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 1000, epsilon : 0.9047424102692003, alpha : 0.9119268439216149\n",
      "episode : 2000, epsilon : 0.818640693009023, alpha : 0.8316871663561808\n",
      "episode : 3000, epsilon : 0.740733027040136, alpha : 0.7585077106700777\n",
      "episode : 4000, epsilon : 0.6702396082111145, alpha : 0.6917672538661829\n",
      "episode : 5000, epsilon : 0.6064548440752158, alpha : 0.6308992338374627\n",
      "episode : 6000, epsilon : 0.5487402913771806, alpha : 0.5753869398011344\n",
      "episode : 7000, epsilon : 0.49651826565897955, alpha : 0.5247591259224881\n",
      "episode : 8000, epsilon : 0.4492660590208904, alpha : 0.4785860108922666\n",
      "episode : 9000, epsilon : 0.40651070816152135, alpha : 0.43647562949787516\n",
      "episode : 10000, epsilon : 0.36782426032832716, alpha : 0.3980705052167768\n",
      "episode : 11000, epsilon : 0.3328194897939159, alpha : 0.36304461558560236\n",
      "episode : 12000, epsilon : 0.3011460219829113, alpha : 0.33110062458388595\n",
      "episode : 13000, epsilon : 0.2724868264544462, alpha : 0.30196735853803136\n",
      "episode : 14000, epsilon : 0.24655504363736472, alpha : 0.27539750411837116\n",
      "episode : 15000, epsilon : 0.22309111355585282, alpha : 0.25116550888753125\n",
      "episode : 16000, epsilon : 0.20186017780594276, alpha : 0.22906566657778343\n",
      "episode : 17000, epsilon : 0.18264972877839583, alpha : 0.20891037084323602\n",
      "episode : 18000, epsilon : 0.1652674825982414, alpha : 0.1905285226629039\n",
      "episode : 19000, epsilon : 0.1495394544905051, alpha : 0.17376407787504544\n",
      "episode : 20000, epsilon : 0.13530821730781176, alpha : 0.15847472251273403\n",
      "episode : 21000, epsilon : 0.12243132578887723, alpha : 0.1445306646955409\n",
      "episode : 22000, epsilon : 0.11077989077576007, alpha : 0.13181353282165464\n",
      "episode : 23000, epsilon : 0.1002372891187318, alpha : 0.12021537070715121\n",
      "episode : 24000, epsilon : 0.09069799635576768, alpha : 0.10963772114211648\n",
      "episode : 25000, epsilon : 0.08206653048255265, alpha : 0.099990790083896\n",
      "episode : 26000, epsilon : 0.07425649624083958, alpha : 0.09119268439227927\n",
      "episode : 27000, epsilon : 0.06718972035911892, alpha : 0.08316871663572549\n",
      "episode : 28000, epsilon : 0.06079546908993181, alpha : 0.07585077106710574\n",
      "episode : 29000, epsilon : 0.05500974021189277, alpha : 0.06917672538670762\n",
      "episode : 30000, epsilon : 0.049774622409831396, alpha : 0.06308992338382775\n",
      "episode : 31000, epsilon : 0.0450377156208723, alpha : 0.05753869398018775\n",
      "episode : 32000, epsilon : 0.04075160654450127, alpha : 0.05247591259231658\n",
      "episode : 33000, epsilon : 0.03687339406682088, alpha : 0.047858601089288466\n",
      "episode : 34000, epsilon : 0.033364259848806435, alpha : 0.043647562949843885\n",
      "episode : 35000, epsilon : 0.030189079780435085, alpha : 0.03980705052172909\n",
      "episode : 36000, epsilon : 0.02731607241160118, alpha : 0.03630446155860712\n",
      "episode : 37000, epsilon : 0.024716480840844147, alpha : 0.03311006245843135\n",
      "episode : 38000, epsilon : 0.022364284877805625, alpha : 0.030196735853842135\n",
      "episode : 39000, epsilon : 0.020235940598352726, alpha : 0.027539750411872684\n",
      "episode : 40000, epsilon : 0.01831014468548656, alpha : 0.025116550888785564\n",
      "episode : 41000, epsilon : 0.016567621197244625, alpha : 0.022906566657807926\n",
      "episode : 42000, epsilon : 0.014990928627284924, alpha : 0.020891037084350583\n",
      "episode : 43000, epsilon : 0.013564285326956013, alpha : 0.019052852266314996\n",
      "episode : 44000, epsilon : 0.012273411541443477, alpha : 0.017376407787526985\n",
      "episode : 45000, epsilon : 0.01110538647887928, alpha : 0.01584747225129387\n",
      "episode : 46000, epsilon : 0.010048518981770406, alpha : 0.014453066469572758\n",
      "episode : 47000, epsilon : 0.009092230506253393, alpha : 0.013181353282182487\n",
      "episode : 48000, epsilon : 0.008226949237874634, alpha : 0.012021537070730646\n",
      "episode : 49000, epsilon : 0.0074440142840654635, alpha : 0.010963772114225808\n",
      "episode : 50000, epsilon : 0.006735588984342178, alpha : 0.009999079008402512\n",
      "episode : 51000, epsilon : 0.0060945824705234694, alpha : 0.009119268439239703\n",
      "episode : 52000, epsilon : 0.005514578691835005, alpha : 0.008316871663583289\n",
      "episode : 53000, epsilon : 0.004989772194489425, alpha : 0.007585077106720369\n",
      "episode : 54000, epsilon : 0.004514910012937894, alpha : 0.006917672538679696\n",
      "episode : 55000, epsilon : 0.004085239091163093, alpha : 0.006308992338390923\n",
      "episode : 56000, epsilon : 0.003696458707735628, alpha : 0.005753869398026207\n",
      "episode : 57000, epsilon : 0.0033446774284401496, alpha : 0.005247591259238435\n",
      "episode : 58000, epsilon : 0.0030263741555954914, alpha : 0.004785860108935027\n",
      "episode : 59000, epsilon : 0.0027383628841983008, alpha : 0.0043647562949900255\n",
      "episode : 60000, epsilon : 0.0024777608121225024, alpha : 0.003980705052178049\n",
      "episode : 61000, epsilon : 0.0022419594851788016, alpha : 0.003630446155865401\n",
      "episode : 62000, epsilon : 0.002028598688215386, alpha : 0.0033110062458474115\n",
      "episode : 63000, epsilon : 0.0018355428209270194, alpha : 0.0030196735853881135\n",
      "episode : 64000, epsilon : 0.0016608595219100297, alpha : 0.0027539750411908247\n",
      "episode : 65000, epsilon : 0.0015028003270041324, alpha : 0.0025116550888818\n",
      "episode : 66000, epsilon : 0.0013597831683238937, alpha : 0.002290656665783751\n",
      "episode : 67000, epsilon : 0.0012303765388067302, alpha : 0.002089103708437756\n",
      "episode : 68000, epsilon : 0.0011132851637750548, alpha : 0.0019052852266339601\n",
      "episode : 69000, epsilon : 0.001007337036094394, alpha : 0.0017376407787549424\n",
      "episode : 70000, epsilon : 0.0009114716851579903, alpha : 0.0015847472251314335\n",
      "episode : 71000, epsilon : 0.0008247295622781977, alpha : 0.0014453066469591422\n",
      "episode : 72000, epsilon : 0.0007462424362394631, alpha : 0.001318135328219951\n",
      "episode : 73000, epsilon : 0.0006752247028787398, alpha : 0.0012021537070746172\n",
      "episode : 74000, epsilon : 0.0006109655217079866, alpha : 0.0010963772114239967\n",
      "episode : 75000, epsilon : 0.0005528217008715504, alpha : 0.0009999079008415426\n",
      "episode : 76000, epsilon : 0.0005002112592215676, alpha : 0.0009119268439251482\n",
      "episode : 77000, epsilon : 0.00045260760107202, alpha : 0.000831687166359403\n",
      "episode : 78000, epsilon : 0.00040953424532459254, alpha : 0.0007585077106730165\n",
      "episode : 79000, epsilon : 0.00037056005620837067, alpha : 0.000691767253868863\n",
      "episode : 80000, epsilon : 0.00033529492789624127, alpha : 0.000630899233839907\n",
      "episode : 81000, epsilon : 0.0003033858798038635, alpha : 0.0005753869398033637\n",
      "episode : 82000, epsilon : 0.0002745135234877381, alpha : 0.0005247591259245212\n",
      "episode : 83000, epsilon : 0.0002483888657783648, alpha : 0.0004785860108941208\n",
      "episode : 84000, epsilon : 0.00022475041614996566, alpha : 0.00043647562949956627\n",
      "episode : 85000, epsilon : 0.00020336156937346308, alpha : 0.00039807050521831905\n",
      "episode : 86000, epsilon : 0.00018400823725479967, alpha : 0.00036304461558700897\n",
      "episode : 87000, epsilon : 0.0001664967057538697, alpha : 0.00033110062458516877\n",
      "episode : 88000, epsilon : 0.00015065169603524145, alpha : 0.0003019673585392013\n",
      "episode : 89000, epsilon : 0.0001363146100430716, alpha : 0.0002753975041194382\n",
      "episode : 90000, epsilon : 0.0001233419430395787, alpha : 0.0002511655088885044\n",
      "episode : 91000, epsilon : 0.00011160384721763662, alpha : 0.00022906566657867093\n",
      "episode : 92000, epsilon : 0.00010098283201020116, alpha : 0.00020891037084404542\n",
      "episode : 93000, epsilon : 9.137258808752791e-05, alpha : 0.00019052852266364208\n",
      "episode : 94000, epsilon : 8.267692327117194e-05, alpha : 0.00017376407787571865\n",
      "episode : 95000, epsilon : 7.480879971397324e-05, alpha : 0.00015847472251334803\n",
      "episode : 96000, epsilon : 6.76894627088369e-05, alpha : 0.0001445306646961009\n",
      "episode : 97000, epsilon : 6.124765240626088e-05, alpha : 0.00013181353282216534\n",
      "episode : 98000, epsilon : 5.5418890550425704e-05, alpha : 0.00012021537070761697\n",
      "episode : 99000, epsilon : 5.0144835094546614e-05, alpha : 0.00010963772114254127\n"
     ]
    }
   ],
   "source": [
    "sarsa = SARSA()\n",
    "Q,π = sarsa.control(s_0 = (2,1),iter_nums = 100000,α = np.exp(np.log(0.0001)/100000),ϵ = 0.9999)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5193f38",
   "metadata": {},
   "source": [
    "- 초기에 학습되지 않은 상태의 $\\pi$에 의해 Q값이 영향을 많이 받을 수 있다.\n",
    "- 적절히 탐험함과 동시에 $\\epsilon$이 빨리 떨어져서 학습된 정보를 쓸 수 있게 해야 정확한 $Q$값을 구할 수 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "489fe608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 0), 'W'): 0,\n",
       " ((0, 0), 'E'): 0,\n",
       " ((0, 0), 'N'): 0,\n",
       " ((0, 0), 'S'): 0,\n",
       " ((0, 1), 'W'): -1.0,\n",
       " ((0, 1), 'E'): -3.3323793522994833,\n",
       " ((0, 1), 'N'): -2.0045782177129015,\n",
       " ((0, 1), 'S'): -3.3983693586776482,\n",
       " ((0, 2), 'W'): -2.123245282636658,\n",
       " ((0, 2), 'E'): -5.530109740776886,\n",
       " ((0, 2), 'N'): -3.9378481042906377,\n",
       " ((0, 2), 'S'): -4.573563369404286,\n",
       " ((0, 3), 'W'): -4.03339097693781,\n",
       " ((0, 3), 'E'): -5.42422622877365,\n",
       " ((0, 3), 'N'): -5.9243412887695985,\n",
       " ((0, 3), 'S'): -5.0411702164485215,\n",
       " ((1, 0), 'W'): -2.1433973390616656,\n",
       " ((1, 0), 'E'): -3.1222171379873207,\n",
       " ((1, 0), 'N'): -1.0,\n",
       " ((1, 0), 'S'): -3.206357191920265,\n",
       " ((1, 1), 'W'): -2.0643897617234646,\n",
       " ((1, 1), 'E'): -4.190076134990604,\n",
       " ((1, 1), 'N'): -2.264986949722798,\n",
       " ((1, 1), 'S'): -4.202230629642308,\n",
       " ((1, 2), 'W'): -3.9117031424234776,\n",
       " ((1, 2), 'E'): -3.657568832911959,\n",
       " ((1, 2), 'N'): -3.77091533257776,\n",
       " ((1, 2), 'S'): -3.050559637632963,\n",
       " ((1, 3), 'W'): -4.623726264709104,\n",
       " ((1, 3), 'E'): -3.7594694049532826,\n",
       " ((1, 3), 'N'): -5.325227602321366,\n",
       " ((1, 3), 'S'): -2.0034778790310765,\n",
       " ((2, 0), 'W'): -3.1713085185107914,\n",
       " ((2, 0), 'E'): -4.22740046617296,\n",
       " ((2, 0), 'N'): -2.0117222298570914,\n",
       " ((2, 0), 'S'): -4.184376265654459,\n",
       " ((2, 1), 'W'): -3.1080038844100484,\n",
       " ((2, 1), 'E'): -3.000438836158547,\n",
       " ((2, 1), 'N'): -3.171386313320444,\n",
       " ((2, 1), 'S'): -3.117540689867569,\n",
       " ((2, 2), 'W'): -4.144258843676126,\n",
       " ((2, 2), 'E'): -2.000139118554603,\n",
       " ((2, 2), 'N'): -4.1377911799736085,\n",
       " ((2, 2), 'S'): -2.1237254775069183,\n",
       " ((2, 3), 'W'): -3.1528738261745333,\n",
       " ((2, 3), 'E'): -2.0336004709980258,\n",
       " ((2, 3), 'N'): -3.069491414847961,\n",
       " ((2, 3), 'S'): -1.0,\n",
       " ((3, 0), 'W'): -4.573778336724294,\n",
       " ((3, 0), 'E'): -3.7076400646155885,\n",
       " ((3, 0), 'N'): -3.0466119557467546,\n",
       " ((3, 0), 'S'): -4.660492506114535,\n",
       " ((3, 1), 'W'): -4.216890101305282,\n",
       " ((3, 1), 'E'): -2.0111838353726657,\n",
       " ((3, 1), 'N'): -4.238596459543881,\n",
       " ((3, 1), 'S'): -3.203730233324354,\n",
       " ((3, 2), 'W'): -3.1959234458939596,\n",
       " ((3, 2), 'E'): -1.0,\n",
       " ((3, 2), 'N'): -3.124331894542941,\n",
       " ((3, 2), 'S'): -2.0586655542334618,\n",
       " ((3, 3), 'W'): 0,\n",
       " ((3, 3), 'E'): 0,\n",
       " ((3, 3), 'N'): 0,\n",
       " ((3, 3), 'S'): 0}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "6f946740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 1): -1.0242829182033806,\n",
       " (0, 2): -2.141202785727903,\n",
       " (0, 3): -3.248416420140333,\n",
       " (1, 0): -1.024492195882349,\n",
       " (1, 1): -2.0431017158579077,\n",
       " (1, 2): -3.0429709841943056,\n",
       " (1, 3): -2.1677109327871933,\n",
       " (2, 0): -2.0489786066301985,\n",
       " (2, 1): -3.041941279994179,\n",
       " (2, 2): -2.041870178294855,\n",
       " (2, 3): -1.0243719054487954,\n",
       " (3, 0): -3.0979206693126837,\n",
       " (3, 1): -2.0470834198523264,\n",
       " (3, 2): -1.0233902362150626,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class DP_prediction(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    def policy_eval(self,π,iter_nums = 10):\n",
    "        V = {tuple(s): 0 for s in self.S}\n",
    "        r = -1;gamma=1\n",
    "        for iter_num in range(iter_nums):\n",
    "            for s in self.S:\n",
    "                s_t = tuple(s) #for each s \\in S\n",
    "                if s_t in self.Terminal_states:\n",
    "                    continue\n",
    "                v = V[s_t]\n",
    "                updt_value = 0\n",
    "                for a in self.A:\n",
    "                    total = 0\n",
    "                    for s_prime in self.S:\n",
    "                        #print(s_prime,r,s_t,a)\n",
    "                        #print(self.dynamics(s_prime,r,s_t,a))\n",
    "                        total += self.dynamics(s_prime,r,s_t,a) * (r + gamma * V[tuple(s_prime)])\n",
    "                    updt_value += π[s_t][a] * total\n",
    "                V[s_t] = updt_value\n",
    "                \n",
    "        return V\n",
    "dp_pred= DP_prediction()\n",
    "V = dp_pred.policy_eval(π,10)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ec0f30",
   "metadata": {},
   "source": [
    "- DP로 구한 optimal value function과 비교\n",
    "- DP의 optimal value function의 각각의 value는 optimal action value function의 각각의 $s_t$에서  max action값이 되어야 함."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a42aa6",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "63c7dcb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon : 0.9990002500000001 alpha : 0.9986194028465246\n",
      "epsilon : 0.6058485196309606 alpha : 0.5004952959591761\n",
      "epsilon : 0.36741975664072807 alpha : 0.25084185282524624\n",
      "epsilon : 0.22282348342150354 alpha : 0.1257187342954265\n",
      "epsilon : 0.13513237616300078 alpha : 0.06300862465664786\n",
      "epsilon : 0.0819516812458937 alpha : 0.031579118286324974\n",
      "epsilon : 0.0496999923314264 alpha : 0.015827050934311866\n",
      "epsilon : 0.030140800044509277 alpha : 0.007932315874245822\n",
      "epsilon : 0.0182790335512516 alpha : 0.0039755754492710235\n",
      "epsilon : 0.011085408054012442 alpha : 0.0019925076614966753\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "class Environment():\n",
    "    def __init__(self,grid_size):\n",
    "        self.grid_size = np.array(grid_size)\n",
    "        self.S = [np.array([i,j]) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "        self.Terminal_states = [(0,0),(self.grid_size[0]-1,self.grid_size[1]-1)]\n",
    "        self.A = [\"W\",\"E\",\"N\",\"S\"]\n",
    "        self.A_to_coord = {\"W\" : [0,-1],\"E\" : [0,1], \"N\" : [-1,0], \"S\" : [1,0]}\n",
    "        for k,v in self.A_to_coord.items():\n",
    "            self.A_to_coord[k] = np.array(v)\n",
    "        self.gamma = 1\n",
    "        self.R = -1\n",
    "    \n",
    "    def move(self,s,a):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        output\n",
    "        s_next : state after one_step transition\n",
    "        ex)\n",
    "        Input : s = [0,1],a = \"W\"\n",
    "        Output : s_next = [0,0]\n",
    "        \"\"\"\n",
    "        s_next = s + self.A_to_coord[a]\n",
    "        if s_next[0] < 0 or s_next[1] <0 or s_next[0] >= self.grid_size[0] or s_next[1] >= self.grid_size[1]:\n",
    "            return s # |S|를 넘어갈 경우, 원래 상태를 반환\n",
    "        else:\n",
    "            return s_next\n",
    "        \n",
    "    def move_test(self):\n",
    "        S = [np.array([i,j]) for i in [0,1,2,3] for j in [0,1,2,3]]\n",
    "        for s in S:\n",
    "            for a in self.A:\n",
    "                print(s,a,self.move(s,a))\n",
    "    def dynamics(self,s_prime,r,s,a):\n",
    "        r=-1\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        s_prime : all of the possible states after one-step transition\n",
    "        r : immediate reward(-1)\n",
    "        output\n",
    "        0 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 다를때\n",
    "        1 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 같을때\n",
    "        즉, a방향으로 움직이면 반드시 a방향으로 감(deterministic)\n",
    "        \"\"\"\n",
    "        s_next = self.move(s,a)\n",
    "        if np.sum(s_next != s_prime)>=1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    def dynamics_test(self,check_p = 1):\n",
    "        r=-1\n",
    "        for s in self.S:\n",
    "            for a in self.A:\n",
    "                for s_prime in self.S: #가능한 모든 next_state\n",
    "                    if self.dynamics(s_prime,r,s,a) == check_p:\n",
    "                        print(f\"state : {s} action : {a} s_prime : {s_prime} dynamics : {self.dynamics(s_prime,r,s,a)}\")\n",
    "    def sampling_action(self,s_t,π):\n",
    "        \"\"\"\n",
    "        Input current state s_t : tuple\n",
    "        ouput action : str (a ~ π(a|s))\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for a,p in π[s_t].items():\n",
    "            actions.append(a)\n",
    "            prob.append(p)\n",
    "        #print(prob)\n",
    "        return np.random.choice(a=actions,p=prob)\n",
    "    def generate_π(self,uniform=False):\n",
    "        \"\"\"\n",
    "        Input : NA\n",
    "        Output : {(x,y) : {\"W\" : pr1,\"E\" : pr2 ,...}}\n",
    "        ex)\n",
    "        {(0,0):{\"W\" : 0.1, \"E\" : 0.2, ...},(0,1):{\"W\":0.4,\"E\":0.5...}}\n",
    "        \"\"\"\n",
    "        π = {(i,j): {} for i in range(self.grid_size[0]) for j in range(self.grid_size[1])}\n",
    "        for t in π.values():\n",
    "            unnormalized_prob = np.random.rand(4)\n",
    "            if uniform == False:\n",
    "                prob = unnormalized_prob/np.sum(unnormalized_prob)\n",
    "            else:\n",
    "                prob = [0.25] * 4\n",
    "            for i in range(len(self.A)):\n",
    "                t[self.A[i]] = prob[i]\n",
    "        return π\n",
    "    def argmax_a_Q(self,Q,s):\n",
    "        max_action = \"W\"\n",
    "        max_value = -5000\n",
    "        for visit,Q_val in Q.items():\n",
    "            if visit[0] == s:\n",
    "                if Q_val > max_value:\n",
    "                    max_action = visit[1]\n",
    "                    max_value = Q_val                    \n",
    "        return max_action\n",
    "class Q_learning(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    \n",
    "    def control(self,s_0,iter_num,alpha,epsilon):\n",
    "        Q = {(tuple(s),a) : 0 for s in self.S for a in self.A}\n",
    "        π = self.generate_π()\n",
    "        γ = 1\n",
    "        for ep_num in range(iter_num):\n",
    "            ϵ = epsilon ** (ep_num + 1)\n",
    "            α = alpha ** (ep_num + 1)\n",
    "            if ep_num % 1000 == True:\n",
    "                print(f\"epsilon : {ϵ} alpha : {α}\")\n",
    "            s_t = s_0\n",
    "            while s_t not in self.Terminal_states:\n",
    "                a_t = self.sampling_action(s_t,π)\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                Q[(s_t,a_t)] = Q[(s_t,a_t)] + α * (r + γ * Q[(s_prime,self.argmax_a_Q(Q,s_prime))] - Q[(s_t,a_t)])\n",
    "                \n",
    "                a_star = self.argmax_a_Q(Q,tuple(s_t))\n",
    "                for (state,action),value in Q.items():\n",
    "                    if action == a_star:\n",
    "                        π[state][action] = 1 - ϵ + ϵ /len(self.A)\n",
    "                    else:\n",
    "                        π[state][action] = ϵ/len(self.A)\n",
    "                s_t = s_prime\n",
    "        return π,Q\n",
    "                \n",
    "q_l = Q_learning()\n",
    "policy,Q= q_l.control(s_0 = (2,1),iter_num = 10000,alpha = np.exp(np.log(0.001)/10000),epsilon = 0.9995)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a0d5f005",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 0), 'W'): 0,\n",
       " ((0, 0), 'E'): 0,\n",
       " ((0, 0), 'N'): 0,\n",
       " ((0, 0), 'S'): 0,\n",
       " ((0, 1), 'W'): -1.0,\n",
       " ((0, 1), 'E'): -3.0,\n",
       " ((0, 1), 'N'): -2.0,\n",
       " ((0, 1), 'S'): -3.0,\n",
       " ((0, 2), 'W'): -2.0,\n",
       " ((0, 2), 'E'): -4.0,\n",
       " ((0, 2), 'N'): -3.0,\n",
       " ((0, 2), 'S'): -4.0,\n",
       " ((0, 3), 'W'): -3.0,\n",
       " ((0, 3), 'E'): -4.0,\n",
       " ((0, 3), 'N'): -4.0,\n",
       " ((0, 3), 'S'): -3.0,\n",
       " ((1, 0), 'W'): -2.0,\n",
       " ((1, 0), 'E'): -3.0,\n",
       " ((1, 0), 'N'): -1.0,\n",
       " ((1, 0), 'S'): -3.0,\n",
       " ((1, 1), 'W'): -2.0,\n",
       " ((1, 1), 'E'): -4.0,\n",
       " ((1, 1), 'N'): -2.0,\n",
       " ((1, 1), 'S'): -4.0,\n",
       " ((1, 2), 'W'): -3.0,\n",
       " ((1, 2), 'E'): -3.0,\n",
       " ((1, 2), 'N'): -3.0,\n",
       " ((1, 2), 'S'): -3.0,\n",
       " ((1, 3), 'W'): -4.0,\n",
       " ((1, 3), 'E'): -3.0,\n",
       " ((1, 3), 'N'): -4.0,\n",
       " ((1, 3), 'S'): -2.0,\n",
       " ((2, 0), 'W'): -3.0,\n",
       " ((2, 0), 'E'): -4.0,\n",
       " ((2, 0), 'N'): -2.0,\n",
       " ((2, 0), 'S'): -4.0,\n",
       " ((2, 1), 'W'): -3.0,\n",
       " ((2, 1), 'E'): -3.0,\n",
       " ((2, 1), 'N'): -3.0,\n",
       " ((2, 1), 'S'): -3.0,\n",
       " ((2, 2), 'W'): -4.0,\n",
       " ((2, 2), 'E'): -2.0,\n",
       " ((2, 2), 'N'): -4.0,\n",
       " ((2, 2), 'S'): -2.0,\n",
       " ((2, 3), 'W'): -3.0,\n",
       " ((2, 3), 'E'): -2.0,\n",
       " ((2, 3), 'N'): -3.0,\n",
       " ((2, 3), 'S'): -1.0,\n",
       " ((3, 0), 'W'): -4.0,\n",
       " ((3, 0), 'E'): -3.0,\n",
       " ((3, 0), 'N'): -3.0,\n",
       " ((3, 0), 'S'): -4.0,\n",
       " ((3, 1), 'W'): -4.0,\n",
       " ((3, 1), 'E'): -2.0,\n",
       " ((3, 1), 'N'): -4.0,\n",
       " ((3, 1), 'S'): -3.0,\n",
       " ((3, 2), 'W'): -3.0,\n",
       " ((3, 2), 'E'): -1.0,\n",
       " ((3, 2), 'N'): -3.0,\n",
       " ((3, 2), 'S'): -2.0,\n",
       " ((3, 3), 'W'): 0,\n",
       " ((3, 3), 'E'): 0,\n",
       " ((3, 3), 'N'): 0,\n",
       " ((3, 3), 'S'): 0}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9680c422",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (0, 1): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (0, 2): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (0, 3): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (1, 0): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (1, 1): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (1, 2): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (1, 3): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (2, 0): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (2, 1): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (2, 2): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (2, 3): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (3, 0): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (3, 1): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (3, 2): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668},\n",
       " (3, 3): {'W': 0.0016823817555366668,\n",
       "  'E': 0.99495285473339,\n",
       "  'N': 0.0016823817555366668,\n",
       "  'S': 0.0016823817555366668}}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
