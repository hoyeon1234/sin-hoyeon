{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c7fcfddf",
   "metadata": {},
   "source": [
    "# Greed World Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6c06fe",
   "metadata": {},
   "source": [
    "![](./ex4-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4aba839",
   "metadata": {},
   "source": [
    "- $\\mathcal{S} = \\{[0,0],[0,1],...,[3,3]\\}$\n",
    "- $\\mathcal{A}(s) = \\{\\text{up,down,right,left}\\}$\n",
    "- $R_t = -1$\n",
    "- $\\gamma = 1$\n",
    "- Deterministically cause the corresponding state transitions\n",
    "- The actions that would take the agent off the grid in fact leave the state unchanged.\n",
    "- undiscounted, episodic task"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaada1b9",
   "metadata": {},
   "source": [
    "## True Value function $v_\\pi$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b118ffc9",
   "metadata": {},
   "source": [
    "- $\\forall s,\\forall a : \\pi(a|s) = 0.25$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759b912",
   "metadata": {},
   "source": [
    "![](./ex4-2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f9eaf96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a07913fd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  0., -14., -20., -22.],\n",
       "       [-14., -18., -20., -20.],\n",
       "       [-20., -20., -18., -14.],\n",
       "       [-22., -20., -14.,   0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array ([[  0,-14,-20 ,-22],\n",
    "           [-14, -18, -20, -20],\n",
    "           [-20, -20, -18, -14.],\n",
    "           [-22, -20, -14,   0.]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc4bad4",
   "metadata": {},
   "source": [
    "## Optimal action value function, $q_{\\star}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ead14d4",
   "metadata": {},
   "source": [
    "![](./ex4-3.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e574e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "q_star = {((0, 0), 'W'): 0,\n",
    " ((0, 0), 'E'): 0,\n",
    " ((0, 0), 'N'): 0,\n",
    " ((0, 0), 'S'): 0,\n",
    " ((0, 1), 'W'): -1.0,\n",
    " ((0, 1), 'E'): -3.0,\n",
    " ((0, 1), 'N'): -2.0,\n",
    " ((0, 1), 'S'): -3.0,\n",
    " ((0, 2), 'W'): -2.0,\n",
    " ((0, 2), 'E'): -4.0,\n",
    " ((0, 2), 'N'): -3.0,\n",
    " ((0, 2), 'S'): -4.0,\n",
    " ((0, 3), 'W'): -3.0,\n",
    " ((0, 3), 'E'): -4.0,\n",
    " ((0, 3), 'N'): -4.0,\n",
    " ((0, 3), 'S'): -3.0,\n",
    " ((1, 0), 'W'): -2.0,\n",
    " ((1, 0), 'E'): -3.0,\n",
    " ((1, 0), 'N'): -1.0,\n",
    " ((1, 0), 'S'): -3.0,\n",
    " ((1, 1), 'W'): -2.0,\n",
    " ((1, 1), 'E'): -4.0,\n",
    " ((1, 1), 'N'): -2.0,\n",
    " ((1, 1), 'S'): -4.0,\n",
    " ((1, 2), 'W'): -3.0,\n",
    " ((1, 2), 'E'): -3.0,\n",
    " ((1, 2), 'N'): -3.0,\n",
    " ((1, 2), 'S'): -3.0,\n",
    " ((1, 3), 'W'): -4.0,\n",
    " ((1, 3), 'E'): -3.0,\n",
    " ((1, 3), 'N'): -4.0,\n",
    " ((1, 3), 'S'): -2.0,\n",
    " ((2, 0), 'W'): -3.0,\n",
    " ((2, 0), 'E'): -4.0,\n",
    " ((2, 0), 'N'): -2.0,\n",
    " ((2, 0), 'S'): -4.0,\n",
    " ((2, 1), 'W'): -3.0,\n",
    " ((2, 1), 'E'): -3.0,\n",
    " ((2, 1), 'N'): -3.0,\n",
    " ((2, 1), 'S'): -3.0,\n",
    " ((2, 2), 'W'): -4.0,\n",
    " ((2, 2), 'E'): -2.0,\n",
    " ((2, 2), 'N'): -4.0,\n",
    " ((2, 2), 'S'): -2.0,\n",
    " ((2, 3), 'W'): -3.0,\n",
    " ((2, 3), 'E'): -2.0,\n",
    " ((2, 3), 'N'): -3.0,\n",
    " ((2, 3), 'S'): -1.0,\n",
    " ((3, 0), 'W'): -4.0,\n",
    " ((3, 0), 'E'): -3.0,\n",
    " ((3, 0), 'N'): -3.0,\n",
    " ((3, 0), 'S'): -4.0,\n",
    " ((3, 1), 'W'): -4.0,\n",
    " ((3, 1), 'E'): -2.0,\n",
    " ((3, 1), 'N'): -4.0,\n",
    " ((3, 1), 'S'): -3.0,\n",
    " ((3, 2), 'W'): -3.0,\n",
    " ((3, 2), 'E'): -1.0,\n",
    " ((3, 2), 'N'): -3.0,\n",
    " ((3, 2), 'S'): -2.0,\n",
    " ((3, 3), 'W'): 0,\n",
    " ((3, 3), 'E'): 0,\n",
    " ((3, 3), 'N'): 0,\n",
    " ((3, 3), 'S'): 0}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0077a94c",
   "metadata": {},
   "source": [
    "# Set up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0cc1dd74",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    def __init__(self,grid_size):\n",
    "        self.grid_size = np.array(grid_size)\n",
    "        self.S = [np.array([i,j]) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "        self.Terminal_states = [(0,0),(self.grid_size[0]-1,self.grid_size[1]-1)]\n",
    "        self.A = [\"W\",\"E\",\"N\",\"S\"]\n",
    "        self.A_to_coord = {\"W\" : [0,-1],\"E\" : [0,1], \"N\" : [-1,0], \"S\" : [1,0]}\n",
    "        for k,v in self.A_to_coord.items():\n",
    "            self.A_to_coord[k] = np.array(v)\n",
    "        self.gamma = 1\n",
    "        self.R = -1\n",
    "    \n",
    "    def move(self,s,a):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        output\n",
    "        s_next : state after one_step transition\n",
    "        ex)\n",
    "        Input : s = [0,1],a = \"W\"\n",
    "        Output : s_next = [0,0]\n",
    "        \"\"\"\n",
    "        s_next = s + self.A_to_coord[a]\n",
    "        if s_next[0] < 0 or s_next[1] <0 or s_next[0] >= self.grid_size[0] or s_next[1] >= self.grid_size[1]:\n",
    "            return s # |S|를 넘어갈 경우, 원래 상태를 반환\n",
    "        else:\n",
    "            return s_next\n",
    "        \n",
    "    def move_test(self):\n",
    "        S = [np.array([i,j]) for i in [0,1,2,3] for j in [0,1,2,3]]\n",
    "        for s in S:\n",
    "            for a in self.A:\n",
    "                print(s,a,self.move(s,a))\n",
    "    def dynamics(self,s_prime,r,s,a):\n",
    "        r=-1\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        s_prime : all of the possible states after one-step transition\n",
    "        r : immediate reward(-1)\n",
    "        output\n",
    "        0 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 다를때\n",
    "        1 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 같을때\n",
    "        즉, a방향으로 움직이면 반드시 a방향으로 감(deterministic)\n",
    "        \"\"\"\n",
    "        s_next = self.move(s,a)\n",
    "        if np.sum(s_next != s_prime)>=1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    def dynamics_test(self,check_p = 1):\n",
    "        r=-1\n",
    "        for s in self.S:\n",
    "            for a in self.A:\n",
    "                for s_prime in self.S: #가능한 모든 next_state\n",
    "                    if self.dynamics(s_prime,r,s,a) == check_p:\n",
    "                        print(f\"state : {s} action : {a} s_prime : {s_prime} dynamics : {self.dynamics(s_prime,r,s,a)}\")\n",
    "    def sampling_action(self,s_t,π):\n",
    "        \"\"\"\n",
    "        Input current state s_t : tuple\n",
    "        ouput action : str (a ~ π(a|s))\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for a,p in π[s_t].items():\n",
    "            actions.append(a)\n",
    "            prob.append(p)\n",
    "        #print(prob)\n",
    "        return np.random.choice(a=actions,p=prob)\n",
    "    def generate_π(self,uniform=False):\n",
    "        \"\"\"\n",
    "        Input : NA\n",
    "        Output : {(x,y) : {\"W\" : pr1,\"E\" : pr2 ,...}}\n",
    "        ex)\n",
    "        {(0,0):{\"W\" : 0.1, \"E\" : 0.2, ...},(0,1):{\"W\":0.4,\"E\":0.5...}}\n",
    "        \"\"\"\n",
    "        π = {(i,j): {} for i in range(self.grid_size[0]) for j in range(self.grid_size[1])}\n",
    "        for t in π.values():\n",
    "            unnormalized_prob = np.random.rand(4)\n",
    "            if uniform == False:\n",
    "                prob = unnormalized_prob/np.sum(unnormalized_prob)\n",
    "            else:\n",
    "                prob = [0.25] * 4\n",
    "            for i in range(len(self.A)):\n",
    "                t[self.A[i]] = prob[i]\n",
    "        return π\n",
    "    def argmax_a_Q(self,Q,s):\n",
    "        max_action = \"W\"\n",
    "        max_value = -5000\n",
    "        for visit,Q_val in Q.items():\n",
    "            if visit[0] == s:\n",
    "                if Q_val > max_value:\n",
    "                    max_action = visit[1]\n",
    "                    max_value = Q_val                    \n",
    "        return max_action\n",
    "class Q_learning(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    \n",
    "    def control(self,s_0,iter_num,alpha,epsilon):\n",
    "        Q = {(tuple(s),a) : 0 for s in self.S for a in self.A}\n",
    "        π = self.generate_π()\n",
    "        γ = 1\n",
    "        for ep_num in range(iter_num):\n",
    "            ϵ = epsilon ** (ep_num + 1)\n",
    "            α = alpha ** (ep_num + 1)\n",
    "            if ep_num % 1000 == True:\n",
    "                print(f\"epsilon : {ϵ} alpha : {α}\")\n",
    "            s_t = s_0\n",
    "            while s_t not in self.Terminal_states:\n",
    "                a_t = self.sampling_action(s_t,π)\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                Q[(s_t,a_t)] = Q[(s_t,a_t)] + α * (r + γ * Q[(s_prime,self.argmax_a_Q(Q,s_prime))] - Q[(s_t,a_t)])\n",
    "                \n",
    "                a_star = self.argmax_a_Q(Q,tuple(s_t))\n",
    "                for (state,action),value in Q.items():\n",
    "                    if action == a_star:\n",
    "                        π[state][action] = 1 - ϵ + ϵ /len(self.A)\n",
    "                    else:\n",
    "                        π[state][action] = ϵ/len(self.A)\n",
    "                s_t = s_prime\n",
    "        return π,Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac6ad58d",
   "metadata": {},
   "source": [
    "# TD_Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f21789b",
   "metadata": {},
   "source": [
    "## strategy1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafdb914",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\alpha(s) = \\frac{1}{N(s)}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3bff4ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead time : 3.84700870513916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({(0, 0): 4991,\n",
       "  (0, 1): 8714,\n",
       "  (0, 2): 11013,\n",
       "  (0, 3): 10582,\n",
       "  (1, 0): 11445,\n",
       "  (1, 1): 15471,\n",
       "  (1, 2): 12949,\n",
       "  (1, 3): 10557,\n",
       "  (2, 0): 19396,\n",
       "  (2, 1): 27376,\n",
       "  (2, 2): 14850,\n",
       "  (2, 3): 8354,\n",
       "  (3, 0): 19121,\n",
       "  (3, 1): 19306,\n",
       "  (3, 2): 11468,\n",
       "  (3, 3): 5009},\n",
       " {(0, 0): 0,\n",
       "  (0, 1): -6.210219153348596,\n",
       "  (0, 2): -8.383241068493124,\n",
       "  (0, 3): -9.002126728634252,\n",
       "  (1, 0): -6.1390042000236225,\n",
       "  (1, 1): -7.7515192854294215,\n",
       "  (1, 2): -8.454972138492304,\n",
       "  (1, 3): -8.392769233108076,\n",
       "  (2, 0): -8.330045542123194,\n",
       "  (2, 1): -8.421539073360337,\n",
       "  (2, 2): -7.734287084877186,\n",
       "  (2, 3): -6.146458879225036,\n",
       "  (3, 0): -8.911924754087085,\n",
       "  (3, 1): -8.321791681471996,\n",
       "  (3, 2): -6.097562056472681,\n",
       "  (3, 3): 0})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TD(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    def prediction(self,s_0,π,iter_nums = 50):\n",
    "        t = time.time()\n",
    "        V = {tuple(s) : 0 for s in self.S}\n",
    "        N = {tuple(s) : 0 for s in self.S}\n",
    "        for ep_num in range(iter_nums):\n",
    "            N[s_0] += 1\n",
    "            s_t = s_0\n",
    "            while s_t not in self.Terminal_states:\n",
    "                a_t = self.sampling_action(s_t,π)\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                N[s_prime] += 1\n",
    "                V[s_t] = V[s_t] + (1/N[s_t]) * (r + 1*V[s_prime] - V[s_t])\n",
    "                s_t = s_prime\n",
    "        print(f\"lead time : {time.time()-t}\")\n",
    "        return N,V\n",
    "\n",
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "N,V = td.prediction(s_0 = (2,1),π = π,iter_nums = 10000)\n",
    "N,V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90dc95a9",
   "metadata": {},
   "source": [
    "- It doesn't converge to a true value.\n",
    "- I thought the number of iterations was too small, so I did some more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "caf6c3d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lead time : 376.06808495521545\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({(0, 0): 500675,\n",
       "  (0, 1): 857102,\n",
       "  (0, 2): 1071635,\n",
       "  (0, 3): 1068941,\n",
       "  (1, 0): 1141288,\n",
       "  (1, 1): 1497208,\n",
       "  (1, 2): 1284379,\n",
       "  (1, 3): 1069050,\n",
       "  (2, 0): 1930091,\n",
       "  (2, 1): 2714779,\n",
       "  (2, 2): 1498691,\n",
       "  (2, 3): 854888,\n",
       "  (3, 0): 1928620,\n",
       "  (3, 1): 1929364,\n",
       "  (3, 2): 1141312,\n",
       "  (3, 3): 499325},\n",
       " {(0, 0): 0,\n",
       "  (0, 1): -7.710187158147642,\n",
       "  (0, 2): -10.649747597920722,\n",
       "  (0, 3): -11.512033496895874,\n",
       "  (1, 0): -7.772392586046007,\n",
       "  (1, 1): -9.839184282402476,\n",
       "  (1, 2): -10.751129025128156,\n",
       "  (1, 3): -10.654188503693032,\n",
       "  (2, 0): -10.819929137588238,\n",
       "  (2, 1): -10.85383247342236,\n",
       "  (2, 2): -9.84822261146621,\n",
       "  (2, 3): -7.712727075121741,\n",
       "  (3, 0): -11.760013045162655,\n",
       "  (3, 1): -10.832300462530744,\n",
       "  (3, 2): -7.79454500536325,\n",
       "  (3, 3): 0})"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class TD(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    def prediction(self,s_0,π,iter_nums = 50):\n",
    "        t = time.time()\n",
    "        V = {tuple(s) : 0 for s in self.S}\n",
    "        N = {tuple(s) : 0 for s in self.S}\n",
    "        for ep_num in range(iter_nums):\n",
    "            N[s_0] += 1\n",
    "            s_t = s_0\n",
    "            while s_t not in self.Terminal_states:\n",
    "                a_t = self.sampling_action(s_t,π)\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                N[s_prime] += 1\n",
    "                V[s_t] = V[s_t] + (1/N[s_t]) * (r + 1*V[s_prime] - V[s_t])\n",
    "                s_t = s_prime\n",
    "        print(f\"lead time : {time.time()-t}\")\n",
    "        return N,V\n",
    "\n",
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "N,V = td.prediction(s_0 = (2,1),π = π,iter_nums = 1000000)\n",
    "N,V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a62ad9",
   "metadata": {},
   "source": [
    "- Increasing the iteration actually increased the difference with the true value function.\n",
    "- It seems wrong to modify $\\alpha$ based on the number of returns like MC does."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b4a42c",
   "metadata": {},
   "source": [
    "## strategy2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6112e1b9",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\alpha = k\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e47d213e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TD(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    def prediction(self,s_0,π,iter_nums,α=0.1):\n",
    "        V = {tuple(s) : 0 for s in self.S}\n",
    "        for ep_num in range(iter_nums):\n",
    "            s_t = s_0\n",
    "            while s_t not in self.Terminal_states:\n",
    "                a_t = self.sampling_action(s_t,π)\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                V[s_t] = V[s_t] + α * (r + 1*V[s_prime] - V[s_t])\n",
    "                s_t = s_prime\n",
    "        return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e099e360",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 1): -3.0256925965875716,\n",
       " (0, 2): -5.332214113492295,\n",
       " (0, 3): -15.617543977702914,\n",
       " (1, 0): -3.0629017222896167,\n",
       " (1, 1): -23.2984430617318,\n",
       " (1, 2): -18.28574411528579,\n",
       " (1, 3): -19.756820878783724,\n",
       " (2, 0): -22.470596366545834,\n",
       " (2, 1): -18.796632804070903,\n",
       " (2, 2): -19.36917079379382,\n",
       " (2, 3): -16.783646559420106,\n",
       " (3, 0): -8.414317785731221,\n",
       " (3, 1): -8.23094812772801,\n",
       " (3, 2): -1.7208495942458732,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "V = td.prediction(s_0 = (2,1),π = π,iter_nums = 10000,α = 0.9)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eab5bcd5",
   "metadata": {},
   "source": [
    "- It was still far from the true value, so I decided to reduce the alpha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "313f9769",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 1): -8.39849373243713,\n",
       " (0, 2): -21.498903370973046,\n",
       " (0, 3): -23.920539288117975,\n",
       " (1, 0): -18.9460598907016,\n",
       " (1, 1): -17.750880979198747,\n",
       " (1, 2): -20.324083266925584,\n",
       " (1, 3): -18.930470481543285,\n",
       " (2, 0): -21.57693034519246,\n",
       " (2, 1): -22.12603238307893,\n",
       " (2, 2): -20.463756775083247,\n",
       " (2, 3): -11.004329018730525,\n",
       " (3, 0): -23.981268316159696,\n",
       " (3, 1): -21.34414457966366,\n",
       " (3, 2): -10.81270597397484,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "V = td.prediction(s_0 = (2,1),π = π,iter_nums = 10000,α = 0.25)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e2a8b46",
   "metadata": {},
   "source": [
    "- It seemed to get a little closer to the true value at alpha=0.25.\n",
    "- So I wonder if it would converge if I increased the number of iterations here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "07c4992f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 1): -10.892909639267481,\n",
       " (0, 2): -16.389984823596123,\n",
       " (0, 3): -20.578998512300483,\n",
       " (1, 0): -10.115580086229546,\n",
       " (1, 1): -16.7374227182455,\n",
       " (1, 2): -19.376736534483427,\n",
       " (1, 3): -17.403295865603702,\n",
       " (2, 0): -17.706264918772543,\n",
       " (2, 1): -18.118900013790565,\n",
       " (2, 2): -19.26210200598588,\n",
       " (2, 3): -6.951457310762545,\n",
       " (3, 0): -20.307685831910902,\n",
       " (3, 1): -21.122118838034453,\n",
       " (3, 2): -15.332188651767787,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "V = td.prediction(s_0 = (2,1),π = π,iter_nums = 100000,α = 0.25)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49f5770",
   "metadata": {},
   "source": [
    "- It seemed to get a little more closer to the true value by increasing the number of iterations.\n",
    "- The intuition here is that by decreasing alpha more and more and increasing iterations, I can get closer to the true value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8cc3bdc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 1): -13.468481120602783,\n",
       " (0, 2): -18.04521926868933,\n",
       " (0, 3): -20.830446568464893,\n",
       " (1, 0): -14.162405826062944,\n",
       " (1, 1): -17.51880803992086,\n",
       " (1, 2): -18.99287999167345,\n",
       " (1, 3): -19.31588268251589,\n",
       " (2, 0): -19.68613223150354,\n",
       " (2, 1): -19.616839196046627,\n",
       " (2, 2): -17.409760812877337,\n",
       " (2, 3): -12.103952443141019,\n",
       " (3, 0): -21.66401720239512,\n",
       " (3, 1): -19.851963302190605,\n",
       " (3, 2): -14.369576703234577,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "V = td.prediction(s_0 = (2,1),π = π,iter_nums = 100000,α = 0.025)\n",
    "V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b2710c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 1): -14.03275753892002,\n",
       " (0, 2): -20.120670066469458,\n",
       " (0, 3): -22.087632181402803,\n",
       " (1, 0): -14.356100397943504,\n",
       " (1, 1): -18.06656086981896,\n",
       " (1, 2): -20.057873409291272,\n",
       " (1, 3): -19.933245308012694,\n",
       " (2, 0): -20.11923861351503,\n",
       " (2, 1): -20.10721474631147,\n",
       " (2, 2): -18.017930371514616,\n",
       " (2, 3): -13.836185455422145,\n",
       " (3, 0): -22.0836078738287,\n",
       " (3, 1): -20.00180431090046,\n",
       " (3, 2): -14.000978315795132,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "V = td.prediction(s_0 = (2,1),π = π,iter_nums = 100000,α = 0.001)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "729f415a",
   "metadata": {},
   "source": [
    "- It approximates the true value fairly well.\n",
    "- I wonder if increasing the number of iterations would increase the value? Is this really convergence?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cc7fd2e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{(0, 0): 0,\n",
       " (0, 1): -14.023947065291312,\n",
       " (0, 2): -19.88466015535378,\n",
       " (0, 3): -22.019204023526456,\n",
       " (1, 0): -13.997593366671204,\n",
       " (1, 1): -17.867270764828138,\n",
       " (1, 2): -19.89726566201253,\n",
       " (1, 3): -19.982618186721407,\n",
       " (2, 0): -19.907105325573927,\n",
       " (2, 1): -19.89049628015864,\n",
       " (2, 2): -17.926426396792216,\n",
       " (2, 3): -13.883623908772902,\n",
       " (3, 0): -21.86589991111874,\n",
       " (3, 1): -19.918957752488122,\n",
       " (3, 2): -14.041869061880227,\n",
       " (3, 3): 0}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "td = TD()\n",
    "π = td.generate_π(True)\n",
    "V = td.prediction(s_0 = (2,1),π = π,iter_nums = 500000,α = 0.001)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a29e6e",
   "metadata": {},
   "source": [
    "- It seems to have converged.\n",
    "- Insight\n",
    "    1. It was stated that decreasing $\\alpha$ will always converge to True. However, decreasing $\\frac{1}{N}$ did not converge to the true value. Perhaps we need to increase the decrease further.\n",
    "    2. The book said that if $\\alpha$ is a constant, it will converge to True on average. In my experiments, as long as $\\alpha$ is appropriate, 100% of them converged to the true value.\n",
    "    3. In TD, the step size alpha should be set small by default. I think it should be at most 0.01 and definitely less than that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693ad4b4",
   "metadata": {},
   "source": [
    "# SARSA(On-Policy TD Control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15c50dd1",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\pi(a|s) = \\begin{cases}\n",
    "1-\\epsilon + \\frac{\\epsilon}{|A(s)|}\\quad(A = \\text{greedy action})\\\\\n",
    "\\epsilon /|A(s)| \\quad\\quad\\quad\\quad (\\text{otherwise}) \n",
    "\\end{cases}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fd54a686",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "class Environment():\n",
    "    def __init__(self,grid_size):\n",
    "        self.grid_size = np.array(grid_size)\n",
    "        self.S = [np.array([i,j]) for i in range(grid_size[0]) for j in range(grid_size[1])]\n",
    "        self.Terminal_states = [(0,0),(self.grid_size[0]-1,self.grid_size[1]-1)]\n",
    "        self.A = [\"W\",\"E\",\"N\",\"S\"]\n",
    "        self.A_to_coord = {\"W\" : [0,-1],\"E\" : [0,1], \"N\" : [-1,0], \"S\" : [1,0]}\n",
    "        for k,v in self.A_to_coord.items():\n",
    "            self.A_to_coord[k] = np.array(v)\n",
    "        self.gamma = 1\n",
    "        self.R = -1\n",
    "    \n",
    "    def move(self,s,a):\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        output\n",
    "        s_next : state after one_step transition\n",
    "        ex)\n",
    "        Input : s = [0,1],a = \"W\"\n",
    "        Output : s_next = [0,0]\n",
    "        \"\"\"\n",
    "        s_next = s + self.A_to_coord[a]\n",
    "        if s_next[0] < 0 or s_next[1] <0 or s_next[0] >= self.grid_size[0] or s_next[1] >= self.grid_size[1]:\n",
    "            return s # |S|를 넘어갈 경우, 원래 상태를 반환\n",
    "        else:\n",
    "            return s_next\n",
    "        \n",
    "    def move_test(self):\n",
    "        S = [np.array([i,j]) for i in [0,1,2,3] for j in [0,1,2,3]]\n",
    "        for s in S:\n",
    "            for a in self.A:\n",
    "                print(s,a,self.move(s,a))\n",
    "    def dynamics(self,s_prime,r,s,a):\n",
    "        r=-1\n",
    "        \"\"\"\n",
    "        Input\n",
    "        s : current_state(position)\n",
    "        a : action(from deterministic policy π)\n",
    "        s_prime : all of the possible states after one-step transition\n",
    "        r : immediate reward(-1)\n",
    "        output\n",
    "        0 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 다를때\n",
    "        1 if s에서 a로 움직였을때의 변화된 상태 next_s와 s_prime의 input이 같을때\n",
    "        즉, a방향으로 움직이면 반드시 a방향으로 감(deterministic)\n",
    "        \"\"\"\n",
    "        s_next = self.move(s,a)\n",
    "        if np.sum(s_next != s_prime)>=1:\n",
    "            return 0\n",
    "        else:\n",
    "            return 1\n",
    "    def dynamics_test(self,check_p = 1):\n",
    "        r=-1\n",
    "        for s in self.S:\n",
    "            for a in self.A:\n",
    "                for s_prime in self.S: #가능한 모든 next_state\n",
    "                    if self.dynamics(s_prime,r,s,a) == check_p:\n",
    "                        print(f\"state : {s} action : {a} s_prime : {s_prime} dynamics : {self.dynamics(s_prime,r,s,a)}\")\n",
    "    def sampling_action(self,s_t,π):\n",
    "        \"\"\"\n",
    "        Input current state s_t : tuple\n",
    "        ouput action : str (a ~ π(a|s))\n",
    "        \"\"\"\n",
    "        actions = []\n",
    "        prob = []\n",
    "        for a,p in π[s_t].items():\n",
    "            actions.append(a)\n",
    "            prob.append(p)\n",
    "        #print(prob)\n",
    "        return np.random.choice(a=actions,p=prob)\n",
    "    def generate_π(self,uniform=False):\n",
    "        \"\"\"\n",
    "        Input : NA\n",
    "        Output : {(x,y) : {\"W\" : pr1,\"E\" : pr2 ,...}}\n",
    "        ex)\n",
    "        {(0,0):{\"W\" : 0.1, \"E\" : 0.2, ...},(0,1):{\"W\":0.4,\"E\":0.5...}}\n",
    "        \"\"\"\n",
    "        π = {(i,j): {} for i in range(self.grid_size[0]) for j in range(self.grid_size[1])}\n",
    "        for t in π.values():\n",
    "            unnormalized_prob = np.random.rand(4)\n",
    "            if uniform == False:\n",
    "                prob = unnormalized_prob/np.sum(unnormalized_prob)\n",
    "            else:\n",
    "                prob = [0.25] * 4\n",
    "            for i in range(len(self.A)):\n",
    "                t[self.A[i]] = prob[i]\n",
    "        return π\n",
    "    def argmax_a_Q(self,Q,s):\n",
    "        max_action = \"W\"\n",
    "        max_value = -5000\n",
    "        for visit,Q_val in Q.items():\n",
    "            if visit[0] == s:\n",
    "                if Q_val > max_value:\n",
    "                    max_action = visit[1]\n",
    "                    max_value = Q_val                    \n",
    "        return max_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e7b2f324",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    def control(self,s_0,iter_nums,α=0.9,ϵ = 0.9995):\n",
    "        Q = {(tuple(s),a):0 for s in self.S for a in self.A}\n",
    "        π = self.generate_π()\n",
    "        action_space_size = len(self.A)\n",
    "        \n",
    "        count=0\n",
    "        for ep_num in range(1,iter_nums):\n",
    "            s_t = s_0;a = self.sampling_action(s_0,π)\n",
    "            a_t = self.sampling_action(s_t,π)\n",
    "            epsilon = ϵ #실수한 부분\n",
    "            alpha = α #GLIE?\n",
    "            \n",
    "            if ep_num % 1000 == 0:\n",
    "                print(f\"episode : {ep_num}, epsilon : {epsilon}, alpha : {alpha}\")\n",
    "            \n",
    "            while s_t not in self.Terminal_states:\n",
    "                count+=1\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                #sampling action ~ ϵ-greedy action\n",
    "                #1. make ϵ-greedy policy\n",
    "                a_star = self.argmax_a_Q(Q,s_prime)\n",
    "                for action,prob in π[s_prime].items():\n",
    "                    if action == a_star:\n",
    "                        π[s_prime][action] = 1 - epsilon + epsilon/action_space_size\n",
    "                    else:\n",
    "                        π[s_prime][action] = epsilon/action_space_size\n",
    "                #2. sampling action\n",
    "                a_prime = self.sampling_action(s_prime,π)\n",
    "                Q[(s_t,a_t)] = Q[(s_t,a_t)] + alpha*(r + 1*Q[(s_prime,a_prime)] - Q[(s_t,a_t)])\n",
    "                #print(alpha*(r + 1*Q[(s_prime,a_prime)] - Q[(s_t,a_t)]))\n",
    "                s_t = s_prime;a_t = a_prime\n",
    "                count+=1\n",
    "        return Q,π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24822384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def difference(q_π,Q):\n",
    "    true_val = []\n",
    "    approx_val = []\n",
    "    for k,v in q_π.items():\n",
    "        true_val.append(v)\n",
    "    for k,v in Q.items():\n",
    "        approx_val.append(v)\n",
    "    return ((np.array(true_val) - np.array(approx_val))**2)**(1/2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "db669847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 1000, epsilon : 0.9, alpha : 0.9\n",
      "episode : 2000, epsilon : 0.9, alpha : 0.9\n",
      "episode : 3000, epsilon : 0.9, alpha : 0.9\n",
      "episode : 4000, epsilon : 0.9, alpha : 0.9\n",
      "episode : 5000, epsilon : 0.9, alpha : 0.9\n",
      "episode : 6000, epsilon : 0.9, alpha : 0.9\n",
      "episode : 7000, epsilon : 0.9, alpha : 0.9\n",
      "episode : 8000, epsilon : 0.9, alpha : 0.9\n",
      "episode : 9000, epsilon : 0.9, alpha : 0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "483.8118478775802"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa = SARSA()\n",
    "Q,π = sarsa.control(s_0 = (2,1),iter_nums = 10000,α = 0.9,ϵ = 0.9)\n",
    "rmse = difference(q_star,Q)\n",
    "np.sum(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "20ec51ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 1000, epsilon : 0.9, alpha : 0.5\n",
      "episode : 2000, epsilon : 0.9, alpha : 0.5\n",
      "episode : 3000, epsilon : 0.9, alpha : 0.5\n",
      "episode : 4000, epsilon : 0.9, alpha : 0.5\n",
      "episode : 5000, epsilon : 0.9, alpha : 0.5\n",
      "episode : 6000, epsilon : 0.9, alpha : 0.5\n",
      "episode : 7000, epsilon : 0.9, alpha : 0.5\n",
      "episode : 8000, epsilon : 0.9, alpha : 0.5\n",
      "episode : 9000, epsilon : 0.9, alpha : 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "556.6158724643351"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa = SARSA()\n",
    "Q,π = sarsa.control(s_0 = (2,1),iter_nums = 10000,α = 0.5,ϵ = 0.9)\n",
    "rmse = difference(q_star,Q)\n",
    "np.sum(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fae26fc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 1000, epsilon : 0.5, alpha : 0.9\n",
      "episode : 2000, epsilon : 0.5, alpha : 0.9\n",
      "episode : 3000, epsilon : 0.5, alpha : 0.9\n",
      "episode : 4000, epsilon : 0.5, alpha : 0.9\n",
      "episode : 5000, epsilon : 0.5, alpha : 0.9\n",
      "episode : 6000, epsilon : 0.5, alpha : 0.9\n",
      "episode : 7000, epsilon : 0.5, alpha : 0.9\n",
      "episode : 8000, epsilon : 0.5, alpha : 0.9\n",
      "episode : 9000, epsilon : 0.5, alpha : 0.9\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "200.9279392085909"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa = SARSA()\n",
    "Q,π = sarsa.control(s_0 = (2,1),iter_nums = 10000,α = 0.9,ϵ = 0.5)\n",
    "rmse = difference(q_star,Q)\n",
    "np.sum(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1eeb86bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 1000, epsilon : 0.5, alpha : 0.2\n",
      "episode : 2000, epsilon : 0.5, alpha : 0.2\n",
      "episode : 3000, epsilon : 0.5, alpha : 0.2\n",
      "episode : 4000, epsilon : 0.5, alpha : 0.2\n",
      "episode : 5000, epsilon : 0.5, alpha : 0.2\n",
      "episode : 6000, epsilon : 0.5, alpha : 0.2\n",
      "episode : 7000, epsilon : 0.5, alpha : 0.2\n",
      "episode : 8000, epsilon : 0.5, alpha : 0.2\n",
      "episode : 9000, epsilon : 0.5, alpha : 0.2\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "125.00452426936465"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa = SARSA()\n",
    "Q,π = sarsa.control(s_0 = (2,1),iter_nums = 10000,α = 0.2,ϵ = 0.5)\n",
    "rmse = difference(q_star,Q)\n",
    "np.sum(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f6071c68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 1000, epsilon : 0.2, alpha : 0.5\n",
      "episode : 2000, epsilon : 0.2, alpha : 0.5\n",
      "episode : 3000, epsilon : 0.2, alpha : 0.5\n",
      "episode : 4000, epsilon : 0.2, alpha : 0.5\n",
      "episode : 5000, epsilon : 0.2, alpha : 0.5\n",
      "episode : 6000, epsilon : 0.2, alpha : 0.5\n",
      "episode : 7000, epsilon : 0.2, alpha : 0.5\n",
      "episode : 8000, epsilon : 0.2, alpha : 0.5\n",
      "episode : 9000, epsilon : 0.2, alpha : 0.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "53.490927304402234"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa = SARSA()\n",
    "Q,π = sarsa.control(s_0 = (2,1),iter_nums = 10000,α = 0.5,ϵ = 0.2)\n",
    "rmse = difference(q_star,Q)\n",
    "np.sum(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14756d28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 1000, epsilon : 0.2, alpha : 0.01\n",
      "episode : 2000, epsilon : 0.2, alpha : 0.01\n",
      "episode : 3000, epsilon : 0.2, alpha : 0.01\n",
      "episode : 4000, epsilon : 0.2, alpha : 0.01\n",
      "episode : 5000, epsilon : 0.2, alpha : 0.01\n",
      "episode : 6000, epsilon : 0.2, alpha : 0.01\n",
      "episode : 7000, epsilon : 0.2, alpha : 0.01\n",
      "episode : 8000, epsilon : 0.2, alpha : 0.01\n",
      "episode : 9000, epsilon : 0.2, alpha : 0.01\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "36.054180468348434"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa = SARSA()\n",
    "Q,π = sarsa.control(s_0 = (2,1),iter_nums = 10000,α = 0.01,ϵ = 0.2)\n",
    "rmse = difference(q_star,Q)\n",
    "np.sum(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba1b064b",
   "metadata": {},
   "source": [
    "- Here I remembered that there are some conditions for the salsa algorithm to converge.\n",
    "- GLIE, Robbins-Monro sequence of step-size $\\alpha_t$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88ae11e",
   "metadata": {},
   "source": [
    "**(GLIE)**\n",
    "\\begin{enumerate}\n",
    "    \\item $\\text{lim}_{k\\to\\infty}N_k(s,a) = \\infty $\n",
    "    \\item $\\text{lim}_{k\\to\\infty}\\pi_k(a|s) = \\boldsymbol{1}(a = \\text{argmax}_{a'\\in \\mathcal{A}}Q_k(s,a'))$\n",
    "\\end{enumerate}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96e73a56",
   "metadata": {},
   "source": [
    "**(Robbins-Monro)**\n",
    "\\begin{enumerate}\n",
    "            \\item $\\sum_{t=1}^{\\infty}\\alpha_t = \\infty $\n",
    "            \\item $\\sum_{t=1}^{\\infty}\\alpha_t^2 $\n",
    "\\end{enumerate}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d803e426",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SARSA(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    def control(self,s_0,iter_nums,α=0.9,ϵ = 0.9995):\n",
    "        Q = {(tuple(s),a):0 for s in self.S for a in self.A}\n",
    "        π = self.generate_π()\n",
    "        action_space_size = len(self.A)\n",
    "        \n",
    "        count=0\n",
    "        for ep_num in range(1,iter_nums):\n",
    "            s_t = s_0;a = self.sampling_action(s_0,π)\n",
    "            a_t = self.sampling_action(s_t,π)\n",
    "            epsilon = ϵ ** (ep_num + 1) #실수한 부분\n",
    "            alpha = α**(ep_num+1) #GLIE?\n",
    "            \n",
    "            if ep_num % (iter_nums / 10) == 0:\n",
    "                print(f\"episode : {ep_num}, epsilon : {epsilon}, alpha : {alpha}\")\n",
    "            \n",
    "            while s_t not in self.Terminal_states:\n",
    "                count+=1\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                #sampling action ~ ϵ-greedy action\n",
    "                #1. make ϵ-greedy policy\n",
    "                a_star = self.argmax_a_Q(Q,s_prime)\n",
    "                for action,prob in π[s_prime].items():\n",
    "                    if action == a_star:\n",
    "                        π[s_prime][action] = 1 - epsilon + epsilon/action_space_size\n",
    "                    else:\n",
    "                        π[s_prime][action] = epsilon/action_space_size\n",
    "                #2. sampling action\n",
    "                a_prime = self.sampling_action(s_prime,π)\n",
    "                Q[(s_t,a_t)] = Q[(s_t,a_t)] + alpha*(r + 1*Q[(s_prime,a_prime)] - Q[(s_t,a_t)])\n",
    "                #print(alpha*(r + 1*Q[(s_prime,a_prime)] - Q[(s_t,a_t)]))\n",
    "                s_t = s_prime;a_t = a_prime\n",
    "                count+=1\n",
    "        return Q,π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c6761523",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 1000, epsilon : 0.9047424102692003, alpha : 0.3977406691050303\n",
      "episode : 2000, epsilon : 0.818640693009023, alpha : 0.1583434123914593\n",
      "episode : 3000, epsilon : 0.740733027040136, alpha : 0.06303764788294978\n",
      "episode : 4000, epsilon : 0.6702396082111145, alpha : 0.025095739637028935\n",
      "episode : 5000, epsilon : 0.6064548440752158, alpha : 0.009990793899844896\n",
      "episode : 6000, epsilon : 0.5487402913771806, alpha : 0.003977406691050414\n",
      "episode : 7000, epsilon : 0.49651826565897955, alpha : 0.0015834341239146371\n",
      "episode : 8000, epsilon : 0.4492660590208904, alpha : 0.0006303764788295154\n",
      "episode : 9000, epsilon : 0.40651070816152135, alpha : 0.00025095739637029634\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "248.8158378933288"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa = SARSA()\n",
    "Q,π = sarsa.control(s_0 = (2,1),iter_nums = 10000,α = np.exp(np.log(0.0001)/10000),ϵ = 0.9999)\n",
    "rmse = difference(q_star,Q)\n",
    "np.sum(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23790956",
   "metadata": {},
   "source": [
    "- Once again, I see that the SARSA algorithm will not converge if the GLIE condition is not met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5deb0289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 1000, epsilon : 0.36732772934619273, alpha : 0.9119268439216149\n",
      "episode : 2000, epsilon : 0.13506472547210194, alpha : 0.8316871663561808\n",
      "episode : 3000, epsilon : 0.049662681604038125, alpha : 0.7585077106700777\n",
      "episode : 4000, epsilon : 0.018260740807661924, alpha : 0.6917672538661829\n",
      "episode : 5000, epsilon : 0.006714390847905722, alpha : 0.6308992338374627\n",
      "episode : 6000, epsilon : 0.002468850794898966, alpha : 0.5753869398011344\n",
      "episode : 7000, epsilon : 0.0009077851417265067, alpha : 0.5247591259224881\n",
      "episode : 8000, epsilon : 0.0003337884432878974, alpha : 0.4785860108922666\n",
      "episode : 9000, epsilon : 0.00012273248343838214, alpha : 0.43647562949787516\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "138.7147404680331"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa = SARSA()\n",
    "Q,π = sarsa.control(s_0 = (2,1),iter_nums = 10000,α = np.exp(np.log(0.0001)/100000),ϵ = 0.999)\n",
    "rmse = difference(q_star,Q)\n",
    "np.sum(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae55d03",
   "metadata": {},
   "source": [
    "- Once again, I see that the SARSA algorithm will not converge if the Robbins-Monro condition is not met."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed7ba7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 10000, epsilon : 0.36782426032832716, alpha : 0.3980705052167768\n",
      "episode : 20000, epsilon : 0.13530821730781176, alpha : 0.15847472251273403\n",
      "episode : 30000, epsilon : 0.049774622409831396, alpha : 0.06308992338382775\n",
      "episode : 40000, epsilon : 0.01831014468548656, alpha : 0.025116550888785564\n",
      "episode : 50000, epsilon : 0.006735588984342178, alpha : 0.009999079008402512\n",
      "episode : 60000, epsilon : 0.0024777608121225024, alpha : 0.003980705052178049\n",
      "episode : 70000, epsilon : 0.0009114716851579903, alpha : 0.0015847472251314335\n",
      "episode : 80000, epsilon : 0.00033529492789624127, alpha : 0.000630899233839907\n",
      "episode : 90000, epsilon : 0.0001233419430395787, alpha : 0.0002511655088885044\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "23.817640205236938"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa = SARSA()\n",
    "Q,π = sarsa.control(s_0 = (2,1),iter_nums = 100000,α = np.exp(np.log(0.0001)/100000),ϵ = 0.9999)\n",
    "rmse = difference(q_star,Q)\n",
    "np.sum(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44880409",
   "metadata": {},
   "source": [
    "- Here, I see that the SARSA algorithm gradually converges to the GLIE and Robbins-monro conditions.\n",
    "- I've also seen that by increasing the number of iterations here, I can approach the optimal action value function and policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "08dca784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode : 100000, epsilon : 0.3678739229922288, alpha : 0.3981035038657425\n",
      "episode : 200000, epsilon : 0.13533257654345773, alpha : 0.15848785951058994\n",
      "episode : 300000, epsilon : 0.04978582369992489, alpha : 0.06309515331650908\n",
      "episode : 400000, epsilon : 0.018315089424785096, alpha : 0.025118632962342347\n",
      "episode : 500000, epsilon : 0.006737711173760915, alpha : 0.009999907896757296\n",
      "episode : 600000, epsilon : 0.0024786530280102824, alpha : 0.00398103503855267\n",
      "episode : 700000, epsilon : 0.0009118409315600251, alpha : 0.0015848785950641953\n",
      "episode : 800000, epsilon : 0.0003354458550964258, alpha : 0.0006309515331484882\n",
      "episode : 900000, epsilon : 0.00012340301669597187, alpha : 0.0002511863296168138\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "8.850716178450071"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sarsa = SARSA()\n",
    "Q,π = sarsa.control(s_0 = (2,1),iter_nums = 1000000,α = np.exp(np.log(0.0001)/1000000),ϵ = 0.99999)\n",
    "rmse = difference(q_star,Q)\n",
    "np.sum(rmse)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3041e8b",
   "metadata": {},
   "source": [
    "- It seems to have converged.\n",
    "- Insight\n",
    "    1. The SARSA algorithm converges only when the GLIE and Robbins-monro conditions are satisfied.\n",
    "    2. It is very slow to converge."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a42aa6",
   "metadata": {},
   "source": [
    "# Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "63c7dcb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learning(Environment):\n",
    "    def __init__(self,grid_size=(4,4)):\n",
    "        super().__init__(grid_size)\n",
    "    \n",
    "    def control(self,s_0,iter_num,alpha,epsilon):\n",
    "        Q = {(tuple(s),a) : 0 for s in self.S for a in self.A}\n",
    "        π = self.generate_π()\n",
    "        γ = 1\n",
    "        for ep_num in range(iter_num):\n",
    "            ϵ = epsilon ** (ep_num + 1)\n",
    "            α = alpha ** (ep_num + 1)\n",
    "            if ep_num % (iter_num / 10) == True:\n",
    "                print(f\"epsilon : {ϵ} alpha : {α}\")\n",
    "            s_t = s_0\n",
    "            while s_t not in self.Terminal_states:\n",
    "                a_t = self.sampling_action(s_t,π)\n",
    "                r,s_prime = -1,tuple(self.move(s_t,a_t))\n",
    "                Q[(s_t,a_t)] = Q[(s_t,a_t)] + α * (r + γ * Q[(s_prime,self.argmax_a_Q(Q,s_prime))] - Q[(s_t,a_t)])\n",
    "                \n",
    "                a_star = self.argmax_a_Q(Q,tuple(s_t))\n",
    "                for (state,action),value in Q.items():\n",
    "                    if action == a_star:\n",
    "                        π[state][action] = 1 - ϵ + ϵ /len(self.A)\n",
    "                    else:\n",
    "                        π[state][action] = ϵ/len(self.A)\n",
    "                s_t = s_prime\n",
    "        return π,Q\n",
    "                \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ae8199eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon : 0.990025 alpha : 0.9862794856312106\n",
      "epsilon : 0.599727876386733 alpha : 0.4943106869868377\n",
      "epsilon : 0.36329741745444855 alpha : 0.24774220576333084\n",
      "epsilon : 0.2200748351473355 alpha : 0.12416523075924281\n",
      "epsilon : 0.13331482894782642 alpha : 0.062230028516917085\n",
      "epsilon : 0.08075818212241154 alpha : 0.031188895840940083\n",
      "epsilon : 0.04892091923449094 alpha : 0.015631476426409968\n",
      "epsilon : 0.029634846598205228 alpha : 0.007834296427662368\n",
      "epsilon : 0.017951913959130546 alpha : 0.003926449353996142\n",
      "epsilon : 0.010874738754866503 alpha : 0.001967886289706926\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.06320977473202194"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_l = Q_learning()\n",
    "π,Q= q_l.control(s_0 = (2,1),iter_num = 1000,alpha = np.exp(np.log(0.001)/1000),epsilon = 0.995)\n",
    "rmse = difference(q_star,Q)\n",
    "np.sum(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a0d5f005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epsilon : 0.9990002500000001 alpha : 0.9986194028465246\n",
      "epsilon : 0.6058485196309606 alpha : 0.5004952959591761\n",
      "epsilon : 0.36741975664072807 alpha : 0.25084185282524624\n",
      "epsilon : 0.22282348342150354 alpha : 0.1257187342954265\n",
      "epsilon : 0.13513237616300078 alpha : 0.06300862465664786\n",
      "epsilon : 0.0819516812458937 alpha : 0.031579118286324974\n",
      "epsilon : 0.0496999923314264 alpha : 0.015827050934311866\n",
      "epsilon : 0.030140800044509277 alpha : 0.007932315874245822\n",
      "epsilon : 0.0182790335512516 alpha : 0.0039755754492710235\n",
      "epsilon : 0.011085408054012442 alpha : 0.0019925076614966753\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_l = Q_learning()\n",
    "π,Q= q_l.control(s_0 = (2,1),iter_num = 10000,alpha = np.exp(np.log(0.001)/10000),epsilon = 0.9995)\n",
    "rmse = difference(q_star,Q)\n",
    "np.sum(rmse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e4ef2797",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{((0, 0), 'W'): 0,\n",
       " ((0, 0), 'E'): 0,\n",
       " ((0, 0), 'N'): 0,\n",
       " ((0, 0), 'S'): 0,\n",
       " ((0, 1), 'W'): -1.0,\n",
       " ((0, 1), 'E'): -3.0,\n",
       " ((0, 1), 'N'): -2.0,\n",
       " ((0, 1), 'S'): -3.0,\n",
       " ((0, 2), 'W'): -2.0,\n",
       " ((0, 2), 'E'): -4.0,\n",
       " ((0, 2), 'N'): -3.0,\n",
       " ((0, 2), 'S'): -4.0,\n",
       " ((0, 3), 'W'): -3.0,\n",
       " ((0, 3), 'E'): -4.0,\n",
       " ((0, 3), 'N'): -4.0,\n",
       " ((0, 3), 'S'): -3.0,\n",
       " ((1, 0), 'W'): -2.0,\n",
       " ((1, 0), 'E'): -3.0,\n",
       " ((1, 0), 'N'): -1.0,\n",
       " ((1, 0), 'S'): -3.0,\n",
       " ((1, 1), 'W'): -2.0,\n",
       " ((1, 1), 'E'): -4.0,\n",
       " ((1, 1), 'N'): -2.0,\n",
       " ((1, 1), 'S'): -4.0,\n",
       " ((1, 2), 'W'): -3.0,\n",
       " ((1, 2), 'E'): -3.0,\n",
       " ((1, 2), 'N'): -3.0,\n",
       " ((1, 2), 'S'): -3.0,\n",
       " ((1, 3), 'W'): -4.0,\n",
       " ((1, 3), 'E'): -3.0,\n",
       " ((1, 3), 'N'): -4.0,\n",
       " ((1, 3), 'S'): -2.0,\n",
       " ((2, 0), 'W'): -3.0,\n",
       " ((2, 0), 'E'): -4.0,\n",
       " ((2, 0), 'N'): -2.0,\n",
       " ((2, 0), 'S'): -4.0,\n",
       " ((2, 1), 'W'): -3.0,\n",
       " ((2, 1), 'E'): -3.0,\n",
       " ((2, 1), 'N'): -3.0,\n",
       " ((2, 1), 'S'): -3.0,\n",
       " ((2, 2), 'W'): -4.0,\n",
       " ((2, 2), 'E'): -2.0,\n",
       " ((2, 2), 'N'): -4.0,\n",
       " ((2, 2), 'S'): -2.0,\n",
       " ((2, 3), 'W'): -3.0,\n",
       " ((2, 3), 'E'): -2.0,\n",
       " ((2, 3), 'N'): -3.0,\n",
       " ((2, 3), 'S'): -1.0,\n",
       " ((3, 0), 'W'): -4.0,\n",
       " ((3, 0), 'E'): -3.0,\n",
       " ((3, 0), 'N'): -3.0,\n",
       " ((3, 0), 'S'): -4.0,\n",
       " ((3, 1), 'W'): -4.0,\n",
       " ((3, 1), 'E'): -2.0,\n",
       " ((3, 1), 'N'): -4.0,\n",
       " ((3, 1), 'S'): -3.0,\n",
       " ((3, 2), 'W'): -3.0,\n",
       " ((3, 2), 'E'): -1.0,\n",
       " ((3, 2), 'N'): -3.0,\n",
       " ((3, 2), 'S'): -2.0,\n",
       " ((3, 3), 'W'): 0,\n",
       " ((3, 3), 'E'): 0,\n",
       " ((3, 3), 'N'): 0,\n",
       " ((3, 3), 'S'): 0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aaaec7",
   "metadata": {},
   "source": [
    "- It seems to have converged very quickly and accurately!\n",
    "- Insight\n",
    "   - Q-learning converges much faster than the SARSA algorithm.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
