{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[강화학습] 2-2 Reward & Return & State Value f & Action Value f\"\n",
    "categories: Reinforcement Learning\n",
    "format: html\n",
    "author: 신호연\n",
    "date: 2023-01-21\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\"> **Reward**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "리워드의 정의는 다음과 같다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\mathcal{R}_{t+1} \\overset{\\Delta}{=} \\mathbb{E}({R}_{t+1}|S_t=s,A_t=a)$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이는 $s_t$에서 $a_t$를 했을때 **t+1에서 얻는 값을 나타내는 확률변수$R_{t+1}$의 기댓값이다.**이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알파고를 예를 들어서 생각해보자. 알파고가 바둑판에($s_t$)에 검은돌을 놓으면($a_{t}$) 상대하는 사람(또는기계)도 어떤 위치에 흰돌을 놓을것이다. 이 흰돌의 위치는 random이기 때문에 따라서 알파고가 확률변수 $R_{t+1}$이 존재하며 그것의 평균값을 리워드$\\mathcal{R}_{t+1}$로 정의한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "생각해보면 리워드는 뭔가 $s_t$에서 $a_t$를 해서 변하는 상황 $s_{t+1}$에 부여되는게 맞을 것 같다.찾아보니 위키피디아에는 reward를 $R_a(s,s')$로 쓴다. 변하는 상황에 따라 부여되는것도 맞고 어떤 액션을 취하면 그것에 상응한다고 봐도 무방할 것 같다.(개인적인 의견입니다.)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그냥 $R_{t+1}$을 리워드라 하는 경우도 많은 것 같다"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "위에서 인식하는 상황이라고 썼는데 사실 state는 agent가 인식하는 것이 아니라 사실은 environment에가 반환(return)하는 것입니다. agent는 state중 일부를 받는데 이것을 observation이라고 합니다. 그러나 실제 논문에서는 딱히 state와 observation을 구별하지않고 쓰는 경우가 많다고 합니다.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\"> **Return**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "&\\text{definition of return } G_t\\\\\n",
    "&G_t \\overset{\\Delta}{=} R_t + \\gamma R_{t+1} + \\gamma R_{t+2} + \\dots\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return($G_t$)은 **현재에서 부터 시작하여 앞으로 받게될 미래의(discounted) reward들의 total sum**이다. 강화학습의 목적은 **return의 평균(기댓값)을 가장 크게 만드는 policy들을 찾는** 것이다. 조금 풀어쓰자면 강화학습은 **agent의 지금 당장의 reward와 미래의 reward를 염두하여 취해야할 action에 대한 policy을 학습**하는 것이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\">  **Discount factor**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\\text{discount factor  } \\gamma \\in [0,1]$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "discount factor($\\gamma$)는 return에서 reward곱해지는 값이다. 이 값이 **작을수록 지금 당장리워드를 받는것에 집중하고(근시안적인 사고) 이 값이 클수록 미래에 받는 리워드를 중요하게 생각(미래지향적)**한다. 또한 효율적인 path결정을 위해서도 중요하다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 크기가 무한히 커질때 크기비교를 못하므로 수학적으로 문제가 된다고 한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color :black\">**Expected value of return**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color :black\">**(state) value function**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습의 목적은 **return의 평균을 가장크게 하는 policy를 찾는것**이다. 그렇다면 어떠한 policy가 return의 평균을 가장 크게 만들 수 있을까? return에 대한 두가지의 함수는 이러한 목적 **즉, return을 가장 크게 만들기 위해 policy가 좋은지 나쁜지 그것의 가치를 평가**한다.(그래서 value function이라 한다.) \n",
    "\n",
    "state value function은 **함수의 입력으로 주어지는 어떤 특정한 state를 가졍했을때 특정 policy가 좋은지 나쁜지 그것의 가치를 평가**한다. 평가는 **현재state부터 마지막 시점까지 Agent가 가능한 모든 action과 놓여질 수 있는 모든 state를 고려하여 기대되는 보수의 총합(return)을 계산**하는 방식이다. 이는 상태가 주어졌을때 함수$G_t$의 조건부 기댓값을 구하는 것과 같다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "&\\text{definition of state value function}\\\\\n",
    "&V^{\\pi}(s_t) \\overset{\\Delta}{=} \\mathbb{E}[G_t|S_t=s_t,\\pi]= \\int_{a_t:a_{\\infty}}G_t\\,p(a_t,s_{t+1},\\dots,a_{\\infty}|s_t)da_t:a_{\\infty}\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "윗식은 $G_t$를 확률분포와 곱해서  $a_t,s_t,a_{t+1},s_{t+1}\\dots a_{\\infty}$에 대해 모두 적분함을 의미한다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color :black\">**optimal policy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal policy는 value function인 $V(s_t)$를 가장 크게 하는 policy들($p(a_t|s_t),p(a_{t+1}|s_{t+1}),\\dots,p(a_{\\infty}|s_{\\infty})$)이다.나중에 더 자세히 공부해야할 것 같다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color :black\">**action value function**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action value function은 state,action의 **함수의 입력으로 주어지는 어떤 state에서 (마찬가지로 주어지는)action을 취했을때 특정 policy가 좋은지 나쁜지(가치)를 평가**한다. 평가는 **action을 취한뒤의 다음 state부터 마지막 시점까지 Agent가 가능한 모든 action과 놓여질 수 있는 모든 state를 고려하여 기대되는 보수의 총합을 계산하는**방식이다. 이것도 마찬가지로 결국은 $G_t$에 대한 조건부 함수이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "Q^{\\pi}(s_t,a_t) \\underset{=}{\\Delta} \\mathbb{E}[G_t|A_t=a_t,S_t=s_t,\\pi] = \\int_{s_{t+1}:a_{\\infty}}G_tp(s_{t+1},\\dots,a_{\\infty})ds_{t+1}:da_{\\infty}\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color :black\">**state value function**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\"> **참고자료**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[jdk.log](https://velog.io/@kjb0531/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B0%9C%EB%85%90%EC%A0%95%EB%A6%AC1)<br>\n",
    "[나무위키 - 강화학습 용어정리](https://namu.wiki/w/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/%EC%9A%A9%EC%96%B4)<br>\n",
    "[stackexchange](https://ai.stackexchange.com/questions/10442/how-are-the-reward-functions-rs-rs-a-and-rs-a-s-equivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.925"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-1 + (1/4)*(-1.7) + (1/4) * (-2) *3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0478b8cb1c47bafb71305148a49d30528a4d9c22ca2de336c01aa5a8230a459a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
