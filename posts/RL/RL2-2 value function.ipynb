{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[강화학습] 2-2 Reward & Return & State Value f & Action Value f\"\n",
    "categories: Reinforcement Learning\n",
    "format: html\n",
    "author: 신호연\n",
    "date: 2023-01-21\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\"> **Reward**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(immediate) reward는  $R_a(s_t,s_{t+1})$로 표기하며 이는 $s_t$에서 action을 취하여 $s_{t+1}$로 **상태가 바뀌었을때의 얻는 값을 나타내는 확률변수**이다. 확률변수인 이유는 모든 시간에서 state와 action은 랜덤이기 때문이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알파고를 예를 들어서 생각해보자. 알파고가 바둑판에($s_t$)에 검은돌을 놓으면($a_{t}$) 상대하는 사람(또는기계)도 어떤 위치에 흰돌을 놓을것이다. 이 흰돌의 위치도 envirionment에 의해 랜덤적으로 결정되 때문에 알파고가 인식하는 다음상황($s_{t+1}$)도 랜덤적인 확률변수이며 reward도 확률변수이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "위에서 인식하는 상황이라고 썼는데 사실 state는 agent가 인식하는 것이 아니라 사실은 environment에가 반환(return)하는 것입니다. agent는 state중 일부를 받는데 이것을 observation이라고 합니다. 그러나 실제 논문에서는 딱히 state와 observation을 구별하지않고 쓰는 경우가 많다고 합니다.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\"> **Return**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "&\\text{definition of return } G_t\\\\\n",
    "&G_t \\overset{\\Delta}{=} R_t + \\gamma R_{t+1} + \\gamma R_{t+2} + \\dots\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return($G_t$)은 현재의 reward와 앞으로 미래에 받게될  **(discounted) reward들의 total sum**이다. 강화학습의 목적은 **return의 평균(기댓값)을 가장 크게 만드는 policy들을 찾는** 것이다. 조금 풀어쓰자면 강화학습의 목적은 **agent의 action은 지금과 당장얻을 보상과 미래의 보상도 염두하여 취해야할 행동에 대한 정책을 학습**하는 것이다\n",
    "\n",
    "discounted reward는 discount factor($\\gamma$)가 곱해진 리워드이며 이 값이 **작을수록 지금 당장받는 리워드 중요시하며 이 값이 클수록 미래에 받는 리워드를 중요시**한다. 또한 효율적인 path결정을 위해서도 중요하다.(Q-learning예시로 설명해야 함)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color :black\">**Expected value of return**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습의 목적은 **return의 평균을 가장크게 하는 policy를 찾는것**이다. 그렇다면 어떠한 policy가 return의 평균을 가장 크게 만들 수 있을까? return에 대한 두가지의 함수는 이러한 목적 **즉, return을 가장 크게 만들기 위해 policy가 좋은지 나쁜지 그것의 가치를 평가**한다.(그래서 value function이라 한다.) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color :black\">**(state) value function**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "state value function은 **함수의 입력으로 주어지는 어떤 특정한 state를 가졍했을때 특정 policy가 좋은지 나쁜지 그것의 가치를 평가**한다. 평가는 **현재state부터 마지막 시점까지 Agent가 가능한 모든 action과 놓여질 수 있는 모든 state를 고려하여 기대되는 보수의 총합(return)을 계산**하는 방식이다. 이는 상태가 주어졌을때 함수$G_t$의 조건부 기댓값을 구하는 것과 같다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "&\\text{definition of state value function}\\\\\n",
    "&V^{\\pi}(s_t) \\overset{\\Delta}{=} \\mathbb{E}[G_t|S_t=s_t,\\pi]= \\int_{a_t:a_{\\infty}}G_t\\,p(a_t,s_{t+1},\\dots,a_{\\infty}|s_t)da_t:a_{\\infty}\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "윗식은 $G_t$를 확률분포와 곱해서  $a_t,s_t,a_{t+1},s_{t+1}\\dots a_{\\infty}$에 대해 모두 적분함을 의미한다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color :black\">**optimal policy**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "optimal policy는 value function인 $V(s_t)$를 가장 크게 하는 policy들($p(a_t|s_t),p(a_{t+1}|s_{t+1}),\\dots,p(a_{\\infty}|s_{\\infty})$)이다.나중에 더 자세히 공부해야할 것 같다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color :black\">**action value function**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "action value function은 state,action의 **함수의 입력으로 주어지는 어떤 state에서 (마찬가지로 주어지는)action을 취했을때 특정 policy가 좋은지 나쁜지(가치)를 평가**한다. 평가는 **action을 취한뒤의 다음 state부터 마지막 시점까지 Agent가 가능한 모든 action과 놓여질 수 있는 모든 state를 고려하여 기대되는 보수의 총합을 계산하는**방식이다. 이것도 마찬가지로 결국은 $G_t$에 대한 조건부 함수이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "Q^{\\pi}(s_t,a_t) \\underset{=}{\\Delta} \\mathbb{E}[G_t|A_t=a_t,S_t=s_t,\\pi] = \\int_{s_{t+1}:a_{\\infty}}G_tp(s_{t+1},\\dots,a_{\\infty})ds_{t+1}:da_{\\infty}\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color :black\">**state value function**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\"> **참고자료**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[jdk.log](https://velog.io/@kjb0531/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B0%9C%EB%85%90%EC%A0%95%EB%A6%AC1)<br>\n",
    "[나무위키 - 강화학습 용어정리](https://namu.wiki/w/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/%EC%9A%A9%EC%96%B4)<br>\n",
    "[stackexchange](https://ai.stackexchange.com/questions/10442/how-are-the-reward-functions-rs-rs-a-and-rs-a-s-equivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0478b8cb1c47bafb71305148a49d30528a4d9c22ca2de336c01aa5a8230a459a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
