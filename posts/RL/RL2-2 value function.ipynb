{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[강화학습] 2-2 Reward & Return & State Value f & Action Value f & Optimal Policy\"\n",
    "categories: Reinforcement Learning\n",
    "format: html\n",
    "author: 신호연\n",
    "date: 2023-01-21\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "지난 포스팅에서는 MDP와 policy에 대해서 공부했다. 또한 사각형 그리드 예제에서 optimal policy가 뭔지 대충 살펴봤었다. 이번 포스팅에서는 강화학습의 reward의 정의와 궁극적인 목적과 연관된 G_t와 state value function,action value function,optimal policy를 공부한다. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\"> **Reward**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(immediate) reward는  $R_a(s_t,s_{t+1})$로 표기하며 이는 $s_t$에서 action을 취하여 $s_{t+1}$로 상태가 바뀌었을때의 확률변수이다. 이것이 확률변수인 이유는 모든 시간에서 state와 action은 랜덤이기 때문이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "알파고를 예를 들어서 생각해보자. 알파고가 바둑판에($s_t$)에 검은돌을 놓으면($a_{t}$) 상대도 어떤 위치에 흰돌을 놓을것이다. 이 흰돌의 위치도 stochastic한 환경에 의해 랜덤적으로 결정되기 때문에 알파고가 다시 인식하는 상황($s_{t+1}$)도 확률변수이며 어떤 확률분포를 따른다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-note}\n",
    "위에서 인식하는 상황이라고 썼는데 사실 state는 agent가 인식하는 것이 아니라 사실은 environment에가 반환(return)하는 것입니다. agent는 state중 일부를 받는데 이것을 observation이라고 합니다. 그러나 실제 논문에서는 딱히 state와 observation을 구별하지않고 쓰는 경우가 많다고 합니다.\n",
    ":::"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\"> **Return**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "&\\text{definition of return } G_t\\\\\n",
    "&G_t \\overset{\\Delta}{=} R_t + \\gamma R_{t+1} + \\gamma R_{t+2} + \\dots\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "return($G_t$)은 현재의 reward뿐만아니라 앞으로 미래에 받게될  discounted reward들의 total sum이다.다시 말하면 sum of discounted rewards이다. discounted reward는 discount factor($\\gamma$)가 곱해진 리워드이며 이 값이 작을수록 지금 당장받는 리워드 중요시하며 이 값이 클수록 미래에 받는 리워드를 중요시한다. 또한 효율적인 path결정을 위해서도 중요하다.(Q-learning예시로 설명해야 함)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "강화학습의 궁극적인 목적은 return의 평균(기댓값)을 가장 크게 만드는 policy들을 찾는 것이다. 조금 풀어쓰자면 강화학습의 목적은 \"agent의 action이 단기적인 보상뿐만이 아니라 미래의 보상도 염두하여 현재 취해야할 행동에 대한 정책을 학습\"이라는 것이다."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\"> **참고자료**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[jdk.log](https://velog.io/@kjb0531/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5-%EA%B0%9C%EB%85%90%EC%A0%95%EB%A6%AC1)<br>\n",
    "[나무위키 - 강화학습 용어정리](https://namu.wiki/w/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5/%EC%9A%A9%EC%96%B4)<br>\n",
    "[stackexchange](https://ai.stackexchange.com/questions/10442/how-are-the-reward-functions-rs-rs-a-and-rs-a-s-equivalent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0478b8cb1c47bafb71305148a49d30528a4d9c22ca2de336c01aa5a8230a459a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
