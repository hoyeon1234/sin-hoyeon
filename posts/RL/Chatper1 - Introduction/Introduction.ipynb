{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[강화학습] Introduction\"\n",
    "format: html\n",
    "date : 2023-03-06\n",
    "author : hoyeon\n",
    "categories: [Reinforcement Learning]\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **우리는 환경과 상호작용(interacting)하면서 배운다**는 것은 우리가 `학습의 본질`에 대해 생각할때 가장 먼저 떠올릴 수 있는 생각이다.\n",
    "- 이 책에서는 `상호작용`으로부터 배우기 위한 `computational approach`를 배운다\n",
    "- 우리가 배울 접근방식 즉, `reinforcement learning`은 기계학습의 다른 접근법들보다 훨씬 `목표지향적 학습`에 중점을 둔다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\"> **Reinforcement Learning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color:black\"> **강화학습이란?**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 강화학습은 어떤 `situation`에서 어떻게 `action`을 `대응`시킬지를 배우는 `학습`이다.\n",
    "- `learner`에게 **어떤 행동을 취해야할지 알려주지 않으며** 따라서 learner는 **어떤 `action`이 가장 많은 `reward`를 주는지 여러번 `시도`하면서 `발견`해야 합니다.**\n",
    "- 대부분의 문제에서 action은 즉각적으로 받는 reward 뿐만 아니라 **다음 situation과 뒤따르는 모든 후속 reward에 영향을 미칠 수 있습니다.**\n",
    "- 이와 같은 강화학습의 두 가지 특성 `trial and error search`  그리고 `delayed reward`는 강화학습의 가장 중요하면서 차별적인 두 가지 특징입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color:black\"> **강화학습 vs 지도학습**\n",
    "- `지도학습`은 labeling이 되어 있는 example(또는 observation)또는 training set으로부터 학습합니다.\n",
    "- 각 example은 situation과 그때의 situation에 대해 시스템이 취해야 하는 올바른 행동(label)에 대한 label입니다.\n",
    "- `지도학습`의 목적은 training set에 없었던 상황에서 올바르게 동작하도록 `extrapolate(추론)`하거나 `generalization(일반화)`하는 것입니다.\n",
    "- 상호작용하는 문제에서 `agent`에게 **바라거나,해야하는 행동을 정확히 대표하는 예시를 얻는 다는 것은 비현실적**입니다. (쉽게 생각해보자면 말하자면 정확히 어떤 상황에서 어떤 행동을 하라고 정확하게 알려주는 것은 불가능하다. 아마도 상황이 수없이 많을 뿐더러 어떤 행동이 최적의 선택인지도 모르기 때문인 것 같다.)\n",
    "- 미지의 영역(학습이 가장 도움이 될 것으로 예상되는)에서 **에이전트는 자신의 경험으로 부터 스스로 배울 수 있어야 합니다.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color:black\"> **강화학습 vs 비지도학습**\n",
    "- `비지도학습`은 unlabeled data에 대한 hidden structure를 찾는 것이 목적입니다.\n",
    "- 강화학습이 correct behavior에 대한 labeled data를 가지고 있지 않기 때문에 누군가는 비지도 학습으로 착각하기도 하지만 강화학습의 목적은 reward signal을 최대화 하는 것이 목적이며 비지도학습 처럼 hidden structure를 찾는 것이 목적이 아닙니다.\n",
    "- 물론 hidden structure를 찾는 것이 강화학습에 유용할 수는 있지만 **reward signal을 최대화하는 것이 목적**이라는 강화학습의 문제를 설명하지 못합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color:black\"> **exploration vs exploitation**\n",
    "- 강화학습에만 존재하는 한가지 문제는 `exploration`과 `exploitation`사이의 `trade-off`입니다.\n",
    "- `exploration`,`exploitation`은 의사결정문제에서 가능한 두가지 행동방식으로 서로간에 장단점을 가집니다. \n",
    "- `exploitation`\n",
    "    - 더 많은 reward를 얻기 위해 지금까지의 데이터를 통해 optimal하다고 여겨지는 decision을 선택하는 것입니다.(알려진 것을 계속 연구하는 것)\n",
    "    - 리워드를 최대화하기위해 알려진 정보를 사용하여 행동하는 것입니다.\n",
    "- `exploration`\n",
    "    - 지금까지 데이터를 통해 optimal하다고 여겨지는 decision을 선택하지 않는 것 입니다. \n",
    "    - environment에 대해서 더 많은 정보를 찾기위해 랜덤하게 행동하는 것입니다.\n",
    "    - 이는 관측된 데이터가 best option을 결정하기에 충분하지 않다는 사실을 가정합니다.(알려진 것을 거부하고 새롭게 탐험하는 것)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
