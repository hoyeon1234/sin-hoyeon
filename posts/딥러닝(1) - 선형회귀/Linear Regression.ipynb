{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["---\n","title : 딥러닝(1) - 선형회귀\n","author : sin ho yeon\n","date : \"2022-12-22\"\n","categories : [Deep learning]\n","---\n","선형회귀에 대해서 정리한 글입니다."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Linear Regression\n","선형회귀란 확률변수사이의 관계가 **선형**이라고 가정했을 때,주어진 표본에 가장 잘 맞는 가중치를 찾는 것입니다.주어진 표본에 가장 잘 맞는 직선을 찾으면 우리는 두 변수사이에 관계가 대략적으로 이렇구나라고 생각할 수 있습니다.<br><br>\n","\n","확률변수사이의 관계가 다음과 같다고 가정해봅시다. 확률변수는 사이의 **관계는 기본적으로 선형**이라고 가정하지만 무시할만한 **아주 작은 오차**를 포함합니다.<br>\n","$Y = w_0 + w_1X_1  \\dots + w_mX_m + \\epsilon$<br><br>\n","\n","위와 같은 관계를 가지는 확률변수로부터 표본추출(sampling)하여 n개의 표본을 얻으면 다음과 같습니다.<br>\n","$y_1 = w_0 + w_1x_{11} + \\dots + w_mx_{1m} + \\epsilon_1$<br>\n","$y_2 = w_0 + w_1x_{21} + \\dots + w_mx_{2m} + \\epsilon_2$<br>\n","$\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\vdots$<br>\n","$y_n = w_0 + w_1x_{n1} + \\dots +w_mx_{nm} + \\epsilon_n$<br><br>\n","\n","샘플을 벡터-행렬로 표현해서 좀 더 간단히 나타낼 수 있습니다.<br>\n","$\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix} = \n","\\begin{bmatrix} \n","1 & x_{11} & x_{12} & \\dots &x_{1m}\\\\\n","1 & x_{21} & x_{22} & \\dots &x_{2m}\\\\\n","1 & x_{31} & x_{32} & \\dots &x_{3m}\\\\\n","\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n","1 & x_{n1} & x_{n2} & \\dots & x_{nm}\\\\ \n","\\end{bmatrix}\n","\\begin{bmatrix}\n","w_0\\\\\n","w_1\\\\\n","w_2\\\\\n","\\vdots\\\\\n","w_m\n","\\end{bmatrix}\n","+\n","\\begin{bmatrix}\n","\\epsilon_0\\\\\n","\\epsilon_1\\\\\n","\\epsilon_2\\\\\n","\\vdots\\\\\n","\\epsilon_n\n","\\end{bmatrix}$<br>\n","$\\Longleftrightarrow\n","\\bf{y} = \\bf{X}\\bf{W} + \\bf{\\epsilon}$<br><br>\n","\n","선형회귀의 목적은 위와 같이 표본이 주어질 때, 우리가 모르는 $\\bf{W}$를 추정하는 것입니다. 표본으로부터 $\\bf{W}$를 추정할 수 있다면 두 변수사이의 관계가 이렇겠구나라고 알 수 있게 됩니다.<br>"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Gradient Descent\n","경사하강법은 $\\bf{w}$를 구하는 방법입니다. 경사하강법의 대략적인 흐름을 요약하면 다음과 같습니다.<br>\n","\n","(경사하강법의 대략적인 흐름)<br>\n","1. $\\bf{W}$에 대한 추정값 $\\hat{\\bf{W}}$을 임의적으로 가정(초기화)한다.<br>\n","2. $\\hat{\\bf{W}}$로 $\\hat{\\bf{y}}$($\\bf{y}$에 대한 추정값)을 구하여 $\\bf{y}$와 **얼마나 비슷하지 않은지,틀린지 확인**한다.<br>\n","3. **기울기(편미분계수,gradient)를 사용**하여 $\\hat{\\bf{W}}$을 수정한다.<br>\n","4. 2번,3번을 정해진 횟수만큼 반복한 후 종료한다.<br><br>\n","\n","## 선형회귀의 Loss function\n","2번에서 구한 추정값 $\\hat{\\bf{W}}$이  **얼마나 틀린지,부정확한지** 알려주는 함수를 Loss function 또는 Cost function이라고 합니다. 선형회귀에서의 Loss function은 MSE를 사용하며 같습니다.<br>\n","\n","(Loss function)<br>\n","$MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2} = \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}}) = \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})$<br>\n","\n","위 함수는 추정값 $\\hat{\\bf{W}}$이 **얼마나 틀렸는지**를 나타내는 $\\hat{\\bf{W}}$에 대한 함수입니다. 우리는 위 수식의 값을 가장 작게만들도록 $\\bf{\\hat{W}}$를 찾아내어 우리가 모르고있는 확률변수사이의 관계를 알아내야합니다.\n","\n","## Parameter update\n","n개의 독립변수를 가지는 다변수 스칼라 함수에 대한 Gradient는 수학적으로 다음과 같습니다.<br>\n","\n","$\\nabla_{X}{f(x_1,x_2,...,x_n)} = (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\dots,\\frac{\\partial f}{\\partial x_n})$<br>\n","출력값이 스칼라인 함수의 그레디언트는 벡터입니다.그러므로,그레디언트를 벡터를 입력했을 때,벡터를 출력으로 하는 벡터함수라고 생각해도 무방합니다.중요한 사실은 **임의의 지점$X$에서 그레디언트의 방향은 스칼라함수가 가장 급격하게 증가하는 방향**이라는 사실입니다. **비슷하게 임의의 지점$x$에서 -그레디언트의 방향은 스칼라함수가 가장 급격하게 감소하는 방향**입니다.(증명생략)<br>\n","\n","이러한 사실을 활용하여 우리는 임의의 $\\hat{\\bf{W}}$에서 Loss function이 가장 급격하게 감소하는 방향을 찾을 수 있습니다. 또한 궁극적인 목적인 틀린정도를 최소화하는 즉,Loss function값이 가장 작은 $\\hat{\\bf{W}}$를 찾을 수 있습니다. $\\bf\\hat{W}$를 수정하는 구체적인 수식은 다음과 같습니다.<br>\n","\n","(Gradient descent parameter update)<br>\n","$\\hat{\\bf{W}}_{t} = \\hat{\\bf{W}}_{t-1} - \\alpha\\times\\nabla_{W}{L}$<br>\n","\n","$\\hat{\\bf{W}}_{t-1}$은 수정되기전의 가중치(벡터)이며 $\\hat{\\bf{W}_{t}}$는 파라미터를 한번 업데이트 한 후의 가중치(벡터)입니다. $t-1$의 $\\hat{\\bf{W_{t-1}}}$에 $-\\alpha\\times\\nabla_{W}{L}$를 더해줌으로서 $\\hat{\\bf{W}}_{t-1}$은 loss function이 가장 급격히(많이)감소하는 방향으로 이동하며 $\\hat{\\bf{W}}_{t}$가 됩니다. $\\alpha$는 학습률(learning rate)입니다. $\\hat{\\bf{W}}_{t-1}$과 곱해져서 얼마나 많이 또는 적게 움직일지를 결정합니다. 한번에 얼마나 이동할지에 비유한 \"보폭\"으로 생각할 수 있습니다.<br>\n","\n","요약하자면, 경사하강법을 통하여 위와 같이 가중치$\\hat{\\bf{W}}$를 재귀적으로 업데이트 하면 loss function $L$이 가장 최소가 되는 지점의 $\\hat{\\bf{W}}$를 찾을 수 있습니다.<br><br>\n","\n","## MSE에 대한 더 상세한 전개\n","MSE를 더 상세히 전개하면 다음과 같습니다.<br>\n","$MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2}$<br>\n","$= \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}})$<br>\n","$= \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})$<br>\n","$= \\frac{1}{n}(\\bf{y^T - \\hat{\\bf{W}}^{T}\\bf{X}^{T})(\\bf{y} - \\bf{X}\\bf{\\hat{W}}})$<br>\n","$= \\frac{1}{n}(\\bf{y^Ty-y^TX\\hat{W}} - \\hat{W}X^Ty + W^TX^TX\\hat{W})$<br>\n","\n","여기서 $\\bf{y^TX\\hat{W}} \\in \\bf{R}^{1 \\times 1}$ 이므로 $\\bf{y^TX\\hat{W}} = (\\bf{y^TX\\hat{W}})^T = (\\bf{\\hat{W}X^Ty})$가 성립합니다.\n","그러므로 MSE를 정리하면 다음과 같습니다.<br>\n","(MSE)<br>\n","$MSE = \\frac{1}{n}(\\bf{y^Ty -2\\hat{W}X^Ty + W^TX^TX\\hat{W}})$<br>\n","\n","## Gradient Descent에 대한 더 상세한 전개($Loss$ = MSE일 경우)\n","\n","(Gradient of MSE)<br>\n","$\\nabla{L} = MSE$<br>\n","$= \\bf{\\frac{1}{n}\\frac{\\partial}{\\partial \\hat{W}}(\\bf{y^Ty - 2W^TX^T + W^TX^TX\\hat{W}})}$<br>\n","$= \\bf{\\frac{1}{n}}(\\bf{\\frac{\\partial}{\\partial \\hat{W}}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2W^{T}X^{T}y + \\frac{\\partial}{\\partial\\hat{W}}W^{T}X^{T}X\\hat{W})$<br>\n","$= \\bf{\\frac{1}{n}(\\frac{\\partial}{\\partial \\hat{W}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2y^TXW + \\frac{\\partial}{\\partial\\hat{W}}W^TX^TX\\hat{W})}$<br>\n","$= \\bf{\\frac{1}{n}[0 - 2X^Ty + (X^TX + X^TX)\\hat{W}]}$<br>\n","$= \\bf{\\frac{2}{n}X^T(X\\hat{W} - y)}$\n","\n","(parameter update)<br>\n","$\\bf{\\hat{W}_{t} = \\hat{W}_{t-1} - \\alpha \\times \\frac{2}{n}X^T(X\\hat{W} - y)}$"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyMa+/K4fxyK6XIjTslAeX36","private_outputs":true,"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"0478b8cb1c47bafb71305148a49d30528a4d9c22ca2de336c01aa5a8230a459a"}}},"nbformat":4,"nbformat_minor":0}
