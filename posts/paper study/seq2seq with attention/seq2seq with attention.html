<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="hoyeon">
<meta name="dcterms.date" content="2023-03-26">

<title>HIHO - [Paper Study] NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-JE4129QJZV"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-JE4129QJZV', { 'anonymize_ip': true});
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">HIHO</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../about.html">
 <span class="menu-text">About me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hoyeon1234/sin-hoyeon/tree/main/posts"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">[Paper Study] NEURAL MACHINE TRANSLATION BY JOINTLY LEARNING TO ALIGN AND TRANSLATE</h1>
                                <div class="quarto-categories">
                <div class="quarto-category">Paper study</div>
              </div>
                  </div>
  </div>
    
  
  <div class="quarto-title-meta">

      <div>
      <div class="quarto-title-meta-heading">Author</div>
      <div class="quarto-title-meta-contents">
               <p>hoyeon </p>
            </div>
    </div>
      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">March 26, 2023</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#introduction" id="toc-introduction" class="nav-link active" data-scroll-target="#introduction"><span style="color:black"> <strong>Introduction</strong></span></a></li>
  <li><a href="#problem-setting" id="toc-problem-setting" class="nav-link" data-scroll-target="#problem-setting"><span style="color:black"> <strong>Problem Setting</strong></span></a></li>
  <li><a href="#background" id="toc-background" class="nav-link" data-scroll-target="#background"><span style="color:black"> <strong>Background</strong></span></a></li>
  <li><a href="#method" id="toc-method" class="nav-link" data-scroll-target="#method"><span style="color:black"> <strong>Method</strong></span></a>
  <ul class="collapse">
  <li><a href="#intuition" id="toc-intuition" class="nav-link" data-scroll-target="#intuition"><span style="color:black"> <strong>Intuition</strong></span></a>
  <ul class="collapse">
  <li><a href="#양방향-vs-단방향-rnn" id="toc-양방향-vs-단방향-rnn" class="nav-link" data-scroll-target="#양방향-vs-단방향-rnn"><span style="color:black"><strong>양방향 vs 단방향 RNN</strong></span></a></li>
  <li><a href="#직관적인-정리" id="toc-직관적인-정리" class="nav-link" data-scroll-target="#직관적인-정리"><span style="color:black"><strong>직관적인 정리</strong></span></a></li>
  </ul></li>
  <li><a href="#modeling" id="toc-modeling" class="nav-link" data-scroll-target="#modeling"><span style="color:black"> <strong>Modeling</strong></span></a>
  <ul class="collapse">
  <li><a href="#decoder" id="toc-decoder" class="nav-link" data-scroll-target="#decoder"><span style="color:black"> <strong>Decoder</strong></span></a></li>
  <li><a href="#encoder" id="toc-encoder" class="nav-link" data-scroll-target="#encoder"><span style="color:black"> <strong>Encoder</strong></span></a></li>
  </ul></li>
  </ul></li>
  <li><a href="#experiments" id="toc-experiments" class="nav-link" data-scroll-target="#experiments"><span style="color:black"> <strong>Experiments</strong></span></a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion"><span style="color:black"> <strong>Conclusion</strong></span></a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="introduction" class="level1">
<h1><span style="color:black"> <strong>Introduction</strong></span></h1>
<ul>
<li>당시 인공신경망을 활용한 기계번역에서는 대부분 <code>encoder</code>와 <code>decoder</code>를 포함한 모델을 사용했습니다</li>
<li>이러한 모델은 <code>고정된 길이</code>의 <code>context vector</code>를 사용하여 <strong>길이가 긴 문장에서 성능저하</strong>를 가져왔습니다.</li>
<li>따라서 논문에서는 <strong>각각의 <code>target world</code>에 대해 서로다른 <code>context vector</code>를 사용함으로서 길이가 긴 문장에 대한 성능저하를 개선</strong>합니다.</li>
<li>이러한 새로운 접근 방식을 통해 그 당시의 state of the art인 phrase-based system과 비슷한 번역 성능을 달성했습니다.</li>
</ul>
</section>
<section id="problem-setting" class="level1">
<h1><span style="color:black"> <strong>Problem Setting</strong></span></h1>
<p><img src="./vanila seq2seq.png" class="img-fluid"></p>
<ul>
<li>기존의 RNN기반의 seq2seq모델은 <code>source sentence</code>가 <code>encoder</code>를 통과하여 <strong>고정된 크기의 <code>context vector</code>가 되고</strong> 이를 <code>decoder</code>의 초기 <code>hidden state</code>사용되어 <code>output sequence</code>를 출력하는 구조를 가졌었습니다.</li>
<li>이러한 구조는 <strong>어떤 문장이던지 고정된 길이의 <code>context vecotr</code>로 변환되야 하는 구조적 한계</strong>때문에 <strong>길이가 긴 문장에서 정보를 과도하게 소실,축소,누락시켰으며</strong> 결과적으로 긴 문장에서의 <strong>성능저하</strong>를 가져왔습니다.</li>
</ul>
</section>
<section id="background" class="level1">
<h1><span style="color:black"> <strong>Background</strong></span></h1>
<ul>
<li>확률론의 관점에서 번역은 source sentence <span class="math inline">\(\bf{x}\)</span>가 주어졌을때 target sentence <span class="math inline">\(\bf{y}\)</span>에 대한 conditional probability를 maximize하는 <span class="math inline">\(\bf{y}\)</span>를 찾는 것과 같습니다.</li>
</ul>
<p><span class="math display">\[\hat{\bf{y}} = \underset{\bf{y}}{\text{argmax}}\,p({\bf{y|x}})\]</span></p>
<ul>
<li>chain rule에 의해 <span class="math inline">\(p(\bf{y|x})\)</span>는 다음과 같습니다.</li>
</ul>
<span class="math display">\[\begin{aligned}
p({\bf{y|x}}) &amp;= \prod_{t=1}^Tp(y_t|y_1,y_2,\dots,y_{t-1}|{\bf{x}}) \\
&amp;= p(y_1|{\bf{x}})p(y_2|y_1,{\bf{x}})p(y_3|y_2,y_1,{\bf{x}})\dots p(y_T|y_1,\dots,y_{t-1}|{\bf{x}})
\end{aligned}\]</span>
<ul>
<li>기존의 seq2seq 모델에서 잘 학습된 decoder에서는 각 time step <span class="math inline">\(t\)</span>마다 chainrule에서 연쇄적으로 곱하는 conditional probability를 구할 수 있습니다. 즉, 다음과 같습니다.</li>
</ul>
<span class="math display">\[\begin{aligned}
&amp;p(y_t|y_1,y_2,\dots,y_{t-1}) = g(y_{t-1},s_{t},c)\\
&amp;\text{where } \\
&amp;s_t = f(y_{t-1},s_{t-1},c)\\
&amp;c = q(\{h_1,h_2,\dots,h_{T_x}\})\\
&amp;s_t\text{ : hidden state of decoder at time t}\\
&amp;c\text{ : context vector,encoder output,initial hidden state of decoder}\\
&amp;h_t\text{ : hidden state of encoder at time t} \\
&amp;q\text{ : arbitary function}
\end{aligned}\]</span>
<ul>
<li>지난번 <a href="https://hoyeon1234.github.io/sin-hoyeon/posts/paper%20study/seq2seq/seq2seq.html">리뷰</a>했던 seq2seq논문의 경우 <span class="math inline">\(q(\{h_1,\dots,h_T\}) = h_T\)</span>였습니다.</li>
</ul>
</section>
<section id="method" class="level1">
<h1><span style="color:black"> <strong>Method</strong></span></h1>
<section id="intuition" class="level2">
<h2 class="anchored" data-anchor-id="intuition"><span style="color:black"> <strong>Intuition</strong></span></h2>
<p><img src="./seq2seq with attention.png" class="img-fluid"><br> 출처 : paper-Figure1</p>
<ul>
<li>위의 그림은 논문에서 제시한 모델로 위는 decoder 아래는 encoder입니다.</li>
<li><code>decoder</code>를 보면 기존의 seq2seq 아키텍쳐와 다를것이 거의 없습니다만 <strong>새로운 정보(휘어져 들어가는 화살표) 들어가며 이는 그림의 아래쪽에 있는 encoder에서 만들어짐</strong>을 알 수 있습니다.</li>
<li><code>encoder</code>는 bidirectional RNN을 사용합니다. 정방향과 역방향으로 읽어들이면서 <strong>input sequence에 대해 전체적이면서도 특히 i-th 단어(토큰)과 연관된 정보 <span class="math inline">\(h_t\)</span>를 만듭니다.</strong></li>
<li>이렇게 만들어진 <strong>input sequence의 각 시점에서의 정보는 각각의 target을 만드는데 얼마나 중요한 정보인지를 의미하는 값인 <span class="math inline">\(\alpha\)</span>와 곱하여 모두 더해집니다.</strong></li>
<li>즉, 더해진 값은 <strong>input sequence</strong>에서 모든 정보를 <strong>target을 예측하는데 얼마나 중요한지,관련있는지</strong>를 고려해서 <strong>재조합한 새로운 정보</strong>라고 할 수 있습니다. 이렇게 더해진 값은 <strong>새로운 출력값 <span class="math inline">\(y_t\)</span>를 만들기 위한 정보인 decoder의 hidden state <span class="math inline">\(s_t\)</span>를 구하는데 사용</strong>됩니다.</li>
</ul>
<section id="양방향-vs-단방향-rnn" class="level3">
<h3 class="anchored" data-anchor-id="양방향-vs-단방향-rnn"><span style="color:black"><strong>양방향 vs 단방향 RNN</strong></span></h3>
<hr>
<section id="단방향-rnn에서-hidden-state-h_t" class="level4">
<h4 class="anchored" data-anchor-id="단방향-rnn에서-hidden-state-h_t"><strong>단방향 RNN에서 hidden state <span class="math inline">\(h_t\)</span></strong><br></h4>
<ul>
<li>어떻게 생성? <span class="math inline">\(\rightarrow\)</span> input word <span class="math inline">\(x_t\)</span>와 이전에 읽어들인 sequence에 대한 정보 <span class="math inline">\(h_{t-1}\)</span>를 합쳐서 새로운 정보 <br></li>
<li>지금 단어 <span class="math inline">\(x_t\)</span>까지 입력된 sequence까지의 대한 정보이자 특히 마지막으로 입력된 단어를 많이 고려한 정보 <br></li>
<li>지금의 input인 <span class="math inline">\(x_t\)</span>다음에 오는 sequence는 고려하지 않음<br></li>
</ul>
</section>
<section id="양방향-rnn에서의-hidden-state-h_t" class="level4">
<h4 class="anchored" data-anchor-id="양방향-rnn에서의-hidden-state-h_t"><strong>양방향 RNN에서의 hidden state <span class="math inline">\(h_t\)</span></strong><br></h4>
<ul>
<li>어떻게 생성? <span class="math inline">\(\rightarrow\)</span> input sequence를 정방향,역방향으로 서로다른 RNN을 통과하여 생성<br></li>
<li>정방향에서는 지금의 단어 <span class="math inline">\(x_t\)</span>까지 순차적으로 입력된 sequence에 대한 정보이자 특히 마지막 <span class="math inline">\(x_t\)</span>를 많이 고려한 정보를 뽑아냄.<br></li>
<li>반대로 역방향에서는 끝에서부터 시작하여 반대로 <span class="math inline">\(x_t\)</span>까지 입력된 sequence 대한 정보이자 특히 마지막 <span class="math inline">\(x_t\)</span>를 많이 고려한 정보를 뽑아냄.<br></li>
<li>정방향이던 역방향이던 다음에 오는 sequence에 대한 정보는 고려하지 않으나 <strong>이 둘을 합쳐서 전체적인 정보 + 특정시점 <span class="math inline">\(x_t\)</span>에 고려한 정보</strong>를 얻을 수 있음.</li>
</ul>
</section>
</section>
<section id="직관적인-정리" class="level3">
<h3 class="anchored" data-anchor-id="직관적인-정리"><span style="color:black"><strong>직관적인 정리</strong></span></h3>
<hr>
<div class="callout-note callout callout-style-default no-icon callout-captioned">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon no-icon"></i>
</div>
<div class="callout-caption-container flex-fill">

</div>
</div>
<div class="callout-body-container callout-body">
<ul>
<li>기존의 seq2seq 모델은 <code>encoder</code>에서 고정된 길이의 <code>context vector</code>로 바꾼 정보만을 <code>decoder</code>에서 사용하여 긴 문장에 대해서는 정보의 손실이 일어났으며 또한 성능이 좋지 못했습니다.</li>
<li>따라서 논문에서는 decoder에서 target(word)을 만들 때 <strong><code>input sequnce</code>의 각각의 위치에서 나오는 모든 정보를 중요도를 반영하여 재조합한 새로운 정보를 만들고 이를 활용합니다.</strong></li>
<li>이렇게 만든 새로운 정보는 decoder가 output sequence의 <strong>각각의 target을 예측할때 가중치 <span class="math inline">\(\alpha\)</span>를 통해 특정위치 근처의 문맥에 주목,집중한 값</strong> 이기때문에 집중,주의를 의미하는 <code>attention</code>이라는 용어를 따와서 <code>attention mechanism</code>이라고 합니다.</li>
<li>이렇게 <code>attention mechanism</code>을 사용함으로서 <strong><code>encoder</code>가 <code>source sentence</code>의 모든정보를 하나의 고정된 길이의 <code>context vector</code>로 인코딩해야 하는 부담을 줄여줍니다.</strong></li>
</ul>
</div>
</div>
</section>
</section>
<section id="modeling" class="level2">
<h2 class="anchored" data-anchor-id="modeling"><span style="color:black"> <strong>Modeling</strong></span></h2>
<section id="decoder" class="level3">
<h3 class="anchored" data-anchor-id="decoder"><span style="color:black"> <strong>Decoder</strong></span></h3>
<ul>
<li>위와 같은 새로운모델에서 chainrule에서 각각의 conditional probability는 다음과 같습니다.</li>
</ul>
<span class="math display">\[\begin{aligned}
p(y_i|y_1,\dots,y_{i-1},{\bf{x}}) = g(y_{i-1},s_{i-1},c_i)
\end{aligned}\]</span>
<ul>
<li>기존의 seq2seq모델에서는 context vector <span class="math inline">\(c\)</span>는 target <span class="math inline">\(y_i\)</span>가 바뀌어도 고정된 바뀌지 않는 값이었습니다.</li>
<li>논문에서 제시된 모델은 이와는 다르게 각각의 target <span class="math inline">\(y_i\)</span>를 계산하기 위해서 서로다른 <code>context vector</code> <span class="math inline">\(c_i\)</span>를 사용합니다.</li>
<li>여기서 <span class="math inline">\(c_i\)</span>는 decoder의 hidden state인 <span class="math inline">\(s_{i}\)</span>를 계산하기 위해서 사용됩니다. 즉,다음과 같습니다.</li>
</ul>
<p><span class="math display">\[s_i = f(s_{i-1},y_{i-1},c_i)\]</span></p>
<ul>
<li><span class="math inline">\(c_i\)</span>는 target <span class="math inline">\(y_i\)</span>를 예측하기 위해서 input sequence에서 나온 정보 <span class="math inline">\(h\)</span>를 중요도 <span class="math inline">\(a\)</span>에 따라 재조합한 정보입니다. 구체적으로 다음과 같습니다.</li>
</ul>
<span class="math display">\[\begin{aligned}
c_i = \sum_{j=1}^{T_x}\alpha_{i,j}h_{j}
\end{aligned}\]</span>
<ul>
<li><p>각각의 annotation 즉 hidden state <span class="math inline">\(h_j\)</span>는 전체문장의 정보를 담고 있으나 특히 <span class="math inline">\(j\)</span>번째 poistion근처의 문맥정보를 많이 담고 있습니다.(bidirectional RNN에 의한 결과입니다.)</p></li>
<li><p>참고 - <span class="math inline">\(i\)</span>값이 바뀌더라도 즉, 또 다른 target을 예측하더라도 참고하는 input sequence의 정보인 annotations는 바뀌지 않음(저장해놨다가 각각의 <span class="math inline">\(y_i\)</span>값을 구하는데 사용할 수 있음,구현시 유의)</p></li>
<li><p>각각의 가중치 <span class="math inline">\(\alpha\)</span>는 다음과 같이 계산할 수 있습니다.</p></li>
</ul>
<span class="math display">\[\begin{aligned}
\alpha_{ij} = \frac{\text{exp}(e_{ij})}{\sum_{k=1}^{T_x}\text{exp}(e_{ik})}\\
\text{where},\,e_{ij} = a(s_{i-1},h_j)
\end{aligned}\]</span>
<ul>
<li><span class="math inline">\(\alpha\)</span>는 softmax의 함숫값이며 <span class="math inline">\(e\)</span>를 0과1사이의 값으로 scailing 했음을 알 수 있습니다.</li>
<li>여기서 <span class="math inline">\(a\)</span>는 alignment model로 input sequence의 j번째 position과 output sequence의 i번째 position이 얼마나 일치하는지,연관되어있는지,관련있는지 알려주는 값이며 이는 feedforward nueral network로부터 계산됩니다.</li>
<li>이와같이 (soft)alignment(일치,정렬)를 직접적으로 계산함으로서 alignment가 잠재적(보이지않던,숨겨져있었던)이었던 기존의 기계번역 모델들과는 다르게 alignment를 더 잘 학습하도록 gradient가 backpropagation 될 수 있으며 따라서 input과 output sequnce에서의 alignment를 기존모델보다 더 잘 학습할 수 있습니다.</li>
</ul>
</section>
<section id="encoder" class="level3">
<h3 class="anchored" data-anchor-id="encoder"><span style="color:black"> <strong>Encoder</strong></span></h3>
<ul>
<li><p>인코더에서는 정방향,역방향으로 input sequence를 모두 읽어들이는 bidirectional RNN을 사용합니다.</p></li>
<li><p>forward RNN <span class="math inline">\(\overset{\rightarrow}{f}\)</span>은 <span class="math inline">\(x_1\)</span>에서 <span class="math inline">\(x_{T_x}\)</span>까지 forward hidden states인 <span class="math inline">\((\overset{\rightarrow}{h_1},\dots,\overset{\rightarrow}{h_{T_x}})\)</span>를 계산합니다.</p></li>
<li><p>backward RNN <span class="math inline">\(\overset{\leftarrow}{f}\)</span>은 <span class="math inline">\(x_{T_x}\)</span>에서 <span class="math inline">\(x_1\)</span>까지 bacward hidden states인 <span class="math inline">\((\overset{\leftarrow}{h_{T_x}},\dots,\overset{\leftarrow}{h_{T_1}})\)</span>을 계산합니다.</p></li>
<li><p>양방향,정방향의 hidden state를 모두 결합하여 다음과 같은 결합된 hidden state값을 계산합니다. <span class="math display">\[h_j = \left[\overset{\rightarrow}{h}_j^{\,\,T};\overset{\leftarrow}{h}_j^{\,\,T}\right]^T\]</span></p></li>
<li><p><span class="math inline">\(h_j\)</span>는 input sequence 전체의 정보를 모두 갖지만 특히 position j근처에 집중된 정보를 가집니다.</p></li>
</ul>
</section>
</section>
</section>
<section id="experiments" class="level1">
<h1><span style="color:black"> <strong>Experiments</strong></span></h1>
<p><img src="./result1.png" class="img-fluid"><br> 출처 : paper-figure2</p>
<ul>
<li>RNNenc는 기존모델 RNNsearch는 논문에서 제안한 모델입니다.</li>
<li><span class="math inline">\(-50\)</span>은 최대 문장의 길이가 <span class="math inline">\(50\)</span>인 데이터셋으로 학습시킨 모델을 의미합니다.</li>
<li><strong>RNNsearch가 RNNenc의 성능보다 높음을 알 수 있습니다.</strong>(심지어 RNNenc-50보다 RNNsearch-30이 더 높아요.)</li>
<li>RNNsearch가 길이가 더 긴 문장에 robust한 모델이며 특히 <strong>RNNsearch-50은 긴 문장에 대해서 성능이 월등히 좋음</strong>을 알 수 있습니다.</li>
</ul>
<p><img src="./result2.png" class="img-fluid"></p>
<p>We can see from the alignments in Fig. 3 that the alignment of words between English and French is largely monotonic. We see strong weights along the diagonal of each matrix. However, we also observe a number of non-trivial, non-monotonic alignments. Adjectives and nouns are typically ordered differently between French and English, and we see an example in Fig. 3 (a). From this figure, we see that the model correctly translates a phrase [European Economic Area] into [zone economique europ ´ een]. The RNNsearch was able to correctly align [zone] with [Area], jumping ´ over the two words ([European] and [Economic]), and then looked one word back at a time to complete the whole phrase [zone economique europ ´ eenne].</p>
<ul>
<li>위의 그림은 가중치 <span class="math inline">\(\alpha\)</span>값을 나타냅니다.</li>
<li>그림을 보면 주대각선의 값이 대부분 큰 것으로 보아 영어와 프랑스어 단어사이의 alingment(일치)는 대부분 단조로움(monotonic)을 알 수 있습니다.</li>
<li>이는 실제로 영어와 프랑스어 사이의 어순이 대부분 일치한다는 사실로 보아 직관적인 사실입니다.</li>
<li>그러나 그렇지 않은 경우도 있습니다. 형용사,명사의 경우에는 두 언어 사이에 어순의 차이가 존재합니다.</li>
<li>이렇게 어순의 차이가 있는 경우에도 마찬가지로 제시된 모델에서는 올바르게 alignment시킴을 알 수 있습니다.</li>
<li>예를 들어 figure3에서 [European Economic Area]는 [zone economique europ´een]로 번역이 됨을 확인할 수 있습니다.</li>
<li>이는 zone이라는 단어가 어순의 차이가 있기에 두 단어를 뛰어넘어서 Area를 올바르게 alignment했음을 알 수 있습니다.(또한 나머지 단어들도 되돌아가서 올바르게 align했음을 알 수 있어요.)</li>
</ul>
<section id="experiments-detail" class="level4">
<h4 class="anchored" data-anchor-id="experiments-detail"><span style="color:black"><strong>experiments detail</strong></span></h4>
<ul>
<li>RNNenc의 경우 encoder,decoder에 1000 hidden units인 RNN사용</li>
<li>RNNsearch의 경우 encoder,decoder에 각각의 1000개의 hidden units을 가진 forward,backward(bidirectional) RNN을 사용했음</li>
<li>RNNsearch,RNNenc 두 경우 모두 target word에 대한 conditional probability 계산을 위해 single maxout을 포함한 multilayer neural network를 사용함.</li>
<li>SGD with Adadelta</li>
<li>minibatch of 80 sentences</li>
<li>beam search</li>
</ul>
</section>
</section>
<section id="conclusion" class="level1">
<h1><span style="color:black"> <strong>Conclusion</strong></span></h1>
<ul>
<li>당시의 인공신경망 기반 기계번역 모델은 <code>encoder</code>-<code>decoder</code> 구조를 사용했으며 여기에는 고정된 크기의 <strong><code>context vector</code>에서 <code>정보의 손실</code></strong>이 일어난다는 문제가 있었습니다.</li>
<li>따라서 <strong>각각의 target word를 생성할때 input sequence에서 나오는 각각의 모든 정보(hidden state)를 중요도에 따라서 재조합한 새로운 정보를 추가적으로 활용</strong>합니다.</li>
<li>이는 input sequence의 <strong>모든 정보를 고정된 크기의 context vector로 함축해야하는 부담을 줄여주며</strong> 동시에 target값을 생성하는데 필요한 <strong>input sequence의 특정한 정보에 집중(attention)</strong>할 수 있도록 합니다.</li>
<li>결과적으로 <strong>긴 문장에 대해서 좋은 결과</strong>를 보였으며 또한 <strong>기존의 모델과 다르게 alignment모델과 번역모델을 동시에 학습</strong>시킬 수 있었다는 점에서 긍정적입니다.</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="hoyeon1234/sin-hoyeon" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>