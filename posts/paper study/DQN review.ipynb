{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"DQN review\"\n",
    "format: html\n",
    "author: 신호연\n",
    "date: 2023-01-20\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color : black\">**개요**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./deep%20rein%20relation%201.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "기존의 딥러닝과 강화학습은 그림의 왼쪽과 같이 공통분모가 거의 없는 분야였습니다. 그러나 2013년 구글딥마인드에서 발표한 D$Q$N논문에서는 강화학습의 한 분야인 $Q$-learning에 딥러닝을 접목시켰고 그 후 계속해서 발전하여 거의 모든 강화학습논문에는 딥러닝이 사용된다고 합니다. 따라서 이제는 오른쪽과 같이 강화학습도 거의 딥러닝의 하나의 분야로 자리잡았습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color : black\">**$Q$-learning**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q$-learning은 **$Q$값을 학습하는 알고리즘으로 그리드형식을 가진 문제에서 활용**될 수 있습니다. 강화학습에서 주체(agent)는 현재의 상태(state)를 관찰하여 어떠한 행동(action)이 가장 큰 보상(reward)를 가져다주는지 학습하며 $Q$-learning에서 이러한 학습의 대상은 $Q$입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://raw.githubusercontent.com/hoyeon1234/sin-hoyeon/master/posts/paper%20study/Q%20learning.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위의 그림은 학습이 끝난 $Q$값의 예시입니다.(실제로 맞는 수치는 아님)$Q$-learning 알고리즘에서 agent는 greedy action을 취합니다. 따라서 **agent가 격자의 시작지점에 들어가게 된다면 greedy action을 통해 가장 큰 $Q$값이 있는 방향으로 이동**하여 시작부터 종료지점까지 일직선으로 가장빠르게 이동합니다. 위와 같은 그리드에서는 더 빠르게 가거나 리워드도 더 좋은 곳은 없으므로 적절하게 학습이 끝났다는 것을 알 수 있습니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color : black\">**$Q$-update**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ Q(s_t,a_t) = (1-\\alpha)Q(s_t,a_t) + \\alpha(R_t + \\gamma \\underset{a_{t+1}}{\\text{argmax}}Q(s_{t+1},a_{t+1}))$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q$-learning에서는 위와 같은 수식으로 각각의 $Q$를 업데이트합니다. 여기서 중요한점은 **$Q$가 state와 action의 함수**라는 점입니다. 위와 같은 길찾기 문제의 경우 그렇게 state(25개)가 많지는 않습니다.그러나 [Atari 벽돌깨기 게임](https://www.youtube.com/watch?v=V1eYniJ0Rnk)과 같은 경우, 움직이는 주체인 가로막대바 위치,벽돌의 갯수,깨진위치,공이 날아오는 각도 등등 ... 매우 많은 state가 가능하고 어떤방향으로 공을 날릴지에 대한 action도 수없이 많이 가능합니다. 기존의 방식으로 Q를 업데이트 하기위해서는 이러한 **수많은 조합에 대하여 state와 action을 기억해놓고** 업데이트 해야합니다. 이러한 방식은 **컴퓨터의 메모리에 부담을 주고 exploration(탐험)하는데 걸리는 시간을 더 오래 만듭니다.**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep-$Q$-learning에서는 DNN을 통해 함수로서 $Q$값을 저장하여 위와 같은 단점을 줄입니다. 또한 loss function을 정의하고 gradient desent를 사용하여 새롭게 Q값을 업데이트 합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![그림출처 : 이것저것 테크 블로그](https://raw.githubusercontent.com/hoyeon1234/sin-hoyeon/master/posts/paper%20study/Q-learning%20vs%20deep%20Q-learning.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0478b8cb1c47bafb71305148a49d30528a4d9c22ca2de336c01aa5a8230a459a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
