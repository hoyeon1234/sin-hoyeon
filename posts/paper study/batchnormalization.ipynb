{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"Batch Normalization: Accelerating Deep Network Training b\n",
    "y Reducing Internal Covariate Shift\"\n",
    "format: html\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\">**Problem setting**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deep Neural Network에서는 **training에서 파라미터 변화**로 layer에 입력되는 **input의 distribution이 학습중에 계속해서 변화(internal covariate shift)**일어난다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- lower learning rate\n",
    "- careful parameter initialization\n",
    "- hard to train models with saturation nonlinearities(비선형성이 너무 과하여 모델을 학습시키기가 어렵다.)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이러한 현상을 논문의 저자는 ***internal covariate shift***라고하며 (좀 더 자세히는 나중에 나온다.) 이것을 막기위해 **normalization for each training mini-batch(Batch Normalization)**을 수행할 것이라고 한다.**Batch Normalization**은 다음과 같은 장점이 있다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- allows us to use much higer learning rates\n",
    "- less careful about initilization\n",
    "- acts as a regularizaer\n",
    "- in some cases eliminating the need for dropout"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\">**Introduction**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD는 **간단하고 효율적인 알고리즘**이지만 그 자체로는 **상당히 민감하고 까다로운** 알고리즘이며 그 이유는 다음과 같다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- requires care ful **tuning of hyper-parameters** (specifically the **learning rate**,**initial values for the parameters**)\n",
    "- The **inputs to each layer** are **affected** by the **parameters of all preceeding layers** => small changing to the network => amplification of gradient(**vanish or slope**) "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SGD로 최적화 하면 **임의의 layer(sub-network)**으로 들어가는 **inputs의 distribution은 계속해서 변화**하며 이는 **각각의 레이어에 존재하는 파라미터**를 **각각의 입력들의 분포에 맞게 다시 학습**해야 함을 의미한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "만약 각 레이어에 들어가는 input의 분포가 고정되어 있다면 어떨까? 크게 다음과 같은 이점을 가진다고 논문에서는 설명한다.<br>\n",
    "- 입력의 분포가 변화하는 것에 대하여 파라미터를 재조정 할 필요가 없다.(따라서 유리하다.)\n",
    "- nonlinearity의 inputs이 saturated regime에 gradient vanish가 되는 현상을 막아준다. => 이는 batchnormalization을 추가하면 학습시간이 덜 들어가는 이유이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "또한 fixed distribution은 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0478b8cb1c47bafb71305148a49d30528a4d9c22ca2de336c01aa5a8230a459a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
