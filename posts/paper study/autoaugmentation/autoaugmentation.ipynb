{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[Paper Study] AutoAugment : Learning  Augmentation Strategies from Data\"\n",
    "subtitle: \"field : CV,RL<br>understanding : 😃😃\"\n",
    "format: html\n",
    "date : 2023-05-17\n",
    "author : hoyeon\n",
    "categories: [Paper study]\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "python"
    }
   },
   "source": [
    "# 한 줄 요약"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- image Augmentation에 강화학습 $\\to$ sota!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro & Abstract"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Data augmentation은 image classifier의 성능을 향상시키는 쉬운 방법.\n",
    "- 왜 image augmentatio이 성능을 향상? $\\to$ 데이터의 `invariance`들을 충분히 학습\n",
    "    - invariance(불변성)란? : 차이가 있거나 변환이 적용된 후에도 무언가가 동일하게 유지되는 속성,상태를 의미함\n",
    "    - 예를 들어 자동차,사과\n",
    "- 그러나 데이터셋마다 이미지의 분포가 다르기 때문에 `수동적으로` augmentation전략을 다뤄줘야 했음.\n",
    "    - 과일이 많은 데이터에 색에 대한 transform을 많이 적용하면? $\\to$ 사과가 사과가 아니게 되겠죠?\n",
    "- 이 논문에서는 데이터마다 적절한 augmentation policy을 `자동적으로` 찾기위해 강화학습을 사용함.\n",
    "- Imagenet과 CIFAR-10에서 Sota를 달성했음. 후에 끝판왕 느낌의 efficient net이 나오는데 그 때의 augmentation에도 활용."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fig1](./Fig1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 크게 두 가지의 파트로 구분 \n",
    "    1. Controller(RNN) : Polict가 있는 Search Space에서 하나의 `augmentation policy`를 sampling. \n",
    "    2. Child network : 이미지 분류기 sampling된 policy로 학습 후 validation accuracy R(reward)을 계산."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 과정(충분히 반복)<br>\n",
    "    policy sampling(by controller) <br>$\\to$ fitting classifier,calculating reward  <br>$\\to$ contoller update,policy sampling <br>$\\to$ fitting classifier,calculating reward <br>$\\to$ contoller update,policy sampling <br> $\\to$ fitting classifier,calculating reward <br>$\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\vdots$ <br>$\\to$ optimal policy(converge to best augmentation strategy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Fig2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- SVHN dataset에 적용한 예시\n",
    "- 하나의 Policy는 5가지의 subpolicy로 구성됨.\n",
    "- subpolicy는 operation 2개와 probability,magnitude로 구성됨\n",
    "    - operation : 이미지 변형하는 방법 (Rotate,Brightness,ShearX,Inver 등등... 총 16가지 방법이 존재함)\n",
    "    - probability : 얼마나 많이 적용할거냐\n",
    "    - magnitude : 어느정도 강도로 할거냐\n",
    "- 위의 예시에서 배치는 총 15개. 각각의 배치마다 균일분포로 어떤 subpolicy가 할당됨.(그림에서는 아예 33333씩 할당되었지만 실제는 아닐 수 있음.이해를 돕기 위함)\n",
    "- Note \n",
    "    - 동일한 subpolicy가 적용되는 배치들이라도 각 배치마다 다른 operation이 적용될 수 있음. 이는 probability 또한 subpolicy에 있기 떄문임"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Fig3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ImageNet에 적용한 예시 vs SVHN에서 적용한예시\n",
    "- SVHN에는 기하학적 변환이 많이 높은 확률로 첫번째 변환으로 적용되게 되어 있음(Shear X,Shear Y)\n",
    "    - SVHN은 숫자 이미지 dataset $\\to $숫자가 비틀려있거나 왜곡되어 있는 경우가 많음.$\\to$ 강인함을 policy는 기하학적 변환을 많이 포함.\n",
    "    - 또한 색변환도 어느정도 포함되어 있음.(이는 데이터에서 확인가능,데이터 자체에 반전되어 있는 경우도 많음)\n",
    "- Imagenet에는 색상 변환이 많이 높은 확률로 첫번째 변환으로 적용되게 되어 있음(Shear X,Shear Y)\n",
    "    - ImageNet은 다양한 물체들을 포함하는 dataset $\\to$ 다양한 색상들이 포함되어 있음. $\\to$ 강인함을 가지기 위해 policy는 색상 변환이 많이 포함"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result on many datasets**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Table2.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Result on Imagenet dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./Table3.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 다양한 데이터셋에 강화학습을 사용한 autoaugmentation 전략을 적용 $\\to$ Sota!!\n",
    "- 추가적으로, 데이콘,캐글 등에서 위에서 나온 augmentation전략들이 많이 사용됨. 적용하기 쉬움 \\\n",
    "링크 [내꺼 코드](https://www.kaggle.com/code/hihoyeon/pp-autoaugumentation/edit)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
