{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[Paper Study] Multi-Agent Reinforcement Learning: A Selective Overview  of  Theories  and  Algorithms\"\n",
    "format: \n",
    "  html:\n",
    "    linkcolor: blue\n",
    "subtitle: \"field : RL<br>understanding : 😃😃\"\n",
    "date: 2023-05-01\n",
    "author: hoyeon\n",
    "categories: [Paper study]\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최근 강화학습은 순차적인 의사결정 문제를 해결하는데에 상당한 성공을 거두었다.\n",
    "- 이러한 성공들에도 불구하고 `multi agent reinforcement learning`(MARL)에 대한 이론적인 기초는 상대적으로 부족하다.\n",
    "- 따라서 이 논문에서는 몇 가지 MARL 알고리즘을 전체적으로 설명하고 이론적으로 분석한다.\n",
    "- 뿐만아니라 중요하지만 다소 도전적인 난관들도 제시한다.\n",
    "- 논문의 궁극적인 목표는 현재의 MARL 분야에 대한 의견,평가을 제시하는 것을 넘어서 유익한 연구방향을 설정할 수 있도록 돕는 것이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 최근 강화학습은 복잡한 함수를 근사할 수 있는 딥러닝의 발전과 더불어 RL은 놀랍게 진보했다.\n",
    "    - 예를 들면 playing real-time strategy games, playing car games, etc."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 대부분의 성공에는 하나의 `agent`가 존재하는 것이 아니라 하나 이상의 `다수의 agent`가 참여하며 이는 `MARL`로 모델링 된다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MARL은 구체적으로 뭘까?\n",
    "    - 다수의 자립하여 움직이는 agent들이 존재하고\n",
    "    - 이러한 agent가 공통적인 환경(environment)에 놓여있을 때의\n",
    "    - 순차적인 의사결정문제를 다룬다.\n",
    "    - 각각의 agent는 환경,다른 agent들과의 상호작용을 통해서 그들이 얻는 각각의 `return`을 최대화 하려고 노력한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 크게 MARL은 다음의 세 가지로 구분된다.\n",
    "    - fully cooperative $\\to$ agent들은 `공동의 return`을 최적화 하기 위해서 협력한다. \n",
    "        - ex) 로봇이 물건을 지정된 장소에 운반하는 경우, 여러대의 자율주행차가 목적지에 도착.\n",
    "    - fully competitive $\\to$ agent들은 `합이 0인 return`을 서로 최적화 하기 위해서 경쟁한다.\n",
    "        - ex) 바둑,체스 \n",
    "    - a mix of two $\\to$ agent가 `보편적인 return`을 최적화 하기위해 협력 or 경쟁할 수 있다.(협력도 경쟁도 모두 가능하다.)\n",
    "        - ex) 축구,농구"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- MARL이 무조건 좋아보이는데? $\\to$ 어려운 점이 많다.(복습)\n",
    "    - 각 에이전트들은 각각의 return을 최대화 하려함. \n",
    "         - `균형점`에서 비효율적일 수 있으며(내시균형에서 발생하는 문제)\n",
    "         - 통신,협력이 효율적으로 이루어질 수 있는가에 대한 추가적인 기준이 필요함.\n",
    "         - 적대적 agent에 대한 robustness는 충분한가?\n",
    "    - 모든 agent는 저마다의 policy를 계속해서 향상(수정).\n",
    "        - B라는 agent가 이전과 다른 action을 하게 되면 environment가 변하게 되고\n",
    "        - A라는 agent가 직면하는 environment는 `non-stationary`해지는 것을 의미.\n",
    "    - 모든 agent에 대한 action들의 조합,결합은 agent한명이 증가할수록 지수적으로 증가.\n",
    "    - 각 agent는 다른 agent가 무엇을 관측했는지는 정확히,모든것을 알 수 없음.\n",
    "        - 다른 agent의 관측에 대한 제한된 정보만을 가지고 각 agent는 결정을함.\n",
    "        - 최적의 결정이 아닌 suboptimal한 결정을 가져옴."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
