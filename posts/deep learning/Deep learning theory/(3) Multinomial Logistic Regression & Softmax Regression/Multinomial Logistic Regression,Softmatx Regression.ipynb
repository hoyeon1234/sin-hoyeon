{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: 신호연\n",
    "date: 2022-12-26\n",
    "title: \"Multinomial Logistic Regression & Softmax Regression\"\n",
    "format: html\n",
    "categories: [Deep learning]\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**로지스틱회귀**에서는 종속변수 y가 0또는1의 2가지 범주만 가지는 **이진분류문제**를 해결할 수 있었습니다. **다항로지스틱 회귀와 소프트맥스 회귀**는 종속변수 y가 더 다양한 범주를 가지는 **일반적인 분류문제**를 해결합니다. 두개의 일반적인 분류모형간의 차이는 마지막 클래스에 대해서 가중치(회귀계수)를 더 사용하는지의 여부입니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#00CC66\">**Problem Setting**</span>\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\text{Given, }D = {(x_{1,i},x_{2,i},\\dots,x_{M,i},y_i})_{i=1}^{i=N} \\\\\n",
    "&\\text{where, $y_i$는 각각의 datapoint의 클래스를 원핫인코딩한 벡터} \\\\ \\\\\n",
    "&\\text{Goal : x가 입력될 때, 어떤 범주(y값)에 속하는지 예측하는 모형 만들기}\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#00CC66\">**Multinomial Logistic Regression**</span>\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **가정**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각각의 관측치에서 샘플$y_i$는 확률변수 $Y_i$의 realization(실현,실현된값)이다. 확률변수 **$Y_i$는 관측치에 포함된 $x$를 조건으로 하는 카테고리분포**를 따른다.<br>\n",
    "\\begin{aligned}\n",
    "&\n",
    "\\begin{aligned}\n",
    "Y_i|x_{1,i},x_{2,i},\\dots,x_{M,i} \\sim \\text{Cat}(y|x_{1,i},x_{2,i},\\dots,x_{M,i};\\mu_i) \n",
    "& = \n",
    "\\begin{cases}\n",
    "\\mu_{1,i} \\text{ if } y = (1,0,\\dots,0,0) \\\\\n",
    "\\mu_{2,i} \\text{ if } y = (0,1,\\dots,0,0) \\\\\n",
    "\\quad\\quad \\vdots \\\\\n",
    "\\mu_{K,i} \\text{ if } y = (0,0,\\dots,0,1) \\\\ \n",
    "\\end{cases} \\\\\n",
    "&= \\mu_{1,i}^{y_1}\\mu_{2,i}^{y_2},\\dots,\\mu_{K,i}^{y_K} \\\\\n",
    "&= \\prod_{K=1}^{K}\\mu_{K,i}y_{K,i} \\\\\n",
    "\\end{aligned} \\\\\n",
    "&\n",
    "\\begin{aligned}\n",
    "&\\text{where, }\\\\\n",
    "&\\mu_i = {\\mu_{1,i},\\mu_{2,i},\\dots,\\mu_{K,i}} \\\\\n",
    "&\\mu_{1,i} = Pr(Y_i|x_{1,i},\\dots,x_{M,i} = (1,0,\\dots,0)) \\\\\n",
    "&\\mu_{2,i} = Pr(Y_i|x_{1,i},\\dots,x_{M,i} = (0,1,\\dots,0)) \\\\\n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\vdots \\\\\n",
    "&\\mu_{K,i} = Pr(Y_i|x_{1,i},\\dots,x_{M,i} = (0,0,\\dots,0,1)) \\\\\n",
    "\\end{aligned}\\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 각 datapoint에서 $Y_i$가 따르는 카테고리분포의 모수$\\mu_i$는 <span style=\"color:blue\">**데이터포인트마다 다른 설명변수(X_i)와 시행마다 변하지 않는 고정된 회귀계수(W)의 일차결합을 포함하는 수식으로 표현됩니다.**</span> 주어진 <span style=\"color:blue\">**X값을 W와 일차함수로 결합하여 추정하고자 하는 값을 표현하는 선형회귀의 핵심아이디어**</span>는 응용되어 대부분의 회귀문제에서 사용한다.<br>\n",
    "\\begin{aligned}\n",
    "&\\mu_{k,i}  = \\mu_{k,i}(X_i;W_{k,i}) = \\mu_{k,i}(X_iW_{k,i}) =  Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i)\\\\\n",
    "&\\text{where},\\\\\n",
    "&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n",
    "&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n",
    "&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n",
    "&W_k = [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **X,W의 선형조합을 포함한 모수의 표현 유도하기**\n",
    "위에서 언급했듯이 대부분의 회귀에서 모델링의 핵심아이디어는 추정하고자 하는 대상을 설명변수와 가중치의 일차결합(선형조합)이 포함되도록 표현하는 것이다. 다항로지스틱회귀도 이 아이디어를 그대로 사용한다. 이진로지스틱회귀와의 차이점은 표현해야하는 모수가 이번에는 더 많다는 것이다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"font-size:15pt\"> **1. 독립변수와 가중치의 linear combination이 K개의 모수를 표현하는 선형방정식을 만든다.**</span>\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n",
    "&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n",
    "&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n",
    "&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n",
    "&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n",
    "&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i-\\text{ln}Z \\\\\n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n",
    "&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\ \n",
    "&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i {\\text{ (임의의 k번째 항)}}\\\\  \n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n",
    "&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n",
    "&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n",
    "&where,\\\\\n",
    "&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n",
    "&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n",
    "&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n",
    "&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "마지막 모수$Pr(Y_i=1|x_1,\\dots,x_K)$에 대한 가중치는 사용하지 않을 것이다. 확률의 합은 1이기 때문에 1-(나머지확률)하면 마지막 $K$번째 모수가 구해지기 때문이다.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style = \"font-size:15pt\">**(2~4). OddsRatio비슷한 무언가  + Logit transform()**</span>\n",
    "좌변을 Odds Ratio(비슷한 무언가,엄밀히 Odds Ratio는 아님) + Logit transform을 취하여 좌변이 우변과 같은 범위$\\,[-\\infty,\\infty]$값을 가질 수 있도록 넓혀줍니다.\n",
    "$$\\text{ln}\\frac{\\mu_{k,i}}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = \\text{ln}\\frac{Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i;W_k)}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = W_k^TX_i$$\n",
    "$\\quad \\quad $분모가 마지막 K번째 클래스에 대한 확률임을 유의!$<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style =\"font-size:15pt\"> **3. 임의의 모수 $\\mu_{k,i}$에 대한 표현**</span>\n",
    "\\begin{aligned}\n",
    "\\mu_{k,i} = Pr(Y_i = (0,\\dots,0,1_{k-th},0,\\dots,0|X_i;W_k) = Pr(Y_i = K|X_i)e^{X_iW_k}\n",
    "\\end{aligned}\n",
    "\n",
    "<span style = \"font-size:15pt\"> **4.마지막 모수 $\\mu_K = Pr(Y_i = (0,\\dots,0,1)|X_i)$를 계산. 확률의 합 = 1이라는 제약조건 활용.**</span>\n",
    "\\begin{aligned}\n",
    "&Pr(Y_i = K|X_i) = 1- \\sum_{k=1}^{K-1}Pr(Y_i = K|X_i)e^{X_iW_k} = 1-Pr(Y_i = K|X_i)\\sum_{k=1}^{K-1}e^{X_iW_k} \\\\\n",
    "&\\Longleftrightarrow Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{k=1}^{K-1}e^{X_iW_k}}\n",
    "\\end{aligned}\n",
    "\n",
    "<span style = \"font-size:15pt\">**5. 임의의 모수 $k$에 대하여 다시 정리**</span>\n",
    "\\begin{aligned}\n",
    "&\\mu_{k,i}=Pr(Y_i = k|X_i) = Pr(Y_i = K|X_i)e^{X_iW_k} = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}}\\\\\n",
    "&\\text{인덱스 겹치므로 시그마의 $k \\rightarrow j$}\n",
    "\\end{aligned}\n",
    "\n",
    "<span style = \"font-size:15pt\">**최종적으로 카테고리분포의 모수는 다음과 같이 표현할 수 있다.**</span><br>\n",
    "\\begin{aligned}\n",
    "&\\mu_{k,i}=Pr(Y_i = k|X_i) = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}} \\text{(단, $k != K$)}\\\\\n",
    "&\\mu_{K,i}=Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{j=1}^{K-1}e^{X_iW_k}}\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **estimation**\n",
    "다항로지스틱회귀의 파리미터 추정은 여려가지 방법이 가능하다고 한다.(추후에 더 공부)<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:#00CC66\">**Softmax Regression**</span>\n",
    "다항 로지스틱회귀에서는 마지막 클래스에 대한 모수를 추정할때에는 회귀계수를 따로 사용하지 않았습니다. 소프트맥스 회귀에서는 마지막 클래스에 대해서도 회귀계수를 사용합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **가정**\n",
    "소프트맥스회귀의 가정은 로지스틱회귀의 가정과 같다.각 datapoint에서의 종속변수의 값은 카테고리분포를 따르는 확률변수에서 샘플링되었으며 카테고리분포의 모수는 각 datapoint마다 변하는 설명변수와 회귀계수(가중치)의 일차결합으로 표현됩니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **X,W의 선형조합을 포함한 모수의 표현 유도하기**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "소프트맥스 회귀마찬가지로 추정하고자 하는 모수를 설명변수와 가중치의 일차결합이 포함된 항으로 표현합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "먼저 설명변수와 가중치의 일차결합형태로 모수를 나타냅니다. 임의의 i번째 관측치가 각각의 범주에 $(1,2,...,K)$ 속할 확률을 의미하는 모수는 다음과 같습니다.<br>\n",
    "\\begin{aligned}\n",
    "&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n",
    "&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n",
    "&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n",
    "&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n",
    "&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n",
    "&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i-\\text{ln}Z \\\\\n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n",
    "&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\ \n",
    "&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i \\\\  \n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n",
    "&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n",
    "&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n",
    "&where,\\\\\n",
    "&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n",
    "&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n",
    "&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n",
    "&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이렇게 나타내고 보니 좌변과 0~1사이의 수만 갖지만 우변은 어떤 수던지 나올 수 있습니다. 범위를 맞춰 주기 위해서 좌변에 로그를 씌워 로그확률로 만들어줍니다. 추가적으로 우변에 $-lnZ$라는 normalizating factor를 더해줍니다. 이는 카테고리분포의 모수의 합이 1이되도록 하는 확률질량함수의 특징을 유지하기 위해서입니다.<br>\n",
    "\\begin{aligned}\n",
    "&\\text{ln}\\mu_{1,i} = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n",
    "\\\\\n",
    "&\\text{ln}\\mu_{2,i} = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n",
    "\\\\\n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n",
    "&\\text{ln}\\mu_{k,i} = w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{M,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i-\\text{ln}Z \\\\\n",
    "\\\\\n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n",
    "&\\text{ln}\\mu_{K-1,i} = w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i-\\text{ln}Z \\\\\n",
    "\\\\\n",
    "\\end{aligned}\n",
    "|\n",
    "따라서,임의의 $k$번째 모수는 다음과 같습니다.<br>\n",
    "$$\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th}|X_i;W_k) = \\frac{1}{Z}e^{W_k^TX_i}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "카테고리분포의 제약조건 즉,모수는 각각의 범주에 속할 확률을 나타내므로 총합이 1임을 활용합니다. 이를 활용하여 Z를 표현하면 다음과 같습니다.<br>\n",
    "\\begin{aligned}\n",
    "&\\sum_{k=1}^{K}{\\mu_{k,i}} =\\sum_{k=1}^{K}{Pr(Y_i=k)}= \\frac{1}{Z}\\sum_{k=1}^{K}e^{W_k^TX_i} = 1\\\\\n",
    "&\\Longleftrightarrow Z = \\sum_{k=1}^{K}e^{W_k^TX_i}\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "최종적으로, 결과를 정리하면 다음과 같습니다.\n",
    "- 추정하고자하는 카테고리분포의 모수는 $\\mu_k$는 $W_k$와 $X_i$의 일차결합으로 표현되었습니다. 이는 소프트맥스 함수이므로 소프트맥스 회귀라는 이름이 붙었습니다.<br>\n",
    "$$\\mu_{c,i}(X_i;W) = Pr(Y_i = (0,0,\\dots,1_{c-th},0,\\dots,0)|X_i;W_k) = \\frac{e^{W_c^TX_i}}{\\sum_{k=1}^{K}e^{W_k^TX_i}} = softmax(c,W_1^TX_i,W_2^TX_i,\\dots,W_K^TX_i)$$\n",
    "- 카테고리분포의 위에서 구한 모수로 다시 정리하면 확률질량 함수는 새로운 모수 $W_1,W_2,\\dots,W_K$를 가집니다.(인덱스 $k->j,c->k$)\n",
    "$$Y_i \\sim Cat(y|X_i;W_1,W_2,\\dots,W_K) = \\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color:\">**MLE**</span>\n",
    "여기까지의 과정으로부터 카테고리분포의 모수는 설명변수와 가중치(회귀계수)의 일차결합으로 표현되며 또한 확률질량함수가 새로운 모수 $W = (W_1,W_2,\\dots,W_K)$로 표현되었습니다.만약 카테고리분포의 모수만 추정할 수 있다면 우리는 데이터포인트가 어떤 범주에 속할 확률이 가장 높은지 알 수 있으며 범주를 분류할 수 있습니다. 여기서는 카테고리분포의 모수$W$를 MLE로 추정합니다.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "확률분포에서 임의의 모수$W = (W_1,W_2,\\dots,W_K)$를 가정할 때, 확률변수 $Y_1,Y_2,\\dots,Y_N$으로부터 realization인 $y_1,y_2,\\dots,y_N$이 나올 가능도는 다음과 같습니다.<br>\n",
    "\\begin{aligned}\n",
    "&\n",
    "\\begin{aligned}\n",
    "L({W};X_i|y_1,y_2,\\dots,y_n) &= Pr_{Y_1,Y_2,\\dots,Y_N}(y1,y2,\\dots,y_n|X_i;W)\\\\\n",
    "&= \\prod_{i=1}^{N}Pr_{Y_i}(Y_i=y_i|X_i;W) \\\\\n",
    "&= \\prod_{i=1}^{N}\\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\\\\\n",
    "\\end{aligned}\n",
    "\\\\\n",
    "&\\text{where } \\{W\\} = \\{W1,W2,\\dots,W_N\\}\n",
    "\\end{aligned}\n",
    "\n",
    "위와 같은 가능도를 최소화 하는 $W$를 찾는 것이 목적입니다.다음과 같습니다\n",
    "\\begin{aligned}\n",
    "\\overset{*}{\\{W\\}} = \\underset{\\{W\\}}{\\text{argmax}} \\prod_{i=1}^{N}\\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13 (main, Aug 25 2022, 23:51:50) [MSC v.1916 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0478b8cb1c47bafb71305148a49d30528a4d9c22ca2de336c01aa5a8230a459a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
