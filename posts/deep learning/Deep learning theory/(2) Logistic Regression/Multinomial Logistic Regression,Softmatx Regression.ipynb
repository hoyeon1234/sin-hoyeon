{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "author: 신호연\n",
    "date: 2022-12-26\n",
    "title: \"Multinomial Logistic Regression\"\n",
    "format: html\n",
    "categories: [Deep learning]\n",
    "---\n",
    "다항로지스틱 회귀에 대한 정리"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem Setting\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\text{Given, }D = {(x_{1,i},x_{2,i},\\dots,x_{M,i},y_i})_{i=1}^{i=n} \\\\\n",
    "&\\text{where, $y_i$는 각각의 datapoint의 클래스를 원핫인코딩한 벡터} \\\\ \\\\\n",
    "&\\text{Goal : x가 입력될 때, 어떤 범주(y값)에 속하는지 예측하는 모형 만들기}\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multinomial Logistic Regression"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "카테고리 분포의 모수 $\\mu_i$는 각각의 범주에 속할 확률이 모두 들어있다. 그러므로,모수 $\\mu_i$를 추정하여 대응하는 확률이 가장 높은 클래스를 주어진 데이터가 속하는 범주로 한다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가정\n",
    "각각의 관측치에서 샘플$y_i$는 확률변수 $Y_i$의 realization(실현,실현된값)이다. 확률변수 $Y_i$는 관측치에 포함된 $x$를 조건으로 하는 카테고리분포를 따른다.<br>\n",
    "\n",
    "\\begin{aligned}\n",
    "Y_i|x_{1,i},x_{2,i},\\dots,x_{M,i} \\sim \\text{Cat}(y|x_{1,i},x_{2,i},\\dots,x_{M,i};\\mu_i) \n",
    "& = \n",
    "\\begin{cases}\n",
    "\\mu_{1,i} \\text{ if } y = (1,0,\\dots,0,0) \\\\\n",
    "\\mu_{2,i} \\text{ if } y = (0,1,\\dots,0,0) \\\\\n",
    "\\quad\\quad \\vdots \\\\\n",
    "\\mu_{K,i} \\text{ if } y = (0,0,\\dots,0,1) \\\\ \n",
    "\\end{cases} \\\\\n",
    "&= \\mu_{1,i}^{y_1}\\mu_{2,i}^{y_2},\\dots,\\mu_{K,i}^{y_K} \\\\\n",
    "&= \\prod_{K=1}^{K}\\mu_{K,i}y_{K,i} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "\\begin{aligned}\n",
    "&\\text{where, }\\mu_i = {\\mu_{1,i},\\mu_{2,i},\\dots,\\mu_{K,i}} \\\\\n",
    "&\\mu_{1,i} = Pr(Y_i|x_{1,i},\\dots,x_{M,i} = (1,0,\\dots,0)) \\\\\n",
    "&\\mu_{2,i} = Pr(Y_i|x_{1,i},\\dots,x_{M,i} = (0,1,\\dots,0)) \\\\\n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\vdots \\\\\n",
    "&\\mu_{K,i} = Pr(Y_i|x_{1,i},\\dots,x_{M,i} = (0,0,\\dots,0,1)) \\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유도"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "선형회귀,이항로지스틱회귀의 핵심아이디어는 추정하려는 모수를 독립변수와 가중치의 linear combination(또는 linearcombination이 포함된)으로 놓는 것이다. 다항로지스틱회귀도 마찬가지로 추정하려는 모수를 독립변수와 가중치의 linear combination이 포함된 놓는다. 다만 차이점은 모수가 이번에는 K개이기 때문에 가중치가 더 많이 필요하다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 독립변수와 가중치의 linear combination이 K개의 모수를 표현하는 선형방정식을 만든다.\n",
    "\\begin{aligned}\n",
    "&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n",
    "&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i\\\\\n",
    "&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n",
    "&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i\\\\\n",
    "&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n",
    "&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i\\\\\n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n",
    "&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\ \n",
    "&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i {\\text{ (임의의 k번째 항)}}\\\\  \n",
    "&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n",
    "&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n",
    "&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n",
    "&where,\\\\\n",
    "&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n",
    "&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n",
    "&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n",
    "&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n",
    "\\end{aligned}\n",
    "\n",
    "마지막 모수$Pr(Y_i=1|x_1,\\dots,x_K)$에 대한 가중치는 사용하지 않을 것이다. 확률의 합은 1이기 때문에 1-(나머지확률)하면 마지막 $K$번째 모수가 구해지기 때문이다.<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. (3~4). OddsRatio비슷한 무언가  + Logit transform()\n",
    "좌변을 Odds Ratio(비슷한 무언가,엄밀히 Odds Ratio는 아님) + Logit transform을 취하여 좌변이 우변과 같은 범위$\\,[-\\infty,\\infty]$값을 가질 수 있도록 넓혀줍니다.\n",
    "$$\\text{ln}\\frac{\\mu_{k,i}}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = \\text{ln}\\frac{Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i;W_k)}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = W_k^TX_i$$\n",
    "$\\quad \\quad $분모가 마지막 K번째 클래스에 대한 확률임을 유의!<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. 임의의 모수 $\\mu_{k,i}$에 대한 표현\n",
    "\\begin{aligned}\n",
    "\\mu_{k,i} = Pr(Y_i = (0,\\dots,0,1_{k-th},0,\\dots,0|X_i;W_k) = Pr(Y_i = K|X_i)e^{X_iW_k}\n",
    "\\end{aligned}\n",
    "\n",
    "4. 마지막 모수 $\\mu_K = Pr(Y_i = (0,\\dots,0,1)|X_i)$를 계산. 확률의 합 = 1이라는 제약조건 활용.\n",
    "\\begin{aligned}\n",
    "&Pr(Y_i = K|X_i) = 1- \\sum_{k=1}^{K-1}Pr(Y_i = K|X_i)e^{X_iW_k} = 1-Pr(Y_i = K|X_i)\\sum_{k=1}^{K-1}e^{X_iW_k} \\\\\n",
    "&\\Longleftrightarrow Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{k=1}^{K-1}e^{X_iW_k}}\n",
    "\\end{aligned}\n",
    "\n",
    "5. 임의의 모수 $k$에 대하여 다시 정리\n",
    "\\begin{aligned}\n",
    "&\\mu_{k,i}=Pr(Y_i = k|X_i) = Pr(Y_i = K|X_i)e^{X_iW_k} = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}}\\\\\n",
    "&\\text{인덱스 겹치므로 시그마의 $k \\rightarrow j$}\n",
    "\\end{aligned}\n",
    "\n",
    "최종적으로 카테고리분포의 모수는 다음과 같이 표현할 수 있다.<br>\n",
    "\\begin{aligned}\n",
    "&\\mu_{k,i}=Pr(Y_i = k|X_i) = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}} \\text{(단, $k != K$)}\\\\\n",
    "&\\mu_{K,i}=Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{k=1}^{K-1}e^{X_iW_k}}\n",
    "\\end{aligned}\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## estimation\n",
    "다항로지스틱회귀의 파리미터 추정은 여려가지 방법이 가능하다고 한다.(추후에 더 공부)<br>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 가정\n",
    "소프트맥스회귀의 가정은 로지스틱회귀의 가정과 같다.각 datapoint에서의 종속변수 $y_i$의 값은 $\\mu_i$를 모수로 하는 카테고리분포를 따른다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 유도"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0478b8cb1c47bafb71305148a49d30528a4d9c22ca2de336c01aa5a8230a459a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
