{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "---\n",
    "title: \"[Probability & Statistics] Bias Variance Tradeoff\"\n",
    "format: \n",
    "  html:  \n",
    "    linkcolor: blue\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style = \"color:black\"> **Bias Variancce Tradeoff**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color:black\"> **Intuition**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `supervised learning`의 목적 중 하나는 unknown data에 대해서 `generalization`을 잘 하도록 모델을 학습시키는 것입니다.\n",
    "- 이는 `expected generalization error`에 포함하는 `bias`와 `variance`를 줄이도록 학습하는 것과 같습니다..\n",
    "- 그러나 학습과정에서 `bias`와 `variance`는 서로간에 상충합니다. \n",
    "- 풀어쓰자면 `bias`가 작아지면 `variance`는 커지고 `variance`가 작아지면 `bias`가 커지는 `bias variance tradeoff` 현상이 나타납니다.\n",
    "- bias와 variance를 조금 구체적으로 살펴보면 다음과 같습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**bias**\n",
    "\n",
    "- `bias`는 학습알고리즘에서의 잘못된 가정으로 인해 발생하는 `오류`입니다.<br> \n",
    "- `High bias`는 학습알고리즘이 **feature와 target간의 `regularities` 를 놓치게 합니다.**(`underfitting의 원인`)<br>\n",
    "- 이는 **`unknown data`에 대해서 `generalization error`가 나오게** 합니다.\n",
    "\n",
    "**vairance**\n",
    "\n",
    "- `variance`는 훈련데이터에서의 작은 변동에 학습알고리즘이 민감하게 반응하여 발생하는 `오류`입니다.<br>\n",
    "- `High variance`는 학습알고리즘이 **feature와 target간의 `regularities` 를 너무 과하게 학습해서 발생합니다.**(`overfitting의 결과`)\n",
    "- 마찬가지로 이는 **`unknown data`에 대해서 `generalization error`가 나오게** 합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./bias%20variance%20tdoff%20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1번그림\n",
    "\n",
    "- bias가 작아서 알고리즘이 어떤 방향으로 편향되지 않습니다.\n",
    "- variance또한 작아서 데이터의 변화에 의한 학습된 알고리즘의 변동,편차는 거의 없습니다.\n",
    "- 전체적으로 bias,variance가 작기 때문에 학습된 알고리즘은 True값에 가깝습니다.\n",
    "\n",
    "2번그림\n",
    "\n",
    "- bias가 크기 때문에 오른쪽 방향으로 편향되어 있습니다.<br>\n",
    "- variance는 작아서 데이터의 변화에 의한 학습된 알고리즘의 변동,편차는 거의 없습니다.<br>\n",
    "\n",
    "3번그림\n",
    "\n",
    "- bias가 작아서 어떤 방향으로 편향되지 않습니다. \n",
    "- variance는 크기때문에 작은 데이터의 변화에 의한 학습된 알고리즘의 성능변화가 매우 심합니다.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color:black\"> **Derivation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = \"color:black\">**Notation**\n",
    "- 위키피디아의 notation과 강의자료 중 notation이 다르고 이외에도 다른부분이 많이 헷갈리기에 notation을 먼저 정리합니다.\n",
    "- $F^*(x) = f(x) = f,\\hat{F}(x) = \\hat{f}(x;D) = \\hat{f}$\n",
    "- $\\mathbb{E}_D = \\mathbb{E}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style = \"color:black\"> **Background**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 통계학에서 데이터는 다음과 같은 `additive error model`로부터 sampling됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\\begin{aligned}\n",
    "&y = f(x) + \\epsilon ,\\,\\, \\epsilon \\overset{\\text{i.i.d}}{\\sim} \\mathcal{N}(0,\\sigma^2)\\\\\n",
    "&\\text{where} \\\\\n",
    "&f(x) : \\text{target function that we are trying to learn,but do not really know} \\\\\n",
    "&\\epsilon : \\text{statistical error}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- $f(x)$는 모집단에서 두 변수 $x,y$사이의 관계를 대표하는 곡선이며 우리가 학습 알고리즘을 통해 찾고자 하는 target function입니다.\n",
    "- target function을 찾게되면 우리는 unknown data에 대해서도 좋은 결과를 얻을 수 있습니다.(good generalization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./fhat.png)<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "출처 : [Pilsung Kang 교수님 깃허브](https://github.com/pilsung-kang/Business-Analytics-IME654-/blob/master/04%20Ensemble%20Learning/04-2_Ensemble%20Learning_Bias-Variance%20Decomposition.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 모집단으로부터 sampling되는 data set에 의해서 같은 알고리즘이라도 다르게 학습이 됩니다.\n",
    "- 만약 서로다른 data set가 N개가 존재한다면 true function인 $f(x)$에 대해 서로다른 N개의 근사값인 $\\hat{f}_1(x),\\hat{f}_2(x),\\dots,\\hat{f}_n(x)$를 구합니다.\n",
    "- 개인적으로 각각의 datapoint는 $p(x,y)$를 따르므로 dataset $D$도 확률변수이며 이로부터 random한 $\\hat{f}(x)$도 확률변수라고 생각합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style = \"color:black\"> **Derivation**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `bias-variance decomposition`은 `bias`,`variance`,`irreducible error`로 구성되는 학습알고리즘의 `expected generalization error`를 살펴봄으로서 학습알고리즘을 분석하는 방법입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
