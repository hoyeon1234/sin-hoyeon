[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "전북대학교 IT응용시스템 공학과 신호연 sinhoyeon0514@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HIHO",
    "section": "",
    "text": "Untitled\n\n\n\n\n\n\n\n\n\n \n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\n \n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\n\n\n\n\nDQN review\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\nDQN review\n\n\n\n1/20/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[PRML 읽기] 1 - 확률론 개요\n\n\nSum rule, Product rule, Baye’s rule,random variable independence\n\n\n\n1/15/23\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n[PRML 읽기] 2 - 확률론 개요(작성중 …)\n\n\nprobability density, expectation and covariance, Bayesian probabilities,Gausian distribution\n\n\n\n1/15/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimension Reduction using Auto Encoder with pytorch\n\n\n\n1/12/23\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nimport\n\n\n\n1/12/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스]Lv.1시저암호\n\n\n\n1/9/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스]Lv1.이상한 문자 만들기\n\n\n\n1/9/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스]Lv1.크기가 작은 부분 문자열\n\n\n\n1/9/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeast Squares\n\n\n열공간(column space)과 정사영(projection)으로 접근한 최소제곱법(least squares)\n\n\n\n1/8/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnp.meshgrid\n\n\n\n1/8/23\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n확률론 용어정리\n\n\nrandom variable,event,sample space,probability distribution,randomsample,realization\n\n\n\n1/8/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinite Difference Method with np.gradient\n\n\n\n1/7/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotly 시각화 모음\n\n\n\n1/6/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAx=b의 해의 갯수 알아내기\n\n\n\n1/5/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrank & null space(kernel)\n\n\n\n1/5/23\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nLinear Combination & Span\n\n\n\n1/3/23\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n행렬곱에 대한 여러가지 관점\n\n\n\n1/2/23\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nMultinomial Logistic Regression & Softmax Regression\n\n\n\n12/30/22\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n카테고리 분포\n\n\n\n12/30/22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum likelyhood estimation\n\n\n\n12/29/22\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n확률분포에서 ;와|의 사용\n\n\n\n12/27/22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n12/26/22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n파이썬 - 변수,할당문,인터닝\n\n\n\n12/25/22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n12/24/22\n\n\n\n\n\n\n\n\n\n\n \n\n\n\npytorch로 Rnn구현하기\n\n\n\n12/24/22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/coading test/programmers시저암호.html",
    "href": "posts/coading test/programmers시저암호.html",
    "title": "[프로그래머스]Lv.1시저암호",
    "section": "",
    "text": "문제\n\n\n\n나의 풀이\n\ndef solution(s, n):\n    upper_ch = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    answer = \"\"\n    for el_s in s:\n        #1.맨 마지막 리턴을 위해 원래 문자가 소문자인지 대문자인지 기억 and 대문자에서 검색할것이기 때문에 대문자로 변환\n        #대문자로 변환된 문자열 s의 각각의 문자,대소문자 여부\n        if el_s == \" \":\n            answer += \" \"\n        elif el_s.isupper() == True:\n            was_upper = True\n        else:\n            was_upper = False\n            el_s = el_s.upper()\n        #2.인덱스 숫자가 upper_ch에서 벗어날때 아닐때 처리\n        for idx,up_ch in enumerate(upper_ch):\n            if el_s == up_ch and idx + n <= len(upper_ch)-1:\n                find_idx = idx+n\n                if was_upper == True:\n                    answer += upper_ch[find_idx]\n                else:\n                    answer += upper_ch[find_idx].lower()        \n            elif el_s == up_ch and idx + n > len(upper_ch)-1:\n                find_idx = n-len(upper_ch[idx:])\n                if was_upper == True:\n                    answer += upper_ch[find_idx]\n                else:\n                    answer += upper_ch[find_idx].lower()  \n    return answer\n\n\n\n다른 풀이\n\ndef caesar(s, n):\n    lower_list = \"abcdefghijklmnopqrstuvwxyz\"  \n    #소문자도 리스트로 만듦 \n    #좋은점->대소문자 여부를 기억하는 코드 불필요(조건문)\n    #안좋은점->소문자로 이뤄진 리스트를 만드는데 그만큼의 메모리 필요\n    upper_list = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \n    \n    \n    \n    result = [] \n    #문자열들을 저장할 list를 만듦\n    #mutable로 붙이는 것과 immutable로 붙이는 것 차이\n    #속도 -> \n    #mutable이 더 빠름,\n    #그러나 garbage collector도 고려시 스캔범위가 너무커서 느려질수도?\n    #메모리 -> immutable이 더 적게들을 듯(리스트는 이중포인터같은 구조라서 이렇게 예상됨)\n    for i in s:\n        if i == \" \":\n            result.append(\" \")\n        elif i.islower() is True:\n            new_ = lower_list.find(i) + n\n            result.append(lower_list[new_ % 26]) \n            #나머지로 계산하는 방식,이게 더 간단하고 좋은듯\n            #반복문이 문자열에 대해서 돌다보니 문자열과 문자열의 인덱스 위주로 너무 생각함\n            #어떤 숫자(여기서는 인덱스)보다 크거나 같을때에 다시 0부터 줘야하는 상황? -> 나머지 활용\n        else:\n            new_ = upper_list.find(i) + n\n            result.append(upper_list[new_ % 26])\n    return \"\".join(result)\n\n나중에 볼 링크 링크1 링크2 링크3"
  },
  {
    "objectID": "posts/coading test/programmers이상한문자만들기.html",
    "href": "posts/coading test/programmers이상한문자만들기.html",
    "title": "[프로그래머스]Lv1.이상한 문자 만들기",
    "section": "",
    "text": "문제\n\n\n\n나의 풀이\n\ndef solution(s):\n    words = s.split(\" \") #스플릿 함수 헷갈리는 부분이 있었음 - 정리\n    answer = \"\"\n    for wd in words:\n        for idx,chr in enumerate(wd):\n            print(idx,chr)\n            if idx % 2 == 0:\n                chr = chr.upper()\n            else:\n                chr = chr.lower()\n            answer += chr\n        answer+= \" \"\n    return answer[:-1]\n\n\n\nsplit 메서드(함수)\n\nt=\"sdsa  sd\"\nhelp(t.split)\n\nHelp on built-in function split:\n\nsplit(sep=None, maxsplit=-1) method of builtins.str instance\n    Return a list of the words in the string, using sep as the delimiter string.\n    \n    sep\n      The delimiter according which to split the string.\n      None (the default value) means split according to any whitespace,\n      and discard empty strings from the result.\n    maxsplit\n      Maximum number of splits to do.\n      -1 (the default value) means no limit.\n\n\n\nstring객체의 인스턴스 즉,문자열에 대해서만 사용할 수 있는 메서드 sep파라미터에 전달한 인수를 구분자로 사용하여 문자열안에 있는 단어들을 가져옴. 가져온 단어들은 리스트의 원소가 되어 반환됨 Parameter - sep - …\nReturn - list[word1,word2,…] (구분자를 통하여 구분된 단어들,기본값은 공백(space))\n\n\n예시\n\n#문자열 내에 구분기준이 없는 경우?\n#sep = \" \"으로 기본값으로 설정되어 있음. 공백이 없으므로 그냥 문자열을 리스트에 넣어서 반환\n_t = \"abcde\"\nanswer = _t.split()\nprint(answer)\n\n['abcde']\n\n\n\n#문자열 내에 구분기준이 있는 경우?\n_t = \"ab cd esda dg\"\nanswer = _t.split()\nprint(answer)\n\n['ab', 'cd', 'esda', 'dg']\n\n\n\n_t = \"abtcdtesdatdg\"\nanswer = _t.split(\"t\")\nprint(answer)\n\n['ab', 'cd', 'esda', 'dg']\n\n\n\n_t = \"ab;;cd;;esd;;atdg\"\nanswer = _t.split(\";;\")\nprint(answer)\n\n['ab', 'cd', 'esd', 'atdg']"
  },
  {
    "objectID": "posts/coading test/programmers크기가작은부분문자열.html",
    "href": "posts/coading test/programmers크기가작은부분문자열.html",
    "title": "[프로그래머스]Lv1.크기가 작은 부분 문자열",
    "section": "",
    "text": "문제\n\n\n\n나의 풀이\n\ndef solution(t, p):\n    t_len = len(t);p_len = len(p)\n    answer = 0\n    p = int(p)\n    for idx in range(t_len-p_len+1):\n        num = int(t[idx:idx+p_len])\n        if num <= p:\n            answer+=1\n        else:\n            pass\n    return answer"
  },
  {
    "objectID": "posts/DL/Auto Encoder Dimension Reduction.html",
    "href": "posts/DL/Auto Encoder Dimension Reduction.html",
    "title": "Dimension Reduction using Auto Encoder with pytorch",
    "section": "",
    "text": "import torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nimport torch\ntest_path = \"./test.csv\"\ntrain_path = \"./train.csv\"\n\n\n# %pip install plotly (jupyter notebook)\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n#pio.renderers.default = 'iframe_connected'\n#pio.renderers.default = \"vscode\"\npio.renderers.default = \"plotly_mimetype+notebook\""
  },
  {
    "objectID": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension3",
    "href": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension3",
    "title": "Dimension Reduction using Auto Encoder with pytorch",
    "section": "encoding dimension=3",
    "text": "encoding dimension=3\n\ntraining autoencoder\n\ntorch.manual_seed(201711375)\nautoencoder_3 = AutoEncoder(47,3)\nloss_fn = torch.nn.MSELoss()\nrelu = torch.nn.LeakyReLU()\noptimizer = torch.optim.Adam(autoencoder_3.parameters(),lr=0.001)\n\n\nfor epoch in range(20000):\n    #1.yhat\n    out = autoencoder_3(X_train_ohe)\n    #2\n    loss = loss_fn(out,X_train_ohe)\n    #3\n    loss.backward()\n    if epoch % 10000 == 0:\n        print(f\"epoch:{epoch} loss:{loss.tolist()}\")\n    #4\n    optimizer.step()\n    optimizer.zero_grad()\n\nepoch:0 loss:0.5274774432182312\nepoch:10000 loss:0.11526338756084442\n\n\n\n\nvisualization\n\nclass_map_inv = {}\nfor key,value in class_map.items():\n    class_map_inv[value] = key\nclass_map_inv\n\n{0: 'A', 1: 'B', 2: 'C'}\n\n\n\ndt_dim3 = pd.DataFrame({\"class\":Y_train_ohe})\ndt_dim3 = pd.concat([pd.DataFrame(np.array(autoencoder_3.encoder(X_train_ohe).tolist())),dt_dim3],axis=1)\ndt_dim3 = dt_dim3.rename(columns = {0:\"x\",1:\"y\",2:\"z\"})\n\n\ncount = 0\ndata = []\nfor cl in dt_dim3[\"class\"].unique():\n    cond = dt_dim3[\"class\"] == cl\n    _data = dt_dim3.loc[cond,:]\n    x = _data.x.tolist()\n    y = _data.y.tolist()\n    z = _data.z.tolist()\n    if count == 0:\n        color = \"red\"\n    elif count == 1:\n        color = \"blue\"\n    else:\n        color = \"black\"\n    trace=go.Scatter3d(\n        x=x,\n        y=y,\n        z=z,\n        mode=\"markers\",\n        marker = dict(color = color,size=2),\n        name = str(class_map_inv[cl])\n        )\n    data.append(trace)\n    count+=1\n\nlayout = go.Layout(title=dict(text = \"3-dimension \"))\n\n#4. figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()"
  },
  {
    "objectID": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension10",
    "href": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension10",
    "title": "Dimension Reduction using Auto Encoder with pytorch",
    "section": "encoding dimension=10",
    "text": "encoding dimension=10\n\ntorch.manual_seed(201711375)\nautoencoder_3 = AutoEncoder(47,10)\nloss_fn = torch.nn.MSELoss()\nrelu = torch.nn.LeakyReLU()\noptimizer = torch.optim.Adam(autoencoder_3.parameters(),lr=0.001)\n\n\nfor epoch in range(40000):\n    #1.yhat\n    out = autoencoder_3(X_train_ohe)\n    #2\n    loss = loss_fn(out,X_train_ohe)\n    #3\n    loss.backward()\n    if epoch % 10000 == 0:\n        print(f\"epoch:{epoch} loss:{loss.tolist()}\")\n    #4\n    optimizer.step()\n    optimizer.zero_grad()\n\nepoch:0 loss:0.3828417658805847\nepoch:10000 loss:0.060432590544223785\nepoch:20000 loss:0.06043253839015961\nepoch:30000 loss:0.060432031750679016"
  },
  {
    "objectID": "posts/DL/Linear Regression.html",
    "href": "posts/DL/Linear Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "선형회귀에 대해서 정리한 글입니다."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#데이터에-대한-가정-모델링",
    "href": "posts/DL/Linear Regression.html#데이터에-대한-가정-모델링",
    "title": "Linear Regression",
    "section": "데이터에 대한 가정 & 모델링",
    "text": "데이터에 대한 가정 & 모델링\n\n\nText(0.5, 1.0, 'n=200')\n\n\n\n\n\n우리가 가진 데이터를 관찰해봅시다. 가장크게 눈에 띄는 사실은 x와 y사이의 관계가 선형적이라는 점입니다. 따라서 간단한 선형모형으로 독립변수와 종속변수사이의 관계를 모델링할 수 있을 것 같습니다. 조금 더 세부적으로 들어가서 각각의 x값에 대해서 y값을 관찰해 봅시다. 첫번째로 알 수 있는 점은 동일한 x값에 대해서도 서로다른 y값을 가지는 점들 데이터가 많이 있다는 것입니다. 이로부터 \\(y\\)가 \\(x\\)뿐만아니라 또다른 확률변수 \\(\\epsilon\\)의 값에 의해 결정된다는 것을 알 수 있습니다. 두번째로 x값에 의해서 찍히는 점의 위치(y)의 양상이 다르다는 점을 알 수 있습니다. x가 작으면 일반적으로 y는 낮은위치에서 점이 찍히고 x가 크면클수록 일반적으로 높은 위치에서 점이 찍히는 것을 알 수 있습니다.\n데이터를 관찰하면서 얻은 사실로부터 \\(x\\)와 \\(y\\)사이의 관계를 수학적으로 모델링 해보겠습니다. 첫번째 사실로부터 우리는 동일한 \\(x\\)라도 각각의 데이터에 대해서 어떤 또다른 값이 더해짐을 알 수 있습니다. 이렇게 뽑힐때마다 그 값이 다른 변수는 확률변수 \\(\\epsilon_i\\)를 더해줌으로서 표현할 수 있습니다. 여기서 오차가 따르는 분포에 대해서 가정을 합니다. 오차는 평균이 0이고 분산이 \\(\\sigma^2\\)인 정규분포를 따르는 확률변수라고 가정합니다.\n또한 두번째 사실로부터 우리는 \\(x\\)와 \\(y\\)는 커지면 커지고 작아지면 작아지는 관계임을 알 수 있습니다. 이러한 관계를 표현할 수 있는 방법은 여러가지가 있지만 선형회귀에서는 선형으로 이 관계를 표현합니다.\n\\[\\begin{aligned}\n&Y_i = w_0 + w_1x_1 + \\dots + w_Mx_M + \\epsilon_i=x_iw + \\epsilon_i\\\\\n&\\text{where , }x_i = \\begin {bmatrix}0,x_{i,1},\\dots,x_{i,M} \\end {bmatrix} \\in \\mathbb{R^{1 \\times (M+1)},w = \\begin {bmatrix} w_0,w_1,\\dots,w_M\\end{bmatrix}}^T \\in \\mathbb{R^{(M+1)\\times 1}}\\\\\n\\quad\\quad\\quad &\\epsilon_i \\overset{i.i.d}{\\sim} \\mathcal{N}(0,\\sigma^2)\\,\\,(\\text{for } i=1,2,\\dots,N)\n\\end{aligned}\\]\n이렇게 \\(x_i\\)와 \\(Y_i\\)사이의 관계를 정할 경우 각각의 \\(Y_i\\)안에 확률변수 \\(\\epsilon_i\\)가 \\(Y_i\\)도 마찬가지로 정규분로를 따르는 확률변수로 그 값이 \\(\\epsilon\\)에 의해 독립변수의 값이 동일하더라도 추출할때마다 다를 수 있습니다. \\(Y_i\\)도 확률변수이므로 정규분포를 따르는 확률변수이므로 중심위치와 변동을 확인하기 위해 기댓값과 분산을 구해볼 수 있습니다. 이때 \\(x\\)는 주어져 있으므로 그때의 확률분포에 대해서 계산하면 다음과 같습니다.\n\\[\\begin{aligned}\n&\\mathbb{E}[Y_i] = \\mathbb{E}[w_0 + \\dots + w_Mx_M+\\epsilon_i] = w_0 + w_1x_1 + \\dots + w_Mx_M=x_iw \\\\\n&\\text{var}[Y_i] = \\text{var}[w_0 + \\dots + w_Mx_M+\\epsilon_i] = \\text{var}[\\epsilon_i] = \\sigma^2\n\\end{aligned}\\]\n위수식으로부터 각각의 \\(Y_i\\)에 대한 확률분포는 정규분포이며(\\(\\epsilon_i\\)에 의해) 기댓값은 \\(x\\)와 \\(w\\)에 의해 정해지지만 분산은 \\(\\sigma^2\\)으로 항상 동일하다는 점을 알 수 있습니다. 모두 정규분포를 따르지만 기댓값만 \\(x\\)에 의하여 달라집니다. 즉 다음과 같습니다.\n\\[\\begin{aligned}\n&Y_i|w;x_i \\sim \\mathcal{N}(x_iw,\\sigma^2)\\, \\text{ for } i = 1,2,\\dots,N\\\\\n&p(y_i|w;x_i) = 정규분포식\n\\end{aligned}\\]\n이미지\n여기까지 우리가 가진 데이터를 기반으로 \\(x\\)와 \\(Y\\)사이의 관계로 선형모형을 만들어봤습니다. 그렇다면 궁극적인 목적인 unseen data에 적절하게 \\(Y\\)값을 예측하려면 어떻게 해야할까요?만약 학습데이터로부터 적절히 \\(w\\)를 구할수만 있다면 주어진 입력\\(x\\)에 대하여 \\(Y\\)가 따르는 정규분포를 대략적으로 알 수 있고 분포가 최댓값을 가지는 곳의 y값인 xw를 예측값으로 하면 됩니다. 기하학적으로 생각했을때 \\(w\\)를 구한다면 데이터를 가장 잘 표현하는 직선(평면,초평면)을 얻은것이므로 입력x에 대하여 직선의 값을 읽어서 예측값으로 하면 됩니다. 두 경우 모두 \\(w\\)를 구해야하므로 우리의 목적은 이제 w를 구하는 것입니다."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#w에-대한-추정---mle",
    "href": "posts/DL/Linear Regression.html#w에-대한-추정---mle",
    "title": "Linear Regression",
    "section": "w에 대한 추정 - MLE",
    "text": "w에 대한 추정 - MLE\n가중치\\(w\\)는 그렇다면 어떻게 구해야 할까요? 어떻게 해야 적절한 \\(w\\)를 구할 수 있을까요? 우리가 가진것은 데이터셋 \\(X,y\\)이므로 이로부터 \\(w\\)를 구해야 합니다. 확률론에서 데이터셋으로부터 \\(w\\)를 구한다는 것은 주어진 데이터로부터 가장 나올 가능성이 높은 w를 찾자라는 말과 같습니다. 이 말은 다른말로 데이터셋이 주어졌다는 조건하에서 가장 확률분포를 최대화 하는 \\(w\\)를 찾자와 같습니다. 최대화 해야하는 조건부확률분포는 다음과 같습니다.\n\\[p(w|y;X)\\]\n하지만 우리는 위와같은 조건부 확률을 현재는 알지 못합니다. 그렇기에 먼저 베이즈 정리의 도움을 받아야 합니다. 베이즈 정리는 다음과 같습니다.\n\\[\\begin{aligned}\n&p(w|y,X) = \\frac{p(\\mathcal{D}|w)p(w)}{p(\\mathcal{D})} \\propto (\\mathcal{D}|w)p(w)\\\\\n&\\text{where, } \\mathcal{D} = (y_1,y_2,\\dots,y_n)\n\\end{aligned}\\]\n\\[\\begin{aligned}\n&p(w|\\mathcal{D}) = \\frac{p(\\mathcal{D}|w)p(w)}{p(\\mathcal{D})} \\propto (\\mathcal{D}|w)p(w)\\\\\n&\\text{where, } \\mathcal{D} = (y_1,y_2,\\dots,y_n)\n\\end{aligned}\\]\n\n\n\n\n\n\nNote\n\n\n\n여기서 는 우리에게 주어진 데이터셋이 아니라 위에서 데이터셋에서의 y값들만을 말합니다. MLE에서는 구하고자(추정하고자)하는 파라미터\\(w\\)는 확률변수가 따르는 어떠한 확률분포의 파라미터이고 그 값을 구하기 위해서 확률변수의 값(realizations)을 사용합니다. 여기서 확률분포로부터 나온 확률변수의 값은 (y_1,y_2,,y_n)입니다.\n\n\n오른쪽 수식에서 분모는 normalization constant라 하는 상수입니다. 그러므로 분자를 최대화하는 \\(w\\)를 구하면 되는데 MLE는 분자의\\(p(\\mathcal{D}|w)\\)를 최대화 하는 것을 목적으로 합니다. \\(p(\\mathcal{D}|w)\\)는 likelyhood(function)라고 하며 이렇게 \\(w\\)에 대해 추정하는 방법을 maximum likelyhood estimation이라고 부르는 이유입니다.\n그렇다면 조건부확률분포 \\(p(\\mathcal{D}|w)\\)는 어떻게 구해야 할까요? 먼저 \\(p(\\mathcal{D}|w)\\)는 w가 주어져있을때 = (y_1,y_2,,y_n)에 대한 확률입니다."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#sample-regression-modelvector-form",
    "href": "posts/DL/Linear Regression.html#sample-regression-modelvector-form",
    "title": "Linear Regression",
    "section": "sample regression model(vector form)",
    "text": "sample regression model(vector form)\n위와 같은 regression model로부터 sampling n개의 표본을 얻으면 다음과 같습니다. \\[\ny_1 = w_0 + w_1x_{11} + \\dots + w_mx_{1m} + \\epsilon_1\n\\] \\[\ny_2 = w_0 + w_1x_{21} + \\dots + w_mx_{2m} + \\epsilon_2\n\\] \\[\n\\vdots\n\\] \\[\ny_n = w_0 + w_1x_{n1} + \\dots +w_mx_{nm} + \\epsilon_n\n\\] \n샘플을 벡터-행렬로 표현해서 좀 더 간단히 나타낼 수 있습니다. \\[\n\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\dots &x_{1m}\\\\\n1 & x_{21} & x_{22} & \\dots &x_{2m}\\\\\n1 & x_{31} & x_{32} & \\dots &x_{3m}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n1 & x_{n1} & x_{n2} & \\dots & x_{nm}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nw_0\\\\\nw_1\\\\\nw_2\\\\\n\\vdots\\\\\nw_m\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\epsilon_0\\\\\n\\epsilon_1\\\\\n\\epsilon_2\\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n\\] \\[\n\\Longleftrightarrow\n\\bf{y} = \\bf{X}\\bf{W} + \\bf{\\epsilon}\n\\tag{1}\\]  위와 같이 샘플링한 표본을 나타내는 모형을 sample regression model이라고 합니다.\n\\(w_1=1,w_0=0\\)인 population regression model로부터 200개의 표본을 추출하여 시각화하면 다음과 같습니다. 선형회귀는 위와 같이 표본만 주어질때(given), 우리가 모르는 \\(w_1,w_0\\)를 추정하는 것입니다. 표본으로부터 \\(\\bf{W}\\)를 추정할 수 있다면 두 변수사이의 관계가 이렇겠구나라고 알 수 있게 됩니다.\n\n\n\n\n\n\nNote\n\n\n\n실제문제에서 population regression model의 가중치 \\(\\bf{w} = (w_0,w_,1,...,w_m)\\)는 정확히 알 수 없습니다. 우리는 표본을 통해서 가중치를 추정할 뿐입니다. 위에서는 샘플데이터를 만들기 위해 어쩔 수 없이 알게되었지만 실제문제에서는 알 수 없습니다."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#선형회귀의-loss-function",
    "href": "posts/DL/Linear Regression.html#선형회귀의-loss-function",
    "title": "Linear Regression",
    "section": "선형회귀의 Loss function",
    "text": "선형회귀의 Loss function\n2번에서 구한 추정값 \\(\\hat{\\bf{W}}\\)이 얼마나 틀린지,부정확한지 알려주는 함수를 Loss function 또는 Cost function이라고 합니다. 선형회귀에서의 Loss function은 일반적으로 MSE를 사용하며 주어진 샘플에서 잔차(residual,\\(\\hat{y}_i-y\\))들을 전부 제곱하여 더한 값입니다.\n(Loss function) \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2} = \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}}) = \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\)\nMSE와 같은 Loss function은 우리의 추정이 얼마나 틀렸는지를 나타내는 \\(\\hat{\\bf{W}}\\)에 대한 함수입니다. 그러므로, loss function을 가장 최소화 하는 \\(\\bf{\\hat{W}}\\)을 찾아내면 확률변수사이의 선형관계인 \\(\\bf{W}\\)를 알아낼 수 있습니다.\n\n\nText(110, 15, 'residual')"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#parameter-update",
    "href": "posts/DL/Linear Regression.html#parameter-update",
    "title": "Linear Regression",
    "section": "Parameter update",
    "text": "Parameter update\nn개의 독립변수를 가지는 다변수 스칼라 함수에 대한 Gradient는 수학적으로 다음과 같습니다.\n\\(\\nabla_{X}{f(x_1,x_2,...,x_n)} = (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\dots,\\frac{\\partial f}{\\partial x_n})\\) 다변수 스칼라 함수에 그레디언트를 취하면 벡터입니다.그러므로,그레디언트를 벡터(다변수)를 입력했을 때,벡터를 출력으로 하는 벡터함수라고 생각해도 무방합니다.중요한 사실은 임의의 공간상의 임의의 point \\(X\\)에서 스칼라함수에 대한 gradient of f = \\(-\\nabla_{X}{f}\\) 방향은 스칼라함수가 가장 급격하게 감소하는 방향이라는 사실입니다.(증명생략)\n위의 사실에 의하면,우리는 임의의 \\(\\hat{\\bf{W}}\\)에서 Loss function이 가장 급격하게 감소하는 방향을 찾을 수 있습니다. 그러므로 감소하는 방향을 찾고 이동하고 감소하는 방향을 찾고 이동하고 반복하다보면… 궁극적인 목적인 틀린정도를 최소화하는 즉,Loss function값이 가장 작은 \\(\\hat{\\bf{W}}\\)를 찾을 수 있습니다. \\(\\bf\\hat{W}\\)를 수정하는 구체적인 수식은 다음과 같습니다.\n(Gradient descent parameter update) \\(\\hat{\\bf{W}}_{t} = \\hat{\\bf{W}}_{t-1} - \\alpha\\times\\nabla_{W}{L}\\)\n\\(\\hat{\\bf{W}}_{t-1}\\)은 수정되기전의 가중치(벡터)이며 \\(\\hat{\\bf{W}_{t}}\\)는 파라미터를 한번 업데이트 한 후의 가중치(벡터)입니다. \\(t-1\\)의 \\(\\hat{\\bf{W_{t-1}}}\\)에 \\(-\\alpha\\times\\nabla_{W}{L}\\)를 더해줌으로서 \\(\\hat{\\bf{W}}_{t-1}\\)은 loss function이 가장 급격히(많이)감소하는 방향으로 이동하며 \\(\\hat{\\bf{W}}_{t}\\)가 됩니다. \\(\\alpha\\)는 학습률(learning rate)입니다. \\(\\hat{\\bf{W}}_{t-1}\\)과 곱해져서 얼마나 많이 또는 적게 움직일지를 결정합니다. 한번에 얼마나 이동할지에 비유한 “보폭”으로 생각할 수 있습니다.\n요약하자면, 경사하강법을 통하여 위와 같이 가중치\\(\\hat{\\bf{W}}\\)를 재귀적으로 업데이트 하면 loss function \\(L\\)이 가장 최소가 되는 지점의 \\(\\hat{\\bf{W}}\\)를 찾을 수 있습니다."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#mse에-대한-더-상세한-전개",
    "href": "posts/DL/Linear Regression.html#mse에-대한-더-상세한-전개",
    "title": "Linear Regression",
    "section": "MSE에 대한 더 상세한 전개",
    "text": "MSE에 대한 더 상세한 전개\nMSE를 더 상세히 전개하면 다음과 같습니다. \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2}\\) \\(= \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}})\\) \\(= \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\) \\(= \\frac{1}{n}(\\bf{y^T - \\hat{\\bf{W}}^{T}\\bf{X}^{T})(\\bf{y} - \\bf{X}\\bf{\\hat{W}}})\\) \\(= \\frac{1}{n}(\\bf{y^Ty-y^TX\\hat{W}} - \\hat{W}X^Ty + \\hat{W}^TX^TX\\hat{W})\\)\n여기서 \\(\\bf{y^TX\\hat{W}} \\in \\bf{R}^{1 \\times 1}\\) 이므로 \\(\\bf{y^TX\\hat{W}} = (\\bf{y^TX\\hat{W}})^T = (\\bf{\\hat{W}X^Ty})\\)가 성립합니다. 그러므로 MSE를 정리하면 다음과 같습니다. (MSE) \\(MSE = \\frac{1}{n}(\\bf{y^Ty -2\\hat{W}X^Ty + \\hat{W}^TX^TX\\hat{W}})\\)"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#gradient-descent에-대한-더-상세한-전개loss-mse일-경우",
    "href": "posts/DL/Linear Regression.html#gradient-descent에-대한-더-상세한-전개loss-mse일-경우",
    "title": "Linear Regression",
    "section": "Gradient Descent에 대한 더 상세한 전개(\\(Loss\\) = MSE일 경우)",
    "text": "Gradient Descent에 대한 더 상세한 전개(\\(Loss\\) = MSE일 경우)\n(Gradient of MSE) \\(\\nabla{L} = MSE\\) \\(= \\bf{\\frac{1}{n}\\frac{\\partial}{\\partial \\hat{W}}(\\bf{y^Ty - 2\\hat{W}^TX^T + \\hat{W}^TX^TX\\hat{W}})}\\) \\(= \\bf{\\frac{1}{n}}(\\bf{\\frac{\\partial}{\\partial \\hat{W}}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2\\hat{W}^{T}X^{T}y + \\frac{\\partial}{\\partial\\hat{W}}\\hat{W}^{T}X^{T}X\\hat{W})\\) \\(= \\bf{\\frac{1}{n}(\\frac{\\partial}{\\partial \\hat{W}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2y^TX\\hat{W} + \\frac{\\partial}{\\partial\\hat{W}}\\hat{W}^TX^TX\\hat{W})}\\) \\(= \\bf{\\frac{1}{n}[0 - 2X^Ty + (X^TX + X^TX)\\hat{W}]}\\) \\(= \\bf{\\frac{2}{n}X^T(X\\hat{W} - y)}\\)\n(parameter update) \\(\\bf{\\hat{W}_{t} = \\hat{W}_{t-1} - \\alpha \\times \\frac{2}{n}X^T(X\\hat{W} - y)}\\)"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#결과해석",
    "href": "posts/DL/Linear Regression.html#결과해석",
    "title": "Linear Regression",
    "section": "결과해석",
    "text": "결과해석\n200개의 샘플로부터 \\(\\bf{w}\\)를 추정하여 \\(\\hat{\\bf{w}}= (0.125,0.969)\\)를 얻었습니다. population regression model의 \\({\\bf{w}} = (w_0,w_1) = (0,1)\\)을 올바르게 추정했음을 알 수 있습니다. 아주 약간의 차이가 존재하는데 이 차이는 모집단에서 샘플을 더 얻거나 더 세밀하게 업데이트하면 최소화할 수 있습니다.\n\n#plt.title(\"w_1 : {} // w_0: {}\".format(round(W_hat[1].tolist()[0],3),round(W_hat[0].tolist()[0],3)))\nplt.title(\"Linear Regression\")\ntext=f\"What = ({round(t[0].tolist()[0],3)},{round(t[1].tolist()[0],3)})\"\nplt.plot(X[:,1],y,\"bo\",alpha=0.5)\nplt.plot(X[:,1],X@W_hat,\"r--\")\nplt.gca().axes.xaxis.set_visible(False)\nplt.gca().axes.yaxis.set_visible(False)\nplt.title(text)\n\nText(0.5, 1.0, 'What = (-0.125,0.969)')"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#참고자료",
    "href": "posts/DL/Linear Regression.html#참고자료",
    "title": "Linear Regression",
    "section": "참고자료",
    "text": "참고자료\nMaximum Likelihood Estimation(MLE) & Maximum A Posterior(MAP)"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html",
    "href": "posts/DL/Logistic Regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "로지스틱회귀에 대해서 정리한 글입니다."
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#기댓값에-대한-고찰",
    "href": "posts/DL/Logistic Regression.html#기댓값에-대한-고찰",
    "title": "Logistic Regression",
    "section": "기댓값에 대한 고찰",
    "text": "기댓값에 대한 고찰\n기댓값은 실험 또는 시행을 무한히 반복했을때 확률변수가 취하는 값의 평균으로(또는 샘플링된 값의 평균) 기대되는 값입니다. 확률변수가 베르누이 분포를 따르는 경우 확률변수에 대한 기댓값(\\(E\\,[y|x_{1,i},x_{2,i}\\.,\\dots,x_{m,i}]\\))과 모수\\((p_i)\\)가 같은 값을 가집니다. 그러므로,만약에 주어진 샘플데이터로부터 베르누이분포의 모수를 적절히 추정할 수 있다면 주어진 조건하에서 실험 또는 시행을 무한히 반복할 경우 확률변수가 1인사건과 0인사건중 어떤 사건이 더 많이 발생할지 알 수 있고 이를 바탕으로 종속변수 Y의 값을 결정하는 것은 타당합니다. - e.g.\n\n\\(E\\,[y]\\, = \\hat{p_i}<0.5\\) => 무한히 실행했을때 0인 경우가 더 많을 것임 => 관측치를 0으로 예측 \n\\(E\\,[y]\\, = \\hat{p_i}\\geq0.5\\)=>무한히 실행했을때 1인 경우가 더 많을 것임 => 관측치를 1로 예측"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#concept",
    "href": "posts/DL/Logistic Regression.html#concept",
    "title": "Logistic Regression",
    "section": "concept",
    "text": "concept\n선형회귀에서 추정하고자하는 변수\\(y\\)는 \\(x_0,x_1,...,x_m\\)과 \\(w_0,w_1,...,w_m\\)과의 linear combination이였습니다.위에서 언급했듯이 특정샘플에 대한 모수를 적절하게 추정할 수 있다면 관측치가 어떤 클래스에 속할지 합리적으로 알 수 있으므로,로지스틱회귀에서도 선형회귀에서의 아이디어를 핵심아이디어를 가지고와서 추정하고자 하는 모수\\(p_i\\)를 \\(x_0,x_1,...,x_m\\)과 \\(w_0,w_1,...,w_m\\)의 linear combination로 표현하고자 합니다.\n선형회귀의 아이디어(linear combination) + 모수에 대한 표현이라는 조건을 만족하기 위해서 최종적인 식은 다음과 조건을 만족해야 것입니다. - \\((x_0,x_1,...,x_m)\\,,(w_0,w_1,..,w_m)\\)의 linear combination 식에 있어야 함. - linearcombination = 모수(추정하고자하는값)여야 함.\nwhy linear combination?"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#본격적인-유도",
    "href": "posts/DL/Logistic Regression.html#본격적인-유도",
    "title": "Logistic Regression",
    "section": "본격적인 유도",
    "text": "본격적인 유도\n\n모수를 로지스틱함수로 바꾸기\n\n\\((x_0,x_1,...,x_m)\\,,(w_0,w_1,..,w_m)\\)의 linear combination이 식에 존재해야 합니다. 그러므로 선형방정식을 하나 만듭니다. \\[\\begin{align}\nf(i) = x_{1,i}w_0 + x_{2,i}w_1 + x_2w_2 + ... + x_{m,i}w_m = X_iW \\nonumber \\\\\nwhere,X_i = \\,[x_{1,i},x_{2,i},\\dots,x_{m,i}]\\, ,W = \\,[w_0,w_1,\\dots,w_m]^\\text{T} \\nonumber \\\\\n\\end{align}\\]\n좌변은 예측하고자 하는 값인 모수여야 합니다. 좌변을 바꿔봅니다. \\[p_i = WX_i\\]\n좌변의 베르누이 분포의 모수 \\(p_i\\)는 확률변수 \\(y = 1\\)인 사건이 일어날 확률입니다. 그러므로 \\([0,1]\\)이라는 범위를 가지는 반면 우변의 값\\(WX_i\\)은 \\(\\,[-\\infty,\\infty]\\,\\)에 범위를 가집니다. 여기서 Odss Ratio를 써서 모수 \\(p_i\\)를 포함하며 더 넓은 range를 갖도록 좌변을 수정합니다. \\[\\text{Odds Ratio} = \\frac{p_i}{1-p_i} = WX_i\\]\n좌변을 Odds Ratio로 수정했지만 여전히 좌변의 범위는\\(\\,[0,\\infty]\\,\\)으로 우변에 비해 좁습니다. 따라서 Odds Ratio에 로짓변환을 취하여 좌변의 범위를 \\(\\,[-\\infty,\\infty]\\)로 넓혀줍니다. \\[\\text{logit}(p) = \\text{ln}\\frac{p_i}{1-p_i} = WX_i\\]\n\n위 식을 해석하기 위해 \\(X\\)의 첫번째 요소인 \\(x_1\\)에 대응하는 회귀계수 \\(w_1\\)이 학습결과 3으로 정해졌다고 가정해봅시다.만약 \\(x_1\\)의 값이 1증가한다면 로그오즈비가 3증가합니다.\n\n이제 양변의 범위는 맞춰졌으므로 추정하고자 하는 변수 \\(p_i\\)가 좌변에 오도록 정리해봅시다. \\[p_i = \\frac{1}{\\,(1 + e^{-WX_i})\\, }\\] (전개) \\(\\frac{p_i}{1-p_i} = e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\,(1-p_i)\\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\,e^{WX_i}-p_ie^{WX_i}\\) \\(\\Longleftrightarrow p_i + p_ie^{WX_i} = \\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i\\,(1 + e^{WX_i})\\, = \\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\frac{\\,e^{WX_i}}{\\,(1 + e^{WX_i})\\, }\\) \\(\\Longleftrightarrow p_i = \\frac{1}{\\,(1 + e^{-WX_i})\\, }\\)\n\n최종적으로, 앞서 목적이었던 X와 W의 선형조합이 수식내부에 존재하도록 새롭게 표현한 모수는 다음과 같습니다. \\[p_i(y) = Pr\\,(y = 1|X_i;W)\\, = \\frac{1}{\\,1 + e^{-WX_i}\\,}\\]\n\n\n베르누이 분포의 pmf 정리\n베르누이분포의 모수 \\(p_i\\)가 새롭게 표현되었으므로 확률질량함수도 새롭게 표현할 수 있습니다. 마지막 수식은 베르누이 분포의 확률질량함수가 모수\\(W\\)에 관한 식으로 바뀌었음을 표현합니다.\n\\[\\begin{align}\nBern(y;p_i) = Pr\\,(Y_{i} = y|x_{1,i},x_{2,i},\\dots,x_{m,i};p_i) &=\n\\begin{cases}\np_i & \\text{if}\\,y=1 \\\\\n1-p_i & \\text{if}\\,y=0\n\\end{cases} \\\\\n&= p_i^{y}(1-p_i)^{1-y} \\\\\n&= \\frac{e^{yWX_i}}{1+e^{WX_i}}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#setting",
    "href": "posts/DL/Logistic Regression.html#setting",
    "title": "Logistic Regression",
    "section": "setting",
    "text": "setting\n\nimport torch\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n#sig = lambda z:torch.exp(z)/(1+torch.exp(z))\nsig = torch.nn.Sigmoid()"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#data",
    "href": "posts/DL/Logistic Regression.html#data",
    "title": "Logistic Regression",
    "section": "data",
    "text": "data\n\ntorch.manual_seed(2022)\nn=400\n\n#1 모수 W가정\nW = torch.tensor(\n    [[-0.8467],\n    [0.041]]).float()\n\n#2 각각의 관측치(데이터요소)에서의 모수 p_i시각화(시그모이드 함수 시각화)\n_x = torch.linspace(-150,150,n).reshape(-1,1)\n_one = torch.ones((n,1))\nX = torch.concat((_one,_x),axis=1)\np_i = sig(X@W)\ny = torch.bernoulli(p_i)\nplt.xlim([-150,150])\nplt.plot(X[:,1],y,\"bo\",alpha=0.5)\nplt.plot(X[:,1],p_i,\"r--\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y,$p_i$\")\nplt.title(\"realizations $y_1,\\dots,y_{300}$ from $Bern(p_1),\\dots,Bern(p_{300})$\")\nplt.legend([\"y\",\"p\"])\n\n<matplotlib.legend.Legend at 0x1f2684bfe20>"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#gradient-descent-1",
    "href": "posts/DL/Logistic Regression.html#gradient-descent-1",
    "title": "Logistic Regression",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nimport torch\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n\ntorch.manual_seed(2022)\nn=400\n\n#2 임의의 W에 대한 estimated value(추정치) What 초기화\nWhat = torch.tensor(\n    [[0.],\n    [-0.03]],requires_grad=True)\n\n_x = torch.linspace(-150,150,n).reshape(-1,1)\n_one = torch.ones((n,1))\nX = torch.concat((_one,_x),axis=1)\nyhat = sig(X@What)\n\nplt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\nplt.plot(X[:,1].data,p_i,\"r\")\nplt.plot(X[:,1].data,yhat.data,\"g\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"realizations $y_1,\\dots,y_{60}$ from $Bern(p_1),\\dots,Bern(p_{60})$\")\nplt.legend([\"y\",\"$p_i$\",\"$\\hat{y}(\\hat{p_i})$\"])\n\n<matplotlib.legend.Legend at 0x1f268e11e50>\n\n\n\n\n\n\nloss_fn = torch.nn.BCELoss()\n\"\"\"\ndef BCE_Loss(yhat,y):\n    return torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat))\n\"\"\"\n\n'\\ndef BCE_Loss(yhat,y):\\n    return torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat))\\n'\n\n\n\n#custom sigmoid + torch.BCELoss 쓰면 오류 발생. 0과 1사이의 범위 아님\n#torch.nn.Sigmoid + custom BCE Loss 써도 오류발생 => nan\nplt.subplots(2,5,figsize=(20,8))\nplt.subplots_adjust(hspace=0.3)\ni=1\n\nfor epoch in range(200):\n    #1 yhat \n    yhat = sig(X@What)\n    #2 loss\n    loss = loss_fn(yhat,y)\n    if epoch % 20 == 0:\n        plt.subplot(2,5,i)\n        #plt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\n        plt.plot(X[:,1].data,p_i,\"r\")\n        plt.plot(X[:,1].data,yhat.data,\"g\")\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        #plt.title(\"realizations $y_1,\\dots,y_{60}$ from $Bern(p_1),\\dots,Bern(p_{60})$\")\n        plt.legend([\"p\",\"yhat\"])\n        title = \"loss : {}\".format(round(loss.tolist(),5))\n        plt.title(title)\n        i+=1\n    #3 derivative\n    loss.backward()\n    #4 update & clean\n    What.data = What.data - 0.00005 * What.grad\n    What.grad = None\n\n\n\n\n\nround(loss.tolist(),4)\n\n0.3109\n\n\n\nplt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\nplt.plot(X[:,1].data,p_i,\"r\")\nplt.plot(X[:,1].data,yhat.data,\"g\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Logistic Regression\")\nplt.legend([\"y\",\"$p_i$\",\"$\\hat{y}(\\hat{p_i})$\"])\nplt.gca().axes.xaxis.set_visible(False)\nplt.gca().axes.yaxis.set_visible(False)"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#베르누이-분포-전개",
    "href": "posts/DL/Logistic Regression.html#베르누이-분포-전개",
    "title": "Logistic Regression",
    "section": "1.베르누이 분포 전개",
    "text": "1.베르누이 분포 전개\n\\[\\begin{aligned}\np_i^y(1-p_i)^{1-y} &= \\ (\\frac{1}{\\,1 + e^{-WX_i}\\,})^y\\,\\,(1-\\frac{1}{\\,1 + e^{-WX_i}\\,})^{1-y} \\\\\n&= (\\frac{1}{1+e^{-WX_i}})^{y}(\\frac{e^{-WX_i}}{1+e^{-WXi}})^{1-y} \\\\\n&= (\\frac{1}{1+e^{-WX_i}})^{y}(\\frac{1}{1+e^{WXi}})^{1-y} \\\\\n&= (\\frac{1+e^{WX_i}}{1+e^{-WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= (\\frac{e^{WX_i}+e^{2WX_i}}{1+e^{WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= (\\frac{e^{WX_i}(1+e^{WX_i})}{1+e^{WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= e^{yWX_i}\\frac{1}{1+e^{WX_i}} \\\\\n&= \\frac{e^{yWX_i}}{1+e^{WX_i}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#nll전개with-parameter-w",
    "href": "posts/DL/Logistic Regression.html#nll전개with-parameter-w",
    "title": "Logistic Regression",
    "section": "2.NLL전개(with parameter \\(W\\))",
    "text": "2.NLL전개(with parameter \\(W\\))\n\\[\\begin{aligned}\nLL &= \\text{ln}(\\underset{i=1}{\\overset{n}{\\large{\\prod}}}\\,\\frac{e^{y_iWX_i}}{1+e^{WX_i}}) \\\\\n&=\\overset{n}{\\underset{i=1}{\\large{\\sum}}}(\\text{ln}\\frac{e^{y_iWX_i}}{1+e^{WX_i}}) \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}\\,[\\text{ln}e^{y_iWX_i} - \\text{ln}(1+e^{WX_i})]\\, \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}\\,[y_iWX_i - \\text{ln}(1+e^{WX_i})], \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}y_iWX_i - \\overset{n}{\\underset{i=1}{\\large{\\sum}}}ln(1+e^{WX_i}) \\\\\n\n\nNLL &= -\\overset{n}{\\underset{i=1}{\\large{\\sum}}}y_iWX_i + \\overset{n}{\\underset{i=1}{\\large{\\sum}}}ln(1+e^{WX_i}) \\\\\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#nll전개cross-entropy-유도하기",
    "href": "posts/DL/Logistic Regression.html#nll전개cross-entropy-유도하기",
    "title": "Logistic Regression",
    "section": "3.NLL전개(Cross Entropy 유도하기)",
    "text": "3.NLL전개(Cross Entropy 유도하기)\n임의의 i번째 항에서의 확률변수 \\(Y_i\\)가 따르는 베르누이 분포는 다음과 같습니다. \n\\[\\begin{aligned}\nBern(y|X_i;p_i) = (p_i)^y(1-p_i)^{y-1}\n\\end{aligned}\\]\n모수가 \\(p_i\\)인 각각의 베르누이 분포를 따르는 확률변수 \\(Y_1,Y_2\\dots Y_n\\)으로부 n개의 realization(sample) \\(y_1,y_2\\dots y_n\\)에 대한 NLL는 다음과 같습니다. \n\\[\\begin{aligned}\nNLL &= -\\text{ln}\\prod_{i=1}^{n}p_i^{y_i}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}\\text{ln}p_i^{y_i}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}\\text{ln}p_i^{y_i} + \\text{ln}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}y_i\\text{ln}p_i + (1-y_i)\\text{ln}(1-p_i)\n\\end{aligned}\\]\n참고링크 1. 로지스틱 회귀 전개 2. 위키피디아 - 로지스틱 회귀 3. ratsgo’s blog"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "",
    "text": "[Deep Learning Series - Part3]\n안녕하세요!!😀 이번 포스트에서는 다항로지스틱 회귀와 소프트맥스 회귀에 대해서 정리해보고자 합니다. 공부하면서 생각보다 모르는 내용이 많아서 다시 처음부터 공부하고 복습해야 하는 내용이 많았네요. 잡담은 그만하고 시작해보겠습니다!! 읽어주셔서 감사해요😎"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#가정-1",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#가정-1",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "가정 (1)",
    "text": "가정 (1)\n로지스틱회귀를 복기해보면… 종속변수 \\(y_i\\)는 베르누이분포를 따르는 확률변수\\(Y_i\\)로부터 샘플링된 값으로 가정했습니다. 또한 베르누이분포의 모수\\(W\\)는 주어진 조건인 \\(X_i\\)와 회귀계수(가중치)\\(W\\)의 일차결합으로 가정했습니다. 이렇게 모수를 가정하면서 베르누이분포의 확률질량함수도 새로운 모수\\(W\\)를 가지게되었고 W를 적절히 추정하면 데이터가 0또는1에 속할 확률을 알아내게 되어 확률이 더 높은 클래스를 주어진데이터에 대한 클래스로 예측했었습니다.다항로지스틱회귀와 소프트맥스회귀에서도 이러한 과정 즉,분포를 가정하고 데이터를 기반으로 모수를 추정하여 확률분포를 기반으로 예측하는 매커니즘은 거의 그대로입니다.\n먼저 다항로지스틱회귀와 소프트맥스회귀에서 종속변수에 대한 가정을 해보겠습니다. 다항로지스틱 회귀와 소프트맥스회귀에서 모두 각각의 관측치(each observation)에서 종속변수의 realization인 \\(y_i\\)는 확률변수\\(Y_i\\)로부터 표본추출(sampling)되었다고 가정합니다. 이때 각각의 관측치에서의 확률변수 \\(Y_i\\)가 따르는 분포는 설명변수 \\(x_{1,i},x_{2,i},\\dots,x_{M,i}\\)가 조건으로 주어질 때, 각각의 범주(클래스)에 속할 확률들을 모수로 가지는 카테고리분포를 따릅니다.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\nY_i|x_{1,i},x_{2,i},\\dots,x_{M,i} \\sim \\text{Cat}(y|x_{1,i},x_{2,i},\\dots,x_{M,i};\\mu_i)\n& =\n\\begin{cases}\n\\mu_{1,i} \\text{ if } y = (1,0,\\dots,0,0) \\\\\n\\mu_{2,i} \\text{ if } y = (0,1,\\dots,0,0) \\\\\n\\quad\\quad \\vdots \\\\\n\\mu_{K,i} \\text{ if } y = (0,0,\\dots,0,1) \\\\\n\\end{cases} \\\\\n&= \\mu_{1,i}^{y_1}\\mu_{2,i}^{y_2},\\dots,\\mu_{K,i}^{y_K} \\\\\n&= \\prod_{K=1}^{K}\\mu_{K,i}y_{K,i} \\\\\n\\end{aligned} \\\\\n&\n\\begin{aligned}\n&\\text{where, }\\\\\n&\\mu_i = {\\mu_{1,i},\\mu_{2,i},\\dots,\\mu_{K,i}} \\\\\n&\\mu_{1,i} = Pr(Y_i = (1,0,\\dots,0)|x_{1,i},\\dots,x_{M,i}) \\\\\n&\\mu_{2,i} = Pr(Y_i = (0,1,\\dots,0)|x_{1,i},\\dots,x_{M,i}) \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\vdots \\\\\n&\\mu_{K,i} = Pr(Y_i = (0,0,\\dots,0,1)|x_{1,i},\\dots,x_{M,i}) \\\\\n\\end{aligned}\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#가정-2",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#가정-2",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "가정 (2)",
    "text": "가정 (2)\n각각의 관측치에서 확률변수\\(Y_i\\)가 따르는 카테고리분포의 모수\\(\\mu_i\\)는 데이터포인트마다 다른 설명변수(X_i)와 시행마다 변하지 않는 고정된 회귀계수(W)의 일차결합을 포함하는 수식으로 표현됩니다. 주어진 X값을 W와 일차결합하여 추정하고자 하는 값을 표현하는 선형회귀의 핵심아이디어이자 대부분의 회귀문제에서 사용하는 중요한 아이디어 입니다.\n\\[\\begin{aligned}\n&\\mu_{k,i}  = \\mu_{k,i}(X_i;W_{k,i}) = \\mu_{k,i}(X_iW_{k,i}) =  Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i)\\\\\n&\\text{where},\\\\\n&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n&W_k = [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n&\\mu_{k,i} = \\text{i-th 관측치의 $k$번째 모수}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xw의-선형조합을-포함한-모수의-표현-유도하기",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xw의-선형조합을-포함한-모수의-표현-유도하기",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "X,W의 선형조합을 포함한 모수의 표현 유도하기",
    "text": "X,W의 선형조합을 포함한 모수의 표현 유도하기\n위에서 언급했듯이 대부분의 회귀에서 모델링의 핵심아이디어는 추정하고자 하는 대상을 설명변수와 가중치의 일차결합(선형조합)이 포함되도록 표현하는 것입니다. 다항로지스틱회귀도 추정하고자 하는 모수\\(\\mu_i = (\\mu_{1,i},\\mu_{2,i},\\dots,\\mu_{K,i})\\)를 각각을 설명변수와 가중치의 일차결합으로 표현해야 합니다.이진로지스틱회귀와에서도 이렇게 모수를 표현했었는데 다항로지스틱회귀에서는 일차결합으로 표현해야할 모수가 좀 더 많습니다. -_-;;\n차근차근 한번 유도해보겠습니다. 일단 K개의 모수를 표현하는 일차결합을 만들어줍니다. 이러한 일차결합에서 x는 관측치마다 존재하는 설명변수의 값에 따라서 회귀계수(가중치)인 W는 관측치에 따라서 변하지 않는 일정한 값입니다.\n\\[\\begin{aligned}\n&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i-\\text{ln}Z \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\\n&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i {\\text{ (임의의 k번째 항)}}\\\\  \n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n&where,\\\\\n&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n\\end{aligned}\\]\n 한 가지 유의해야 할 점은 마지막 모수는 일차결합으로 표현하지 않는다는 것입니다. 카테고리분포에서 모수의 총합은 1이기 때문에 마지막 \\(K\\)번째 모수는 1에서 전부 빼면 되기 때문입니다.\n그런데 섣불리 일차결합을 만들다보니 … 좌변에 있는 모수는 \\([0,1]\\)의 범위이고 우변은 \\([-\\infty,\\infty]\\)의 범위이므로 가지므로 양변의 범위가 전혀 맞지 않습니다. 그러므로 좌변을 Odds Ratio(엄밀히 Odds Ratio는 아니지만 통일성을 위해 Odds Ratio라고 하겠습니다.) + Logit transform을 취하여 좌변이 우변과 같은 범위를 가질 수 있도록 확장하여 줍니다. (로그안에 있는 분모가 K번째 클래스에 대한 항임을 유의합니다.) \\[\\text{ln}\\frac{\\mu_{k,i}}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = \\text{ln}\\frac{Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i;W_k)}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = W_k^TX_i\\]\n원래의 목적은 모수에 대한 일차결합이 포함된 항을 얻는 것이었습니다. 그러므로 정리하여 모수에 대한 표현을 얻습니다.\n\\[\\begin{aligned}\n\\mu_{k,i} = Pr(Y_i = (0,\\dots,0,1_{k-th},0,\\dots,0|X_i;W_k) = Pr(Y_i = K|X_i)e^{X_iW_k}\n\\end{aligned}\\]\n여기까지 해서 모수에 대한 표현을 얻었습니다. 다만 \\(Y_i\\)가 \\(K\\)번째 클래스에 대한 확률은 카테고리분포에서의 모수에 대한 제약조건을 활용하여 더 간단하게 바꿀 수 있습니다.\n\\[\\begin{aligned}\n&Pr(Y_i = K|X_i) = 1- \\sum_{k=1}^{K-1}Pr(Y_i = K|X_i)e^{X_iW_k} = 1-Pr(Y_i = K|X_i)\\sum_{k=1}^{K-1}e^{X_iW_k} \\\\\n&\\Longleftrightarrow Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{k=1}^{K-1}e^{X_iW_k}}\n\\end{aligned}\\]\n더 간단하게 표현된 항으로 다시 정리하여 쓰면 다음과 같습니다.\n\\[\\begin{aligned}\n&\\mu_{k,i}=Pr(Y_i = k|X_i) = Pr(Y_i = K|X_i)e^{X_iW_k} = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}}\\\\\n&\\text{인덱스 겹치므로 시그마의 $k \\rightarrow j$}\n\\end{aligned}\\]\n 최종적으로 카테고리 분포의 모수는 다음과 같습니다. 전개하는 과정이 마지막 \\(K\\)번째 항은 제외한채 진행되었으므로 K번째 항에대한 확률은 따로 써줍니다.\n\\[\\begin{aligned}\n&\\mu_{k,i}=Pr(Y_i = k|X_i) = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}} \\text{(단, $k != K$)}\\\\\n&\\mu_{K,i}=Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{j=1}^{K-1}e^{X_iW_k}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#estimation",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#estimation",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "Estimation",
    "text": "Estimation\n더 공부해 오겠습니다 ^__^;;"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#가정",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#가정",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "가정",
    "text": "가정\n소프트맥스회귀의 가정은 로지스틱회귀의 가정과 습니다. 각 datapoint에서의 종속변수의 값은 카테고리분포를 따르는 확률변수에서 샘플링되었으며 카테고리분포의 모수는 각 datapoint마다 변하는 설명변수와 회귀계수(가중치)의 일차결합으로 표현됩니다."
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xw의-선형조합을-포함한-모수의-표현-유도하기-1",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xw의-선형조합을-포함한-모수의-표현-유도하기-1",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "X,W의 선형조합을 포함한 모수의 표현 유도하기",
    "text": "X,W의 선형조합을 포함한 모수의 표현 유도하기\n소프트맥스 회귀마찬가지로 추정하고자 하는 모수를 설명변수와 가중치의 일차결합이 포함된 항으로 표현합니다.\n먼저 설명변수와 가중치의 일차결합형태로 모수를 나타냅니다. 임의의 i번째 관측치가 각각의 범주에 \\((1,2,...,K)\\) 속할 확률을 의미하는 모수는 다음과 같습니다.\n\\[\\begin{aligned}\n&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i-\\text{ln}Z \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\\n&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i \\\\  \n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n&where,\\\\\n&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n\\end{aligned}\\]\n이렇게 나타내고 보니 좌변과 0~1사이의 수만 갖지만 우변은 어떤 수던지 나올 수 있습니다. 범위를 맞춰 주기 위해서 좌변에 로그를 씌워 로그확률로 만들어줍니다. 추가적으로 우변에 \\(-lnZ\\)라는 normalizating factor를 더해줍니다. 다음과정에서 카테고리분포의 모수의 합이 1이되도록 하는 확률질량함수의 특징을 유지하기 위해서 사용합니다.\n\\[\\begin{aligned}\n&\\text{ln}\\mu_{1,i} = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\text{ln}\\mu_{2,i} = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\text{ln}\\mu_{k,i} = w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{M,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\text{ln}\\mu_{K-1,i} = w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i-\\text{ln}Z \\\\\n\\\\\n\\end{aligned}\\]\n따라서,임의의 \\(k\\)번째 모수는 다음과 같습니다. \\[\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th}|X_i;W_k) = \\frac{1}{Z}e^{W_k^TX_i}\\]\n카테고리분포의 제약조건 즉,모수는 각각의 범주에 속할 확률을 나타내므로 총합이 1임을 활용합니다. 이를 활용하여 Z를 표현하면 다음과 같습니다.\n\\[\\begin{aligned}\n&\\sum_{k=1}^{K}{\\mu_{k,i}} =\\sum_{k=1}^{K}{Pr(Y_i=k)}= \\frac{1}{Z}\\sum_{k=1}^{K}e^{W_k^TX_i} = 1\\\\\n&\\Longleftrightarrow Z = \\sum_{k=1}^{K}e^{W_k^TX_i}\n\\end{aligned}\\]\n최종적으로, 결과를 정리하면 다음과 같습니다. - 추정하고자하는 카테고리분포의 모수는 \\(\\mu_k\\)는 \\(W_k\\)와 \\(X_i\\)의 일차결합으로 표현되었습니다. 이는 소프트맥스 함수이므로 소프트맥스 회귀라는 이름이 붙었습니다. \\[\\mu_{c,i}(X_i;W) = Pr(Y_i = (0,0,\\dots,1_{c-th},0,\\dots,0)|X_i;W_k) = \\frac{e^{W_c^TX_i}}{\\sum_{k=1}^{K}e^{W_k^TX_i}} = softmax(c,W_1^TX_i,W_2^TX_i,\\dots,W_K^TX_i)\\] - 카테고리분포의 위에서 구한 모수로 다시 정리하면 확률질량 함수는 새로운 모수 \\(W_1,W_2,\\dots,W_K\\)를 가집니다.(인덱스 \\(k->j,c->k\\)) \\[Y_i \\sim Cat(y|X_i;W_1,W_2,\\dots,W_K) = \\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#mle",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#mle",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "MLE",
    "text": "MLE\n여기까지의 과정으로부터 카테고리분포의 모수는 설명변수와 가중치(회귀계수)의 일차결합으로 표현되며 또한 확률질량함수가 새로운 모수 \\(W = (W_1,W_2,\\dots,W_K)\\)로 표현되었습니다.만약 카테고리분포의 모수만 추정할 수 있다면 우리는 데이터포인트가 어떤 범주에 속할 확률이 가장 높은지 알 수 있으며 범주를 분류할 수 있습니다. 여기서는 카테고리분포의 모수\\(W\\)를 MLE로 추정합니다.\n확률분포에서 임의의 모수\\(W = (W_1,W_2,\\dots,W_K)\\)를 가정할 때, 확률변수 \\(Y_1,Y_2,\\dots,Y_N\\)으로부터 realization인 \\(y_1,y_2,\\dots,y_N\\)이 나올 가능도는 다음과 같습니다.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\nL({W};X_i|y_1,y_2,\\dots,y_n) &= Pr_{Y_1,Y_2,\\dots,Y_N}(y1,y2,\\dots,y_n|X_i;W)\\\\\n&= \\prod_{i=1}^{N}Pr_{Y_i}(Y_i=y_i|X_i;W) \\\\\n&= \\prod_{i=1}^{N}\\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\\\\\n\\end{aligned}\n\\\\\n&\\text{where } \\{W\\} = \\{W1,W2,\\dots,W_N\\}\n\\end{aligned}\\]\n위와 같은 가능도를 최소화 하는 \\(W\\)를 찾는 것이 목적입니다.다음과 같습니다\n\\[\\begin{aligned}\n\\overset{*}{\\{W\\}} = \\underset{\\{W\\}}{\\text{argmax}} \\prod_{i=1}^{N}\\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn 구현.html",
    "href": "posts/DL/Pytorch Rnn 구현.html",
    "title": "pytorch로 Rnn구현하기",
    "section": "",
    "text": "hi?hi!가 반복되는 텍스트 데이터에서 다음 문자가 뭐가 나올지 예측하는 RNN모형 만들기"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn 구현.html#vectorization",
    "href": "posts/DL/Pytorch Rnn 구현.html#vectorization",
    "title": "pytorch로 Rnn구현하기",
    "section": "vectorization",
    "text": "vectorization\n\n여러가지 방법이 있으나(tf-idf,dense vector,one-hot encoding 등등…) 여기서는 원핫인코딩 사용\n\n\ndef mapping(txt,map_dict):\n    return [map_dict[chr]for chr in txt]\ntxt_mapped = mapping(txt,map_dict)\nprint(txt_mapped[:10])\n\ndef onehot_encoding(txt_mapped):\n    seq_encoded = torch.nn.functional.one_hot(torch.tensor(txt_mapped))\n    return seq_encoded.float()\nsequence_data_encoded = onehot_encoding(txt_mapped)\nprint(sequence_data_encoded[:10])\n\n[2, 3, 0, 2, 3, 1, 2, 3, 0, 2]\ntensor([[0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.]])\n\n\n데이터 살짝 변형 하나의 긴 sequence data를 RNN의 입력으로 해도 되지만 처리속도,성능을 고려했을 때 자그마한 sequencedata로 분리하여 입력해주는게 더 좋은 방법임. 분리하는 방법도 여러가지가 있을 수 있겠는데 여기서는 다음과 같이 분리함 raw sequence data : hi?hi!hi?hi!hi?hi! ……….. sequence1 : (x,y) = (hi?,h) sequence2 : (x,y) = (i?h,i) sequence3 : (x,y) = (?hi,!) …\n\ndef create_seqdataset(seq_data,seq_length):\n    #x = seq_data[:-1]\n    #y = seq_data[1:]\n    seqs_x = []\n    seqs_y = []\n    for idx in range(0,len(seq_data)-seq_length):\n        seqs_x.append(seq_data[idx:idx+seq_length])\n        seqs_y.append(seq_data[idx+seq_length])\n    return torch.stack(seqs_x),torch.stack(seqs_y)\n    #return seq_x,seq_y\n\nx_data,y_data = create_seqdataset(sequence_data_encoded,3)\nprint(x_data.shape,y_data.shape)\n\ntorch.Size([57, 3, 4]) torch.Size([57, 4])\n\n\n\n왜 저런 shape을 맞춰 주는가?\n여기서 나오는 x_data.shape = \\((57,3,4)\\)가 살짝 난해함.  파이토치 공식문서에 따르면 batch_first = True로 설정할 경우,rnn계열의 모델에 넣어줘야 하는 텐서의 shape은 \\((N,L,H_{in})\\) = (batch size,sequnce length,input_size)이고 dataloader라는 일종의 데이터 중간관리자?를 한 번 거쳐서 모델에 입력됨. dataloader에서 나오는 output.shape = \\((N,L,H_{in})\\)이 되기 위해서는 input.shape = \\((D,L,H_{in}\\)(D는 분리된 시퀀스의 갯수)이어야 함(즉 입력텐서의 차원이 3개여야 출력텐서의 차원도3개이고 차원이 나오는 순서도 저런식이 되어야 함). 따라서 저렇게 설정함.\n\n\n파라미터 잠깐 설명\nbatch size는 배치의 총 갯수(배치안에 있는 원소의 갯수 아님!), sequnce length는 시퀀스데이터의 길이이자 timestemp(시점)의 총 갯수(길이), \\(H_{in}\\)은 each timestep(각 시점)마다 입력되는 벡터의 길이라고 볼 수 있음. 위처럼 원핫인코딩을 한 경우 \\(H_{in}\\)은 시퀀스데이터에 있는 문자의 갯수로 결정되므로 4이고 L은 create_seqdataset함수에서 인수로 넣어준 3(sequnce_length)이고 마지막으로 N(batch_size)은 torch.utils.data.DataLoader안에 인수로 넣어주는 batch_size로 인해서 일정한 갯수로 배치를 나누었을때 나오는 배치들의 총 숫자임.rnn 문서에서 설명하는 batch_size는 torch.utils.dada.DataLoader에서 설정한 batch_size의 갯수만큼 데이터를 모아서 여러개의 배치로 만들었을때 나오는 배치의 총 갯수라고 보면됨.(헷갈리는 부분….)"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn 구현.html#학습-준비하기",
    "href": "posts/DL/Pytorch Rnn 구현.html#학습-준비하기",
    "title": "pytorch로 Rnn구현하기",
    "section": "학습 준비하기",
    "text": "학습 준비하기\n\ndefine architecture,loss,optimizer\ndata check\n\n\n#architecture,loss,optimizer \ntorch.manual_seed(2022)\nrnn = torch.nn.RNN(4,20,batch_first = True)\nlinr = torch.nn.Linear(20,4)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=1e-3)\n\n\nds = torch.utils.data.TensorDataset(x_data,y_data)\ndl = torch.utils.data.DataLoader(ds,batch_size=8,drop_last=True)\n\nfor idx,(x,y) in enumerate(dl):\n    if idx ==5:\n        break\n    print(x.shape,y.shape)\n\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\n\n\n위에서 언급했듯이 데이터로더를 거쳐서 나오는 텐서는 RNN에 바로 입력될 것임. input.shape = \\((N,L,H_{in}) = (8,3,4)\\)"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn 구현.html#모형학습",
    "href": "posts/DL/Pytorch Rnn 구현.html#모형학습",
    "title": "pytorch로 Rnn구현하기",
    "section": "모형학습",
    "text": "모형학습\n\nfor epoch in range(0,101):\n    for tr_x,tr_y in dl:\n        #1 output\n        hidden,hT = rnn(tr_x)\n        #print(hidden.shape)\n        output = linr(hT[-1])\n        #2 loss\n        loss = loss_fn(output,tr_y)\n        #3 derivative\n        loss.backward()\n        #4 update & clean\n        optimizer.step()\n        optimizer.zero_grad()\n    if epoch % 10 == 0:\n        print(f'epoch : {epoch},loss : {round(loss.tolist(),5)}')\n\nepoch : 0,loss : 1.31779\nepoch : 10,loss : 0.69453\nepoch : 20,loss : 0.19338\nepoch : 30,loss : 0.05891\nepoch : 40,loss : 0.02861\nepoch : 50,loss : 0.01791\nepoch : 60,loss : 0.0126\nepoch : 70,loss : 0.00947\nepoch : 80,loss : 0.00744\nepoch : 90,loss : 0.00602\nepoch : 100,loss : 0.00499\n\n\npytorch의 rnn을 거쳐서 나오는 output은 두 가지임. - hidden : 가장 깊이 위치한 히든레이어의 각각의 시점에서의 출력값을 모아놓은 텐서 - hT : 모든 히든레이어에의 마지막 시점(시점T)에서의 출력값을 모아놓은 텐서 - 외우기! 위치 : 가장깊은 <=> 모든 , 시점 : 각각의 <=> 마지막\n위와같은 설정에서는 가장 깊이 위치한 히든레이어의 마지막시점에서의 출력값만이 우리는 다음에올 문자열을 예측할 때 필요하므로 hT[-1]을 하여 그 값을 가져옴."
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory.html",
    "href": "posts/DL/[PRML 정독하기] Probability Theory.html",
    "title": "[PRML 읽기] 1 - 확률론 개요",
    "section": "",
    "text": "Figure1.9\n\n\n빨강색,파랑색 상자 중 하나를 선택하고 선택한 상자안에서 사과(초록) 또는 오렌지(주황)를 꺼낸다고 합시다. 여러번 반복했을 때, 빨강색 상자와 파랑색 상자가 선택된 비율이 각각 40%,60%라고 알려져 있는 상태입니다. 미래에 선택된 상자를 나타내는 변수를 B라고 하면 실제로 상자를 선택하기 전까지는 각각의 상자를 뽑을 확률(가능성)만이 존재하므로 변수B는 확률변수 입니다. 이 확률변수가 취할 수 있는 값은 r 또는 b로 두 가지 입니다. 마찬가지로 미래에 선택된 과일을 나타내는 확률변수 F를 놓을 수 있고 F가 취할 수 있는 값을 a 또는 o로 놓을 수 있습니다.\n빈도주의 관점에서 어떠한 사건이 발생할 확률은 매우 여러번 시행을 반복했을때, 어떤 사건이 나오는 경우의 비율(fraction,ratio)입니다. 예를 들어 주사위 눈이 3이나올 확률이 50%라고 하면 100번 던졌을때 50번정도는 3이 나오는 것으로 이해할 수 있습니다. 위의 문제에서 여러번 반복했을때 빨강색 또는 파랑색상자인 사건이 발생하는 경우가 각각 전체에서 40%,60%였다고 했으므로 이는 확률입니다. 또한 확률변수 B가 r을 취하는 사건에 대한 확률과 확률변수 B가 b를 취하는 사건에 대한 확률이라고 말하며 다음과 같이 적을 수 있습니다.\n\\[\\begin{aligned}\np(B = r) = 0.4 \\\\\np(B = b) = 0.6\n\\end{aligned}\\]\n각각의 상자를 선택하는 사건의 확률은 확률의 정의에 의해서 [0,1]사이의 구간에만 존재합니다. 또한 각각의 상자를 선택하는 사건은 상호베타적이면서 시행으로부터 나올 수 있는 모든 결과들 입니다. 그러므로 확률의 합은 1입니다.\n몇 가지 궁금한 점이 생겼습니다. “사과(초록)이나 오렌지(주황)가 나올 확률은?” 또는 “사과를 뽑았을 때 어떤 상자를 선택할 가능성이 높은지?”에 대해서 궁금합니다. 이는 sum rule과 product rule을 알아야 합니다.\n\n\n\n\n\n\nFigure1.9\n\n\n확률변수 \\(X,Y\\)가 존재하고 \\(X\\)가 취할 수 있는 값은 \\(x_i(i=1,2,\\dots,M)\\) \\(Y\\)가 취할 수 있는 값은 \\(y_j(j=1,2,\\dots,L)\\)라고 합시다. 총 \\(N\\)번을 시행했다고 할 때,시행의 결과 중 \\(X = x_i\\)이면서 동시에 \\(Y=y_i\\)인 경우는 \\(n_{ij}\\)번 나왔으며 \\(X = x_i\\)인 경우는 \\(c_i\\)번 나왔고 확률변수 \\(Y = y_j\\)인 경우는 \\(r_j\\)번이 나왔습니다.\n확률변수 \\(X = x_i\\)이고 \\(Y=y_j\\)인 사건이 동시에 발생할 확률을 \\(X=x_i,Y=y_j\\)일 때의 결합확률(joint probability)라고 합니다.\n\\[p(X = x_i,Y = y_j)\\]\n결합확률은 매우 여러번 시행했을 때, \\(X=x_i,Y=y_j\\)인 사건이 나오는 경우의 \\(n_{ij}\\) 비율입니다.\n\\[N \\rightarrow \\infty,\\,\\, p(X=x_i,Y=y_j) = \\frac{n_{ij}}{N}\\]\n시행으로부터 확률변수 \\(X = x_i\\)인 사건이 몇 번 나왔는지 알기위해서는 \\(c_i\\)는 \\(X = x_i,Y = y_j(\\text{for } j=1,2,\\dots,L)\\)인 사건이 발생하는 모든 경우를 전부 다 더해야 합니다. 예를 들어서 사과를 선택하는 \\(F = a\\)이 경우는 사과를 선택하고 상자가 파랑색 상자인 \\(F=a,B=r\\) 사건이 발생한 경우와 사과를 선택하고 상자가 빨강색 상자인\\(F=a,B=b\\) 사건이 발생한 경우이므로 시행으로부터 두 가지 케이스에 해당하는 모든 경우를 모두 세어야 합니다.\n\\[c_i = \\sum_{j=1}^{L}n_{ij}\\]\n결과적으로 ,\\(X = x_i\\)인 사건의 확률은 다음과 같습니다.\n\\[\\begin{aligned}\np(X = x_i) &= \\frac{c_{i}}{N} \\\\\n&=\\frac{\\sum_{j=1}^{L}n_{ij}}{N}\\\\\n&= p(X=x_i,Y=y_1) + p(X=x_i,Y=y_2) + \\dots + p(X=x_i,Y=y_L) \\\\\n&= \\sum_{j=1}^{L}p(X=x_i,Y=y_j) \\\\\n\\end{aligned}\\]\n위와 같이 하나의 확률변수에 대한 확률을 구할 때, 다른 확률변수와의 모든 결합확률을 더하여 구하는 법칙을 sum rule of probability라고 합니다. 이때 다른 확률변수와 결합확률을 marginalizing 또는 summing out하여 구하므로 marginal probability라고 합니다.\n시행의 결과가 \\(X=x_i\\)인 특정 조건을 만족하는 경우에 대해서만 고려해본다고 합시다. 조건에 맞는 경우안에서 \\(Y = y_j\\)인 사건이 나오는 횟수의 비는 \\(p(Y=y_j|X=x_i)\\)라고 표기하며 다음과 같습니다.\n\\[p(Y=y_j|X=x_i) = \\frac{n_{ij}}{c_j}\\]\n이를 \\(X=x_i\\)로 주어졌을 때, \\(Y=y_j\\)인 사건에 대한 조건부확률이라고 합니다. 조건부 확률의 경우 기호\\(|\\) 다음에 조건이 오며 분모에는 조건에 해당하는 사건이 나오는 경우가 몇 번인지 그 횟수에 값이 오지만 조건부확률이 아닌 그냥 확률의 경우 분모는 몇 번 시행했는지 입니다. 이러한 이유는 조건부확률은 일반적인 확률과 다르게 어떤 특정한 조건안에서 다른사건이 나오는 비율이기 때문입니다.\n위에서 정의한 조건부확률로 결합확률을 다시 적어보면 다음과 같습니다.\n\\[p(X=x_i,Y=y_j) = \\frac{n_{ij}}{N} = \\frac{n_{ij}}{c_i}\\frac{c_i}{N} = p(Y=y_j|X=x_i)p(X = x_i)\\]\n즉 결합확률은 \\(X=x_i\\)인 사건이 발생한 확률과 발생한 사건을 조건\\(X=x_i\\)으로하고 \\(Y=y_j\\)가 발생할 확률의 곱과 같습니다. 이는 어느정도 직관과 일치한다고 볼 수 있는데 예를 들자면 빨강색상자에서 사과를 뽑을 가능성은 먼저 빨강색상자를 고르고 그 다음 사과를 뽑을 가능성이기 때문입니다.\n\n\n\n위에서 적은 결합확률로부터 다음과 같은 식을 얻어낼 수 있습니다.\n\\[p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)}\\]\n이를 베이즈정리 라고 합니다. 베이즈 정리에서 \\(p(Y|X)\\)는 posterior probability(사후확률)로 어떤 조건 또는 증거가 발견되었을때의 확률입니다. \\(p(Y)\\)는 prior probability로 어떤 증거 또는 조건이 발견되기전의 확률입니다. 베이즈 정리로부터 우리는 posterior와 prior의 관계 즉,조건 또는 증거가 발견되기 전,후의 확률사이의 수식을 알 수 있습니다. 그러므로 사전확률을 알고 있다면 그 확률을 통하여 사후확률을 구할 수 있습니다.\n\\[p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)} = \\frac{p(X|Y)p(Y)}{\\sum_{Y}p(X,Y)} = \\frac{p(X|Y)p(Y)}{\\sum_{Y}p(X|Y)p(Y)}\\]\n분모를 sum rule과 product rule에 의하여 더 전개하면 위와 같습니다. 사후확률 \\(p(Y|X)\\)는 \\(X\\)라는 증거,조건이 주어질 때 결과 \\(Y\\)에 대한 확률이었습니다. 위의 수식을 곰곰히 보면 … \\(X\\)가 조건으로 주어질때 결과\\(Y\\)에 대한 조건부 확률을 구하기 위하여 \\(Y\\)가 주어질때의 \\(X\\)에 대한 조건부 확률로 계산합니다. 여기서 나타나는 베이즈 정리에서 핵심은 어떤 조건이 주어지고 결과에 대한 확률을 구할 때, 결과를 조건으로 조건을 결과로 역으로 바꾼 확률을 사용한다는 점입니다. 즉 \\(X\\)가 조건일 때, \\(Y\\)에 대한 조건부 확률이 잘 구해지지 않는다면 이를 뒤집어서 \\(Y\\)가 조건이고 \\(X\\)가 결과일때의 확률을 이용할 수 있습니다.\n베이즈 정리에서 분모는 normalization 상수로 모든 \\(Y\\)에 대하여 확률의 합이 1이 되도록 합니다.\n예시로 돌아가서 오렌지 또는 사과가 나올 확률과 사과를 골랐을 때 어떤 상자를 골랐을 확률이 높은지를 계산해봅시다. 각각의 경우에 대해 확률을 정리하면 다음과 같습니다.\n\\[\\begin{align}\n&p(B = r) = 0.4 \\\\\n&p(B = b) = 0.6 \\\\\n&p(F = a | B = r) = \\frac{1}{4}\\\\\n&p(F = o | B = r) = \\frac{3}{4}\\\\\n&p(F = a | B = b) = \\frac{3}{4}\\\\\n&p(F = o | B = b) = \\frac{1}{4}\\\\\n\\end{align}\\]\n(3)(4),(5)(6) 각각의 합은 normalization constant로 인해 합이 1이 되는것을 알 수 있습니다.\n이어서 원래 궁금했던 첫번째 문제인 사과 또는 오렌지가 나올 확률을 Product rule로 계산해볼 수 있습니다.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\np(F = a) &= p(F=a,B=r) + p(F=a,B=b) \\\\\n&=p(F=a|B=r)p(B=r) + p(F=a|B=b)p(B=b) \\\\\n&=\\frac{1}{4}\\times\\frac{4}{10} + \\frac{3}{4}\\times\\frac{6}{10}\\\\\n&= \\frac{11}{20}\\\\\n\\end{aligned}\n\\\\\n&p(F=b) = 1-\\frac{11}{20}= \\frac{9}{20}\n\\end{aligned}\\]\n또다른 문제인 오렌지 또는 사과를 뽑았을때 어떤 박스를 선택했는지 알고 싶습니다. 즉 알고싶은 확률은 \\(p(B|F=a)\\) 또는 \\(p(B|F=o)\\)입니다. 그런데 우리에게 주어진 확률들은 사전확률인 \\(p(F)\\)와 조건과 결과가 역으로 뒤집힌 확률인 \\(p(F|B)\\)입니다. 그러므로 ,Bayes Rule을 사용하여 원하는 확률을 구할 수 있습니다.\n\\[\\begin{align}\n&p(B|F) = \\frac{p(F|B)p(F)}{p(B)} \\\\\n&p(B=r|F=o) = \\frac{p(F=o|B=r)p(B=r)}{p(F=o)} = \\frac{2}{3}\\\\\n&\\leftrightarrow p(B=b|F=o) = 1-\\frac{2}{3} = \\frac{1}{3}\n\\end{align}\\]\n오렌지를 확인하기전까지는 빨강색 박스일 확률이 절반이 안되는 0.4 였는데 오렌지를 확인하고는 \\(\\frac{2}{3}\\)로 확률이 상승했습니다. 이는 주어지는 정보,주건이 확률에 큰 영향을 미치는 것을 확인할 수 있습니다. 또한 구한 확률이 직관적으로 그림에서 확인할 수 있는 사실과도 일치함을 알 수 있습니다.\n\n\n\n두 확률변수가 독립이면 다음과 같습니다.\n\\[\\text{X and Y are independent random variables} \\iff p(X,Y) = p(X)p(Y)\\]\n예시에서 확인해봅시다.\n\\[\\begin{aligned}\n&p(B=r|F=o) = \\frac{2}{3} ,p(B=r) = \\frac{4}{10}\\\\\n&p(B=r|F=o) \\not = p(B=r) \\Longleftrightarrow \\text{X and Y are dependent}\n\\end{aligned}\\]\n만약 두 박스안에 들어있는 오랜지와 사과가 같은 비율로 들어있다고 한다면…\n\\[\\begin{aligned}\np(F=o | B=r) = p(F = o) \\\\\np(F=a | B=r) = p(F = a)  \\\\\np(F=o | B=b) = p(F = o)  \\\\\np(F=a | B=b) = p(F = a)\n\\end{aligned}\\]\n따라서 상자안에 있는 과일의 갯수가 같을 경우, 두 확률변수는 독립입니다.\n\n\n\n\n빈도주의 관점에서 확률은 매우여러번 시행했을 때, 어떤 사건이 발생하는(나오는) 비율(ratio)입니다.\nSum rule은 여러확률변수에 대한 결합확률이 주어질 때, 특정한 하나의 변수에 대한 확률(marginal probability)을 구함\n\n\\[p(X) = \\sum_Yp(X,Y)\\]\n\nProduct rule은 조건부확률과 주변확률사이의 곱으로 결합확률을 구함\n\n\\[p(X,Y) = p(Y|X)p(X) = p(X|Y)p(Y)\\]\n\n베이즈정리는 posterior(사후확률)를 prior(사전확률)과 조건과 결과가 바뀌었을때의 확률을 통해서 구합니다. 또한 사전확률과 사후확률사이의 관계(수식)입니다.\n\n\\[p(X|Y) = \\frac{p(Y|X)p(Y)}{p(X)} = \\frac{p(Y|X)p(Y)}{\\sum_Yp(X,Y)p(Y)}\\]\n\n두 확률변수가 독립일 경우 , 결합확률(분포)는 각각의 변수에 대한 (주변)확률의 곱입니다\n\n\\[\\text{X and Y are independent random variables} \\iff p(X,Y) = p(X)p(Y)\\]\n\n\n\n\n\n확률변수가 독립이라면 \\(Y\\)의 조건부확률은 조건\\(X\\)가 어떤 값이던간에 전혀 영향이 없습니다(독립적입니다).\n\\[\\begin{aligned}\n&p(Y|X) = p(Y) \\\\\n&\\leftrightarrow p(Y|X) = \\frac{p(X,Y)}{p(X)} = p(Y)\\\\\n&\\leftrightarrow p(X,Y) = p(X)p(Y)\n\\end{aligned}\\]\n반대로 해도 성립합니다.\n\n\n\n\n가장 쉽게 이해하는 베이즈 정리(Bayes’ Law)\nIndependent Random Variables\nPRML 1.2-probability theory"
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory2.html",
    "href": "posts/DL/[PRML 정독하기] Probability Theory2.html",
    "title": "[PRML 읽기] 2 - 확률론 개요(작성중 …)",
    "section": "",
    "text": "PRML을 읽고 정리한 내용입니다."
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory2.html#probability-density-function",
    "href": "posts/DL/[PRML 정독하기] Probability Theory2.html#probability-density-function",
    "title": "[PRML 읽기] 2 - 확률론 개요(작성중 …)",
    "section": "Probability density function",
    "text": "Probability density function\n확률밀도함수는 연속확률변수가 미소구간안에 속하는 사건에 대한 확률을 미소구간의 길이로 나눈 확률밀도값을 함숫값으로 가지는 확률함수로 정의합니다.\n\\[p(x) \\overset{\\Delta}{=} \\lim_{\\Delta x \\rightarrow 0}\\frac{p(x<X\\leq x+\\Delta x))}{\\Delta x}\\]\n확률밀도함수를 정적분하면 확률변수가 임의의 구간안에 속하는 사건에 대한 확률을 얻을 수 있습니다.(증명)\n\\[\\begin{aligned}\nP(a < X \\leq b) = \\int_{a}^{b}p(u)d(u)\n\\end{aligned}\\]\n확률밀도함수는 다음의 두 가지 조건을 만족해야 합니다. 첫번째 식은 확률(밀도)는 반드시 0보다 크거나 같음을 의미합니다. 두번째 식에서 확률변수는 반드시 \\((-\\infty,\\infty]\\)인 구간안에 속함을 의미합니다.\n\\[\\begin{align}\np(x) \\geq 0 \\\\\n\\int_{-\\infty}^{\\infty}f(t)dt = 1\n\\end{align}\\]"
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory2.html#probabiltiy-variable-transform",
    "href": "posts/DL/[PRML 정독하기] Probability Theory2.html#probabiltiy-variable-transform",
    "title": "[PRML 읽기] 2 - 확률론 개요(작성중 …)",
    "section": "Probabiltiy variable transform",
    "text": "Probabiltiy variable transform\n이 부분의 내용은 PRML에 있는 내용을 각색한 부분입니다. 틀린부분이 있다면 알려주세요!!\n연속확률변수 \\(X\\)의 확률밀도함수를 \\(p_X(x)\\)라 할 때, 변수를 변환하여 \\(X\\)를 \\(Y\\)에 관한 식\\(X = g(Y)\\)로 표현했다고 해봅시다. 목적은 확률변수 \\(Y\\)의 확률밀도함수 \\(p_Y(y)\\)를 얻는 것입니다. \\(\\Delta x\\rightarrow 0 \\Delta y \\rightarrow 0\\)이라고 한다면 다음이 성립합니다.\n\\[\\begin{aligned}\n&\\lim_{\\Delta x \\rightarrow 0}\\frac{p(x < X \\leq x + \\Delta x)}{\\Delta x} \\times \\Delta x \\overset{\\sim}{=} \\lim_{\\Delta y \\rightarrow 0}\\frac{p(y < Y \\leq y + \\Delta y)}{\\Delta y} \\times \\Delta y \\\\\n&\\Longleftrightarrow p_X(x)dx \\overset{\\sim}{=} p_Y(y)dy\n\\end{aligned}\\]\n윗식은 Jacobian factor에 의해 등식으로 바꿀 수 있습니다.\n\\[\\begin{aligned}\np_Y(y) &= p_X(x) \\begin {vmatrix} \\frac{dx}{dy} \\end {vmatrix} \\\\\n&= p_X(g(y))|g^{'}(y)|\n\\end{aligned}\\]\n확률변수의 변환은 확률분포함수를 최대화 하는 문제에서 유용하게 사용할 수 있다고 합니다. 변환할 변수를 선택하면 최대화해야하는 확률함수를 바꿀 수 있습니다."
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory2.html#sum-rule-product-rule-of-continuous-variable",
    "href": "posts/DL/[PRML 정독하기] Probability Theory2.html#sum-rule-product-rule-of-continuous-variable",
    "title": "[PRML 읽기] 2 - 확률론 개요(작성중 …)",
    "section": "Sum rule & Product rule of continuous variable",
    "text": "Sum rule & Product rule of continuous variable\n이산확률변수에 대해서는 Sum rule과 Product rule을 살펴봤었지만 연속확률변수 대해서는 보지 않았었습니다. 연속확률변수의 경우 다음과 같습니다. 엄밀한 증명은 measure theroy로 증명해야 하므로 .. 생략하겠습니다.(간략한 증명)\n\\[\\begin{aligned}\n&p(x) = \\int_y f(x,y)dy \\\\\n&p(x,y) = p(y|x)p(x)\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory2.html#expectations-and-variances",
    "href": "posts/DL/[PRML 정독하기] Probability Theory2.html#expectations-and-variances",
    "title": "[PRML 읽기] 2 - 확률론 개요(작성중 …)",
    "section": "Expectations and Variances",
    "text": "Expectations and Variances\n함수의 기댓값(또는 평균)은 함숫값이 어떤 값을 중심으로 분포하는지를 알려줍니다. 가능한 모든\\(x\\)에 대하여 함숫값과 그때의 확률분포의 값을 곱하여 얻은 가중평균입니다.\n\\[\\begin{aligned}\n&\\mathbb{E}[f] = \\sum_x p(x)f(x) \\quad \\text{If X is a discrete R.V} \\\\\n&\\mathbb{E}[f] = \\int_x p(x)f(x)dx \\quad \\text{If X is a continuous R.V}\n\\end{aligned}\\]\n표본의 크기가 무한할 경우, 표본으로 부터 구한 함숫값의 평균과 기댓값은 값이 같습니다. 이를 통해서 확률분포의 기댓값을 알 수 있다면 표본이 적당히 크기가 클 경우 함숫값이 어느정도 일지 대략적으로 예측할 수 있습니다.\n\\[\\mathbb{E}[f] = \\lim_{N \\rightarrow \\infty}\\frac{1}{N}\\sum_{n=1}^{N}f(x_n)\\]\n다변수함수는 여러개의 변수를 가지는 함수입니다. 따라서 각각의 변수가 따르는 확률분포중에서 하나를 선택하여 그때의 확률분포와 함숫값의 기댓값을 구할 수 있습니다. 이때 기댓값은 나머지 확률변수에 대한 함수가 됩니다.\n\\[\\mathbb{E}_x[f(x,y)] = f(y)\\]\n함수의 조건부 기댓값은 조건부 확률분포와의 가중평균으로 정의할 수 있습니다. \\(y\\)가 조건으로 주어질 때, \\(x\\)의 조건부 기댓값은 다음과 같습니다.\n\\[\\mathbb{E}_x[f|y] = \\sum_x{p(x|y)}{f(x)}\\]\n확률변수 \\(f(x)\\)의 분산(variance)는 함수가 기댓값을 중심으로 얼마나 퍼져있는지 알려줍니다. 편차제곱의 기댓값(평균)으로 정의합니다.\n\\[\\begin{aligned}\n&\\mathbb{E}[x] = \\int_xxp(x)dx \\text{ or } \\sum_xxp(x)\\\\\n&\n\\begin{aligned}\n\\text{var}[x]\n&= \\mathbb{E}[(x - \\mathbb{E}[x])^2] \\\\\n&= \\mathbb{E}[x^2] - \\mathbb{E}[x]^2\\\\\n\\end{aligned}\n\\end{aligned}\\]\n두 개의 확률변수에 대해서 공분산은 다음과 같습니다.(2번째 식에 대한 전개)\n\\[\\begin{align}\n\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}] \\\\\n&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}[x]\\mathbb{E}[y]\n\\end{align}\\]"
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory2.html#appendix",
    "href": "posts/DL/[PRML 정독하기] Probability Theory2.html#appendix",
    "title": "[PRML 읽기] 2 - 확률론 개요(작성중 …)",
    "section": "Appendix",
    "text": "Appendix\n\n확률밀도함수에 관한 여러가지 증명\n누적분포함수는 연속확률변수가 \\((-\\infty,x]\\)인 구간안에 속할 확률입니다.\n\\[F(x) = P(-\\infty<X\\leq x)\\]\n따라서,연속확률분포의 분자를 누적분포함수로 나타낼 수 있습니다. 이는 누적분포함수의 도함수가 확률밀도함수이며 누적분포함수의 기울기,변화율이 확률밀도함수임을 나타냅니다.\n\\[p(x) = \\lim_{\\Delta x \\rightarrow 0}\\frac{p(x<X\\leq x+\\Delta x))}{\\Delta x} = \\lim_{\\Delta x \\rightarrow 0}\\frac{F(x+\\Delta x) - F(x)}{\\Delta x} = \\frac{dF}{dx}\\]\n누적분포함수의 도함수가 확률밀도함수이므로 확률밀도함수의 적분은 누적분포함수입니다.\n\\[\\int_{-\\infty}^{x}f(t)dt = F(x) = P(-\\infty<X\\leq x)\\]\n임의의 구간 \\((a,b]\\)사이에 확률변수 \\(X\\)가 속하는 사건에 대한 확률은 다음과 같습니다.\n\\[\\begin{aligned}\nP(a < X \\leq b) &= P(-\\infty < X \\leq b) - P(-\\infty < X \\leq a) \\\\\n&= F(b) - F(a) \\\\\n&= \\int_{-\\infty}^{b}f(t)dt - \\int_{-\\infty}^{a}f(t)dt \\\\\n&= \\int_{a}^{b}f(t)dt\n\\end{aligned}\\]\n\n\n공분산 전개하기\n\\[\\begin{aligned}\n\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}] \\\\\n&=\\mathbb{E}_{x,y}[xy - x\\mathbb{E}[y] - y\\mathbb{E}[x] + \\mathbb{E}[x]\\mathbb{E}[y]]\\\\\n&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}_{x,y}[x\\mathbb{E}[y]] - \\mathbb{E}_{x,y}[y\\mathbb{E}[x]] + \\mathbb{E}[x]\\mathbb{E}[y]]\\\\\n\\end{aligned}\\]\n여기서 \\(\\mathbb{E}_{x,y}[x\\mathbb{E}[y]]\\)는 다음과 같다.\n\\[\\begin{aligned}\n\\int_{\\infty}^{\\infty}\\int_{\\infty}^{\\infty}x\\mathbb{E}[y]p(y)p(x)dydx &=  \\int_{\\infty}^{\\infty}x\\mathbb{E}[y]p(x)\\bigg(\\int_{\\infty}^{\\infty}p(y)dy\\bigg)dx \\\\\n&= \\int_{\\infty}^{\\infty}x\\mathbb{E}[y]p(x)dx \\\\\n&= \\mathbb{E}[y]\\int_{\\infty}^{\\infty}xp(x)dx \\\\\n&= \\mathbb{E}[y]\\mathbb{E}[x]\n\\end{aligned}\\]\n마찬가지로 \\(\\mathbb{E}_{x,y}[y\\mathbb{E}[x]]\\)도 같은 값을 가진다. 따라서 다음과 같다.\n\\[\\begin{aligned}\n\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[xy] - \\mathbb{E}_{x,y}[x\\mathbb{E}[y]] - \\mathbb{E}_{x,y}[y\\mathbb{E}[x]] + \\mathbb{E}[x]\\mathbb{E}[y]]\\\\\n&= \\mathbb{E}_{x,y}[xy] - \\mathbb{E}[y]\\mathbb{E}[x] - \\mathbb{E}[x]\\mathbb{E}[y] + \\mathbb{E}[x]\\mathbb{E}[y] \\\\\n&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}[x]\\mathbb{E}[y]\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Linear Algebra/Ax=b의 해의 갯수 알아내기/Ax = b의 해석.html",
    "href": "posts/Linear Algebra/Ax=b의 해의 갯수 알아내기/Ax = b의 해석.html",
    "title": "Ax=b의 해의 갯수 알아내기",
    "section": "",
    "text": "유투브 - 혁펜하임님의 선형대수학강의를 정리하기 위해 작성한 글입니다.Rank와 Null space로 Ax=b의 해의 갯수를 파악합니다.\n연립일차방정식은 행렬 Ax=b로 나타내고 행렬의 랭크와 열공간을 기반으로 해의 갯수를 나타낼 수 있습니다.\n\n열공간을 기반으로 한 Ax=b의 해석\n행렬\\(A = \\begin{bmatrix}a_1 & a_2 & \\dots & a_n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\)와 벡터\\(x = \\begin{bmatrix}x_1,x_2,\\dots,x_n\\end{bmatrix}^T\\in \\mathbb{R}^{n \\times 1}\\)의 곱을 생각해보자. 행렬과 벡터의 곱은 다음과 같이 열벡터의 일차결합으로 바라볼 수 있다.\n\\[\\begin{aligned}\nAx = \\begin{bmatrix}a_1 & a_2 & \\dots & a_n\\end{bmatrix}\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots\\\\\nx_n\n\\end{bmatrix}\n= a_1x_1 + a_2x_2 + \\dots a_nx_n\n\\end{aligned}\\]\n행렬 A의 열공간은 열벡터들의 선형생성이다.\n\\[\\begin{aligned}\n&\\text{C}(A) = \\text{span}(a_1,a_2,\\dots,a_n) = \\{c_1a_1 + c_2a_2 + \\dots c_na_n|c_1,c_2,\\dots,c_n \\in K\\}\n\\end{aligned}\\]\n그러므로, 행렬 \\(A\\)의 열공간은 임의의 \\(x\\)에 대하여 가능한 \\(Ax\\)의 모든 집합과 같다. 가능한 모든 스칼라\\(x_1,x_2,\\dots,x_n\\)로 벡터의 일차결합이 이루는 집합은 생성(span)과 같기 때문이다 \\[\\text{C}(A) = \\text{span}(a_1,a_2,\\dots,a_n) = \\{Ax |x\\in K^n\\}\\]\n만약 \\(Ax = b\\)인 방정식의 해 \\(x\\)를 구하려 한다 하자. \\(\\text{C}(A)\\)는 가능한 \\(Ax\\)의 모든 집합이였으므로 만약 \\(\\text{C}(A)\\)가 \\(b\\)를 포함한다면(즉,\\(b\\)가 열공간의 원소라면) \\(Ax=b\\)인 \\(x\\)가 존재하여 방정식의 해가 존재하고 \\(\\text{C}(A)\\)가 \\(b\\)를 포함하지 않는다면 \\(Ax = b\\)인 \\(x\\)값이 존재하지 않고 따라서 방정식의 해가 존재하지 않는다.\n\n\nCase1 : A가 full column rank일 때\n문제 : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) 일때, \\(Ax = b\\)를 만족하는 x의 갯수는?\n 위의 문제에서 A는 full column rank이다. full column rank일 경우 \\(Ax\\)의 모양은 위와 같고 열공간은 m차원 벡터공간의 부분공간이자 n차원 벡터공간이다.앞에서 “m차원 벡터공간의 부분공간인 n차원 벡터공간” 이라 했는데 그 이유는 (열)벡터가 m차원 벡터공간의 원소(벡터)이기 때문이다. m차원 벡터공간에 있는 (열)벡터 n개를 기저로 생성된 공간이기 때문에 m차원 벡터공간의 부분공간이면서 동시에 n차원 벡터공간이 된다. \n위에서 언급했듯이 열공간은 임의의 x에 대하여 가능한 \\(Ax\\)의 집합이기에 방정식의 해가 존재하려면 b가 열공간안에 있으면 되고 아니면 바깥에 있으면 된다. 그렇다면 이 경우 b의 위치는 어떻게 될까?\n\n가능한 b의 위치는 2가지이다. 1의 경우는 열공간안에 b가 없는 경우다. 이 경우 b는 m차원 벡터공간에는 있으면서(\\(b \\in R^{m \\times 1}\\)이기에 당연하다) 부분공간인 n차원 열공간안에는 없게된다. 따라서 이 경우 해는 존재하지 않는다. 2의 경우는 열공간안에 b가 있는 경우다. 이 경우 b는 m차원 벡터공간에 속하면서 동시에 부분공간인 n차원 벡터공간에도 속한다.\n이렇게 생각하면 끝난 것 같은데 한가지 더 생각해야 할 것이 있다. 바로 null space이다. 만약 Ax = b를 만족하는 하나의 해인 \\(x_{particular}\\)가 있다고 해보자. 이때 \\(Ax=0\\)을 만족하는 널공간의 임의의 원소인 \\(x\\)를 \\(x_{null}\\in N(A)\\)이라고 하면 다음이 성립한다.\n\\[A(x_{particular} + x_{null}) = Ax_{particular} + Ax_{null} = b\\]\n윗식에 의해서 null space의 원소인 \\(x_{null}\\)과 \\(x_{particular}\\)의 합도 방정식의 해이므로 해를 구할때 null space도 확인해야 한다. null space에 대한 x를 추가한 완전해는 다음과 같다. \\[x_{complete} = x_{particular} + x_{null}\\]\n그렇다면 x_{null}을 어떻게 확인할 수 있을까? 랭크-널리티 정리로 확인할 수 있는데 위와같이 full column rank인 경우는 다음과 같다.\n\\[\\begin{aligned}\n&\\text{rank}(A) + \\text{nulity(A)} = n\\\\\n&\\Longleftrightarrow \\text{nulity(A)} = n - rank(A) = n - n = 0\n\\end{aligned}\\]\n랭크-널리티 정리에 의하여 null space의 차원이 0임을 확인했다. 차원이 0인 널공간(벡터공간)은 \\(\\{\\bf{0}\\}\\)이므로 \\(x_{null} \\in N(A)\\)인 \\(x_{null} ={\\bf 0}\\)이다. 따라서 2번의 경우, \\(x_{particular}\\)가 존재하여 \\(Ax = 0\\)일 경우의 완전해는 다음과 같다. \\[\\therefore  x_{total} = x_{particular} + x_{null} = x_{particular} + {\\bf 0} = x_{particular}\\]\n결론적으로, full column rank일 경우 해가 존재하지 않거나 단 하나 존재한다.\n\n\nCase2 : A가 full row rank일 때\n문제 : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = m<n\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) 일때, \\(Ax = b\\)를 만족하는 x의 갯수는?\n 위의 문제에서 A는 full row rank이다. 위해서 한 것처럼 먼저 \\(C(A)\\)와 \\(b\\)가 어떻게 위치하고 있을지 파악하고 null space를 따져 완전해를 구하는 것이다. 먼저 \\(C(A)\\)를 생각해보자. \\(C(A)\\)는 행렬의 랭크가 m이기에 n개의 (열)벡터 중 선형독립인 column vector는 m개 뿐이다. 따라서 벡터의 span인 열공간은 m차원 벡터공간이다. (n개의 열벡터가 존재하지만,선형종속이기떄문이다.)\n이전문제와 다른점도 존재하는데 바로 열공간이 (열)벡터가 존재하는 벡터공간의 부분공간이 아니라는 점이다. 이 문제에서 각각의 열벡터는 m차원 공간의 벡터이고 열벡터의 생성도 m차원 벡터공간이기때문에 열벡터가 존재하는 바로 그 공간이 열공간이다.\n위와 같이 열공간에 대해서 생각했으면 이제 b에 대해서 생각해볼 차례다. b의 위치는 어떻게 될까? b는 A의 열공간에 속하는 벡터일까 아닐까?\n\nb의 경우 \\(b \\in \\mathbb{R}^{m \\times 1}\\)이기에 열공간에 속하는 벡터이다. 그러므로 full row rank인 경우는 반드시 해가 존재한다.\n여기서 완전해를 구하기 위해서 null space도 생각해야한다. 랭크-널리티 정리에 의해 다음과 같다.\n\\[\\begin{aligned}\n&rank(A) + nulity(A) = n \\\\\n&\\Longleftrightarrow nulity(A) = n - rank(A) =  n - m\n\\end{aligned}\\]\n랭크정리에 의해서 null space는 n-m차원의 벡터공간이다. 이 경우 가능한 \\(x_{null}\\)은 무한히 많이 존재하므로 해가 무수히 많다. 완전해는 다음과 같다.\n\\[\\begin{aligned}\n\\therefore\\,\\,  &x_{complete} = x_{particular} + x_{null}\\\\\n&\\text{where, } x_{null} \\in N(A) = \\mathbb{R}^{n-m}\n\\end{aligned}\\]\n결론적으로, full row rank의 경우 해는 무수히 많다.\n\n\nCase3 : A가 full rank일 때\n문제 : \\(A \\in \\mathbb{R}^{m \\times m},\\text{rank}(A) = m\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) 일때, \\(Ax = b\\)를 만족하는 x의 갯수는?\n\n사실 이 문제의 해는 다음과 같다. \\[x = A^{-1}b\\] 그러므로,full rank인 경우 해의 갯수가 1개이다.\n위처럼 간단하게 해를 구할 수 있지만 … 그래도 기하학적으로 생각해보기 위에서 했던 것처럼 따져보자. column space는 m차원 벡터공간이다. column vector는 column space 그 자체인 m차원 벡터공간의 원소이다. \\(b\\in \\mathbb{R}^{m \\times 1}\\)의 경우 m차원 벡터공간의 원소이다. 그러므로, column space는 반드시 b를 포함하며 그림으로 표현하면 다음과 같다.\n\n널공간을 따지기 위해 랭크-널리티 정리를 사용해 보면 다음과 같다.\n\\[\\begin{aligned}\n&\\text{rank}(A)+\\text{nulity}(A) = m\\\\\n&\\Longleftrightarrow \\text{nulity}(A) = m - \\text{rank}(A) = m - m = 0\n\\end{aligned}\\]\nnull space는 차원이 0이므로 \\(N(A) = \\{{\\bf 0}\\}\\)이고 다음과 같다. \\[x_{complete} = x_{particular} + x_{null} = x_{particular} + {\\bf 0} = x_{particular}\\]\n결과적으로, full rank인 경우 해는 반드시 존재하며 갯수는 1개이다.\n\n\nCase4 : A가 rank deficient 일 때\n문제 : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = \\alpha < \\text{min}(m,n)\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) 일때, \\(Ax = b\\)를 만족하는 x의 갯수는?\n\n위의 예시에서는 A가 rank deficient인 경우 중, \\(m<n\\) 인 경우를 생각해보자. 행렬 A의 column space는 m차원 공간의 부분공간이자 \\(\\alpha\\)차원의 벡터공간이다.\\(b \\in \\mathbb{R}^{m \\times 1}\\)이므로 가능한 경우는 아래와 같다.\n\n1번의 경우라면 b는 column space의 원소가 아니므로 방정식의 해는 존재하지 않는다. 만약 2번의 경우라면 해가 존재한다. 이때에는 null space를 고려한 완전해를 따져야하므로 랭크-널리티 정리를 확인한다.\n\\[\\begin{aligned}\n&\\text{rank}(A) + \\text{nulity}(A) = \\alpha + \\text{nulity}(A) = n \\\\\n&\\Longleftrightarrow \\text{nulity}(A) = n - \\alpha > 0\n\\end{aligned}\\]\n랭크정리에 의해서 null space는 \\(n - \\alpha\\)차원의 벡터공간이다. 이 경우 null space의 임의의 원소인 \\(x_{null}\\)는 무한히 많이 존재하므로 해가 무수히 많다. 완전해는 다음과 같다.\n\\[\\begin{aligned}\n\\therefore\\,\\,  &x_{complete} = x_{particular} + x_{null}\\\\\n&\\text{where, } x_{null} \\in N(A) = \\mathbb{R}^{n-\\alpha}\n\\end{aligned}\\]\n결론적으로, rank deficient의 경우 해는 무수히 많거나 해는 존재하지 않는다.\n\n\n정리\n\n\n\n\n\n\n\n\nrank type\nexpression\n해의 갯수\n\n\n\n\nfull column rank\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) = n<m\\)\n해가 없거나 해가 한개만 존재한다.\n\n\nfull row rank\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) = m<n\\)\n해가 무수히 많다.\n\n\nfull rank\n\\(A \\in \\mathbb{R}^{m \\times m}\\),\\(\\text{rank}(A) = m\\)\n해가 한개만 존재한다.\n\n\nrank deficient\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) < \\text{min}(m,n)\\)\n해가 없거나 해가 무수히 많다.\n\n\n\n\n\n참고자료\n혁펜하임 - [선대] 2-11강. Ax=b 의 해의 수 알아내기 프린키피아"
  },
  {
    "objectID": "posts/Linear Algebra/Least Squares/Least Squares.html",
    "href": "posts/Linear Algebra/Least Squares/Least Squares.html",
    "title": "Least Squares",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m,x \\in \\mathbb{R}^{n \\times 1},b \\in \\mathbb{R}^{m \\times 1}\\) 가 주어지고 방정식 \\(Ax = b\\)를 만족하는 해인 \\(x\\)를 구할 수 없을 때, \\(Ax\\)가 \\(b\\)와 가장 비슷하게 하는 \\(x\\)를 찾는 것이 목적입니다."
  },
  {
    "objectID": "posts/Linear Algebra/Least Squares/Least Squares.html#projection-matrix",
    "href": "posts/Linear Algebra/Least Squares/Least Squares.html#projection-matrix",
    "title": "Least Squares",
    "section": "projection matrix",
    "text": "projection matrix\n위해서 구한 \\(\\hat{x}\\)를 \\(A\\hat{x}\\)에 대입하면 다음과 같습니다. \\[A\\hat{x} = (A^TA)^{-1}A^Tb\\]\n위 식은 우변의 \\(b\\)에 \\(A(A^TA)^{-1}A^T\\)를 곱하여 \\(C(A)\\)에서 \\(b\\)와 가장 비슷하면서(거리가 가장 가까우면서) \\(b\\)를 \\(C(A)\\)에 정사영(projection) 한 벡터 \\(A\\hat{x}\\)을 얻음을 의미합니다. 따라서 \\(A(A^TA)^{-1}A^T\\)를 projection matrix라 부르고 \\(p_A\\)로 표기합니다."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html",
    "title": "rank & null space(kernel)",
    "section": "",
    "text": "유투브 - 혁펜하임님의 선형대수학강의를 정리하기 위해 작성한 글입니다."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#예제",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#예제",
    "title": "rank & null space(kernel)",
    "section": "예제",
    "text": "예제\n\\[\\begin{pmatrix}\n1&2&3\\\\\n0&0&0\n\\end{pmatrix}\\]\n위에 있는 행렬에서 선형독립인 열벡터의 갯수 = 1이다. 또한 선형독립인 열벡터의 갯수 = 선형독립인 행벡터의 갯수 = 열공간의 차원 = 행공간의 차원이므로 랭크정리도 성립한다.\n위와 같은 \\(2 \\times 3\\) 행렬은 랭크보다 행,열의 갯수가 많이 부족하므로 rank-deficient 라고 한다.\n\\[\\begin{pmatrix}\n1&0&1\\\\\n0&1&1\n\\end{pmatrix}\\]\n위에 있는 행렬에서 선형독립인 열벡터의 갯수 = 2이다. 또한 선형독립인 열벡터의 갯수 = 선형독립인 행벡터의 갯수 = 열공간의 차원 = 행공간의 차원이므로 랭크정리도 성립한다.\n위와 같은 행렬은 행의 갯수만큼 랭크가 다 차있으므로 full row rank라 한다."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#용어정리",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#용어정리",
    "title": "rank & null space(kernel)",
    "section": "용어정리",
    "text": "용어정리\n\\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m \\Rightarrow\\) 열벡터가 모두 선형독립,full column rank,위아래로 길쭉하고 양옆은 좁은 직사각행렬 \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n>m \\Rightarrow\\) 행벡터가 모두 선형독립,full row rank,위아래가 좁고 좌우 양옆으로 길쭉한 직사각행렬 \\(A \\in \\mathbb{R}^{n \\times n},\\text{rank}(A) = n \\Rightarrow\\) 행백터,열벡터가 모두 선형독립,full rank,위,아래,양,옆의 길이가 모두 같은 정사각행렬 \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) < \\text{min}(n,m)\\) = 선형종속인 행벡터,열벡터 반드시 존재,rank deficient"
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#예제1",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#예제1",
    "title": "rank & null space(kernel)",
    "section": "예제1)",
    "text": "예제1)\n행렬 A = \\(\\begin{pmatrix}1&0&1 \\\\0&1&1\\end{pmatrix}\\)일 때, 행렬A의 영공간은?\n먼저 만족하는 \\(x\\)를 찾아보자. 어떤 방법이던 사용가능하지만 문제가 간단하므로 직관적으로 풀이한다.\n\\[\\begin{aligned}\n&Ax = x_1\\begin{pmatrix}1\\\\0\\end{pmatrix} + x_2\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + x_3\\begin{pmatrix}1\\\\1\\end{pmatrix} = {\\bf 0} \\\\\n&\\Longleftrightarrow\n\\begin{cases}\nx_1 + x_3 = 0 \\\\\nx_2 + x_3 = 0\n\\end{cases}\n\\\\\n&\\Longleftrightarrow x_3 = t,x_2 = -t,x_1 = t \\\\\n&\\Longleftrightarrow x = t\\begin{pmatrix}1\\\\-1\\\\1\\end{pmatrix} \\\\\n&\\therefore \\text{N}(A) = \\Bigg\\{t\\begin{pmatrix}1\\\\-1\\\\1 \\end{pmatrix}|t \\in \\mathbb{R}\\Bigg\\}\n\\end{aligned}\\]\n방정식의 해를 구해보니 1)\\([1,-1,1]^T\\)의 span이 영공간이며 2)영공간은 행렬곱에 의해서(중간에서 차원일치 \\(2 \\times 3 과 3 \\times 1\\)) 행벡터가 존재하는 차원인 3차원 벡터공간의 부분공간인 1차원 벡터공간을 생성함을 알 수 있다.\n영공간의 원소(방정식의 해)는 무수히 많음이 자명한데 왜냐하면 \\(Ax = 0\\)을 만족하는 임의의 x벡터에 대한 스칼라배에 대해 다음이 성립하기 때문이다.\n\\[\\begin{aligned}\n&Ax = {\\bf 0} \\\\\n&\\Longleftrightarrow cAx = c{\\bf 0} = {\\bf 0} \\\\\n&\\Longleftrightarrow A(cx) ={\\bf 0} \\\\\n\\end{aligned}\\]\n따라서 \\(x\\)의 스칼라배인 \\(cx\\)도 \\(A\\)와 곱해져서 \\({\\bf 0}\\) 만들기 \\(x\\)의 스칼라배도 영공간의 원소이다. \nrank-nulity theoreom도 성립함을 확인할 수 있다. 영공간의 차원은 영공간의 기저의 갯수인데 \\([1,-1,1]^T\\)이 기저의 조건인 1)선형생성 = 벡터공간 2)선형독립 이라는 두 조건을 만족하므로 차원은 1이다.랭크는 다른방식으로도 구할 수 있지만 행렬이 간단해서 바로2임을 확인할 수 있으므로 다음과 같다 \\[\\text{nulity}(A) + \\text{rank}(A)= n   \\Longleftrightarrow  1 + 2 = 3\\]"
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#예제2",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#예제2",
    "title": "rank & null space(kernel)",
    "section": "예제2)",
    "text": "예제2)\n행렬 A = \\(\\begin{pmatrix}1&2&3 \\\\0&0&0\\end{pmatrix}\\)일 때, 행렬A의 영공간은?\n마찬가지로 만족하는 \\(x\\)를 먼저 찾아보자.\n\\[\\begin{aligned}\n&Ax = x_1\\begin{pmatrix}1\\\\0\\end{pmatrix} + x_2\\begin{pmatrix}2 \\\\ 0\\end{pmatrix} + x_3\\begin{pmatrix}3\\\\0\\end{pmatrix} = {\\bf 0} \\\\\n&\\Longleftrightarrow\nx_1 + 2x_2 + 3x_3 = 0 \\\\\n&\\Longleftrightarrow x_3 = t,x_2 = q,x_1 = -2q -3t \\\\\n&\\Longleftrightarrow x = \\begin{pmatrix}-2q-3t\\\\q\\\\t\\end{pmatrix} = q\\begin{pmatrix}-2\\\\1\\\\0\\end{pmatrix} + t\\begin{pmatrix}-3\\\\0\\\\1\\end{pmatrix} \\\\\n&\\therefore \\text{N}(A) = \\Bigg\\{q\\begin{pmatrix}-2\\\\1\\\\0 \\end{pmatrix} + t\\begin{pmatrix}-3\\\\0\\\\1\\end{pmatrix}|t,q \\in \\mathbb{R}\\Bigg\\}\n\\end{aligned}\\]\n방정식의 해를 구해보니 1)두 벡터의 span이 영공간이며 2)생성된 영공간은 행렬곱에 의해서(중간에서 차원일치 \\(2 \\times 3\\) 과 \\(3 \\times 1\\)) 행벡터가 존재하는 차원인 3차원 벡터공간안에서 부분공간인 1차원 벡터공간을 생성함을 알 수 있다.\n\\(Ax = 0\\)을 만족하는 2개의 x의 선형조합이 모두 영공간의 원소임을 확인해보자.\n\\[\\begin{aligned}\n&Ax_1 = {\\bf 0},Ax_2 = {\\bf 0} \\\\\n&\\Longleftrightarrow cAx_1 = c{\\bf 0}={\\bf 0},cAx_2 = c{\\bf 0}={\\bf 0} \\\\\n&\\Longleftrightarrow A(cx_1) + A(cx_2) = A(cx_1 + cx_2) = {\\bf 0} \\\\\n\\end{aligned}\\]\n따라서 \\(x\\) 방정식을 만족하는 두 벡터의 선형조합인 \\(c(x_1 + cx_2)\\)도 \\(A\\)와 곱해져서 영공간의 원소임을 알 수 있다. \n마찬가지로 rank-nulity theoreom도 성립함을 확인할 수 있다. 영공간의 차원은 영공간의 기저의 갯수이고 \\([-2,1,0]^T,[-3,0,1]^T\\)가 기저의 조건인 1)선형생성 = 벡터공간(여기서 영공간) 2)선형독립 이라는 두 조건을 만족하므로 영공간의 기저는 \\([-2,1,0]^T,[-3,0,1]^T\\)이고 차원은 2이다.랭크는 다른방식으로도 구할 수 있지만 행렬이 간단해서 바로1임을 확인할 수 있으므로 다음과 같다 \\[\\text{nulity}(A) + \\text{rank}(A)= n   \\Longleftrightarrow  2 + 1 = 3\\]"
  },
  {
    "objectID": "posts/Linear Algebra/span과 columns space.html",
    "href": "posts/Linear Algebra/span과 columns space.html",
    "title": "Linear Combination & Span",
    "section": "",
    "text": "유투브 - 혁펜하임님의 선형대수학강의를 정리하기 위해 작성한 글입니다.\n\nlinear Combination\n벡터 \\(\\bf{v_1,v_2,\\dots,v_n}\\)의 선형결합(또는 일차결합)은 다음과 같다. \\[w_1{\\bf v_1} + w_2{\\bf v_2} + \\dots + w_n{\\bf v_n}\\] 선형결합의 결과는 여러가지 벡터들의 결합이다. 여기서 각각의 \\(w_1,w_2,\\dots,w_n\\)은 스칼라이며 대응하는 \\(x_1,x_2,\\dots,x_n\\)를 결합에 사용하는 정도를 의미한다. 만약 \\(w_1 = 0.0001\\)이면 여러 벡터들을 결합하지만 그 결합 중 \\(\\bf v_1\\)이 아주 사용하여 결합하는 것이고 \\(w_2 = 120\\)이라면 결합에서 \\(\\bf v_2\\)를 아주 많이 사용하는 것이다.\n\n\nspan\n벡터 \\(v_1,v_2,\\dots,v_n\\)의 선형결합에서 \\(w_1,w_2,\\dots,w_n\\)(스칼라,결합에 사용하는 정도)를 바꿨을때 가능한 모든 벡터들의 집합이며 즉,벡터의 선형결합으로 가능한 모든 집합들이며 정의는 다음과 같다. \\[\\text{span}({\\bf{v_1,v_2,\\dots,v_n}}) := \\{w_1{\\bf{v_1}} + w_2{\\bf{v_2}} + \\dots + w_n{\\bf{v_n}}:w_1,w_2,\\dots,w_n \\in K\\}\\]  span은 선형결합으로 만들어지는 또다른 벡터공간이다. 여기서 K는 field를 의미하는데 스칼라는 field라는 또다른 집합으로부터 가져온 원소이기때문에 그렇다.임의의 벡터들의 span은 어떨까? 아래의 그림을 확인해보자.\n<참고> span은 동사로도 사용한다. ex : 벡터공간을 생성한다. 열공간은 열벡터들이 생성하는 공간이다.(=열공간은 열벡터의 생성이다.).기저들이 벡터공간을 생성한다.\n:는 조건을 의미합니다.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nplt.style.use(\"ggplot\")\n\ndef linrcmb(v1,v2):\n    linrcomb_x= []\n    linrcomb_y= []\n\n    _w = np.linspace(-50,50,300).tolist()\n    for i in range(2000):\n    #w1,w2를 -100~100사이의 임의의 숫자로\n        w1 = random.sample(_w,1)\n        w2 = random.sample(_w,1)\n        #print(w1,w2)\n    #선형결합 계산\n        linrcomb = w1 * v1 + w2 * v2\n    #시각화를 위해 선형결합의 x값 y값 따로 모아놓기\n        linrcomb_x.append(linrcomb[0][0])\n        linrcomb_y.append(linrcomb[1][0])\n    return linrcomb_x,linrcomb_y\n\nv1 = np.array([[0],[0]])\nv2 = np.array([[0],[0]])\nx,y = linrcmb(v1,v2)\nfig,ax = plt.subplots(figsize=(30,5))\nplt.subplot(1,5,1)\nplt.title(\"$v_1 = [0,0]^{T},v_2 = [0,0]^T$\")\nplt.scatter(x,y,color=\"black\")\n\nv1 = np.array([[1],[0]])\nv2 = np.array([[-3],[0]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,2)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [-3,0]^T$\")\nplt.scatter(x,y)\n\nv1 = np.array([[1],[1]])\nv2 = np.array([[0],[0]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,3)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [0,0]^T$\")\nplt.scatter(x,y,color=\"green\",alpha=1)\n\n\nv1 = np.array([[1],[0]])\nv2 = np.array([[0],[1]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,4)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [0,1]^T$\")\nplt.scatter(x,y,color=\"purple\",alpha=1)\n\ndef linrcmb2(v1,v2):\n    linrcomb_x= []\n    linrcomb_y= []\n\n    _w = (np.linspace(-250,250,300)).tolist()\n    for i in range(2000):\n        w1 = random.sample(_w,1)[0] * 100 #그래프를 그리기 위한 값 조절\n        w2 = random.sample(_w,1)[0] \n        #선형결합 계산\n        linrcomb = w1 * v1 + w2 * v2\n        #시각화를 위해 선형결합의 x값 y값 따로 모아놓기\n        linrcomb_x.append(linrcomb[0][0])\n        linrcomb_y.append(linrcomb[1][0])\n    \n    return linrcomb_x,linrcomb_y\nlinrcmb2(v1,v2)\n\nv1 = np.array([[-1],[0]])\nv2 = np.array([[2],[2]])\nx,y = linrcmb2(v1,v2)\nplt.subplot(1,5,5)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [2,2]^T$\")\nplt.scatter(x,y,color=\"blue\",alpha=1)\nplt.subplots_adjust(wspace=0.4,hspace=0.5)\nplt.suptitle(\"span$(v_1,v_2)$\",y=1.02,fontsize=20)\n\n\nText(0.5, 1.02, 'span$(v_1,v_2)$')\n\n\n\n\n\n점 하나는 벡터 하나를 나타낸다. 1번째 그림에서 벡터들의 생성(span)은 2차원 벡터공간의 부분공간이자 0차원 벡터공간(점)이며 2,3번째 그림에서 벡터들의 생성(span)은 2차원 벡터공간의 부분공간이자 1차원 벡터공간(직선)이다. 이와는 다르게 3번째 그림에서의 벡터들의 생성(span)은 3차원 벡터공간 그 자체인데 그 이유는 3차원 벡터공간의 모든 점을 표현할 수 있기 때문이다.\n\n\ncolumn Space\n열공간(columns space)은 행렬의 열벡터들의 span 즉, 행렬의 열벡터들로 가능한 모든 선형조합(벡터)의 집합입니다. \\(v_1,v_2,\\dots,v_n\\)이 행렬A의 열벡터들이라고 할 때, 열공간은 다음과 같습니다. \\[\\text{C}(A) = \\text{span}(v_1,v_2,\\dots,v_n) = \\{w_1{\\bf v_1} + w_2{\\bf v_2} + \\dots + w_n{\\bf v_n}:w_1,w_2,\\dots,w_n \\in K\\}\\] 열공간은 방정식 \\(Ax = b\\)의 해의 갯수를 파악하는데 쓰입니다.\n\n\n참고자료\n[선형대수] 벡터공간(vector space), 벡터 부분공간(vector subspace), 생성공간(span), 차원(dimension) 위키피디아-Linear span 위키피디아-Row and columns spaces 혁펜하임 - [선대] 2-6강. span 과 column space (열공간) 직관적 설명"
  },
  {
    "objectID": "posts/Linear Algebra/행렬곱에 대한 여러가지 시각.html",
    "href": "posts/Linear Algebra/행렬곱에 대한 여러가지 시각.html",
    "title": "행렬곱에 대한 여러가지 관점",
    "section": "",
    "text": "유튜브 - 혁펜하임님의 선형대수학 강의 정리용 입니다.\n\n행렬곱은 내적이다.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},B \\in \\mathbb{R}^{n \\times p} \\\\\n&AB =\n\\begin{bmatrix}\na_1^T\\\\\na_2^T\\\\\n\\vdots\\\\\na_m^T\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 & b_2 & \\dots & b_p\n\\end{bmatrix}=\n\\begin{bmatrix}\na_1^Tb_1 & a_1^Tb_2 & \\dots & a_1^Tb_p \\\\\na_2^Tb_1 & a_2^Tb_2 & \\dots & a_2^Tb_p \\\\\n\\vdots & \\vdots  & \\vdots & \\vdots \\\\\na_{m}^Tb_1 & a_m^Tb_2 & \\dots & a_m^Tb_p \\\\\n\\end{bmatrix}\n\\end{aligned}\\]\n\n\n행렬곱은 rank-1 matrix의 합이다.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},B \\in \\mathbb{R}^{n \\times p} \\\\\n&AB =\n\\begin{bmatrix}\na_1 & a_2 & \\dots & a_n\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1^T \\\\\nb_2^T \\\\\n\\vdots \\\\\nb_n^T\n\\end{bmatrix}\n= a_1b_1^T + a_2b_2^T + \\dots + a_nb_n^T\n\\end{aligned}\\]\n\n\n행렬과 벡터의 곱은 열공간에 속한 임의의 벡터이다.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},x \\in \\mathbb{R}^{n \\times 1} \\\\\n&Ax =\n\\begin{bmatrix}\na_1 & a_2 & \\dots & a_n\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n= a_1x_1 + a_2x_2 + \\dots + a_nx_n\n\\end{aligned}\\]\n\\(a_1,a_2,\\dots,a_n\\)은 벡터 \\(x_1,x_2,\\dots,x_n\\)은 스칼라이다. 행렬곱은 위와같이 열공간의 기저(행렬\\(A\\)의 열벡터)와 스칼라(미지수벡터\\(x\\)의 원소)와의 일차결합이므로 기저인 열벡터가 생성(span)하는 열공간의 원소이다. 이는 방정식 \\(Ax=b\\)의 해의 갯수를 알아내는데에 사용하는 중요한 개념이다.(참고 : 방정식 Ax = b의 해의 갯수 알아내기)\n열공간(column space) : 행렬에서 (열)벡터의 일차결합으로 생성되는 벡터공간. 열벡터의 span\n\n\n행벡터와 행렬의 곱은 row space(행공간)에 속한 임의의 벡터이다.\n\\[\\begin{aligned}\n&\\text{Let }x \\in \\mathbb{R}^{1 \\times n},X \\in \\mathbb{R}^{n \\times p} \\\\\n&xA =\n\\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\n\\vdots \\\\\na_n\n\\end{bmatrix}\n= x_1a_1^T + x_2a_2^T + \\dots + x_na_n^T\n\\end{aligned}\\]\n\\(a_1^T,a_2^T,\\dots,a_n^T\\)은 벡터 \\(x_1,x_2,\\dots,x_n\\)은 스칼라이다. 행렬곱은 위와같이 행공간의 기저(행렬\\(A\\)의 행벡터)와 스칼라(\\(x\\)의 원소)와의 일차결합으로 기저인 행벡터가 생성하는 행공간의 원소이다.\n행공간 : 행렬에서 행벡터의 일차결합으로 생성되는 벡터공간. 행벡터의 span\n\n\n참고문헌\n혁펜하임 - 선대 2-5강. 행렬의 곱셈과 네 가지 관점 (열공간 (column space) 등)"
  },
  {
    "objectID": "posts/numpy/np.meshgrid.html",
    "href": "posts/numpy/np.meshgrid.html",
    "title": "np.meshgrid",
    "section": "",
    "text": "np.meshgrid"
  },
  {
    "objectID": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html",
    "href": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html",
    "title": "Finite Difference Method with np.gradient",
    "section": "",
    "text": "\\({\\bf x} =\\begin{bmatrix}x_1&x_2&\\dots&x_m\\end{bmatrix}^T\\)일 때, \\(x\\)에 대한 다변수함수 \\(f({\\bf x})\\)의 gradient는 다음과 같다.\n\\[\\begin{aligned}\n&\\text{gradient of }f({\\bf{x}}) = \\frac{\\partial f}{\\partial {\\bf x}} = \\nabla f(\\bf{x}) =\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_m}\n\\end{pmatrix}\n\\end{aligned}\\]\n함수가 가지는 모든 변수에 대해서 편미분 한 뒤 모아놓은 벡터라고 생각하면 된다. 함수의 수식을 알고 미분이 가능하면 우리는 해석적으로 미분해서(미분공식써서) 그레디언트를 구하고 각각의 어떤 point에서의 편미분계수들도 구할 수 있다. 그러나 우리가 주어진 데이터는 함수f의 함숫값들이 주어진다. 예를 들면 다음과 같다.\n\nimport numpy as np\nf = np.array([1,2,4,7,11,17],dtype = float)\nprint(\"f(x)\")\nprint(f)\n\nf(x)\n[ 1.  2.  4.  7. 11. 17.]\n\n\n위와 같은 함숫값들만 주어질때에는 원래의 함수를 알기는 불가능하다. 따라서 도함수를 통한 정확한 미분계수를 구하기가 불가능하므로 주어진 데이터로 \\(\\bf x\\)에서의 미분계수의 값을 근사적으로 구할 수 있는데 이를 수치미분이라 한다."
  },
  {
    "objectID": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#차원-배열의-경우",
    "href": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#차원-배열의-경우",
    "title": "Finite Difference Method with np.gradient",
    "section": "1차원 배열의 경우",
    "text": "1차원 배열의 경우\n\nEx) \\(dx\\) = 1\n넘파이 1차원 배열이 다음과 같이 주어져 있다고 하자.\n\nfrom IPython.display import display, Markdown\nf = np.array([1,2,4,7,11,16],dtype=float)\nprint(f)\n\n[ 1.  2.  4.  7. 11. 16.]\n\n\nnp.gradient함수는 1차원 배열의 내부에 있는 각각의 값들은 \\(x\\)값이 거리가 \\(dx\\)=1씩 변화할때마다의 함숫값\\(f(x)\\)들로 이해한다. 즉,다음과 같다.\n\nfor i in range(len(f)):\n    display(Markdown(rf'$x_{i+1}$에서의 함숫값 $f(x_{i+1})$ = {f[i]}'))\n\n\\(x_1\\)에서의 함숫값 \\(f(x_1)\\) = 1.0\n\n\n\\(x_2\\)에서의 함숫값 \\(f(x_2)\\) = 2.0\n\n\n\\(x_3\\)에서의 함숫값 \\(f(x_3)\\) = 4.0\n\n\n\\(x_4\\)에서의 함숫값 \\(f(x_4)\\) = 7.0\n\n\n\\(x_5\\)에서의 함숫값 \\(f(x_5)\\) = 11.0\n\n\n\\(x_6\\)에서의 함숫값 \\(f(x_6)\\) = 16.0\n\n\n위에서 넘파이의 그래디언트는 끝값을 제외한 내부의 요소에는 중앙차분근사 양끝값에 대해서는 후향차분 또는 전향차분을 사용한다고 언급했었다. 계산한,\\(x_2,x_5\\)에서 미분계수의 2차중앙차분근사는 다음과 같다.\n\\[\\begin{aligned}\n&f^{'}(x_2) \\overset{\\sim}{=} \\frac{f(x_3)-f(x_1)}{2h} = \\frac{4-1}{2} = 1.5 \\\\\n&f^{'}(x_5) \\overset{\\sim}{=} \\frac{f(x_6)-f(x_4)}{2h} = \\frac{16-7}{2} = 4.5 \\\\\n&\\text{where, } h = x_3-x_2 = x_2-x_1 = 1\n\\end{aligned}\\]\n1차원 배열의 가장 처음에 오는 값에 전향차분근사를 사용하고 가장 마지막에 오는 값에서는 후향차분근사를 사용한다.\n\\[\\begin{aligned}\n&f^{'}(x_1) = \\frac{f(x_2) - f(x_1)}{h} = \\frac{2-1}{1} = 1 \\\\\n&f^{'}(x_6) = \\frac{f(x_6) - f(x_5)}{h} = \\frac{16-11}{1} = 5\n\\end{aligned}\\]\n계산한 값과 실제로 일치하는지 확인.\n\nnum_diff = np.gradient(f)\nprint(\"np.gradient의 출력값\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$에서의 도함수의 근삿값 $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradient의 출력값\n[1.  1.5 2.5 3.5 4.5 5. ]\n\n\n\\(x_1\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 1.0\n\n\n\\(x_2\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 1.5\n\n\n\\(x_3\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 2.5\n\n\n\\(x_4\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 3.5\n\n\n\\(x_5\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 4.5\n\n\n\\(x_6\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 5.0\n\n\n\n\nEx) \\(dx \\not = 1\\) (default가 아닐 경우)\n거리\\(dx=2\\)일때 계산한,\\(x_2,x_5\\)에서 미분계수의 2차중앙차분근사는 다음과 같다.\n\\[\\begin{aligned}\n&f^{'}(x_2) \\overset{\\sim}{=} \\frac{f(x_3)-f(x_1)}{2h} = \\frac{4-1}{4} = 0.75 \\\\\n&f^{'}(x_5) \\overset{\\sim}{=} \\frac{f(x_6)-f(x_4)}{2h} = \\frac{16-7}{4} = 2.25 \\\\\n&\\text{where, } h = x_3-x_1 = x_6-x_4 = 2\n\\end{aligned}\\]\n거리가 \\(dx=2\\)일때 배열의 양 끝값에서 전향,후향차분근사를 통한 미분계수의 값은 다음과 같다.\n\\[\\begin{aligned}\n&f^{'}(x_1) = \\frac{f(x_2) - f(x_1)}{h} = \\frac{2-1}{2} = 0.5 \\\\\n&f^{'}(x_6) = \\frac{f(x_6) - f(x_5)}{h} = \\frac{16-11}{2} = 2.5\n\\end{aligned}\\]\nx값 사이의 거리\\(dx\\)를 바꾸고 싶다면? => 두번째 인수에 스칼라 대입하면 된다.\n\ndx = 2\nnum_diff = np.gradient(f,dx)\nprint(\"np.gradient의 출력값\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$에서의 도함수의 근삿값 $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradient의 출력값\n[0.5  0.75 1.25 1.75 2.25 2.5 ]\n\n\n\\(x_1\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 0.5\n\n\n\\(x_2\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 0.75\n\n\n\\(x_3\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 1.25\n\n\n\\(x_4\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 1.75\n\n\n\\(x_5\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 2.25\n\n\n\\(x_6\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 2.5\n\n\n\n\nEx) x값의 좌표를 직접 정해주는 경우\n이전에는 각각의 인덱스간의 거리는 모두 동일하게 기본값 1이거나 다른값을 사용했다. 그러지 않고 \\(x_1,x_2,\\dots,x_6\\)의 좌표를 직접 지정해주는 것도 가능하다. 함수의 2번재 인수에 좌표를 직접 넣어주면 된다.\n먼저 x값의 좌표를 다음과 같다고 해보자.\n\nx = np.array([0., 1., 1.5, 3.5, 4., 6.], dtype=float)\nf = np.array([1,2,4,7,11,16],dtype=float)\nprint(\"각각의 좌표와 함숫값\")\nfor i in range(len(x)):\n    display(Markdown(rf'$x_{i+1}$ = {x[i]}, $f(x_{i+1})$ = {f[i]}'))\n\n각각의 좌표와 함숫값\n\n\n\\(x_1\\) = 0.0, \\(f(x_1)\\) = 1.0\n\n\n\\(x_2\\) = 1.0, \\(f(x_2)\\) = 2.0\n\n\n\\(x_3\\) = 1.5, \\(f(x_3)\\) = 4.0\n\n\n\\(x_4\\) = 3.5, \\(f(x_4)\\) = 7.0\n\n\n\\(x_5\\) = 4.0, \\(f(x_5)\\) = 11.0\n\n\n\\(x_6\\) = 6.0, \\(f(x_6)\\) = 16.0\n\n\n각각의 좌표에서 도함수의 근삿값을 구하면 아래와 같다.(수식 계산은 잘 모르겠네요 … 추후에 더 공부하겠습니다!)\n\nnum_diff = np.gradient(f,x)\nprint(\"np.gradient의 출력값\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$에서의 도함수의 근삿값 $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradient의 출력값\n[1.  3.  3.5 6.7 6.9 2.5]\n\n\n\\(x_1\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 1.0\n\n\n\\(x_2\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 2.9999999999999996\n\n\n\\(x_3\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 3.5\n\n\n\\(x_4\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 6.700000000000001\n\n\n\\(x_5\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 6.899999999999999\n\n\n\\(x_6\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 2.5"
  },
  {
    "objectID": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#차원-배열의-경우-1",
    "href": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#차원-배열의-경우-1",
    "title": "Finite Difference Method with np.gradient",
    "section": "2차원 배열의 경우",
    "text": "2차원 배열의 경우\n2차원 배열의 경우 axis=0(세로축)과 axis=1(가로축) 두 축방향으로 계산한 도함수의 근삿값을 반환한다.axis=0일 경우 각각의 열마다 따로따로 독립적으로 \\(x_1,x_2...\\)에 대한 함숫값\\(f(x_1),f(x_2),\\dots\\)이 있다고 생각하면 되고 axis=1일 경우 각각의 행마다 따로따로 독립적으로 \\(x_1,x_2...\\)에 대한 함숫값\\(f(x_1),f(x_2),\\dots\\)이 있다고 생각하면 된다.또한 1차원 배열과 유사하게 각각의 행,열의 끝값에는 전향or후향차분근사를 행,열의 내부에 있는 값은 중앙차분근사를 사용한다.\n\nEx) \\(dx=1,dy=1\\)\n2차원 배열은 다음과 같다.\n\nnp.array([[1, 2, 6], [3, 4, 5]], dtype=float)\n\narray([[1., 2., 6.],\n       [3., 4., 5.]])\n\n\n\nax0_difcoef,ax1_difcoef= np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float))\nprint(f'axis = 0 방향으로 도함수의 근삿값 계산 \\n{ax0_difcoef}')\nprint(f'axis = 1 방향으로 도함수의 근삿값 계산 \\n{ax1_difcoef}')\n\naxis = 0 방향으로 도함수의 근삿값 계산 \n[[ 2.  2. -1.]\n [ 2.  2. -1.]]\naxis = 1 방향으로 도함수의 근삿값 계산 \n[[1.  2.5 4. ]\n [1.  1.  1. ]]\n\n\n\n\nEx) \\(dx \\not = 1,dy \\not = 1\\) (default가 아닌 경우)\n각각의 행,열마다 거리를 따로 설정해주고 싶은 경우? => 스칼라 2개 인수로 전달.\n\ndx = 2;dy = 2\nax0_difcoef,ax1_difcoef= np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float),dx,dy)\nprint(f'axis = 0 방향으로 도함수의 근삿값 계산 \\n{ax0_difcoef}')\nprint(f'axis = 1 방향으로 도함수의 근삿값 계산 \\n{ax1_difcoef}')\n\naxis = 0 방향으로 도함수의 근삿값 계산 \n[[ 1.   1.  -0.5]\n [ 1.   1.  -0.5]]\naxis = 1 방향으로 도함수의 근삿값 계산 \n[[0.5  1.25 2.  ]\n [0.5  0.5  0.5 ]]"
  },
  {
    "objectID": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#전향차분근사-유도",
    "href": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#전향차분근사-유도",
    "title": "Finite Difference Method with np.gradient",
    "section": "전향차분근사 유도",
    "text": "전향차분근사 유도\n\\(x_1,x_2,\\dots,x_{i-1},x_i,x_{i+1},\\dots,x_n\\)과 각각에 대응하는 함숫값 \\(f(x_1),f(x_2),\\dots,f(x_{i-1}),f(x_i),f(x_{i+1}),\\dots,f(x_n)\\) 주어진 데이터라고 가정하자. 목적은 x_i에서의 미분계수를 구하는 것이다. \\(a = x_i\\)에서 함수\\(f(x)\\)의 테일러 급수 근사는 다음과 같다. \\[f(x) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x-x_i) + \\frac{f^{''}(x_i)}{2!}(x-x_i)^2 + \\dots \\]\n\\(x=x_{i+1}\\)에서의 함숫값은 다음과 같다. \\[f(x_{i+1}) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x_{i+1}-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x_{i+1}-x_i) + \\frac{f^{''}(x_i)}{2!}(x_{i+1}-x_i)^2 + \\dots \\]\n\\(f'(x_i)\\)가 포함된항만 남겨두고 나머지는 이항하면 다음과 같다. \\[f^{'}(x_i)(x_{i+1}-x_i) = f(x_{i+1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}(x_{i+1}-x_i)^2 + \\dots\\]\n\\(h = x_{i+1}-x_i\\)로 두고 양변을 h로 나누면 다음과 같다. \\[f'(x_i) = \\frac{f(x_{i+1})}{h} - \\frac{f(x_i)}{h} - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!}\\]\n여기서 우변의 두개의 항만 남겨두고 \\(O(h) = - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\)라 하면 다음과 같다. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i+1})-f(x_i)}{h}\\\\\n&O(h) = - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\\\\n&\\text{where, } h = x_{i+1} - x_i \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#후향차분근사-유도",
    "href": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#후향차분근사-유도",
    "title": "Finite Difference Method with np.gradient",
    "section": "후향차분근사 유도",
    "text": "후향차분근사 유도\n\\(x_1,x_2,\\dots,x_{i-1},x_i,x_{i+1},\\dots,x_n\\)과 각각에 대응하는 함숫값 \\(f(x_1),f(x_2),\\dots,f(x_{i-1}),f(x_i),f(x_{i+1}),\\dots,f(x_n)\\) 주어진 데이터라고 가정하자. 목적은 x_i에서의 미분계수를 구하는 것이다. \\(a = x_i\\)에서 함수\\(f(x)\\)의 테일러 급수 근사는 다음과 같다. \\[f(x) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x-x_i) + \\frac{f^{''}(x_i)}{2!}(x-x_i)^2 + \\dots \\]\n\\(f(x_i)\\)는 다음과 같다. \\[f(x_{i-1}) = \\sum_{n=0}^{\\infty}\\frac{f^n(x_i)}{n!}(x_{i-1}-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x_{i-1}-x_i) + \\frac{f^{''}(x_i)}{2!}(x_{i-1}-x_i)^2+\\dots \\]\n1차미분이 포함된 항만 남기고 나머지는 이항하면 다음과 같다. \\[f^{'}(x_i)(x_{i-1}-x_i) = f(x_{i-1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}(x_{i-1}-x_i)^2-\\frac{f^{'''}(x_i)}{3!}(x_{i-1}-x_i)^3-\\dots \\]\n\\(h = x_{i} - x_{i-1}\\)로 놓으면 다음과 같다. \\[f^{'}(x_i)(-h) = f(x_{i-1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}h^2+\\frac{f^{'''}(x_i)}{3!}h^3-\\dots \\]\n양변을 \\(-h\\)로 나누면 다음과 같다.\n\\[\\begin{aligned}\nf^{'}(x_i) &= \\frac{f(x_{i-1})}{-h} + \\frac{f(x_i)}{h} + \\frac{f^{''}(x_i)}{2!}h-\\frac{f^{'''}(x_i)}{3!}h^2+\\dots \\\\\n&=\\frac{f(x_i)-f(x_{i-1}) }{h} +  \\frac{f^{''}(x_i)}{2!}h-\\frac{f^{'''}(x_i)}{3!}h^2+\\dots\n\\end{aligned}\\]\n마찬가지로 우변의 두개 항만 남겨두고 \\(O(h) = \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} + \\dots\\)라 하면 다음과 같다. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i})-f(x_{i-1})}{h}\\\\\n&O(h) = \\frac{h}{2!}f^{''}(x_i) - \\frac{h^2}{3!}f{'''}(x_i)+\\dots \\\\\n&\\text{where, } h = x_{i} - x_{i-1}, \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#중앙차분근사-유도",
    "href": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#중앙차분근사-유도",
    "title": "Finite Difference Method with np.gradient",
    "section": "중앙차분근사 유도",
    "text": "중앙차분근사 유도\n전향차분근사와 후향차분근사의 유도과정에서의 테일러 전개식은 다음과 같다. \\[f'(x_i) = \\frac{f(x_{i+1})}{h} - \\frac{f(x_i)}{h} - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - ...\\] \\[f'(x_i) = \\frac{f(x_{i})}{h} - \\frac{f(x_{i-1})}{h} + \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} + ...\\]\n두 식을 더해주면 다음과 같다.\n\\[\\begin{aligned}\n&2f^{'}(x_i) = \\frac{f(x_{i+1})-f(x_{i-1})}{h} - \\frac{2h^2f^{'''}(x_i)}{3!} \\\\\n&\\Leftrightarrow f^{'}(x_i) = \\frac{f(x_{i+1})-f(x_{i-1})}{2h} - \\frac{h^2f^{'''}(x_i)}{3!} - ...\n\\end{aligned}\\]\n마찬가지로 우변의 두개 항만 남겨두고 절단오차\\(O(h^2) = -\\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\)라 하면 다음과 같다. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i+1})-f(x_{i-1})}{2h}\\\\\n&O(h) = - \\frac{h^2}{3!}f{'''}(x_i)+\\dots \\\\\n&\\text{where, } h = x_{i} - x_{i-1}\\,\\,\\text{or}\\,\\, h = x_{i+1} - x_i\\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html",
    "href": "posts/open/Dakon competetion chisquare test.html",
    "title": "Untitled",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import chisquare\nimport warnings\nwarnings.filterwarnings('ignore')\ntest_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/test.csv\"\ntrain_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/train.csv\"\n\n\nclass CFG:\n    SEED = 42\n\n\ntrain = pd.read_csv(train_path)\ntrain_len = len(train)\ntest = pd.read_csv(test_path)\nid_test = test[\"id\"]\ntest = pd.read_csv(test_path)\n\n\ndataset = pd.concat([train,test],axis=0).reset_index(drop=True)\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      id\n      father\n      mother\n      gender\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      ...\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      0\n      0\n      0\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      ...\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      0\n      0\n      0\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      ...\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      0\n      0\n      0\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      ...\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      TRAIN_003\n      0\n      0\n      0\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      ...\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      TRAIN_004\n      0\n      0\n      0\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      ...\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      0\n      0\n      0\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      ...\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      0\n      0\n      0\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      ...\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      0\n      0\n      0\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      ...\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      0\n      0\n      0\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      ...\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      0\n      0\n      0\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      ...\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 21 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#fathermothergender",
    "href": "posts/open/Dakon competetion chisquare test.html#fathermothergender",
    "title": "Untitled",
    "section": "father,mother,gender",
    "text": "father,mother,gender\n\nfather,mother,gender = 0 => drop\n\n\ndataset = dataset.drop(columns = [\"father\",\"mother\",\"gender\"])\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      id\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      TRAIN_003\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      TRAIN_004\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 18 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_01",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_01",
    "title": "Untitled",
    "section": "SNP_01",
    "text": "SNP_01\n\nclass = B 인 경우, AA는 아예 없음 = > feature extraction hasGG 추가\n\n추후 고려사항 - class = A 인 경우, AA가 좀 높음 - class = B 인 경우, GG가 압도적으로 높음 - class = C 인 경우, GG가 좀 높음\n\ncol = \"SNP_01\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_02",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_02",
    "title": "Untitled",
    "section": "SNP_02",
    "text": "SNP_02\n\nclass = A인 경우, AA는 없음 => f.e has02AA\n\n추후 고려 - B,의 경우 AG->GG-AA 순 - C,의 경우 AG->GG->AA 순\n\ncol = \"SNP_02\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_03",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_03",
    "title": "Untitled",
    "section": "SNP_03",
    "text": "SNP_03\n\nA인 경우 , 무조건 AA만 존재 => feature extraction\n\n추후 고려 나머지는 뭐 대충 고르게\n\ncol = \"SNP_03\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    plt.ylim(20,60)\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_04",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_04",
    "title": "Untitled",
    "section": "SNP_04",
    "text": "SNP_04\n\nC인 경우 , GG는 없음 => feature extraction has04GG\n\n추후 고려 나머지는 뭐 대충 고르게 - class A인 경우,AA가 압도적으로 낮음 => feature extraction\n\ncol = \"SNP_04\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_05",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_05",
    "title": "Untitled",
    "section": "SNP_05",
    "text": "SNP_05\n\nclass = A인 경우, CC는 없음\n\n\ncol = \"SNP_05\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_06",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_06",
    "title": "Untitled",
    "section": "SNP_06",
    "text": "SNP_06\n\nclass = A인 경우, AA는 없음 => feature extraction has06AA\n\n\ncol = \"SNP_06\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_07",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_07",
    "title": "Untitled",
    "section": "SNP_07",
    "text": "SNP_07\n\nA의 경우 => AA는 없음 =>f.e has07AA\nB,C의 경우 => GG는 없음=>f.e has07GG\n\n\ncol = \"SNP_07\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_08",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_08",
    "title": "Untitled",
    "section": "SNP_08",
    "text": "SNP_08\n\nA의 경우 => GG는 없음 =>f.e has08GG\n\n\ncol = \"SNP_08\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_09",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_09",
    "title": "Untitled",
    "section": "SNP_09",
    "text": "SNP_09\n\nC의 경우 => 대부분이 AA이고 GA와 GG는 사실 없다고 봐도 무방=> f.e has09AA\nB의 경우 => GG는 거의 없음 => f.e has09GG\n\n앞선 경우들은 빈도수가 낮은 경우네는 따로 feature extraction을 안해줬는데 왜 여기서는 해? => 여기서는 낮은 것들의 빈도수가 1로 너무 낮음,그래서 여기는 함.\n\ncol = \"SNP_09\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset.loc[dataset[\"class\"] == \"B\",col].value_counts()\n\nA A    91\nG A    22\nG G     1\nName: SNP_09, dtype: int64\n\n\n\ndataset.loc[dataset[\"class\"] == \"C\",col].value_counts()\n\nA A    78\nG A     1\nName: SNP_09, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_10",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_10",
    "title": "Untitled",
    "section": "SNP_10",
    "text": "SNP_10\n\nA와 B에서 낮은값들이 있긴 한데 … 그래도 특이점 1인 정도는 아님\n\n\ncol = \"SNP_10\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"B\",col]\nx.value_counts()\n\nG G    110\nA G      4\nName: SNP_10, dtype: int64\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"A\",col]\nx.value_counts()\n\nA G    34\nA A    32\nG G     3\nName: SNP_10, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_11",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_11",
    "title": "Untitled",
    "section": "SNP_11",
    "text": "SNP_11\n\nA의 경우 => AA는 없음 =>f.e has11AA\n\n\ncol = \"SNP_11\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_12",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_12",
    "title": "Untitled",
    "section": "SNP_12",
    "text": "SNP_12\n\nA의 경우 => AA가 있는 관측치가 거의 없음,GG가 있는 관측치는 많음\nB,C의 경우 => GG가 있는 관측치가 거의 없음,AA가 있는 관측치는 많음\n\n=>f.e has12AA =>f.e has12GG\n\ncol = \"SNP_12\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset.loc[dataset[\"class\"] == \"A\",col].value_counts()\n\nG G    48\nG A    20\nA A     1\nName: SNP_12, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_13",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_13",
    "title": "Untitled",
    "section": "SNP_13",
    "text": "SNP_13\n\nA의 경우 => AA는 없음 => f.e has13AA\n\n\ncol = \"SNP_13\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"A\",col]\nx.value_counts()\n\nG G    66\nA G     3\nName: SNP_13, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_14",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_14",
    "title": "Untitled",
    "section": "SNP_14",
    "text": "SNP_14\n\nB의 경우 => AA만 존재 =>f.e has14AA 를 추가해서 AA인것들의 계수를 결정하도록\nC의 경우 CC가 있긴 한데 .. 엄청낮긴함(그래도 1은 아니니까 f.e는 안함)\n\n\ncol = \"SNP_14\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"C\",col]\nx.value_counts()\n\nA A    60\nC A    17\nC C     2\nName: SNP_14, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_15",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_15",
    "title": "Untitled",
    "section": "SNP_15",
    "text": "SNP_15\n\n별다른 특징 없음\n\n\ncol = \"SNP_15\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      id\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      TRAIN_003\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      TRAIN_004\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 18 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#class별-관측치의-숫자",
    "href": "posts/open/Dakon competetion chisquare test.html#class별-관측치의-숫자",
    "title": "Untitled",
    "section": "class별 관측치의 숫자",
    "text": "class별 관측치의 숫자\n\nclass imbalance? => No\n\n\nx = dataset[\"class\"].value_counts().index\ny = dataset[\"class\"].value_counts().values\nplt.bar(x,y)\n\n<BarContainer object of 3 artists>"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#chi2-test-between-abc",
    "href": "posts/open/Dakon competetion chisquare test.html#chi2-test-between-abc",
    "title": "Untitled",
    "section": "chi^2 test, between [“A”,“B”,“C”]",
    "text": "chi^2 test, between [“A”,“B”,“C”]\n\n목적 : 각각의 클래스에서 각각의 SNP변수에서 나오는 G G or A g 등등.. 의 값의 빈도수가 같은지 아니면 다른지 카이제곱검정을 통해서 파악한 후, 클래스별로 빈도수 차이있으면 그 행만 추가\n\n\n\"\"\"\n#1 각각의 독립변수(검정에서는 class)별로 SNP_01의 특정값(예를 들면 GG)가 나오는 관측치 세어보기\n\n#컬럼,클래스 지정\nsnp_name = [col_name for col_name in dataset.columns if \"SNP\" in col_name]\nclass_name = [\"A\",\"B\",\"C\"]\n\nsignif_col = []\nunsignif_col = []\nfor snp in snp_name:\n    _snp_unique = dataset.loc[:,snp].unique().tolist()\n    for unq_vl in _snp_unique:\n        _condition = (dataset.loc[:,snp] == unq_vl)\n        _data = dataset.loc[_condition,[snp,\"class\"]].dropna(axis=0).value_counts().droplevel(axis=0,level=0).copy()\n        _data_class = _data.index\n        for cl_name in class_name:\n            if cl_name not in _data_class:\n                #print(\"존재하지 않는 class : \",cl_name)\n                #print(\"존재하지 않는 클래스 추가후 df\")\n                _data = _data.append(pd.Series({cl_name:0}))\n\n        _data = _data[class_name]\n        #특정컬럼의 특정값에 대해서 카이제곱검정 수행\n        #예를 들어서 각각의 클래스 A,B,C에서 SNP_01의 A A값을 가지는 빈도가 같은지 다른지 수행\n        f_obs = _data.values.tolist()\n        p_value = chisquare(f_obs)[1].round(5)\n        if p_value < 0.01: #유의확률 0.01\n            #print(f\"{snp}={unq_vl}\")\n            #print(\"p_value :\",p_value)\n            name = snp+\"=\"+unq_vl\n            signif_col.append(name)\n        else:\n            name = snp+\"=\"+unq_vl\n            unsignif_col.append(name)\nlen(signif_col),len(unsignif_col)\n\"\"\"\n\n'\\n#1 각각의 독립변수(검정에서는 class)별로 SNP_01의 특정값(예를 들면 GG)가 나오는 관측치 세어보기\\n\\n#컬럼,클래스 지정\\nsnp_name = [col_name for col_name in dataset.columns if \"SNP\" in col_name]\\nclass_name = [\"A\",\"B\",\"C\"]\\n\\nsignif_col = []\\nunsignif_col = []\\nfor snp in snp_name:\\n    _snp_unique = dataset.loc[:,snp].unique().tolist()\\n    for unq_vl in _snp_unique:\\n        _condition = (dataset.loc[:,snp] == unq_vl)\\n        _data = dataset.loc[_condition,[snp,\"class\"]].dropna(axis=0).value_counts().droplevel(axis=0,level=0).copy()\\n        _data_class = _data.index\\n        for cl_name in class_name:\\n            if cl_name not in _data_class:\\n                #print(\"존재하지 않는 class : \",cl_name)\\n                #print(\"존재하지 않는 클래스 추가후 df\")\\n                _data = _data.append(pd.Series({cl_name:0}))\\n\\n        _data = _data[class_name]\\n        #특정컬럼의 특정값에 대해서 카이제곱검정 수행\\n        #예를 들어서 각각의 클래스 A,B,C에서 SNP_01의 A A값을 가지는 빈도가 같은지 다른지 수행\\n        f_obs = _data.values.tolist()\\n        p_value = chisquare(f_obs)[1].round(5)\\n        if p_value < 0.01: #유의확률 0.01\\n            #print(f\"{snp}={unq_vl}\")\\n            #print(\"p_value :\",p_value)\\n            name = snp+\"=\"+unq_vl\\n            signif_col.append(name)\\n        else:\\n            name = snp+\"=\"+unq_vl\\n            unsignif_col.append(name)\\nlen(signif_col),len(unsignif_col)\\n'"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#chi2-test-between-bc",
    "href": "posts/open/Dakon competetion chisquare test.html#chi2-test-between-bc",
    "title": "Untitled",
    "section": "chi^2 test, between “B”,“C”",
    "text": "chi^2 test, between “B”,“C”\n\n목적 : 각각의 클래스에서 각각의 SNP변수에서 나오는 G G or A g 등등.. 의 값의 빈도수가 같은지 아니면 다른지 카이제곱검정을 통해서 파악한 후, 클래스별로 빈도수 차이있으면 그 행만 추가\n\n\n# EDA과정에서 trait == 1 이면 반드시 A였음,따라서 trait 변수는 제거하고 나중에 trait == 1이면 반드시 1로 제출\nidx = dataset[dataset.trait == 1].index\n_dt = dataset.drop(index=idx)\n\n\n_dt\n\n\n\n\n\n  \n    \n      \n      id\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      4\n      TRAIN_004\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      5\n      TRAIN_005\n      2\n      G G\n      G G\n      C A\n      A A\n      C C\n      A A\n      A A\n      G A\n      A A\n      G G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n317 rows × 18 columns\n\n\n\n\n#1 각각의 독립변수(검정에서는 class)별로 SNP_01의 특정값(예를 들면 GG)가 나오는 관측치 세어보기\n\n#컬럼,클래스 지정\nsnp_name = [col_name for col_name in _dt.columns if \"SNP\" in col_name]\nclass_name = [\"B\",\"C\"]\n\nsignif_col = []\nunsignif_col = []\nfor snp in snp_name:\n    _snp_unique = _dt.loc[:,snp].unique().tolist()\n    for unq_vl in _snp_unique:\n        _condition = (_dt.loc[:,snp] == unq_vl)\n        _data = _dt.loc[_condition,[snp,\"class\"]].dropna(axis=0).value_counts().droplevel(axis=0,level=0).copy()\n        _data_class = _data.index\n        for cl_name in class_name:\n            if cl_name not in _data_class:\n                #print(\"존재하지 않는 class : \",cl_name)\n                #print(\"존재하지 않는 클래스 추가후 df\")\n                _data = _data.append(pd.Series({cl_name:0}))\n\n        _data = _data[class_name]\n        #특정컬럼의 특정값에 대해서 카이제곱검정 수행\n        #예를 들어서 각각의 클래스 A,B,C에서 SNP_01의 A A값을 가지는 빈도가 같은지 다른지 수행\n        f_obs = _data.values.tolist()\n        p_value = chisquare(f_obs)[1].round(5)\n        if p_value < 0.05: #유의확률 0.01\n            #print(f\"{snp}={unq_vl}\")\n            #print(\"p_value :\",p_value)\n            name = snp+\"_\"+unq_vl\n            signif_col.append(name)\n        else:\n            name = snp+\"_\"+unq_vl\n            unsignif_col.append(name)\nlen(signif_col),len(unsignif_col)\n\n(27, 17)\n\n\n\nsignif_col\n\n['SNP_01_G G',\n 'SNP_01_A A',\n 'SNP_02_A G',\n 'SNP_02_G G',\n 'SNP_02_A A',\n 'SNP_03_C A',\n 'SNP_03_C C',\n 'SNP_04_G A',\n 'SNP_04_A A',\n 'SNP_04_G G',\n 'SNP_05_A A',\n 'SNP_05_C C',\n 'SNP_06_A G',\n 'SNP_07_A A',\n 'SNP_07_G A',\n 'SNP_08_G G',\n 'SNP_08_A A',\n 'SNP_09_G A',\n 'SNP_10_G G',\n 'SNP_10_A G',\n 'SNP_10_A A',\n 'SNP_11_A G',\n 'SNP_11_G G',\n 'SNP_13_A A',\n 'SNP_14_A A',\n 'SNP_14_C A',\n 'SNP_15_A A']\n\n\n\nunsignif_col\n\n['SNP_01_A G',\n 'SNP_03_A A',\n 'SNP_05_C A',\n 'SNP_06_A A',\n 'SNP_06_G G',\n 'SNP_08_G A',\n 'SNP_09_A A',\n 'SNP_09_G G',\n 'SNP_11_A A',\n 'SNP_12_A A',\n 'SNP_12_G A',\n 'SNP_12_G G',\n 'SNP_13_G G',\n 'SNP_13_A G',\n 'SNP_14_C C',\n 'SNP_15_G A',\n 'SNP_15_G G']\n\n\n\n\"\"\"\nimport scipy.stats as stats\nimport numpy as np\n  \n# Make a 3 x 3 table\ndataset = np.array([[13, 17, 11], [4, 6, 9],\n                    [20, 31, 42]])\n  \n# Finding Chi-squared test statistic,\n# sample size, and minimum of rows\n# and columns\nX2 = stats.chi2_contingency(dataset, correction=False)\nN = np.sum(dataset)\nminimum_dimension = min(dataset.shape)-1\nX2\n\"\"\"\n\n'\\nimport scipy.stats as stats\\nimport numpy as np\\n  \\n# Make a 3 x 3 table\\ndataset = np.array([[13, 17, 11], [4, 6, 9],\\n                    [20, 31, 42]])\\n  \\n# Finding Chi-squared test statistic,\\n# sample size, and minimum of rows\\n# and columns\\nX2 = stats.chi2_contingency(dataset, correction=False)\\nN = np.sum(dataset)\\nminimum_dimension = min(dataset.shape)-1\\nX2\\n'"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#add-column-delete-column",
    "href": "posts/open/Dakon competetion chisquare test.html#add-column-delete-column",
    "title": "Untitled",
    "section": "add column & delete column",
    "text": "add column & delete column\n\ndef create_col(dataset,col,value):\n    _t = []\n    for val in dataset[col] == value:\n        if val == True:\n            _t.append(1)\n        else:\n            _t.append(0)\n    \n    col_name_base = \"has\"+col[-2:]\n    value_name = \"\"\n    for chr in value:\n        if chr != \" \":\n            value_name+=chr\n    col_name = col_name_base+value_name\n    print(col_name)\n    dataset[col_name] = _t\n\n    return dataset\n\n\"\"\"\ndataset = create_col(dataset,\"SNP_01\",\"G G\")\ndataset = create_col(dataset,\"SNP_02\",\"A A\")\ndataset = create_col(dataset,\"SNP_03\",\"A A\")\ndataset = create_col(dataset,\"SNP_04\",\"G G\")\ndataset = create_col(dataset,\"SNP_05\",\"C C\")\ndataset = create_col(dataset,\"SNP_06\",\"A A\")\ndataset = create_col(dataset,\"SNP_07\",\"A A\")\ndataset = create_col(dataset,\"SNP_07\",\"G G\")\ndataset = create_col(dataset,\"SNP_08\",\"G G\")\ndataset = create_col(dataset,\"SNP_09\",\"A A\")\ndataset = create_col(dataset,\"SNP_09\",\"G G\")\ndataset = create_col(dataset,\"SNP_11\",\"A A\")\ndataset = create_col(dataset,\"SNP_12\",\"A A\")\ndataset = create_col(dataset,\"SNP_12\",\"G G\")\ndataset = create_col(dataset,\"SNP_13\",\"A A\")\ndataset = create_col(dataset,\"SNP_14\",\"A A\")\n\"\"\"\n#dataset = dataset.drop(columns = [\"SNP_03\",\"SNP_04\",\"SNP_05\",\"SNP_06\",\"SNP_07\",\"SNP_08\",\"SNP_09\",\"SNP_11\",\"SNP_12\",\"SNP_13\",\"SNP_14\"])\n\n'\\ndataset = create_col(dataset,\"SNP_01\",\"G G\")\\ndataset = create_col(dataset,\"SNP_02\",\"A A\")\\ndataset = create_col(dataset,\"SNP_03\",\"A A\")\\ndataset = create_col(dataset,\"SNP_04\",\"G G\")\\ndataset = create_col(dataset,\"SNP_05\",\"C C\")\\ndataset = create_col(dataset,\"SNP_06\",\"A A\")\\ndataset = create_col(dataset,\"SNP_07\",\"A A\")\\ndataset = create_col(dataset,\"SNP_07\",\"G G\")\\ndataset = create_col(dataset,\"SNP_08\",\"G G\")\\ndataset = create_col(dataset,\"SNP_09\",\"A A\")\\ndataset = create_col(dataset,\"SNP_09\",\"G G\")\\ndataset = create_col(dataset,\"SNP_11\",\"A A\")\\ndataset = create_col(dataset,\"SNP_12\",\"A A\")\\ndataset = create_col(dataset,\"SNP_12\",\"G G\")\\ndataset = create_col(dataset,\"SNP_13\",\"A A\")\\ndataset = create_col(dataset,\"SNP_14\",\"A A\")\\n'"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#encoding",
    "href": "posts/open/Dakon competetion chisquare test.html#encoding",
    "title": "Untitled",
    "section": "encoding",
    "text": "encoding\n\n_dataset = dataset\n_dataset\n\n\n\n\n\n  \n    \n      \n      id\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      TRAIN_003\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      TRAIN_004\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 18 columns\n\n\n\n\nonehot-encoding trait != 1 (A클래스 빼고)\n\n_dt = _dataset.loc[_dataset.trait != 1,:].copy()\ncl = _dt[\"class\"]\n#print(cl)\none_hot_label = [col_name for col_name in _dt.columns if \"SNP\" in col_name]\nget_class = [\"id\"]+signif_col + [\"trait\",\"class\"]\ntrait_map = {1:0,2:1}\n\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\n_dt = pd.get_dummies(_dt,columns = one_hot_label)\n_dt[\"class\"] = cl.map(class_map)\ncond = ~pd.isna(_dt[\"class\"])\ntrain_ohe = _dt.loc[cond,:]\ntrain_ohe = train_ohe.loc[:,get_class]\ntrain_ohe[\"trait\"] = train_ohe[\"trait\"].map(trait_map)\ntrain_ohe[\"class\"] = train_ohe[\"class\"].astype(int)\n\ntest_ohe = _dt.loc[~cond,:][get_class]\ntest_ohe = test_ohe.drop(columns = \"class\")\ntest_ohe[\"trait\"] = test_ohe[\"trait\"].map(trait_map)\n\n\ntrain_id = train_ohe[\"id\"]\nX_train_ohe = train_ohe.drop(columns = [\"class\",\"id\"])\nY_train_ohe = train_ohe[\"class\"]\n\n\nX_train_ohe\n\n\n\n\n\n  \n    \n      \n      SNP_01_G G\n      SNP_01_A A\n      SNP_02_A G\n      SNP_02_G G\n      SNP_02_A A\n      SNP_03_C A\n      SNP_03_C C\n      SNP_04_G A\n      SNP_04_A A\n      SNP_04_G G\n      ...\n      SNP_10_G G\n      SNP_10_A G\n      SNP_10_A A\n      SNP_11_A G\n      SNP_11_G G\n      SNP_13_A A\n      SNP_14_A A\n      SNP_14_C A\n      SNP_15_A A\n      trait\n    \n  \n  \n    \n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      2\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      4\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      5\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      255\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      0\n      1\n      1\n      1\n      0\n      0\n      1\n    \n    \n      256\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      257\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      258\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      261\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n  \n\n193 rows × 28 columns\n\n\n\n\n\nlabel-encoding trait != 0 (A클래스 빼고)\n\n_dt = _dataset.loc[_dataset.trait != 1,:].copy()\ncl = _dt[\"class\"]\n#print(cl)\none_hot_label = [col_name for col_name in _dt.columns if \"SNP\" in col_name]\nget_class = [\"id\"]+signif_col + [\"trait\",\"class\"]\ntrait_map = {1:0,2:1}\n\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\n_dt = pd.get_dummies(_dt,columns = one_hot_label)\n_dt[\"class\"] = cl.map(class_map)\ncond = ~pd.isna(_dt[\"class\"])\ntrain_ohe = _dt.loc[cond,:]\ntrain_ohe = train_ohe.loc[:,get_class]\ntrain_ohe[\"trait\"] = train_ohe[\"trait\"].map(trait_map)\ntrain_ohe[\"class\"] = train_ohe[\"class\"].astype(int)\n\ntest_ohe = _dt.loc[~cond,:][get_class]\ntest_ohe = test_ohe.drop(columns = \"class\")\ntest_ohe[\"trait\"] = test_ohe[\"trait\"].map(trait_map)\n\n\n\nOne-hot encoding\n\n\"\"\"\n#one-hot encoding for distance base algorithm\ndataset_ohe = pd.get_dummies(_dataset,columns = _dataset.columns.drop(\"class\"),drop_first=True) #multicollinearity를 막기위한 drop_first 옵션\ntrain_ohe = dataset_ohe[:train_len].copy()\ntest_ohe = dataset_ohe[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain_ohe[\"class\"]=train_ohe[\"class\"].map(class_map).astype(int)\nX_train_ohe = train_ohe.drop(columns = \"class\")\nY_train_ohe = train_ohe[\"class\"]\n\"\"\"\n\n'\\n#one-hot encoding for distance base algorithm\\ndataset_ohe = pd.get_dummies(_dataset,columns = _dataset.columns.drop(\"class\"),drop_first=True) #multicollinearity를 막기위한 drop_first 옵션\\ntrain_ohe = dataset_ohe[:train_len].copy()\\ntest_ohe = dataset_ohe[train_len:].copy().drop(columns=\"class\")\\n\\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\\ntrain_ohe[\"class\"]=train_ohe[\"class\"].map(class_map).astype(int)\\nX_train_ohe = train_ohe.drop(columns = \"class\")\\nY_train_ohe = train_ohe[\"class\"]\\n'\n\n\n\n\nLabel encoding\n\n\"\"\"\nle_col = []\nfor col_name in dataset.columns.tolist():\n    if \"SNP\" in col_name:\n        le_col.append(col_name)\nle_col.append(\"trait\")\nfrom sklearn import preprocessing\nfor col in le_col:\n    le = preprocessing.LabelEncoder()\n    _col = dataset[col].tolist()\n    le.fit(_col)\n    dataset[col] = le.transform(_col)\ndataset\n\"\"\"\n\n'\\nle_col = []\\nfor col_name in dataset.columns.tolist():\\n    if \"SNP\" in col_name:\\n        le_col.append(col_name)\\nle_col.append(\"trait\")\\nfrom sklearn import preprocessing\\nfor col in le_col:\\n    le = preprocessing.LabelEncoder()\\n    _col = dataset[col].tolist()\\n    le.fit(_col)\\n    dataset[col] = le.transform(_col)\\ndataset\\n'\n\n\n\n\"\"\"\ntrain = dataset[:train_len].copy()\ntest = dataset[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain[\"class\"]=train[\"class\"].map(class_map).astype(int)\nX_train = train.drop(columns = \"class\")\nY_train = train[\"class\"]\n\"\"\"\n\n'\\ntrain = dataset[:train_len].copy()\\ntest = dataset[train_len:].copy().drop(columns=\"class\")\\n\\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\\ntrain[\"class\"]=train[\"class\"].map(class_map).astype(int)\\nX_train = train.drop(columns = \"class\")\\nY_train = train[\"class\"]\\n'"
  },
  {
    "objectID": "posts/open/Dakon competetion.html",
    "href": "posts/open/Dakon competetion.html",
    "title": "Untitled",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\ntest_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/test.csv\"\ntrain_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/train.csv\"\n\n\nclass CFG:\n    SEED = 42\n\n\ntrain = pd.read_csv(train_path).drop(columns = [\"id\"])\ntrain_len = len(train)\ntest = pd.read_csv(test_path)\nid_test = test[\"id\"]\ntest = pd.read_csv(test_path).drop(columns = [\"id\"])\n\n\ndataset = pd.concat([train,test],axis=0)\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      father\n      mother\n      gender\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      0\n      0\n      0\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      0\n      0\n      0\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      0\n      0\n      0\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      0\n      0\n      0\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      170\n      0\n      0\n      0\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      171\n      0\n      0\n      0\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      172\n      0\n      0\n      0\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      173\n      0\n      0\n      0\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      174\n      0\n      0\n      0\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 20 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#fathermothergender",
    "href": "posts/open/Dakon competetion.html#fathermothergender",
    "title": "Untitled",
    "section": "father,mother,gender",
    "text": "father,mother,gender\n\nfather,mother,gender = 0 => drop\n\n\ndataset = dataset.drop(columns = [\"father\",\"mother\",\"gender\"])\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 17 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_01",
    "href": "posts/open/Dakon competetion.html#snp_01",
    "title": "Untitled",
    "section": "SNP_01",
    "text": "SNP_01\n\nclass = B 인 경우, AA는 아예 없음 = > feature extraction hasGG 추가\n\n추후 고려사항 - class = A 인 경우, AA가 좀 높음 - class = B 인 경우, GG가 압도적으로 높음 - class = C 인 경우, GG가 좀 높음\n\ncol = \"SNP_01\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_02",
    "href": "posts/open/Dakon competetion.html#snp_02",
    "title": "Untitled",
    "section": "SNP_02",
    "text": "SNP_02\n\nclass = A인 경우, AA는 없음 => f.e has02AA\n\n추후 고려 - B,의 경우 AG->GG-AA 순 - C,의 경우 AG->GG->AA 순\n\ncol = \"SNP_02\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_03",
    "href": "posts/open/Dakon competetion.html#snp_03",
    "title": "Untitled",
    "section": "SNP_03",
    "text": "SNP_03\n\nA인 경우 , 무조건 AA만 존재 => feature extraction\n\n추후 고려 나머지는 뭐 대충 고르게\n\ncol = \"SNP_03\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_04",
    "href": "posts/open/Dakon competetion.html#snp_04",
    "title": "Untitled",
    "section": "SNP_04",
    "text": "SNP_04\n\nC인 경우 , GG는 없음 => feature extraction has04GG\n\n추후 고려 나머지는 뭐 대충 고르게 - class A인 경우,AA가 압도적으로 낮음 => feature extraction\n\ncol = \"SNP_04\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_05",
    "href": "posts/open/Dakon competetion.html#snp_05",
    "title": "Untitled",
    "section": "SNP_05",
    "text": "SNP_05\n\nclass = A인 경우, CC는 없음\n\n\ncol = \"SNP_05\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_06",
    "href": "posts/open/Dakon competetion.html#snp_06",
    "title": "Untitled",
    "section": "SNP_06",
    "text": "SNP_06\n\nclass = A인 경우, AA는 없음 => feature extraction has06AA\n\n\ncol = \"SNP_06\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_07",
    "href": "posts/open/Dakon competetion.html#snp_07",
    "title": "Untitled",
    "section": "SNP_07",
    "text": "SNP_07\n\nA의 경우 => AA는 없음 =>f.e has07AA\nB,C의 경우 => GG는 없음=>f.e has07GG\n\n\ncol = \"SNP_07\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_08",
    "href": "posts/open/Dakon competetion.html#snp_08",
    "title": "Untitled",
    "section": "SNP_08",
    "text": "SNP_08\n\nA의 경우 => GG는 없음 =>f.e has08GG\n\n\ncol = \"SNP_08\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_09",
    "href": "posts/open/Dakon competetion.html#snp_09",
    "title": "Untitled",
    "section": "SNP_09",
    "text": "SNP_09\n\nC의 경우 => 대부분이 AA이고 GA와 GG는 사실 없다고 봐도 무방=> f.e has09AA\nB의 경우 => GG는 거의 없음 => f.e has09GG\n\n앞선 경우들은 빈도수가 낮은 경우네는 따로 feature extraction을 안해줬는데 왜 여기서는 해? => 여기서는 낮은 것들의 빈도수가 1로 너무 낮음,그래서 여기는 함.\n\ncol = \"SNP_09\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset.loc[dataset[\"class\"] == \"B\",col].value_counts()\n\nA A    91\nG A    22\nG G     1\nName: SNP_09, dtype: int64\n\n\n\ndataset.loc[dataset[\"class\"] == \"C\",col].value_counts()\n\nA A    78\nG A     1\nName: SNP_09, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_10",
    "href": "posts/open/Dakon competetion.html#snp_10",
    "title": "Untitled",
    "section": "SNP_10",
    "text": "SNP_10\n\nA와 B에서 낮은값들이 있긴 한데 … 그래도 특이점 1인 정도는 아님\n\n\ncol = \"SNP_10\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"B\",col]\nx.value_counts()\n\nG G    110\nA G      4\nName: SNP_10, dtype: int64\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"A\",col]\nx.value_counts()\n\nA G    34\nA A    32\nG G     3\nName: SNP_10, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_11",
    "href": "posts/open/Dakon competetion.html#snp_11",
    "title": "Untitled",
    "section": "SNP_11",
    "text": "SNP_11\n\nA의 경우 => AA는 없음 =>f.e has11AA\n\n\ncol = \"SNP_11\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_12",
    "href": "posts/open/Dakon competetion.html#snp_12",
    "title": "Untitled",
    "section": "SNP_12",
    "text": "SNP_12\n\nA의 경우 => AA가 있는 관측치가 거의 없음,GG가 있는 관측치는 많음\nB,C의 경우 => GG가 있는 관측치가 거의 없음,AA가 있는 관측치는 많음\n\n=>f.e has12AA =>f.e has12GG\n\ncol = \"SNP_12\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset.loc[dataset[\"class\"] == \"A\",col].value_counts()\n\nG G    48\nG A    20\nA A     1\nName: SNP_12, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_13",
    "href": "posts/open/Dakon competetion.html#snp_13",
    "title": "Untitled",
    "section": "SNP_13",
    "text": "SNP_13\n\nA의 경우 => AA는 없음 => f.e has13AA\n\n\ncol = \"SNP_13\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"A\",col]\nx.value_counts()\n\nG G    66\nA G     3\nName: SNP_13, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_14",
    "href": "posts/open/Dakon competetion.html#snp_14",
    "title": "Untitled",
    "section": "SNP_14",
    "text": "SNP_14\n\nB의 경우 => AA만 존재 =>f.e has14AA 를 추가해서 AA인것들의 계수를 결정하도록\nC의 경우 CC가 있긴 한데 .. 엄청낮긴함(그래도 1은 아니니까 f.e는 안함)\n\n\ncol = \"SNP_14\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"C\",col]\nx.value_counts()\n\nA A    60\nC A    17\nC C     2\nName: SNP_14, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_15",
    "href": "posts/open/Dakon competetion.html#snp_15",
    "title": "Untitled",
    "section": "SNP_15",
    "text": "SNP_15\n\n별다른 특징 없음\n\n\ncol = \"SNP_15\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#class별-관측치의-숫자",
    "href": "posts/open/Dakon competetion.html#class별-관측치의-숫자",
    "title": "Untitled",
    "section": "class별 관측치의 숫자",
    "text": "class별 관측치의 숫자\n\nclass imbalance? => No\n\n\nx = dataset[\"class\"].value_counts().index\ny = dataset[\"class\"].value_counts().values\nplt.bar(x,y)\n\n<BarContainer object of 3 artists>"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#one-hot-encoding",
    "href": "posts/open/Dakon competetion.html#one-hot-encoding",
    "title": "Untitled",
    "section": "One-hot encoding",
    "text": "One-hot encoding\n\n#one-hot encoding for distance base algorithm\ndataset_ohe = pd.get_dummies(dataset,columns = dataset.columns.drop(\"class\"),drop_first=True) #multicollinearity를 막기위한 drop_first 옵션\ntrain_ohe = dataset_ohe[:train_len].copy()\ntest_ohe = dataset_ohe[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain_ohe[\"class\"]=train_ohe[\"class\"].map(class_map).astype(int)\nX_train_ohe = train_ohe.drop(columns = \"class\")\nY_train_ohe = train_ohe[\"class\"]\n\n\nX_train_ohe\n\n\n\n\n\n  \n    \n      \n      trait_1\n      SNP_01_1\n      SNP_01_2\n      SNP_02_1\n      SNP_02_2\n      SNP_03_1\n      SNP_03_2\n      SNP_04_1\n      SNP_04_2\n      SNP_05_1\n      ...\n      has07AA_1\n      has07GG_1\n      has08GG_1\n      has09AA_1\n      has09GG_1\n      has11AA_1\n      has12AA_1\n      has12GG_1\n      has13AA_1\n      has14AA_1\n    \n  \n  \n    \n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n      ...\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      1\n      1\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      3\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n    \n    \n      4\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      257\n      1\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      258\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      259\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      260\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      261\n      1\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n  \n\n262 rows × 47 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#label-encoding",
    "href": "posts/open/Dakon competetion.html#label-encoding",
    "title": "Untitled",
    "section": "Label encoding",
    "text": "Label encoding\n\nle_col = []\nfor col_name in dataset.columns.tolist():\n    if \"SNP\" in col_name:\n        le_col.append(col_name)\nle_col.append(\"trait\")\nle_col\n\n['SNP_01',\n 'SNP_02',\n 'SNP_03',\n 'SNP_04',\n 'SNP_05',\n 'SNP_06',\n 'SNP_07',\n 'SNP_08',\n 'SNP_09',\n 'SNP_10',\n 'SNP_11',\n 'SNP_12',\n 'SNP_13',\n 'SNP_14',\n 'SNP_15',\n 'trait']\n\n\n\nfrom sklearn import preprocessing\nfor col in le_col:\n    le = preprocessing.LabelEncoder()\n    _col = dataset[col].tolist()\n    le.fit(_col)\n    dataset[col] = le.transform(_col)\ndataset\n\n\n\n\n\n  \n    \n      \n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      ...\n      has07AA\n      has07GG\n      has08GG\n      has09AA\n      has09GG\n      has11AA\n      has12AA\n      has12GG\n      has13AA\n      has14AA\n    \n  \n  \n    \n      0\n      1\n      2\n      1\n      0\n      1\n      1\n      0\n      0\n      2\n      0\n      ...\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      1\n      1\n      1\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      2\n      2\n      0\n      1\n      2\n      2\n      0\n      1\n      1\n      ...\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      3\n      0\n      0\n      2\n      0\n      1\n      0\n      2\n      2\n      0\n      2\n      ...\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n    \n    \n      4\n      1\n      2\n      2\n      2\n      0\n      2\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      170\n      1\n      1\n      2\n      2\n      0\n      1\n      1\n      0\n      2\n      0\n      ...\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      171\n      1\n      2\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      172\n      1\n      2\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      173\n      1\n      1\n      2\n      1\n      1\n      2\n      2\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      174\n      1\n      2\n      2\n      2\n      1\n      1\n      0\n      1\n      2\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n  \n\n437 rows × 33 columns\n\n\n\n\ntrain = dataset[:train_len].copy()\ntest = dataset[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain[\"class\"]=train[\"class\"].map(class_map).astype(int)\nX_train = train.drop(columns = \"class\")\nY_train = train[\"class\"]\n\n\n#원핫인코딩 vs 레이블인코딩\nlen(X_train_ohe.columns),len(X_train.columns)\n\n(47, 32)\n\n\n\nlen(Y_train),len(Y_train_ohe)\n\n(262, 262)"
  },
  {
    "objectID": "posts/open/Dakon competition modeling.html",
    "href": "posts/open/Dakon competition modeling.html",
    "title": "HIHO",
    "section": "",
    "text": "import torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nimport torch\ntest_path = \"./test.csv\"\ntrain_path = \"./train.csv\"\n\n\n# %pip install plotly (jupyter notebook)\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n#pio.renderers.default = 'iframe_connected'\n#pio.renderers.default = \"vscode\"\npio.renderers.default = \"plotly_mimetype+notebook\""
  },
  {
    "objectID": "posts/open/Dakon competition modeling.html#탐지한-이상치-제외",
    "href": "posts/open/Dakon competition modeling.html#탐지한-이상치-제외",
    "title": "HIHO",
    "section": "탐지한 이상치 제외",
    "text": "탐지한 이상치 제외\n\noutliers = pd.read_csv(\"./outlierdetect.csv\").drop(columns = \"Unnamed: 0\")\nnormal_index = ~outliers.isoutlier\n\n\nX_train_ohe = X_train_ohe[normal_index];Y_train_ohe = Y_train_ohe[normal_index]\n\n\nX_train_ohe.shape\n\ntorch.Size([243, 47])"
  },
  {
    "objectID": "posts/open/Dakon competition torch.html",
    "href": "posts/open/Dakon competition torch.html",
    "title": "Untitled",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nimport torch\ntest_path = \"./test.csv\"\ntrain_path = \"./train.csv\"\n\n\ntrain = pd.read_csv(train_path).drop(columns = [\"id\"])\ntrain_len = len(train)\ntest = pd.read_csv(test_path)\nid_test = test[\"id\"]\ntest = pd.read_csv(test_path).drop(columns = [\"id\"])\n\n\ndataset = pd.concat([train,test],axis=0)\ndataset = dataset.drop(columns = [\"father\",\"mother\",\"gender\"])\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 17 columns\n\n\n\n\n_t = []\nfor val in dataset.SNP_01 == \"G G\":\n    if val == True:\n        _t.append(1)\n    else:\n        _t.append(0)\ndataset[\"has01GG\"] = _t\n\n_t = []\nfor val in dataset.SNP_02 == \"A A\":\n    if val == True:\n        _t.append(1)\n    else:\n        _t.append(0)\ndataset[\"has02AA\"] = _t\n\n\ndef create_col(dataset,col,value):\n    _t = []\n    for val in dataset[col] == value:\n        if val == True:\n            _t.append(1)\n        else:\n            _t.append(0)\n    \n    col_name_base = \"has\"+col[-2:]\n    value_name = \"\"\n    for chr in value:\n        if chr != \" \":\n            value_name+=chr\n    col_name = col_name_base+value_name\n    #print(col_name)\n    dataset[col_name] = _t\n\n    return dataset\n\ndataset = create_col(dataset,\"SNP_03\",\"A A\")\ndataset = create_col(dataset,\"SNP_04\",\"G G\")\ndataset = create_col(dataset,\"SNP_05\",\"C C\")\ndataset = create_col(dataset,\"SNP_06\",\"A A\")\ndataset = create_col(dataset,\"SNP_07\",\"A A\")\ndataset = create_col(dataset,\"SNP_07\",\"G G\")\ndataset = create_col(dataset,\"SNP_08\",\"G G\")\n\ndataset = create_col(dataset,\"SNP_09\",\"A A\")\ndataset = create_col(dataset,\"SNP_09\",\"G G\")\ndataset = create_col(dataset,\"SNP_11\",\"A A\")\n\ndataset = create_col(dataset,\"SNP_12\",\"A A\")\ndataset = create_col(dataset,\"SNP_12\",\"G G\")\n\ndataset = create_col(dataset,\"SNP_13\",\"A A\")\ndataset = create_col(dataset,\"SNP_14\",\"A A\")\n\n\n#one-hot encoding for distance base algorithm\ndataset_ohe = pd.get_dummies(dataset,columns = dataset.columns.drop(\"class\"),drop_first=True) #multicollinearity를 막기위한 drop_first 옵션\ntrain_ohe = dataset_ohe[:train_len].copy()\ntest_ohe = dataset_ohe[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain_ohe[\"class\"]=train_ohe[\"class\"].map(class_map).astype(int)\nX_train_ohe = train_ohe.drop(columns = \"class\")\nY_train_ohe = train_ohe[\"class\"]\n\n\nimport torch\nimport torch.nn as nn\n\n\nX_train_ohe = torch.from_numpy(X_train_ohe.values).float()\n#y_train_ohe = torch.from_numpy(pd.get_dummies(Y_train_ohe).values).float()\nY_train_ohe = torch.from_numpy(Y_train_ohe.values).long()\n\n\nclass mynet(nn.Module):\n    def __init__(self,in_features,dropout_p,l1_out=64):\n        super().__init__()\n        self.linr1 = torch.nn.Linear(in_features,l1_out)\n        self.relu1 = torch.nn.ReLU()\n        self.d1 = torch.nn.Dropout(p=dropout_p)\n        self.b1 = torch.nn.BatchNorm1d(l1_out)\n        # 256\n        \n        self.linr2 = torch.nn.Linear(l1_out,l1_out//2)\n        self.relu2 = torch.nn.ReLU()\n        self.d2 = torch.nn.Dropout(p=dropout_p)\n        self.b2 = torch.nn.BatchNorm1d(l1_out//2)\n        # 128\n\n        self.linr3 = torch.nn.Linear(l1_out//2,l1_out//4)\n        self.relu3 = torch.nn.ReLU()\n        self.d3 = torch.nn.Dropout(p=dropout_p)\n        self.b3 = torch.nn.BatchNorm1d(l1_out//4)\n        # 64\n        self.linr4 = torch.nn.Linear(l1_out//4,l1_out//8)\n        self.relu4 = torch.nn.ReLU()\n        self.d4 = torch.nn.Dropout(p=dropout_p)\n        self.b4 = torch.nn.BatchNorm1d(l1_out//8)\n        # 32\n        self.linr5 = torch.nn.Linear(l1_out//8,l1_out//16)\n        self.relu5 = torch.nn.ReLU()\n        self.d5 = torch.nn.Dropout(p=dropout_p)\n        self.b5 = torch.nn.BatchNorm1d(l1_out//16)\n        #16\n        self.linr6 = torch.nn.Linear(l1_out//16,3)\n\n    def forward(self,x):\n        out = self.b1(self.d1(self.relu1(self.linr1(x))))\n        out = self.b2(self.d2(self.relu2(self.linr2(out))))\n        out = self.b3(self.d3(self.relu3(self.linr3(out))))\n        out = self.b4(self.d4(self.relu4(self.linr4(out))))\n        out = self.b5(self.d5(self.relu5(self.linr5(out))))\n        out = self.linr6(out)\n        #out = self.b3(self.d3(self.relu3(self.linr3(x))))\n        return out\n\n\nfrom sklearn.model_selection import StratifiedKFold\nimport random\nepochs_list = [i for i in range(1400,2000,20)]\nweight_decay = np.linspace(0.001,0.0001,500).tolist()\nlr = np.linspace(1e-2,1e-5,500).tolist()\nhidden_nodes = [i for i in range(1300,1800)]\ndropout_p = np.linspace(0.6,0.8,1000).tolist()\nrs = [i for i in range(0,500)]\n\n\ndef dl_cv(epochs,weight_decay,learning_rate,hidden1_nodes,dropout_p,rs):\n\n    try_number=0\n    skf = StratifiedKFold(n_splits=5,shuffle=True)\n    skf.get_n_splits(X_train_ohe,Y_train_ohe)\n    \n    while True:\n        try_number+=1\n        print(f'try:{try_number}...')\n        train_accs = []\n        val_accs = []\n        hdly1 = random.sample(hidden1_nodes,1)[0]\n        lr = random.sample(learning_rate,1)[0]\n        wght_decay = random.sample(weight_decay,1)[0]\n        epoch = random.sample(epochs,1)[0]\n        drop_p = random.sample(dropout_p,1)[0]\n        r_seed = random.sample(rs,1)[0]\n        print(f'lr:{lr} wght_decay:{wght_decay} epochs:{epoch} hidden_l1nodes:{hdly1} dropout_prob:{drop_p}')\n        for train_index,valid_index in skf.split(X_train_ohe,Y_train_ohe): \n            torch.manual_seed(r_seed)\n            net = mynet(47,drop_p,l1_out=hdly1)\n            optimizer = torch.optim.Adam(net.parameters(),lr=lr,weight_decay = wght_decay)        \n            loss_fn = torch.nn.CrossEntropyLoss()\n            #KFold\n            train_index = train_index.tolist();valid_index = valid_index.tolist()\n            X_tr = X_train_ohe[train_index,:];y_tr = Y_train_ohe[train_index]\n            X_tst = X_train_ohe[valid_index,:];y_valid = Y_train_ohe[valid_index]\n\n            #fitting\n            for ep in range(epoch):\n                net.train()\n                #1 yhat\n                yhat = net(X_tr)\n                #2 loss\n                loss = loss_fn(yhat,y_tr)\n                if ep % 50 == 0:\n                    pass\n                    #print(loss)\n                #3 derivative\n                loss.backward()\n                #4 update\n                optimizer.step()\n                optimizer.zero_grad()\n            train_yhat = torch.argmax(net(X_tr),dim=1)\n            train_acc = torch.mean((train_yhat==y_tr).float())\n            print(\"trainacc : \",train_acc)\n            train_accs.append(train_acc)\n            net.eval()\n            with torch.no_grad():\n                val_yhat = torch.argmax(net(X_tst),dim=1)\n                val_acc = torch.mean((val_yhat == y_valid).float()).tolist()\n                print(\"validacc : \",val_acc)\n                val_accs.append(val_acc)\n            if val_acc < 0.97:\n                break\n\n        valid_accuracy = torch.mean(torch.tensor(val_accs))\n        print(f\"K-Fold train accuracy {torch.mean(torch.tensor(train_accs))}\")\n        print(f\"K-Fold valid accuracy {torch.mean(torch.tensor(val_accs))}\")\n        print(\"==========================================================\")\n        if valid_accuracy > 0.99:\n            path = \"./model{}.pth\".format(try_number)\n            torch.save(net,path)\n\n\nt = dl_cv(epochs_list,weight_decay,lr,hidden_nodes,dropout_p,rs)\n\ntry:1...\nlr:0.0039939879759519036 wght_decay:0.00039218436873747497 epochs:1400 hidden_l1nodes:1433 dropout_prob:0.6748748748748749\n\n\nKeyboardInterrupt: \n\n\n\nAssemble\n\ntest_ohe = torch.from_numpy(test_ohe.values).float()\n\n\nimport os\npath = \"C:/Users/22668/Desktop/새 폴더\"\nmodels = []\nfor model in os.listdir(path):\n    md_path = os.path.join(path,model)\n    models.append(torch.load(md_path))\n\n\nassemble = torch.zeros(test_ohe.shape[0],3)\nassemble.shape\n\ntorch.Size([175, 3])\n\n\n\nsoft = torch.nn.Softmax(dim=1)\n\n\nfor model in models:\n    net = model\n    yhat = soft(model(test_ohe))\n    assemble +=yhat\n\n\ntest_predict = torch.argmax(assemble,dim=1)\nresult = pd.concat([pd.Series(id_test),pd.Series(test_predict)],axis=1)\nresult.columns = [\"id\",\"class\"]\nclass_map_inv = {0:\"A\",1:\"B\",2:\"C\"}\nresult[\"class\"] = result[\"class\"].map(class_map_inv)\n\n\nresult\n\n\n\n\n\n  \n    \n      \n      id\n      class\n    \n  \n  \n    \n      0\n      TEST_000\n      A\n    \n    \n      1\n      TEST_001\n      B\n    \n    \n      2\n      TEST_002\n      C\n    \n    \n      3\n      TEST_003\n      B\n    \n    \n      4\n      TEST_004\n      A\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      170\n      TEST_170\n      B\n    \n    \n      171\n      TEST_171\n      C\n    \n    \n      172\n      TEST_172\n      C\n    \n    \n      173\n      TEST_173\n      B\n    \n    \n      174\n      TEST_174\n      B\n    \n  \n\n175 rows × 2 columns\n\n\n\n\nresult.to_csv(\"./submission_dl.csv\",index=False)"
  },
  {
    "objectID": "posts/paper study/DQN review - 복사본.html",
    "href": "posts/paper study/DQN review - 복사본.html",
    "title": "DQN review",
    "section": "",
    "text": "개요\n\n기존의 딥러닝과 강화학습은 그림의 왼쪽과 같이 공통분모가 거의 없는 분야였습니다. 그러나 2013년 구글딥마인드에서 발표한 DQN논문에서는 강화학습의 한 분야인 Q-learning에 딥러닝을 접목시켰고 그 후 계속해서 발전하여 거의 모든 강화학습논문에는 딥러닝이 사용된다고 합니다. 따라서 이제는 오른쪽과 같이 강화학습도 거의 딥러닝의 하나의 분야로 자리잡았습니다.\n\n\nQ-learning\nQ-learning은 Q값을 학습하는 알고리즘으로 그리드형식을 가진 문제에서 활용될 수 있습니다. 강화학습에서 주체(agent)는 현재의 상태(state)를 관찰하여 어떠한 행동(action)이 가장 큰 보상(reward)를 가져다주는지 학습하며 Q-learning에서 이러한 학습의 대상은 Q입니다.\n\n위의 그림은 학습이 끝난 Q값의 예시입니다.(실제로 맞는 수치는 아님)Q-learning 알고리즘에서 agent는 greedy action을 취합니다. 따라서 agent가 격자의 시작지점에 들어가게 된다면 greedy action을 통해 가장 큰 Q값이 있는 방향으로 이동하여 시작부터 종료지점까지 일직선으로 가장빠르게 이동합니다. 위와 같은 그리드에서는 더 빠르게 가거나 리워드도 더 좋은 곳은 없으므로 적절하게 학습이 끝났다는 것을 알 수 있습니다.\n\n\nQ-update\n\\[Q(s_t,a_t) = (1-\\alpha)Q(s_t,a_t) + \\alpha(R_t + \\gamma \\underset{a_{t+1}}{\\text{argmax}}Q(s_{t+1},a_{t+1}))\\]"
  },
  {
    "objectID": "posts/paper study/DQN review.html",
    "href": "posts/paper study/DQN review.html",
    "title": "DQN review",
    "section": "",
    "text": "개요\n\n기존의 딥러닝과 강화학습은 그림의 왼쪽과 같이 공통분모가 거의 없는 분야였습니다. 그러나 2013년 구글딥마인드에서 발표한 D\\(Q\\)N논문에서는 강화학습의 한 분야인 \\(Q\\)-learning에 딥러닝을 접목시켰고 그 후 계속해서 발전하여 거의 모든 강화학습논문에는 딥러닝이 사용된다고 합니다. 따라서 이제는 오른쪽과 같이 강화학습도 거의 딥러닝의 하나의 분야로 자리잡았습니다.\n\n\n\\(Q\\)-learning\n\\(Q\\)-learning은 \\(Q\\)값을 학습하는 알고리즘으로 그리드형식을 가진 문제에서 활용될 수 있습니다. 강화학습에서 주체(agent)는 현재의 상태(state)를 관찰하여 어떠한 행동(action)이 가장 큰 보상(reward)를 가져다주는지 학습하며 \\(Q\\)-learning에서 이러한 학습의 대상은 \\(Q\\)입니다.\n\n위의 그림은 학습이 끝난 \\(Q\\)값의 예시입니다.(실제로 맞는 수치는 아님)\\(Q\\)-learning 알고리즘에서 agent는 greedy action을 취합니다. 따라서 agent가 격자의 시작지점에 들어가게 된다면 greedy action을 통해 가장 큰 \\(Q\\)값이 있는 방향으로 이동하여 시작부터 종료지점까지 일직선으로 가장빠르게 이동합니다. 위와 같은 그리드에서는 더 빠르게 가거나 리워드도 더 좋은 곳은 없으므로 적절하게 학습이 끝났다는 것을 알 수 있습니다.\n\n\n\\(Q\\)-update\n\\[ Q(s_t,a_t) = (1-\\alpha)Q(s_t,a_t) + \\alpha(R_t + \\gamma \\underset{a_{t+1}}{\\text{argmax}}Q(s_{t+1},a_{t+1}))\\]\n\\(Q\\)-learning에서는 위와 같은 수식으로 각각의 \\(Q\\)를 업데이트합니다. 여기서 중요한점은 \\(Q\\)가 state와 action의 함수라는 점입니다. 위와 같은 길찾기 문제의 경우 그렇게 state(25개)가 많지는 않습니다.그러나 Atari 벽돌깨기 게임과 같은 경우, 움직이는 주체인 가로막대바 위치,벽돌의 갯수,깨진위치,공이 날아오는 각도 등등 … 매우 많은 state가 가능하고 어떤방향으로 공을 날릴지에 대한 action도 수없이 많이 가능합니다. 기존의 방식으로 Q를 업데이트 하기위해서는 이러한 수많은 조합에 대하여 state와 action을 기억해놓고 업데이트 해야합니다. 이러한 방식은 컴퓨터의 메모리에 부담을 주고 exploration(탐험)하는데 걸리는 시간을 더 오래 만듭니다.\nDeep-\\(Q\\)-learning에서는 DNN을 통해 함수로서 \\(Q\\)값을 저장하여 위와 같은 단점을 줄입니다. 또한 loss function을 정의하고 gradient desent를 사용하여 새롭게 Q값을 업데이트 합니다.\n\n\n\n그림출처 : 이것저것 테크 블로그"
  },
  {
    "objectID": "posts/Probability&Statistics/categori distribution.html",
    "href": "posts/Probability&Statistics/categori distribution.html",
    "title": "카테고리 분포",
    "section": "",
    "text": "카테고리 분포에 대한 정리\n\n카테고리 분포\n카테고리분포는 시행의 한번의 시행(또는 실험)으로부터 나올 수 있는 사건이 K개인 확률분포를 모델링할때 쓰이며 다음과 같습니다.\n\\[\\begin{aligned}\n&Cat({\\bf{x};\\bf{\\mu}}) =\n\\begin{cases}\n\\mu_1\\, (\\text{if } x = (1,0,0,0,\\dots,1)) \\\\\n\\mu_2\\, (\\text{if } x = (0,1,0,0,\\dots,1)) \\\\\n\\mu_3\\, (\\text{if } x = (0,0,1,0,\\dots,1)) \\\\\n\\vdots \\\\\n\\mu_k\\, (\\text{if } x = (0,0,0,0,\\dots,1)) \\\\\n\\end{cases}\n\\\\\n&\\text{where, }x = (x_1,x_2,\\dots,x_K),\\mu = (\\mu_1,\\mu_2,\\dots,\\mu_k)\n\\end{aligned}\\]\n카테고리분포의 변수\\(\\bf{X}\\)는 K개의 원소를 가지는 원핫인코딩(one-hot encoded)된 벡터이며 각원소는 indicate number(어떤 클래스에 속하는지 나타내는)인 1또는0입니다. 모수(벡터)\\(\\mu\\)도 K개의 원소를 가지며 각각의 원소는 카테고리 확률분포로부터 대응하는 결과값(원핫벡터)에 대한 확률입니다. 즉,각각의 원핫벡터가 표본추출될 가능성(확률)을 알려줍니다.\n위와 같은 사실로부터 다음과 같은 4가지의 제약조건이 존재합니다.\n\n\\(\\mu_i\\)는 원핫벡터가 나올 확률입니다.\n\n\\[0\\leq\\mu_i\\leq1\\]\n\n확률의 합은 1입니다.\n\n\\[\\sum_{i=1}^{K}\\mu_i = 1\\]\n\n원핫벡터의 각 원소는 indicate number인 1또는 0입니다.\n\n\\[\\begin{aligned}\nx_i =\n\\begin{cases}\n0\\\\\n1\n\\end{cases}\n\\end{aligned}\\]\n\n원핫벡터의 모든 원소의 합은 1입니다. \\[\\sum_{i=1}^{K}x_i = 1\\]"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html",
    "title": "Maximum likelyhood estimation",
    "section": "",
    "text": "최대가능도 추정법(MLE)에 대한 정리"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#problem-setting",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#problem-setting",
    "title": "Maximum likelyhood estimation",
    "section": "Problem Setting",
    "text": "Problem Setting\n\\[\\begin{aligned}\n&\\text{Given}\\,x_1,x_2,...,x_N\\ \\text{which are realizations of a random sample }X_1,X_2,\\dots,X_N \\\\\n&\\text{where,}\\, X_1 \\sim f_1(x_1;\\theta),X_2 \\sim f_2(x_1;\\theta),\\dots,X_N \\sim f_N(x_N;\\theta) \\\\ \\\\\n\n&\\text{Goal : 확률분포의 모수 $\\theta$를 점추정하기}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#what-is-estimation",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#what-is-estimation",
    "title": "Maximum likelyhood estimation",
    "section": " What is estimation? ",
    "text": "What is estimation? \n\nimport scipy as sp\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n\nplt.figure(figsize=(10,5))\nrv = sp.stats.norm(175,10)\nx = np.linspace(150,200,500)\nplt.plot(x,rv.pdf(x),\"b\")\nplt.title(\"$N(175,10)$\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nText(0, 0.5, 'y')\n\n\n\n\n\n대한민국 20대 남성들의 키가 다음과 확률분포를 따른다고 가정해보겠습니다. 우리는 “키가 175근처인 사람이 대부분이네??” 또는 “키가 160이하이거나 190이상인 사람은 거의 없겠네?” 등의 해석을 할 수 있습니다. 이러한 해석을 가능하게 하는것은 우리가 확률분포의 모양(정규분포)과 모수(평균,분산)을 가정했기 때문입니다. 극단적으로,같은 평균이라는 모수를 가지지만 전혀다른 분포인 베르누이분포라고 가정을 하면 확률변수의 값이 딱 떨어지는 2개의 이산적인 값만을 가지므로 “이하”라는 표현이나 “이상”이라는 표현은 쓸 수 없습니다.(물론 베르누이분포는 분산은 모수가 아니긴합니다.) 모양이 같지만 모수가 다른 경우도 마찬가지입니다. 같은 정규분포라도 평균이나 분산이 다르면 전혀 다른 해석이 가능합니다.\n만약 위와 같은 해석을 하고싶으나 20대 남성들의 키가 따르는 분포자체를 모른다면 해야할까요? 위와 같은 해석을 하기위해서는 확률분포 더 상세히말하자면 확률분포의 모양과 확률분포의 모수를 알아야 합니다. 이를 확률분포의 추정이라고 합니다.\n확률분포의 추정에서 보통 분포의 모양은 어떠한 분포로 가정합니다(ex 정규분포,베타분포,베르누이분포,다항분포 등등…). 그 다음은 확률분포의 모수를 추정해야 합니다. 이때 모수를 추정하는 방법 중 하나가 바로 최대가능도 추정법(Maximum likelihood estimation)입니다.\n아까전 상황을 다시 생각해봅시다. 우리는 확률분포로부터 몇 가지의 해석을 할 수 있었습니다. 이는 확률분포의 모양과 모수(평균,분산)를 먼저 가정을 했기 때문이었고,이로부터 특정한 표본이 나올 수 있는 가능성을 알 수 있었습니다.이와는 반대로,최대가능도 추정법에서는 역으로 샘플이 주어져있다고 가정하고 가정한 확률분포의 모양에 대해서 가장 가능성이 높은 모수를 이 모수를 모수에 대한 추정량(estimated value)로 합니다."
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#mle최대가능도-추정법",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#mle최대가능도-추정법",
    "title": "Maximum likelyhood estimation",
    "section": "MLE(최대가능도 추정법)",
    "text": "MLE(최대가능도 추정법)"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#likelyhood-function",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#likelyhood-function",
    "title": "Maximum likelyhood estimation",
    "section": "Likelyhood function ",
    "text": "Likelyhood function \n위에서 샘플이 주어져있다고 가정하고 가장 가능성이 높은 모수가 모수에 대한 추정량이라고 했습니다. 그렇다면 가능성이 가장 높은 모수를 정하기 위해서 모수의 가능성이라는 지표를 정의할 필요가 있습니다. 모수의 가능성은 어떻게 정의해야 할까요?\n\nrv1 = sp.stats.norm(175,10)\nrv2 = sp.stats.norm(185,10)\nrv3 = sp.stats.norm(160,10)\nx = np.linspace(150,200,500)\nplt.figure(figsize=(10,5))\nplt.plot(x,rv1.pdf(x),\"b\")\nplt.plot(x,rv2.pdf(x),\"g\")\nplt.plot(x,rv3.pdf(x),\"purple\")\nplt.plot([177],[0],\"o\",color=\"black\",ms=\"10\")\nplt.plot \nplt.axvline(177,color = \"black\",linewidth = 1,linestyle = \"--\",ymin=0.07)\nplt.title(\"Normal distributions\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend([\"$N(175,10)$\",\"$N(170,10)$\",\"$N(165,10)$\"],loc=\"upper right\")\n\n<matplotlib.legend.Legend at 0x21d4e82fb20>\n\n\n\n\n\n그림과 같이 크기가1인 표본을 177 얻었고 확률분포의 모양은 정규분포로 가정했다고 해봅시다. 샘플이 주어져 있을때 가능성이 가장 높은 모수는 무엇일까요? 그래프를 들여다보니 보라색 확률분포에서는 뭔가 표본을 얻기는 힘들 것 같습니다. 표본을 뽑을 확률(엄밀히는 확률밀도이지만 확률로 적겠습니다.)이 너무 낮아서 가능성이 너무 낮기 때문입니다. 초록색의 경우는 보라색보다는 확률이 더 크기때문에 가능성이 더 커보입니다. 파랑색의 경우는 확률이 초록색보다도 더 큽니다. 그러므로 3개의 분포중에서는 파랑색 확률분포의 모수인 175가 가능성이 가장 커보이고 이를 실제 모수에 대한 추정값으로 하는 것이 합리적입니다.\n위의 예시에서 가능성을 확인하기 위해서 확률분포의 값(y축,확률,확률밀도)을 확인했습니다.결국 우리가 정의하기로 지표인 모수의 가능성은 결국은 확률(또는 확률밀도) 그 자체임을 알 수 있습니다. 그러나 확률과는 약간 다르게 확률(또는 확률밀도)은 모수를 고정하고 확률분포의 읽는 확률변수에 대한 함수이고 반면에 모수의 가능성은 샘플은 고정되있고 모수를 바꿔가며 서로다른 확률분포에서 값을 읽는 모수에 대한 함수라는 점에 차이가 있습니다. 모수의 가능성에는 가능도(likelyhood)라는 이름을 붙여줍니다.\n\nrv1 = sp.stats.norm(175,10)\nrv2 = sp.stats.norm(185,10)\nrv3 = sp.stats.norm(160,10)\nx = np.linspace(150,200,500)\nplt.figure(figsize=(10,5))\nplt.plot(x,rv1.pdf(x),\"b\")\nplt.plot(x,rv2.pdf(x),\"g\")\nplt.plot(x,rv3.pdf(x),\"purple\")\ns = [173,177,175,180]\nplt.plot(s,[0,0,0,0],\"o\",color=\"black\",ms=\"10\")\nfor i in s:\n    temp = [i]\n    plt.axvline(temp,linestyle = \"--\",color=\"black\",linewidth=1,ymin=0.07,ymax = 0.96)\nplt.title(\"Normal distributions\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend([\"$N(175,10)$\",\"$N(170,10)$\",\"$N(165,10)$\"],loc=\"upper right\")\n\n<matplotlib.legend.Legend at 0x21d4e89d160>\n\n\n\n\n\n이번에는 크기가 4인 표본을 얻었다고 해봅시다. 분포의 모양은 마찬가지로 정규분포로 가정했습니다. 이번에는 샘플의 크기가4이므로 이전과는 다르게 하나의 데이터포인트만 반영하는 것이 아니라 4개의 데이터포인트를 모두 반영하여 가능성이 가장 높은 모수를 고려해야 합니다. \n위에서 1개의 데이터포인트(표본)를 고려할때에는 여러가지 모수를 따르는 분포에서 1개의 확률값만 읽고 어떤 모수가 가능성이 가장 높은지 확인하고 가능도라는 것을 정의했습니다. 4개일때도 다르지 않습니다. 4개일때에도 확률값을 읽는데 다만 달라진 것은 4개의 확률값을 동시에 함께 반영해야 한다는 점입니다. 4개일 경우에는 4개의 표본 모두에 대하여 동시에 함께 그 가능성을 고려해야 합니다. 그러므로 여기서는 가능도인 확률을 각각의 데이터포인트를 뽑을 동시에 함께 뽑을 확률이자 확률의 곱인 확률인 결합확률(joint distribution)로 계산해야 합니다.\n정리 모수의 가능도는 샘플과 확률분포의 모양이 주어져있을 때,샘플이 모수로부터 나오기가 얼마나 가능한지를 알려주며 결과적으로는 (결합)확률 입니다. 다만 확률(또는 확률분포)의 경우에는 목적이 모수가 정해진 확률분포로부터 읽는 그 값을 읽어 어떤 샘플 x값을 취할 가능성을 파악하는 것이고 반면 가능도는 샘플이 이미 뽑혀서 정해져있을때, 서로다른 모수를 가지는 확률분포의 값을 읽어서 어떤 모수가 가장 가능성이 높은지 파악하는 것입니다. 수학적으로 표기하면 다음과 같습니다.\n\\[\\begin{align}\nL(\\theta|D) = L(\\theta|x_1,x_2,\\dots,x_N) &= f_{X_1,X_2,\\dots,X_N}(x_1,x_2,\\dots,x_N;\\theta) \\\\\n&= \\prod_{i=1}^{N}f_{X_i}(x_i;\\theta)\n\\end{align}\\]\n식(1)은 가능도 = 결합확률임을 의미합니다. 식(2)는 확률변수가 독립일 경우의 결합확률 = 확률의 곱입니다. 각각의 샘플에 대한 확률변수는 모두 독립이라고 가정합니다."
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#likelyhood의-최댓값-구하기",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#likelyhood의-최댓값-구하기",
    "title": "Maximum likelyhood estimation",
    "section": "Likelyhood의 최댓값 구하기 ",
    "text": "Likelyhood의 최댓값 구하기 \n여기까지 가능도함수를 구하는 방법을 알아봤습니다. 가능도함수를 최대화 할때의 값이 주어진 샘플에 대해서 가장 가능성이 높은 확률분포의 모수이므로 그때의 값으로 모수를 추정합니다. MLE추정량은 다음과 같습니다.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MLE} &= \\underset{\\theta}{\\text{argmax}}\\,L(\\theta|x_1,x_2 \\dots x_n) \\\\\n&=  \\underset{\\theta}{\\text{argmax}},f_{X_1,X_2,\\dots,X_n}(x_1,x_2,\\dots,x_n|\\theta) \\\\\n&= \\underset{\\theta}{\\text{argmax}}\\,\\prod_{i=1}^{N}f_{X_i}(x_i;\\theta)\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#베이즈-정리에-의한-mle",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#베이즈-정리에-의한-mle",
    "title": "Maximum likelyhood estimation",
    "section": "베이즈 정리에 의한 MLE",
    "text": "베이즈 정리에 의한 MLE\n\\(D=(t_1,t_2,\\dots,t_n)\\)는 샘플(데이터셋), \\(\\theta\\)는 우리가 알고싶은 확률분포의 모수라고 합시다. 베이즈정리는 증거 또는 조건이 주어지기 전의 사전확률 \\(p(D)\\)와 주어진 후의 사후확률 \\(p(\\theta|D)\\)사이의 관계를 알려줍니다.\n\\[\\begin{aligned}\np(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{p(D)} \\propto p(\\theta|D) \\times p(\\theta)\n\\end{aligned}\\]\n여기서 사후확률은 \\(\\theta\\)에 대한 확률분포로 데이터(샘플)이 주어질때 확률분포의 임의의 모수\\(\\theta\\)가 얼마나 가능한지 또는 불확실한지 그 정도를 알려주는 확률을 함숫값으로 가지는 확률함수입니다. 그러므로 사후확률을 최대로 하는 모수\\(\\theta\\)가 데이터셋이 주어져있을 때 가장 확률이 높은,가능성이 높은 모수이므로 그때의 값을 확률분포의 모수로 추정하면 됩니다.\n문제는 왼쪽의 확률분포는 바로 알기가 쉽지 않다는 점입니다. 따라서 베이즈정리를 통하여 우변의 식을 최대화 하는 값을 구합니다. 우변의 식에서 분모는 주어진 데이터에 의하여 고정된 상수(normalization constant라고 합니다)입니다. 그러므로 최댓값을 구하는데 영향을 주지 않습니다. 우리는 분자에 있는 \\(p(D|\\theta)p(\\theta)\\)를 최대화하는 \\(\\theta\\)를 찾으면 됩니다.\n여기서 \\(p(D|\\theta)\\)를 가능도(likelyhood)라 합니다. MLE에서는 분자에서 가능도만 최대로 하는 \\(\\theta\\)를 구합니다. MAP라는 다른 방법은 분자에 있는 \\(p(D|\\theta)p(\\theta)\\)를 최대화 하는 \\(\\theta\\)를 구한다고 합니다.\n\n예시 - 정규분포 데이터가 주어진 경우\n대한민국 20대 남성들의 키의 분포가 정규분포를 따른다고 가정해보겠습니다. 샘플링하여 크기가 4인표본[173,177,175,180]을 얻은 상태입니다. 목적은 정규분포의 모수인 \\(\\mu\\)를 MLE로 추정하는 것입니다.\n다음을 계산해야 합니다\n\\[\\begin{aligned}\n\\underset{\\theta}{\\text{argmax }}L(\\theta;x_1,x_2,x_3) &= \\prod_{i=1}^{4}f_{X_i}(x_i;\\theta)\\\\\n&= \\underset{\\theta}{\\text{argmax }}f_{X_1}(x_1;\\theta)f_{X_2}(x_2;\\theta)f_{X_2}(x_2;\\theta)f_{X_2}(x_2;\\theta)\\\\\n&= \\underset{\\theta}{\\text{argmax }}\\mathcal{N}(x_1;\\mu,\\sigma^2)\\mathcal{N}(x_2;\\mu,\\sigma^2)\\mathcal{N}(x_3;\\mu,\\sigma^2)\\mathcal{N}(x_4;\\mu,\\sigma^2) \\\\\n&= \\underset{\\theta}{\\text{argmax }}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left({-\\frac{(173-\\mu)^2}{2\\sigma^2}}\\right) \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left({-\\frac{(177-\\mu)^2}{2\\sigma^2}}\\right) \\\\ \\cdot &\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left({-\\frac{(175-\\mu)^2}{2\\sigma^2}}\\right) \\cdot\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left({-\\frac{(178-\\mu)^2}{2\\sigma^2}}\\right)\\\\\n&= \\underset{\\theta}{\\text{argmax }}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(-\\frac{(173-\\mu)^2 + (177-\\mu)^2 + (175-\\mu)^2 + (178-\\mu)^2}{\\sigma^2})\n\\end{aligned}\\]\n위의 가능도 함수가 최대가 되려면 exp의 지수의 분자인 \\(-[(173-\\mu)^2 + (177-\\mu)^2 + (175-\\mu)^2 + (178-\\mu)^2]\\)가 최대가 되면 됩니다.(분산은 고려하지 않겠습니다.)분자는 2차함수이므로 최대가 되는 지점을 구할 수 있습니다. 여기서는 넘파이를 활용하여 계산하겠습니다.\n\nx = np.linspace(120,232,50000)\ny = -((173-x)**2 +(177-x)**2+(175-x)**2+(178-x)**2)\nmax_idx = np.where(y == np.max(y))\nprint(x[max_idx])\nplt.axvline(x[max_idx],color=\"black\",linestyle=\"--\")\nplt.plot(x,y,\"b\")\nplt.scatter(x[max_idx],y[max_idx],s=100,c=\"black\",marker=\"s\")\nplt.title(f\"max(y) = {round(np.max(y),3)} if $\\mu$ = {round(x[max_idx][0],3)}, \")\n\n[175.750235]\n\n\nText(0.5, 1.0, 'max(y) = -14.75 if $\\\\mu$ = 175.75, ')\n\n\n\n\n\n가능도함수를 최대로 하는 모숫값은 175.75입니다. 따라서 MLE에 의한 모수에 대한 추정값은 175.75입니다. \\[\\hat{\\theta}_{MLE} = 175.75\\]\n\n\nLL\nLL은 log likelyhood의 약자로 likelyhood에 log를 취해준 값입니다. 로그함수를 취해도 함수가 MLE의 계산결과(가능도가 최대인 모수에 대한 추정값)의 위치가 변하지 않고 계산을 곱셈을 더하기로 바꿔서 계산하기에 더 편리하기 때문에 LL을 사용할 수도 있습니다. \\[LL = \\text{ln }L(\\theta|x_1,x_2 \\dots,x_N)\\]\n\n\nNLL\nNLL은 LL에 -(negative)를 곱해준 값입니다. 함수가 최대인 지점을 찾는 문제를 최소인 지점을 찾는 문제로 바꿀 수 있습니다. 머신러닝이나 최적화에서 사용하는 방법입니다. \\[NLL = -\\text{ln }L(\\theta|x_1,X_2 = x_2 \\dots x_N)\\]"
  },
  {
    "objectID": "posts/Probability&Statistics/notation.html",
    "href": "posts/Probability&Statistics/notation.html",
    "title": "확률분포에서 ;와|의 사용",
    "section": "",
    "text": "확률분포에서 ;와|에 관한 notation 정리\n\n;의 사용\n; 함수에서 특정 변수를 고정시켜놓을때 사용할 수 있다. 다음과 같은 이변수함수를 생각해보자 \\[f(x,y)\\] 독립변수 x와 y를 갖는 함수이며 두 변수 모두에 대해서 미분가능하다. 여기서 변수 y가 어떤 어떤 값으로 고정된것이 알려지거나 또는 고정된상황을 가정한 후 x에 대해서만 관심이 있다고 해보자(미분,극한등을 아마도?취하고 싶다..아마도…) 이때는 이변수함수로 표기하는것이 아닌 일변수함수로 표기해야한다. 그때는 다음과 같이 표기하면 된다.\n\\[\nf(x;y)\n\\]\n이렇게 표기하면 y값은 이제 어떤 값으로 이미 설정(setting)되거나 고정(fixed)되어있음을 의미한다. 비슷하게 그냥 고정된 파라미터가 이 다음에도 올 수 있다. 베르누이분포의 확률질량함수는 다음과 같다.\n\\[f_X(x;p) = p^x(1-p)^{1-x}\\]\nsetting,fixed된 파라미터인 p를 ;다음에 표시했다.\n\n\n|의 사용\n|는 given that,if이라는 의미이며 |의 다음에는 보통 먼저 깔고가거나 주어지는 전제,상황,조건,증거 등이 나온다. 확률,통계 분야에서만 쓰이며 조건부 확률이나,베이지안에서 많이 쓰이는 기호라고 한다. 중요한 점은 특히 |다음에 오는 상황,조건,증거와 같은 것들은 어떤 관점이냐에 따라서 상수나 확률변수의 관점으로 생각될 수 있다는 것이다. 특히 베이지안에서는 확률변수로 생각한다.\n조건부 확률분포(조건부확률밀도함수,조건부 확률함수)는 다음과 같다. \\[\np_{X|Y}(x|y) = \\frac{p_{XY}(x,y)}{p_Y(y)}\\\\\np_{X|Y}(y|x) = \\frac{p_{XY}(x,y)}{p_X(x)}\n\\]  가장 윗식을 해석해보면 다음과 같다. - 좌변 : 확률변수 Y가 주어진(given)상황에서 확률변수X의 확률(또는 확률밀도)를 나타내는 함수이며 시행으로부터 확률변수 X가 취할 수 있는 값 x를 입력으로 받는다. 조건부 확률분포에서 Y의 값은 y로 주어져 있다고 가정하므로 입력하는 변수가 아니다. 따라서 확률변수 Y는 변수가 아니라 모수(parameter)이다. - 우변 : 조건부확률분포 공식.\n##  결론 ; vs |\n\\[p_{\\theta}(x) = p(x;\\theta) = p(x|\\theta)\\]  ;는 수학의 모든 분야에서 쓰이지만 |는 확률통계분야에서만 쓰인다. ;다음에는 이미 설정되거나 고정된 값이 나오며 |다음에는 먼저 주어지는 조건이 나온다. 근데 ;를 많이 쓰지 않고 |뒤에 그냥 고정된 값을 써버리는 경우가 많다. 이는 지양해야 할 표현이다.\n참고링크 링크1(;의 사용법) 링크2(조건부확률분포) 링크3(표기법의 혼용)"
  },
  {
    "objectID": "posts/Probability&Statistics/sample space and random variable.html",
    "href": "posts/Probability&Statistics/sample space and random variable.html",
    "title": "확률론 용어정리",
    "section": "",
    "text": "Experiment and trial\n가능한 결과들이 미리 정해져있고 무한히 반복가능한 과정을 확률실험(experiment) 또는 시행(trial)이라고 합니다.\n\n\nsample space & event\n어떤 임의의 시행을 가정해본다고 합시다. 시행으로부터 나올 수 있는 모든 결과들의 집합을 우리는 표본공간(sample space)라고 합니다. 사건은 표본공간이라는 집합의 부분집합입니다. 여러개의 원소(결과)들이 모여서 하나의 사건이 될 수 있으며 단 하나의 원소로도 사건이 될 수 있습니다. 시행의 결과가 어떠한 사건(부분집합)에 속하는 경우 우리는 “~인 사건이 발생했다!”라고 표현합니다.\n\n\nrandom variable\n확률변수는 확률실험 또는 시행으로부터 나올 수 있는 결과를 대신 나타내는 변수입니다. 시행을 하기전까지는 그 값이 정해지지 않고 확률분포만 존재하며 시행을 하면 확률분포에 의해서 결과가 정해지고 실수가 부여됩니다. 수학적으로는 표본공간의 원소를 정의역으로 하여 실수를 대응시키는 “함수”입니다.\n\n\nprobability distribution(function)\n확률분포(확률함수)란 확률변수가 취할 수 있는 실수값에 각각의 실수를 취할 가능성인 확률 또는 확률밀도를 대응시키는 함수입니다.\n\n\n예시\n동전을 두번 던지는 시행을 3번 반복하여 크기가 3인 표본을 얻었다고 가정해봅시다. 동전의 앞면을 H(head)라 하고 뒷면을 T(tail)이라고 할 때 표본공간은 다음과 같습니다.\n\\[\\Omega = \\{HH,HT,TH,TT\\}\\]\n표본공간에 있는 결과들 중에서 동전의 앞면이 1개라도 있는 경우 사건A 동전의 앞면이 하나도 없는 경우를 사건B라 합시다. 사건A와 B는 다음과 같습니다.\n\\[A = \\{HH,HT,TH\\},B=\\{TT\\}\\]\n\\(X_1,X_2,X_3\\)는 각각 첫번째 두번째 세번째 시행의 결과를 나타내는 변수이며 확률분포는 다음과 같다고 합시다.\n\n\n\n                                                \n\n\n위의 확률분포를 반영하여 시행의 결과가 결정됩니다. 시행으로부터 얻은 표본은 \\((1,1,0)\\)이며 \\(X_1 = 1 ,X_2 =1 ,X_3 =0\\) 입니다. 시행의 결과 얻은 실제로 관찰된 표본은 \\((x_1,x_2,x_3)\\) 이런식으로 각각의 원소를 소문자인 미지수로 표현할 수도 있습니다.\n\n\ni.i.d & randomsample & realization\n위의 동전던지기 실험에서 각각의 확률변수는 이전에 던진 결과가 이후에 던지는 결과에 영향을 미치지 않고(즉,확률분포에 영향을 미치지 않고) 동일한 분포를 따르는 확률분포였습니다. 이와 여러개의 확률변수가 서로간에 독립이며 동일한 분포를 따르는 확률변수들을 Independent and identically distributed random variables라고 합니다. 위의 분포는 베르누이 분포를 따르므로 다음과 같이 표현할 수 있습니다.\n\\[X_1,X_2,X_3 \\overset{i.i.d}{\\sim} \\text{Bernoulli(p = 0.7)}\\]\nrandomsample은 i.i.d인 여러개의 확률변수의 모음입니다. \\(X1,X2,X3\\)를 말합니다.\nrealization은 관찰된 결과 각각을 말합니다. \\(x1\\)도 realization \\(x2\\)도 realization \\(x3\\)도? 모두 realization입니다. \\(x1,x2,x3\\)를 모아서 확률변수 \\(X_1,X_2,X_3\\)의 realizations이라고 합니다.\n\n\n참고자료\nwikipedia - Experiment (probability theory) StackExchange - What is the difference between random variable and random sample? wikipedia - i.i.d 정보통신기술용어해설"
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "",
    "text": "파이썬 변수,할당문,인터닝\n전북대학교 최규빈 교수님의 딥러닝 deepcopy-shallowcopy 특강 중,변수와 할당문 부분을 재구성한 글입니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-변수",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-변수",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "파이썬에서의 변수",
    "text": "파이썬에서의 변수\n\n파이썬의 변수는 메모리상에 저장된 객체를 참조(reference)합니다.\n따라서 변수는 자바에서 참조변수와 같습니다.\n변수는 객체(object)를 부르는 별칭(다른이름),객체에 붙여진 포스트잇,또다른 레이블 이라고 생각하는 것이 비유적으로 맞습니다.\n앞으로 객체에 접근하기 위해서는 a라는 변수(별칭,객체)를 찾으면 됩니다.\n마치 C언어의 포인터와 유사하게 동작합니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-할당문",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-할당문",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "파이썬에서의 할당문(=)",
    "text": "파이썬에서의 할당문(=)\n\n=은 파이썬(뿐만아니라 대부분의 프로그래밍언어)에서 할당문입니다.\n할당문을 실행하면 = 오른쪽에 있는 객체를 먼저 메모리 상에 생성하거나 가져옵니다.\n=의 왼쪽에는 변수가 존재하며 파이썬은 변수에 생성된 객체를 할당합니다.\n변수는 객체에 붙여지는 별칭,레이블로 표현할 수 있습니다. 이렇게 객체에 변수가 할당되고나면 할당된 변수와 객체에 대해서 “변수가 객체에 바인딩(묶이다)되어있다”고 표현합니다.\n\n\na = [1,2,3]\nprint(id(a))\n\n1819154486912\n\n\n위 코드에서 객체[1,2,3]에 변수 a가 바인딩 되었습니다.내부동작은 다음과 같습니다. 1. 메모리 주소(1819161042880)에 리스트 객체[1,2,3]을 생성합니다 2. 생성된 리스트객체를 변수 a에 할당합니다. a는 객체[1,2,3]에 붙여지는 별칭,레이블이라고 할 수 있습니다.\n\n에일리어싱\n에일리어싱은 하나의 객체를 여러개의 변수가 참조하게 하는 것입니다. 하나의 객체에 여러개의 별칭,별명,레이블을 붙이는 것이라고도 할 수 있습니다.\n\nb = a\nprint(id(a));print(id(b))\nprint(a);print(b)\n\n1819154486912\n1819154486912\n[1, 2, 3]\n[1, 2, 3]"
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#idvalue",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#idvalue",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "id,value",
    "text": "id,value\nid는 객체가 가지는 메모리상의 고유한 주소입니다. 서로다른 객체는 다른 값을 가질수도 같은값을 가질수도 있습니다.\n\na=[1,2,3]\nb=a\na.append(4)\nc=[1,2,3,4]\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\n\na:[1, 2, 3, 4] b:[1, 2, 3, 4] c:[1, 2, 3, 4]\nid(a):1819154849280,id(b):1819154849280,id(c):1819155097408\n\n\n변수a,b,c는 모두 같은 value(값)을 가집니다.a와b는 같은 객체에 바인딩되어있지만 c는 또다른 객체에 바인딩되어 있습니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#is-vs",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#is-vs",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "is vs ==",
    "text": "is vs ==\n- is는 객체비교연산자로 두 변수가 동일한 객체를 참조하는지 아니면 다른객체를 참조하는지 확인한 후 True or False를 반환합니다. 파이썬은 내부적으로 동일한 객체인지 아닌지를 판단할때에는 메모리주소를 확인한다고 합니다. - ==는 값비교연산자로 두 변수가 참조하는 객체의 값이 같은지 아니면 값이 다른지를 확인한 후 True or False를 반환합니다. - 참조하다는 뭔가 잘 와닿는데 참조`라는 용어는 잘 와닿지가 않습니다…변수가 참조하는(또는 가리키는,지칭하는)객체(object) 그 자체입니다.\n\ncode1\n\na=[1,2,3] #1\nprint(\"append하기 전 id(a):\",id(a))\nb=a #2 에일리어싱,동일한 객체를 가리키도록 함.\na.append(4) #3\nc=[1,2,3,4] #4\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\nprint(\"a의 참조(reference)와 b의 참조는 동일한 객체인가요??\",a is b)\nprint(\"a의 참조와 b의 참조는 값(value)이 같나요?\",a == b)\nprint(\"a의 참조(reference)와 c의 참조는 동일한 객체인가요??\",a is c)\nprint(\"a의 참조와 c의 참조는 값(value)이 같나요?\",a == c)\n\nappend하기 전 id(a): 1819155097408\na:[1, 2, 3, 4] b:[1, 2, 3, 4] c:[1, 2, 3, 4]\nid(a):1819155097408,id(b):1819155097408,id(c):1819155103296\na의 참조(reference)와 b의 참조는 동일한 객체인가요?? True\na의 참조와 b의 참조는 값(value)이 같나요? True\na의 참조(reference)와 c의 참조는 동일한 객체인가요?? False\na의 참조와 c의 참조는 값(value)이 같나요? True\n\n\n코드설명 1. 변수a에 [1,2,3]을 할당합니다.a는 [1,2,3]을 참조합니다. 2. 에일리어싱으로 변수b도 [1,2,3]을 참조합니다. 3. 변수a가 참조하는 리스트객체[1,2,3]에 4를 추가합니다. 4. 변수c에 [1,2,3,4]를 할당합니다.\n\n\ncode2 - 살짝 심화\n\na=[1,2,3] #1\nprint(\"재할당 하기 전 id(a):\",id(a))\nb=a #2에일리어싱,동일한 객체를 가리키도록 함\na=[1,2,3]+[4] #3재할당\nc=[1,2,3,4] #4\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\nprint(\"a의 참조(reference)와 b의 참조는 동일한 객체인가요??\",a is b)\nprint(\"a의 참조와 b의 참조는 값(value)이 같나요?\",a == b)\nprint(\"a의 참조(reference)와 c의 참조는 동일한 객체인가요??\",a is c)\nprint(\"a의 참조와 c의 참조는 값(value)이 같나요?\",a == c)\n\n재할당 하기 전 id(a): 1819155102592\na:[1, 2, 3, 4] b:[1, 2, 3] c:[1, 2, 3, 4]\nid(a):1819184303168,id(b):1819155102592,id(c):1819184334272\na의 참조(reference)와 b의 참조는 동일한 객체인가요?? False\na의 참조와 b의 참조는 값(value)이 같나요? False\na의 참조(reference)와 c의 참조는 동일한 객체인가요?? False\na의 참조와 c의 참조는 값(value)이 같나요? True\n\n\n코드설명 1. 변수a에 [1,2,3]을 할당합니다.a는 [1,2,3]을 참조합니다. 2. 에일리어싱으로 변수b도 [1,2,3]을 참조합니다. 3. 변수a에 리스트객체[1,2,3,4]를 재할당합니다. 4. 변수c에 [1,2,3,4]를 할당합니다.\na,b,c 각각의 값은 code1과 code2에서 모두 같습니다. 차이점은 code1에서는 a가 참조하는 리스트[1,2,3]에 4를 추가하고 code2에서는 a에 리스트[1,2,3,4]를 재할당한다는 점입니다.중요한 차이점은 다음과 같습니다.\n- code1에 append전 후의 a가 참조하는 객체는 주소는 변하지 않은 것으로 보아 동일한 객체에 원소만 추가되었음을 알 수 있습니다. - 반면 code2에서 할당문 전 후의 a가 참조하는 객체의 주소가 변합니다. - 이전에 없었던 1)객체가 메모리에 생성되고 2)변수a는 이전의 [1,2,3]을 더 이상 참조하지 않고 생성된 객체[1,2,3,4]를 참조**하는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#인터닝",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#인터닝",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "인터닝",
    "text": "인터닝\n인터닝이란 이미 생성된 객체를 재사용하는 것을 말합니다. 객체의 빠른 재사용을 가능하게 하며 메모리를 절약한다고 합니다. 내부적으로는 다음과 같이 구현됩니다. 1. 임의의 할당문을 실행합니다. 2. = 오른쪽에 있는 객체가 Intern 컬렉션에 등록되어 있는지 아닌지 확인합니다. 3. 등록되어 있는 객체의 경우 그 객체를 그대로 참조합니다. 등록되지 않은 경우 메모리에 객체를 생성하며 변수는 생성된 객체의 주소를 참조합니다\n자주 사용하는 객체의 경우 직접 Intern 컬렉션에 등록할 수 있고 빠르게 재사용할 수 있습니다. -5~256사이의 정수이거나 20자 미만의 문자열은 할당문을 실행하면 자동으로 Inter 컬렉션에 등록됩니다. 따라서 해당하는 정수나 문자열을 또다른 할당문에 실행하면 변수는 같은 객체를 참조합니다.\n예제1-인터닝 X\n\na=1+2021\nb=2023-1\nc=2022\nprint(id(a),id(b),id(c))\nprint(a,b,c)\n\n1819155040400 1819155039600 1819155040688\n2022 2022 2022\n\n\na,b,c는 서로다른 객체를 참조하며 객체들은 모두 같은 값을 가집니다.\n예제2-인터닝 O\n\na=1+2 \nb=4-1\nc=3\nprint(id(a),id(b),id(c))\nprint(a,b,c)\n\n1819075897712 1819075897712 1819075897712\n3 3 3\n\n\na,b,c는 모두같은 객체를 참조합니다. 내부적인 동작은 다음과 같습니다. 1. a=1+2 할당문을 실행합니다. 2. 할당되는 객체가 -5~256사이의 정수이므로 자동으로 Intern 컬렉션에 등록됩니다. 3. b와c에도 정수 3을 할당합니다. 3은 Intern 컬렉션 등록되어있는 객체이며 메모리상에 생성되어있는 객체이므로 새로운 3객체가 생성되지 b,c는 이미 생성된 3객체를 가리킵니다.\n참고링크1 : https://guebin.github.io/DL2022/posts/Appendix/2022-12-14-A1.html 참고링크2 : http://pythonstudy.xyz/python/article/512-%ED%8C%8C%EC%9D%B4%EC%8D%AC-Object-Interning"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html",
    "title": "plotly 시각화 모음",
    "section": "",
    "text": "Code\n# %pip install plotly (jupyter notebook)\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\npio.renderers.default = \"plotly_mimetype+notebook\"\n\n\n\n\nCode\nimport pandas as pd\ntimesData = pd.read_csv(\"./timesData.csv\")\ntimesData.head(5)\n\n\n\n\n\n\n  \n    \n      \n      world_rank\n      university_name\n      country\n      teaching\n      international\n      research\n      citations\n      income\n      total_score\n      num_students\n      student_staff_ratio\n      international_students\n      female_male_ratio\n      year\n    \n  \n  \n    \n      0\n      1\n      Harvard University\n      United States of America\n      99.7\n      72.4\n      98.7\n      98.8\n      34.5\n      96.1\n      20,152\n      8.9\n      25%\n      NaN\n      2011\n    \n    \n      1\n      2\n      California Institute of Technology\n      United States of America\n      97.7\n      54.6\n      98.0\n      99.9\n      83.7\n      96.0\n      2,243\n      6.9\n      27%\n      33 : 67\n      2011\n    \n    \n      2\n      3\n      Massachusetts Institute of Technology\n      United States of America\n      97.8\n      82.3\n      91.4\n      99.9\n      87.5\n      95.6\n      11,074\n      9.0\n      33%\n      37 : 63\n      2011\n    \n    \n      3\n      4\n      Stanford University\n      United States of America\n      98.3\n      29.5\n      98.1\n      99.2\n      64.3\n      94.3\n      15,596\n      7.8\n      22%\n      42 : 58\n      2011\n    \n    \n      4\n      5\n      Princeton University\n      United States of America\n      90.9\n      70.3\n      95.4\n      99.9\n      -\n      94.2\n      7,929\n      8.4\n      27%\n      45 : 55\n      2011\n    \n  \n\n\n\n\n\n\nCode\ntimesData.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2603 entries, 0 to 2602\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   world_rank              2603 non-null   object \n 1   university_name         2603 non-null   object \n 2   country                 2603 non-null   object \n 3   teaching                2603 non-null   float64\n 4   international           2603 non-null   object \n 5   research                2603 non-null   float64\n 6   citations               2603 non-null   float64\n 7   income                  2603 non-null   object \n 8   total_score             2603 non-null   object \n 9   num_students            2544 non-null   object \n 10  student_staff_ratio     2544 non-null   float64\n 11  international_students  2536 non-null   object \n 12  female_male_ratio       2370 non-null   object \n 13  year                    2603 non-null   int64  \ndtypes: float64(4), int64(1), object(9)\nmemory usage: 284.8+ KB"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#add-markers-and-text",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#add-markers-and-text",
    "title": "plotly 시각화 모음",
    "section": "add markers and text",
    "text": "add markers and text\n\n\nCode\n#1 data frame\ndf = timesData.iloc[:100]\n\n#2 trace and data\ntrace = go.Scatter(\n    x = df.world_rank,\n    y = df.citations,\n    mode = \"lines+markers\", #add marker,\n    marker = dict(color = \"rgba(16,112,2,0.8)\"),\n    text = df.university_name #add text\n)\ndata = [trace]\n#3 layout and data\nlayout = go.Layout(\n    title = \"citation\",\n    xaxis = dict(title = \"World Rank\",ticklen = 5)\n)\n\n#4 create figure\nfig = go.Figure(data = data,layout = layout)\n\n#5 plot figure\nfig.show()\n\n\n\n                                                \n\n\nversion2가 뭔가 더 좋을듯?"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#add-markers-and-text-1",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#add-markers-and-text-1",
    "title": "plotly 시각화 모음",
    "section": "add markers and text",
    "text": "add markers and text\n\n\nCode\n#1 data frame\ndf2014 = timesData[timesData.year == 2014].iloc[:100,:]\n\n#2 trace,data\ntrace = go.Scatter(\n    x = df2014.world_rank,\n    y = df2014.citations,\n    mode = \"markers\",\n    #marker = dict(color = \"green\",opacity=0.8), #alpha(불투명도) 조절 vs1\n    marker = dict(color = \"rgba(255,128,2,0.8)\"), #alpha(불투명도) 조절 vs2\n    text = df2014.university_name,\n\n)\ndata = [trace]\n\n#3 layout\nlayout = go.Layout(xaxis = dict(title = \"World Rank\"),yaxis = dict(title = \"Citation\"))\n\n#4 create figure\nfig = go.Figure(data=data,layout=layout)\n\n#5 plot\nfig.show()"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#여러개의-차트-겹처-그리기",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#여러개의-차트-겹처-그리기",
    "title": "plotly 시각화 모음",
    "section": "여러개의 차트 겹처 그리기",
    "text": "여러개의 차트 겹처 그리기\n\n여기서는 histogram으로 했으나 다른차트들도 가능\n\n\n\nCode\n#1.dataframe\nx2011 = timesData.student_staff_ratio[timesData.year == 2011]\nx2012 = timesData.student_staff_ratio[timesData.year == 2012]\n#2.trace&data\ntrace1 = go.Histogram(\n    x=x2011,\n    #opacity=0.7, #불투명도 조절\n    name=\"2011\", #범례(legend)를 설정하기 위한 이름 설정\n    marker=dict(color=\"rgb(171,50,96)\",opacity=0.7)\n)\n\ntrace2 = go.Histogram(\n    x=x2012,\n    name=\"2012\",\n    marker=dict(color=\"blue\",opacity=0.7)\n)\ndata=[trace1,trace2]\n#3.layout\nlayout = go.Layout(\n    barmode = \"overlay\", #trace 겹쳐 그리기\n    xaxis=dict(title=\"students-staff ratio\"),\n    yaxis=dict(title=\"count\"),\n    title = dict(text = \"histogram\",x = 0.5)\n)\n#4 figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()\n\n\n\n                                                \n\n\n참고자료 - Opacity와 alpha? : Opacity는 marker안팎에서 모두 쓰일 수 있으며 alpha는 rgba와 쓸때만 입력,같은 역할을 함. 단,Opacity를 marker의 밖에서 입력하면 trace안에서 밀도를 표현 하지 못함. 다른 trace끼리 겹칠때에는 밀도표현됨.(같은 trace에서만 안됨.)\n\n\nCode\n# 1.data frame\ndataframe = timesData[timesData.year == 2015]\n\n#2.trace and data\ndata = []\nfor col in [\"world_rank\",\"citations\",\"income\",\"total_score\"]:\n    _trace = go.Scatter(\n        x = dataframe[\"world_rank\"],\n        y = dataframe[col],\n        mode = \"lines\"\n    )\n    data.append(_trace)\n\n#3. layout\nlayout = go.Layout(\n    xaxis=dict(\n        domain=[0, 0.45]\n    ),\n    yaxis=dict(\n        domain=[0, 0.45]\n    ),\n    xaxis2=dict(\n        domain=[0.55, 1]\n    ),\n    xaxis3=dict(\n        domain=[0, 0.45],\n        anchor='y3'\n    ),\n    xaxis4=dict(\n        domain=[0.55, 1],\n        anchor='y4'\n    ),\n    yaxis2=dict(\n        domain=[0, 0.45],\n        anchor='x2'\n    ),\n    yaxis3=dict(\n        domain=[0.55, 1]\n    ),\n    yaxis4=dict(\n        domain=[0.55, 1],\n        anchor='x4'\n    ),\n    title = 'Research, citation, income and total score VS World Rank of Universities'\n)\n\n#4. fig\nfig = make_subplots(rows=2,cols=2)\n#5. plot\nrow = 1\ncol = 1\nfor trace in data:\n    fig.append_trace(trace,row=row,col=col)\n    col+=1\n    if col > 2:\n        col = 1\n        row+=1\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{\"type\": \"xy\"}, {\"type\": \"polar\"}],\n           [{\"type\": \"domain\"}, {\"type\": \"scene\"}]],\n)\n\nfig.add_trace(go.Bar(y=[2, 3, 1]),\n              row=1, col=1)\n\nfig.add_trace(go.Barpolar(theta=[0, 45, 90], r=[2, 3, 1]),\n              row=1, col=2)\n\nfig.add_trace(go.Pie(values=[2, 3, 1]),\n              row=2, col=1)\n\nfig.add_trace(go.Scatter3d(x=[2, 3, 1], y=[0, 0, 0],\n                           z=[0.5, 1, 2], mode=\"lines\"),\n              row=2, col=2)\n\nfig.update_layout(height=700, showlegend=False)\n\nfig.show()"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#vector-fieldquiver-plot",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#vector-fieldquiver-plot",
    "title": "plotly 시각화 모음",
    "section": "Vector field(quiver plot)",
    "text": "Vector field(quiver plot)\n\n사전준비\n\nnp.meshgrid : x좌표,y좌표를 가지는 벡터를 입력했을때, 두 벡터로 만들 수 있는 격자의 좌표(x,y)를 출력\n\n\n\nCode\nimport numpy as np\nx_coord = np.arange(0,2,.2)\ny_coord = np.arange(0,2,.2)\nx,y = np.meshgrid(np.arange(0,2,.2),np.arange(0,2,.2))\nprint(x_coord.shape,y_coord.shape)\nprint(x.shape,y.shape)\n\n\n(10,) (10,)\n(10, 10) (10, 10)\n\n\n\n격자(grid,matrix)에 함수 적용하면? => matrix(x,y 각각의 좌표)의 모든 요소에 함수가 적용됨\n\n\n\nCode\nprint(np.cos(x).shape,np.sin(x).shape)\n\n\n(10, 10) (10, 10)\n\n\n\n배열의 요소 값 차례대로 읽어보기 …\n\n(0,0),(0.2,0),(0.4,0) … (1.8,0) => (0,0.2),(0.2,0.2),(0.4,0.2)… x좌표 다 읽고 y좌표증가 그 다음 x좌표 다 읽고 y좌표 증가 …\n\n\nCode\nx,y\n\n\n(array([[0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8]]),\n array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n        [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n        [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n        [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6],\n        [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],\n        [1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],\n        [1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2],\n        [1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4],\n        [1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6],\n        [1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8]]))\n\n\n\n\nGradient Vector Field\n\n\\(\\nabla f = xe^{-x^2-y^2}\\)\n\n\nCode\n#1.prepare data\nx,y = np.meshgrid(np.arange(-2,2,0.2),np.arange(-2,2,.25)) #좌표\nz = x*np.exp(-x**2-y**2) #함수\n\ndx=0.2;dy=0.25 #dx,dy\nv,u = np.gradient(z,dx,dy) #함수의 그레디언트(각좌표에서의 미분계수)\n\n\n\n\nCode\n#2.trace and data => 생략\n#3.fig\nfig = ff.create_quiver(x,y,u,v,scale=.25,arrow_scale=.4,name=\"quiver\",line_width=1)\nfig.add_trace(go.Scatter(x=[-.7,.75],y=[0,0],\n                         mode=\"markers\",\n                         marker_size=12,\n                         name=\"points\"))\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nx = np.linspace(-1,1,100)\ny = np.linspace(-1,1,100)\nxx,yy = np.meshgrid(x,y)\nfor i in range()\n\n\n(array([[-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        ...,\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ]]),\n array([[-1.        , -1.        , -1.        , ..., -1.        ,\n         -1.        , -1.        ],\n        [-0.97979798, -0.97979798, -0.97979798, ..., -0.97979798,\n         -0.97979798, -0.97979798],\n        [-0.95959596, -0.95959596, -0.95959596, ..., -0.95959596,\n         -0.95959596, -0.95959596],\n        ...,\n        [ 0.95959596,  0.95959596,  0.95959596, ...,  0.95959596,\n          0.95959596,  0.95959596],\n        [ 0.97979798,  0.97979798,  0.97979798, ...,  0.97979798,\n          0.97979798,  0.97979798],\n        [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n          1.        ,  1.        ]]))\n\n\n\n\n\n1. 시점\n\n종점은 화살표로 표시해야 하므로 시점만 만들기\n\n\n\nCode\nimport plotly.graph_objs as go\n\n\n\n\nCode\n#1. prepare data\n\n#첫번째 벡터의 시점 x[0],y[0],z[0] 종점 x[1],y[1],z[1]\n#두번째 벡터의 시점 x[2],y[2],z[2] 종점 x[2],y[2],z[2]\n#두 개씩 묶임\nx = [10.1219, 10.42579, 15.21396, 15.42468, 20.29639,20.46268, 25.36298, 25.49156]\ny = [5.0545,  5.180104, 5.0545,   5.20337,  5.0545,  5.194271, 5.0545,   5.231627]\nz = [5.2713,  5.231409, 5.2713,   5.231409, 5.2713 ,  5.235852,  5.2713, 5.231627]\n#pairs = [(0,1),(2,3),(4,5),(6,7)]\n[coord for coord in range(0,len(x),2)]\n\n\n[0, 2, 4, 6]\n\n\n\n\nCode\n#2. trace,data(trace set)\ntrace1 = go.Scatter3d(\n    x=[x[coord] for coord in range(0,len(x),2)],\n    y=[y[coord] for coord in range(0,len(y),2)],\n    z=[z[coord] for coord in range(0,len(z),2)],\n    mode = \"markers\",\n    line=dict(color=\"red\")\n)\ndata = [trace1]\n\n#3. Layout\nlayout = go.Layout(title=dict(text = \"vectors\"))\n\n#4. figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()\n\n\n\n                                                \n\n\n\n\n2. 선 만들기\n\n\nCode\n#1.prepare data\nx_lines = list()\ny_lines = list()\nz_lines = list()\n\nfor i in range(len(x)):\n    x_lines.append(x[i])\n    y_lines.append(y[i])\n    z_lines.append(z[i])\n    #plotly에서 Scatter의 line mode는 점과 점 사이에 선을 만듦\n    #0,1번째 자리의 좌표에는 시점,종점을 넣고 3번째 자리에 None을 추가하여 점을 만들지 않음 \n    #따라서, 선이 생기지 않음\n    if i % 2 == 1:    \n        x_lines.append(None)\n        y_lines.append(None)\n        z_lines.append(None)\n\n#2.trace and tr_set(=data)\ntrace2 = go.Scatter3d(\n    x=x_lines,\n    y=y_lines,\n    z=z_lines,\n    mode = \"lines\",\n    line = dict(width = 2, color = 'rgb(255, 0,0)')\n)\ndata = [trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\n#중간체크\ndata = [trace1,trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\n3.종점 만들기\n\n\nCode\ndata = [trace1,trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nimport plotly.graph_objs as go\n# plotly.offline.init_notebook_mode()\n\nx = [10.1219, 10.42579, 15.21396, 15.42468, 20.29639,20.46268, 25.36298, 25.49156]\ny = [5.0545,  5.180104, 5.0545,   5.20337,  5.0545,  5.194271, 5.0545,   5.231627]\nz = [5.2713,  5.231409, 5.2713,   5.231409, 5.2713 ,  5.235852,  5.2713, 5.231627]\n\npairs = [(0,1), (2,3),(4,5), (6,7)]\n\n## plot ONLY the first ball in each pair of balls\ntrace1 = go.Scatter3d(\n    x=[x[p[0]] for p in pairs],\n    y=[y[p[0]] for p in pairs],\n    z=[z[p[0]] for p in pairs],\n    mode='markers',\n    name='markers',\n    line=dict(color='red')\n)\n\ngo.Figure(data=trace1)"
  }
]