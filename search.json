[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "전북대학교 IT응용시스템 공학과 신호연 sinhoyeon0514@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hoyeon's Blog",
    "section": "",
    "text": "Untitled\n\n\n\n\n\n\n\n\n\n \n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\n \n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\n \n\n\n\n차원축소를 위한 AutoEncoder\n\n\n\n1/12/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스]Lv.1시저암호\n\n\n\n1/9/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스]Lv1.이상한 문자 만들기\n\n\n\n1/9/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[프로그래머스]Lv1.크기가 작은 부분 문자열\n\n\n\n1/9/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeast Squares\n\n\n열공간(column space)과 정사영(projection)으로 접근한 최소제곱법(least squares)\n\n\n\n1/8/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnp.meshgrid\n\n\n\n1/8/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinite Difference Method with np.gradient\n\n\n\n1/7/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotly 시각화 모음\n\n\n\n1/6/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAx=b의 해의 갯수 알아내기\n\n\n\n1/5/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrank & null space(kernel)\n\n\n\n1/5/23\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nLinear Combination & Span\n\n\n\n1/3/23\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n행렬곱에 대한 여러가지 관점\n\n\n\n1/2/23\n\n\n\n\n\n\n\n\n\n\n \n\n\n\nMultinomial Logistic Regression & Softmax Regression\n\n\n\n12/30/22\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n카테고리 분포\n\n\n\n12/30/22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum likelyhood estimation\n\n\n\n12/29/22\n\n\n\n\n\n\n\n\n\n\n \n\n\n\n확률분포에서 ;와|의 사용\n\n\n\n12/27/22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n12/26/22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n파이썬 - 변수,할당문,인터닝\n\n\n\n12/25/22\n\n\n\n\n\n\n\n\n\n\n \n\n\n\npytorch로 Rnn구현하기\n\n\n\n12/24/22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n12/22/22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/coading test/[프로그래머스]Lv1.시저암호.html",
    "href": "posts/coading test/[프로그래머스]Lv1.시저암호.html",
    "title": "[프로그래머스]Lv.1시저암호",
    "section": "",
    "text": "문제\n\n\n\n나의 풀이\n\ndef solution(s, n):\n    upper_ch = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    answer = \"\"\n    for el_s in s:\n        #1.맨 마지막 리턴을 위해 원래 문자가 소문자인지 대문자인지 기억 and 대문자에서 검색할것이기 때문에 대문자로 변환\n        #대문자로 변환된 문자열 s의 각각의 문자,대소문자 여부\n        if el_s == \" \":\n            answer += \" \"\n        elif el_s.isupper() == True:\n            was_upper = True\n        else:\n            was_upper = False\n            el_s = el_s.upper()\n        #2.인덱스 숫자가 upper_ch에서 벗어날때 아닐때 처리\n        for idx,up_ch in enumerate(upper_ch):\n            if el_s == up_ch and idx + n <= len(upper_ch)-1:\n                find_idx = idx+n\n                if was_upper == True:\n                    answer += upper_ch[find_idx]\n                else:\n                    answer += upper_ch[find_idx].lower()        \n            elif el_s == up_ch and idx + n > len(upper_ch)-1:\n                find_idx = n-len(upper_ch[idx:])\n                if was_upper == True:\n                    answer += upper_ch[find_idx]\n                else:\n                    answer += upper_ch[find_idx].lower()  \n    return answer\n\n\n\n다른 풀이\n\ndef caesar(s, n):\n    lower_list = \"abcdefghijklmnopqrstuvwxyz\"  \n    #소문자도 리스트로 만듦 \n    #좋은점->대소문자 여부를 기억하는 코드 불필요(조건문)\n    #안좋은점->소문자로 이뤄진 리스트를 만드는데 그만큼의 메모리 필요\n    upper_list = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \n    \n    \n    \n    result = [] \n    #문자열들을 저장할 list를 만듦\n    #mutable로 붙이는 것과 immutable로 붙이는 것 차이\n    #속도 -> \n    #mutable이 더 빠름,\n    #그러나 garbage collector도 고려시 스캔범위가 너무커서 느려질수도?\n    #메모리 -> immutable이 더 적게들을 듯(리스트는 이중포인터같은 구조라서 이렇게 예상됨)\n    for i in s:\n        if i == \" \":\n            result.append(\" \")\n        elif i.islower() is True:\n            new_ = lower_list.find(i) + n\n            result.append(lower_list[new_ % 26]) \n            #나머지로 계산하는 방식,이게 더 간단하고 좋은듯\n            #반복문이 문자열에 대해서 돌다보니 문자열과 문자열의 인덱스 위주로 너무 생각함\n            #어떤 숫자(여기서는 인덱스)보다 크거나 같을때에 다시 0부터 줘야하는 상황? -> 나머지 활용\n        else:\n            new_ = upper_list.find(i) + n\n            result.append(upper_list[new_ % 26])\n    return \"\".join(result)\n\n나중에 볼 링크 링크1 링크2 링크3"
  },
  {
    "objectID": "posts/coading test/[프로그래머스]Lv1.이상한 문자 만들기.html",
    "href": "posts/coading test/[프로그래머스]Lv1.이상한 문자 만들기.html",
    "title": "[프로그래머스]Lv1.이상한 문자 만들기",
    "section": "",
    "text": "문제\n\n\n\n나의 풀이\n\ndef solution(s):\n    words = s.split(\" \") #스플릿 함수 헷갈리는 부분이 있었음 - 정리\n    answer = \"\"\n    for wd in words:\n        for idx,chr in enumerate(wd):\n            print(idx,chr)\n            if idx % 2 == 0:\n                chr = chr.upper()\n            else:\n                chr = chr.lower()\n            answer += chr\n        answer+= \" \"\n    return answer[:-1]\n\n\n\nsplit 메서드(함수)\n\nt=\"sdsa  sd\"\nhelp(t.split)\n\nHelp on built-in function split:\n\nsplit(sep=None, maxsplit=-1) method of builtins.str instance\n    Return a list of the words in the string, using sep as the delimiter string.\n    \n    sep\n      The delimiter according which to split the string.\n      None (the default value) means split according to any whitespace,\n      and discard empty strings from the result.\n    maxsplit\n      Maximum number of splits to do.\n      -1 (the default value) means no limit.\n\n\n\nstring객체의 인스턴스 즉,문자열에 대해서만 사용할 수 있는 메서드 sep파라미터에 전달한 인수를 구분자로 사용하여 문자열안에 있는 단어들을 가져옴. 가져온 단어들은 리스트의 원소가 되어 반환됨 Parameter - sep - …\nReturn - list[word1,word2,…] (구분자를 통하여 구분된 단어들,기본값은 공백(space))\n\n\n예시\n\n#문자열 내에 구분기준이 없는 경우?\n#sep = \" \"으로 기본값으로 설정되어 있음. 공백이 없으므로 그냥 문자열을 리스트에 넣어서 반환\n_t = \"abcde\"\nanswer = _t.split()\nprint(answer)\n\n['abcde']\n\n\n\n#문자열 내에 구분기준이 있는 경우?\n_t = \"ab cd esda dg\"\nanswer = _t.split()\nprint(answer)\n\n['ab', 'cd', 'esda', 'dg']\n\n\n\n_t = \"abtcdtesdatdg\"\nanswer = _t.split(\"t\")\nprint(answer)\n\n['ab', 'cd', 'esda', 'dg']\n\n\n\n_t = \"ab;;cd;;esd;;atdg\"\nanswer = _t.split(\";;\")\nprint(answer)\n\n['ab', 'cd', 'esd', 'atdg']"
  },
  {
    "objectID": "posts/coading test/[프로그래머스]Lv1.크기가 작은 부분 문자열.html",
    "href": "posts/coading test/[프로그래머스]Lv1.크기가 작은 부분 문자열.html",
    "title": "[프로그래머스]Lv1.크기가 작은 부분 문자열",
    "section": "",
    "text": "문제\n\n\n\n나의 풀이\n\ndef solution(t, p):\n    t_len = len(t);p_len = len(p)\n    answer = 0\n    p = int(p)\n    for idx in range(t_len-p_len+1):\n        num = int(t[idx:idx+p_len])\n        if num <= p:\n            answer+=1\n        else:\n            pass\n    return answer"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html",
    "href": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "선형회귀에 대해서 정리한 글입니다."
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#population-regression-model",
    "href": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#population-regression-model",
    "title": "Linear Regression",
    "section": "population regression model",
    "text": "population regression model\n\\[\nY = w_0 + w_1X_1  \\dots + w_mX_m + \\epsilon\n\\tag{1}\\] 위와 모집단의 확률변수사이의 관계를 선형이라고 가정한 모형을 모형을 population regression model이라고 합니다. population regression model은 아주 작은 오차(error)를 포함합니다."
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#sample-regression-modelvector-form",
    "href": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#sample-regression-modelvector-form",
    "title": "Linear Regression",
    "section": "sample regression model(vector form)",
    "text": "sample regression model(vector form)\n위와 같은 regression model로부터 sampling n개의 표본을 얻으면 다음과 같습니다. \\[\ny_1 = w_0 + w_1x_{11} + \\dots + w_mx_{1m} + \\epsilon_1\n\\] \\[\ny_2 = w_0 + w_1x_{21} + \\dots + w_mx_{2m} + \\epsilon_2\n\\] \\[\n\\vdots\n\\] \\[\ny_n = w_0 + w_1x_{n1} + \\dots +w_mx_{nm} + \\epsilon_n\n\\] \n샘플을 벡터-행렬로 표현해서 좀 더 간단히 나타낼 수 있습니다. \\[\n\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\dots &x_{1m}\\\\\n1 & x_{21} & x_{22} & \\dots &x_{2m}\\\\\n1 & x_{31} & x_{32} & \\dots &x_{3m}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n1 & x_{n1} & x_{n2} & \\dots & x_{nm}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nw_0\\\\\nw_1\\\\\nw_2\\\\\n\\vdots\\\\\nw_m\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\epsilon_0\\\\\n\\epsilon_1\\\\\n\\epsilon_2\\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n\\] \\[\n\\Longleftrightarrow\n\\bf{y} = \\bf{X}\\bf{W} + \\bf{\\epsilon}\n\\tag{2}\\]  위와 같이 샘플링한 표본을 나타내는 모형을 sample regression model이라고 합니다.\n\n\nText(0.5, 1.0, 'n=200')\n\n\n\n\n\n\\(w_1=1,w_0=0\\)인 population regression model로부터 200개의 표본을 추출하여 시각화하면 다음과 같습니다. 선형회귀는 위와 같이 표본만 주어질때(given), 우리가 모르는 \\(w_1,w_0\\)를 추정하는 것입니다. 표본으로부터 \\(\\bf{W}\\)를 추정할 수 있다면 두 변수사이의 관계가 이렇겠구나라고 알 수 있게 됩니다.\n\n\n\n\n\n\nNote\n\n\n\n실제문제에서 population regression model의 가중치 \\(\\bf{w} = (w_0,w_,1,...,w_m)\\)는 정확히 알 수 없습니다. 우리는 표본을 통해서 가중치를 추정할 뿐입니다. 위에서는 샘플데이터를 만들기 위해 어쩔 수 없이 알게되었지만 실제문제에서는 알 수 없습니다."
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#선형회귀의-loss-function",
    "href": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#선형회귀의-loss-function",
    "title": "Linear Regression",
    "section": "선형회귀의 Loss function",
    "text": "선형회귀의 Loss function\n2번에서 구한 추정값 \\(\\hat{\\bf{W}}\\)이 얼마나 틀린지,부정확한지 알려주는 함수를 Loss function 또는 Cost function이라고 합니다. 선형회귀에서의 Loss function은 일반적으로 MSE를 사용하며 주어진 샘플에서 잔차(residual,\\(\\hat{y}_i-y\\))들을 전부 제곱하여 더한 값입니다.\n(Loss function) \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2} = \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}}) = \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\)\nMSE와 같은 Loss function은 우리의 추정이 얼마나 틀렸는지를 나타내는 \\(\\hat{\\bf{W}}\\)에 대한 함수입니다. 그러므로, loss function을 가장 최소화 하는 \\(\\bf{\\hat{W}}\\)을 찾아내면 확률변수사이의 선형관계인 \\(\\bf{W}\\)를 알아낼 수 있습니다.\n\n\nText(110, 15, 'residual')"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#parameter-update",
    "href": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#parameter-update",
    "title": "Linear Regression",
    "section": "Parameter update",
    "text": "Parameter update\nn개의 독립변수를 가지는 다변수 스칼라 함수에 대한 Gradient는 수학적으로 다음과 같습니다.\n\\(\\nabla_{X}{f(x_1,x_2,...,x_n)} = (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\dots,\\frac{\\partial f}{\\partial x_n})\\) 다변수 스칼라 함수에 그레디언트를 취하면 벡터입니다.그러므로,그레디언트를 벡터(다변수)를 입력했을 때,벡터를 출력으로 하는 벡터함수라고 생각해도 무방합니다.중요한 사실은 임의의 공간상의 임의의 point \\(X\\)에서 스칼라함수에 대한 gradient of f = \\(-\\nabla_{X}{f}\\) 방향은 스칼라함수가 가장 급격하게 감소하는 방향이라는 사실입니다.(증명생략)\n위의 사실에 의하면,우리는 임의의 \\(\\hat{\\bf{W}}\\)에서 Loss function이 가장 급격하게 감소하는 방향을 찾을 수 있습니다. 그러므로 감소하는 방향을 찾고 이동하고 감소하는 방향을 찾고 이동하고 반복하다보면… 궁극적인 목적인 틀린정도를 최소화하는 즉,Loss function값이 가장 작은 \\(\\hat{\\bf{W}}\\)를 찾을 수 있습니다. \\(\\bf\\hat{W}\\)를 수정하는 구체적인 수식은 다음과 같습니다.\n(Gradient descent parameter update) \\(\\hat{\\bf{W}}_{t} = \\hat{\\bf{W}}_{t-1} - \\alpha\\times\\nabla_{W}{L}\\)\n\\(\\hat{\\bf{W}}_{t-1}\\)은 수정되기전의 가중치(벡터)이며 \\(\\hat{\\bf{W}_{t}}\\)는 파라미터를 한번 업데이트 한 후의 가중치(벡터)입니다. \\(t-1\\)의 \\(\\hat{\\bf{W_{t-1}}}\\)에 \\(-\\alpha\\times\\nabla_{W}{L}\\)를 더해줌으로서 \\(\\hat{\\bf{W}}_{t-1}\\)은 loss function이 가장 급격히(많이)감소하는 방향으로 이동하며 \\(\\hat{\\bf{W}}_{t}\\)가 됩니다. \\(\\alpha\\)는 학습률(learning rate)입니다. \\(\\hat{\\bf{W}}_{t-1}\\)과 곱해져서 얼마나 많이 또는 적게 움직일지를 결정합니다. 한번에 얼마나 이동할지에 비유한 “보폭”으로 생각할 수 있습니다.\n요약하자면, 경사하강법을 통하여 위와 같이 가중치\\(\\hat{\\bf{W}}\\)를 재귀적으로 업데이트 하면 loss function \\(L\\)이 가장 최소가 되는 지점의 \\(\\hat{\\bf{W}}\\)를 찾을 수 있습니다."
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#mse에-대한-더-상세한-전개",
    "href": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#mse에-대한-더-상세한-전개",
    "title": "Linear Regression",
    "section": "MSE에 대한 더 상세한 전개",
    "text": "MSE에 대한 더 상세한 전개\nMSE를 더 상세히 전개하면 다음과 같습니다. \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2}\\) \\(= \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}})\\) \\(= \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\) \\(= \\frac{1}{n}(\\bf{y^T - \\hat{\\bf{W}}^{T}\\bf{X}^{T})(\\bf{y} - \\bf{X}\\bf{\\hat{W}}})\\) \\(= \\frac{1}{n}(\\bf{y^Ty-y^TX\\hat{W}} - \\hat{W}X^Ty + \\hat{W}^TX^TX\\hat{W})\\)\n여기서 \\(\\bf{y^TX\\hat{W}} \\in \\bf{R}^{1 \\times 1}\\) 이므로 \\(\\bf{y^TX\\hat{W}} = (\\bf{y^TX\\hat{W}})^T = (\\bf{\\hat{W}X^Ty})\\)가 성립합니다. 그러므로 MSE를 정리하면 다음과 같습니다. (MSE) \\(MSE = \\frac{1}{n}(\\bf{y^Ty -2\\hat{W}X^Ty + \\hat{W}^TX^TX\\hat{W}})\\)"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#gradient-descent에-대한-더-상세한-전개loss-mse일-경우",
    "href": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#gradient-descent에-대한-더-상세한-전개loss-mse일-경우",
    "title": "Linear Regression",
    "section": "Gradient Descent에 대한 더 상세한 전개(\\(Loss\\) = MSE일 경우)",
    "text": "Gradient Descent에 대한 더 상세한 전개(\\(Loss\\) = MSE일 경우)\n(Gradient of MSE) \\(\\nabla{L} = MSE\\) \\(= \\bf{\\frac{1}{n}\\frac{\\partial}{\\partial \\hat{W}}(\\bf{y^Ty - 2\\hat{W}^TX^T + \\hat{W}^TX^TX\\hat{W}})}\\) \\(= \\bf{\\frac{1}{n}}(\\bf{\\frac{\\partial}{\\partial \\hat{W}}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2\\hat{W}^{T}X^{T}y + \\frac{\\partial}{\\partial\\hat{W}}\\hat{W}^{T}X^{T}X\\hat{W})\\) \\(= \\bf{\\frac{1}{n}(\\frac{\\partial}{\\partial \\hat{W}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2y^TX\\hat{W} + \\frac{\\partial}{\\partial\\hat{W}}\\hat{W}^TX^TX\\hat{W})}\\) \\(= \\bf{\\frac{1}{n}[0 - 2X^Ty + (X^TX + X^TX)\\hat{W}]}\\) \\(= \\bf{\\frac{2}{n}X^T(X\\hat{W} - y)}\\)\n(parameter update) \\(\\bf{\\hat{W}_{t} = \\hat{W}_{t-1} - \\alpha \\times \\frac{2}{n}X^T(X\\hat{W} - y)}\\)"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#결과해석",
    "href": "posts/Deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#결과해석",
    "title": "Linear Regression",
    "section": "결과해석",
    "text": "결과해석\n200개의 샘플로부터 \\(\\bf{w}\\)를 추정하여 \\(\\hat{\\bf{w}}= (0.125,0.969)\\)를 얻었습니다. population regression model의 \\({\\bf{w}} = (w_0,w_1) = (0,1)\\)을 올바르게 추정했음을 알 수 있습니다. 아주 약간의 차이가 존재하는데 이 차이는 모집단에서 샘플을 더 얻거나 더 세밀하게 업데이트하면 최소화할 수 있습니다.\n\n#plt.title(\"w_1 : {} // w_0: {}\".format(round(W_hat[1].tolist()[0],3),round(W_hat[0].tolist()[0],3)))\nplt.title(\"Linear Regression\")\ntext=f\"What = ({round(t[0].tolist()[0],3)},{round(t[1].tolist()[0],3)})\"\nplt.plot(X[:,1],y,\"bo\",alpha=0.5)\nplt.plot(X[:,1],X@W_hat,\"r--\")\nplt.gca().axes.xaxis.set_visible(False)\nplt.gca().axes.yaxis.set_visible(False)\nplt.title(text)\n\nText(0.5, 1.0, 'What = (-0.125,0.969)')"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html",
    "href": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "로지스틱회귀에 대해서 정리한 글입니다."
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#기댓값에-대한-고찰",
    "href": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#기댓값에-대한-고찰",
    "title": "Logistic Regression",
    "section": "기댓값에 대한 고찰",
    "text": "기댓값에 대한 고찰\n기댓값은 실험 또는 시행을 무한히 반복했을때 확률변수가 취하는 값의 평균으로(또는 샘플링된 값의 평균) 기대되는 값입니다. 확률변수가 베르누이 분포를 따르는 경우 확률변수에 대한 기댓값(\\(E\\,[y|x_{1,i},x_{2,i}\\.,\\dots,x_{m,i}]\\))과 모수\\((p_i)\\)가 같은 값을 가집니다. 그러므로,만약에 주어진 샘플데이터로부터 베르누이분포의 모수를 적절히 추정할 수 있다면 주어진 조건하에서 실험 또는 시행을 무한히 반복할 경우 확률변수가 1인사건과 0인사건중 어떤 사건이 더 많이 발생할지 알 수 있고 이를 바탕으로 종속변수 Y의 값을 결정하는 것은 타당합니다. - e.g.\n\n\\(E\\,[y]\\, = \\hat{p_i}<0.5\\) => 무한히 실행했을때 0인 경우가 더 많을 것임 => 관측치를 0으로 예측 \n\\(E\\,[y]\\, = \\hat{p_i}\\geq0.5\\)=>무한히 실행했을때 1인 경우가 더 많을 것임 => 관측치를 1로 예측"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#concept",
    "href": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#concept",
    "title": "Logistic Regression",
    "section": "concept",
    "text": "concept\n선형회귀에서 추정하고자하는 변수\\(y\\)는 \\(x_0,x_1,...,x_m\\)과 \\(w_0,w_1,...,w_m\\)과의 linear combination이였습니다.위에서 언급했듯이 특정샘플에 대한 모수를 적절하게 추정할 수 있다면 관측치가 어떤 클래스에 속할지 합리적으로 알 수 있으므로,로지스틱회귀에서도 선형회귀에서의 아이디어를 핵심아이디어를 가지고와서 추정하고자 하는 모수\\(p_i\\)를 \\(x_0,x_1,...,x_m\\)과 \\(w_0,w_1,...,w_m\\)의 linear combination로 표현하고자 합니다.\n선형회귀의 아이디어(linear combination) + 모수에 대한 표현이라는 조건을 만족하기 위해서 최종적인 식은 다음과 조건을 만족해야 것입니다. - \\((x_0,x_1,...,x_m)\\,,(w_0,w_1,..,w_m)\\)의 linear combination 식에 있어야 함. - linearcombination = 모수(추정하고자하는값)여야 함.\nwhy linear combination?"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#본격적인-유도",
    "href": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#본격적인-유도",
    "title": "Logistic Regression",
    "section": "본격적인 유도",
    "text": "본격적인 유도\n\n모수를 로지스틱함수로 바꾸기\n\n\\((x_0,x_1,...,x_m)\\,,(w_0,w_1,..,w_m)\\)의 linear combination이 식에 존재해야 합니다. 그러므로 선형방정식을 하나 만듭니다. \\[\\begin{align}\nf(i) = x_{1,i}w_0 + x_{2,i}w_1 + x_2w_2 + ... + x_{m,i}w_m = X_iW \\nonumber \\\\\nwhere,X_i = \\,[x_{1,i},x_{2,i},\\dots,x_{m,i}]\\, ,W = \\,[w_0,w_1,\\dots,w_m]^\\text{T} \\nonumber \\\\\n\\end{align}\\]\n좌변은 예측하고자 하는 값인 모수여야 합니다. 좌변을 바꿔봅니다. \\[p_i = WX_i\\]\n좌변의 베르누이 분포의 모수 \\(p_i\\)는 확률변수 \\(y = 1\\)인 사건이 일어날 확률입니다. 그러므로 \\([0,1]\\)이라는 범위를 가지는 반면 우변의 값\\(WX_i\\)은 \\(\\,[-\\infty,\\infty]\\,\\)에 범위를 가집니다. 여기서 Odss Ratio를 써서 모수 \\(p_i\\)를 포함하며 더 넓은 range를 갖도록 좌변을 수정합니다. \\[\\text{Odds Ratio} = \\frac{p_i}{1-p_i} = WX_i\\]\n좌변을 Odds Ratio로 수정했지만 여전히 좌변의 범위는\\(\\,[0,\\infty]\\,\\)으로 우변에 비해 좁습니다. 따라서 Odds Ratio에 로짓변환을 취하여 좌변의 범위를 \\(\\,[-\\infty,\\infty]\\)로 넓혀줍니다. \\[\\text{logit}(p) = \\text{ln}\\frac{p_i}{1-p_i} = WX_i\\]\n\n위 식을 해석하기 위해 \\(X\\)의 첫번째 요소인 \\(x_1\\)에 대응하는 회귀계수 \\(w_1\\)이 학습결과 3으로 정해졌다고 가정해봅시다.만약 \\(x_1\\)의 값이 1증가한다면 로그오즈비가 3증가합니다.\n\n이제 양변의 범위는 맞춰졌으므로 추정하고자 하는 변수 \\(p_i\\)가 좌변에 오도록 정리해봅시다. \\[p_i = \\frac{1}{\\,(1 + e^{-WX_i})\\, }\\] (전개) \\(\\frac{p_i}{1-p_i} = e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\,(1-p_i)\\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\,e^{WX_i}-p_ie^{WX_i}\\) \\(\\Longleftrightarrow p_i + p_ie^{WX_i} = \\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i\\,(1 + e^{WX_i})\\, = \\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\frac{\\,e^{WX_i}}{\\,(1 + e^{WX_i})\\, }\\) \\(\\Longleftrightarrow p_i = \\frac{1}{\\,(1 + e^{-WX_i})\\, }\\)\n\n최종적으로, 앞서 목적이었던 X와 W의 선형조합이 수식내부에 존재하도록 새롭게 표현한 모수는 다음과 같습니다. \\[p_i(y) = Pr\\,(y = 1|X_i;W)\\, = \\frac{1}{\\,1 + e^{-WX_i}\\,}\\]\n\n\n베르누이 분포의 pmf 정리\n베르누이분포의 모수 \\(p_i\\)가 새롭게 표현되었으므로 확률질량함수도 새롭게 표현할 수 있습니다. 마지막 수식은 베르누이 분포의 확률질량함수가 모수\\(W\\)에 관한 식으로 바뀌었음을 표현합니다.\n\\[\\begin{align}\nBern(y;p_i) = Pr\\,(Y_{i} = y|x_{1,i},x_{2,i},\\dots,x_{m,i};p_i) &=\n\\begin{cases}\np_i & \\text{if}\\,y=1 \\\\\n1-p_i & \\text{if}\\,y=0\n\\end{cases} \\\\\n&= p_i^{y}(1-p_i)^{1-y} \\\\\n&= \\frac{e^{yWX_i}}{1+e^{WX_i}}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#setting",
    "href": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#setting",
    "title": "Logistic Regression",
    "section": "setting",
    "text": "setting\n\nimport torch\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n#sig = lambda z:torch.exp(z)/(1+torch.exp(z))\nsig = torch.nn.Sigmoid()"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#data",
    "href": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#data",
    "title": "Logistic Regression",
    "section": "data",
    "text": "data\n\ntorch.manual_seed(2022)\nn=400\n\n#1 모수 W가정\nW = torch.tensor(\n    [[-0.8467],\n    [0.041]]).float()\n\n#2 각각의 관측치(데이터요소)에서의 모수 p_i시각화(시그모이드 함수 시각화)\n_x = torch.linspace(-150,150,n).reshape(-1,1)\n_one = torch.ones((n,1))\nX = torch.concat((_one,_x),axis=1)\np_i = sig(X@W)\ny = torch.bernoulli(p_i)\nplt.xlim([-150,150])\nplt.plot(X[:,1],y,\"bo\",alpha=0.5)\nplt.plot(X[:,1],p_i,\"r--\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y,$p_i$\")\nplt.title(\"realizations $y_1,\\dots,y_{300}$ from $Bern(p_1),\\dots,Bern(p_{300})$\")\nplt.legend([\"y\",\"p\"])\n\n<matplotlib.legend.Legend at 0x1f2684bfe20>"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#gradient-descent-1",
    "href": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#gradient-descent-1",
    "title": "Logistic Regression",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nimport torch\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n\ntorch.manual_seed(2022)\nn=400\n\n#2 임의의 W에 대한 estimated value(추정치) What 초기화\nWhat = torch.tensor(\n    [[0.],\n    [-0.03]],requires_grad=True)\n\n_x = torch.linspace(-150,150,n).reshape(-1,1)\n_one = torch.ones((n,1))\nX = torch.concat((_one,_x),axis=1)\nyhat = sig(X@What)\n\nplt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\nplt.plot(X[:,1].data,p_i,\"r\")\nplt.plot(X[:,1].data,yhat.data,\"g\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"realizations $y_1,\\dots,y_{60}$ from $Bern(p_1),\\dots,Bern(p_{60})$\")\nplt.legend([\"y\",\"$p_i$\",\"$\\hat{y}(\\hat{p_i})$\"])\n\n<matplotlib.legend.Legend at 0x1f268e11e50>\n\n\n\n\n\n\nloss_fn = torch.nn.BCELoss()\n\"\"\"\ndef BCE_Loss(yhat,y):\n    return torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat))\n\"\"\"\n\n'\\ndef BCE_Loss(yhat,y):\\n    return torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat))\\n'\n\n\n\n#custom sigmoid + torch.BCELoss 쓰면 오류 발생. 0과 1사이의 범위 아님\n#torch.nn.Sigmoid + custom BCE Loss 써도 오류발생 => nan\nplt.subplots(2,5,figsize=(20,8))\nplt.subplots_adjust(hspace=0.3)\ni=1\n\nfor epoch in range(200):\n    #1 yhat \n    yhat = sig(X@What)\n    #2 loss\n    loss = loss_fn(yhat,y)\n    if epoch % 20 == 0:\n        plt.subplot(2,5,i)\n        #plt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\n        plt.plot(X[:,1].data,p_i,\"r\")\n        plt.plot(X[:,1].data,yhat.data,\"g\")\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        #plt.title(\"realizations $y_1,\\dots,y_{60}$ from $Bern(p_1),\\dots,Bern(p_{60})$\")\n        plt.legend([\"p\",\"yhat\"])\n        title = \"loss : {}\".format(round(loss.tolist(),5))\n        plt.title(title)\n        i+=1\n    #3 derivative\n    loss.backward()\n    #4 update & clean\n    What.data = What.data - 0.00005 * What.grad\n    What.grad = None\n\n\n\n\n\nround(loss.tolist(),4)\n\n0.3109\n\n\n\nplt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\nplt.plot(X[:,1].data,p_i,\"r\")\nplt.plot(X[:,1].data,yhat.data,\"g\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Logistic Regression\")\nplt.legend([\"y\",\"$p_i$\",\"$\\hat{y}(\\hat{p_i})$\"])\nplt.gca().axes.xaxis.set_visible(False)\nplt.gca().axes.yaxis.set_visible(False)"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#베르누이-분포-전개",
    "href": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#베르누이-분포-전개",
    "title": "Logistic Regression",
    "section": "1.베르누이 분포 전개",
    "text": "1.베르누이 분포 전개\n\\[\\begin{aligned}\np_i^y(1-p_i)^{1-y} &= \\ (\\frac{1}{\\,1 + e^{-WX_i}\\,})^y\\,\\,(1-\\frac{1}{\\,1 + e^{-WX_i}\\,})^{1-y} \\\\\n&= (\\frac{1}{1+e^{-WX_i}})^{y}(\\frac{e^{-WX_i}}{1+e^{-WXi}})^{1-y} \\\\\n&= (\\frac{1}{1+e^{-WX_i}})^{y}(\\frac{1}{1+e^{WXi}})^{1-y} \\\\\n&= (\\frac{1+e^{WX_i}}{1+e^{-WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= (\\frac{e^{WX_i}+e^{2WX_i}}{1+e^{WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= (\\frac{e^{WX_i}(1+e^{WX_i})}{1+e^{WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= e^{yWX_i}\\frac{1}{1+e^{WX_i}} \\\\\n&= \\frac{e^{yWX_i}}{1+e^{WX_i}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#nll전개with-parameter-w",
    "href": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#nll전개with-parameter-w",
    "title": "Logistic Regression",
    "section": "2.NLL전개(with parameter \\(W\\))",
    "text": "2.NLL전개(with parameter \\(W\\))\n\\[\\begin{aligned}\nLL &= \\text{ln}(\\underset{i=1}{\\overset{n}{\\large{\\prod}}}\\,\\frac{e^{y_iWX_i}}{1+e^{WX_i}}) \\\\\n&=\\overset{n}{\\underset{i=1}{\\large{\\sum}}}(\\text{ln}\\frac{e^{y_iWX_i}}{1+e^{WX_i}}) \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}\\,[\\text{ln}e^{y_iWX_i} - \\text{ln}(1+e^{WX_i})]\\, \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}\\,[y_iWX_i - \\text{ln}(1+e^{WX_i})], \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}y_iWX_i - \\overset{n}{\\underset{i=1}{\\large{\\sum}}}ln(1+e^{WX_i}) \\\\\n\n\nNLL &= -\\overset{n}{\\underset{i=1}{\\large{\\sum}}}y_iWX_i + \\overset{n}{\\underset{i=1}{\\large{\\sum}}}ln(1+e^{WX_i}) \\\\\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#nll전개cross-entropy-유도하기",
    "href": "posts/Deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#nll전개cross-entropy-유도하기",
    "title": "Logistic Regression",
    "section": "3.NLL전개(Cross Entropy 유도하기)",
    "text": "3.NLL전개(Cross Entropy 유도하기)\n임의의 i번째 항에서의 확률변수 \\(Y_i\\)가 따르는 베르누이 분포는 다음과 같습니다. \n\\[\\begin{aligned}\nBern(y|X_i;p_i) = (p_i)^y(1-p_i)^{y-1}\n\\end{aligned}\\]\n모수가 \\(p_i\\)인 각각의 베르누이 분포를 따르는 확률변수 \\(Y_1,Y_2\\dots Y_n\\)으로부 n개의 realization(sample) \\(y_1,y_2\\dots y_n\\)에 대한 NLL는 다음과 같습니다. \n\\[\\begin{aligned}\nNLL &= -\\text{ln}\\prod_{i=1}^{n}p_i^{y_i}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}\\text{ln}p_i^{y_i}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}\\text{ln}p_i^{y_i} + \\text{ln}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}y_i\\text{ln}p_i + (1-y_i)\\text{ln}(1-p_i)\n\\end{aligned}\\]\n참고링크 1. 로지스틱 회귀 전개 2. 위키피디아 - 로지스틱 회귀 3. ratsgo’s blog"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html",
    "href": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "",
    "text": "[Deep Learning Series - Part3]\n안녕하세요!!😀 이번 포스트에서는 다항로지스틱 회귀와 소프트맥스 회귀에 대해서 정리해보고자 합니다. 공부하면서 생각보다 모르는 내용이 많아서 다시 처음부터 공부하고 복습해야 하는 내용이 많았네요. 잡담은 그만하고 시작해보겠습니다!! 읽어주셔서 감사해요😎"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#가정-1",
    "href": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#가정-1",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "가정 (1)",
    "text": "가정 (1)\n로지스틱회귀를 복기해보면… 종속변수 \\(y_i\\)는 베르누이분포를 따르는 확률변수\\(Y_i\\)로부터 샘플링된 값으로 가정했습니다. 또한 베르누이분포의 모수\\(W\\)는 주어진 조건인 \\(X_i\\)와 회귀계수(가중치)\\(W\\)의 일차결합으로 가정했습니다. 이렇게 모수를 가정하면서 베르누이분포의 확률질량함수도 새로운 모수\\(W\\)를 가지게되었고 W를 적절히 추정하면 데이터가 0또는1에 속할 확률을 알아내게 되어 확률이 더 높은 클래스를 주어진데이터에 대한 클래스로 예측했었습니다.다항로지스틱회귀와 소프트맥스회귀에서도 이러한 과정 즉,분포를 가정하고 데이터를 기반으로 모수를 추정하여 확률분포를 기반으로 예측하는 매커니즘은 거의 그대로입니다.\n먼저 다항로지스틱회귀와 소프트맥스회귀에서 종속변수에 대한 가정을 해보겠습니다. 다항로지스틱 회귀와 소프트맥스회귀에서 모두 각각의 관측치(each observation)에서 종속변수의 realization인 \\(y_i\\)는 확률변수\\(Y_i\\)로부터 표본추출(sampling)되었다고 가정합니다. 이때 각각의 관측치에서의 확률변수 \\(Y_i\\)가 따르는 분포는 설명변수 \\(x_{1,i},x_{2,i},\\dots,x_{M,i}\\)가 조건으로 주어질 때, 각각의 범주(클래스)에 속할 확률들을 모수로 가지는 카테고리분포를 따릅니다.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\nY_i|x_{1,i},x_{2,i},\\dots,x_{M,i} \\sim \\text{Cat}(y|x_{1,i},x_{2,i},\\dots,x_{M,i};\\mu_i)\n& =\n\\begin{cases}\n\\mu_{1,i} \\text{ if } y = (1,0,\\dots,0,0) \\\\\n\\mu_{2,i} \\text{ if } y = (0,1,\\dots,0,0) \\\\\n\\quad\\quad \\vdots \\\\\n\\mu_{K,i} \\text{ if } y = (0,0,\\dots,0,1) \\\\\n\\end{cases} \\\\\n&= \\mu_{1,i}^{y_1}\\mu_{2,i}^{y_2},\\dots,\\mu_{K,i}^{y_K} \\\\\n&= \\prod_{K=1}^{K}\\mu_{K,i}y_{K,i} \\\\\n\\end{aligned} \\\\\n&\n\\begin{aligned}\n&\\text{where, }\\\\\n&\\mu_i = {\\mu_{1,i},\\mu_{2,i},\\dots,\\mu_{K,i}} \\\\\n&\\mu_{1,i} = Pr(Y_i = (1,0,\\dots,0)|x_{1,i},\\dots,x_{M,i}) \\\\\n&\\mu_{2,i} = Pr(Y_i = (0,1,\\dots,0)|x_{1,i},\\dots,x_{M,i}) \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\vdots \\\\\n&\\mu_{K,i} = Pr(Y_i = (0,0,\\dots,0,1)|x_{1,i},\\dots,x_{M,i}) \\\\\n\\end{aligned}\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#가정-2",
    "href": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#가정-2",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "가정 (2)",
    "text": "가정 (2)\n각각의 관측치에서 확률변수\\(Y_i\\)가 따르는 카테고리분포의 모수\\(\\mu_i\\)는 데이터포인트마다 다른 설명변수(X_i)와 시행마다 변하지 않는 고정된 회귀계수(W)의 일차결합을 포함하는 수식으로 표현됩니다. 주어진 X값을 W와 일차결합하여 추정하고자 하는 값을 표현하는 선형회귀의 핵심아이디어이자 대부분의 회귀문제에서 사용하는 중요한 아이디어 입니다.\n\\[\\begin{aligned}\n&\\mu_{k,i}  = \\mu_{k,i}(X_i;W_{k,i}) = \\mu_{k,i}(X_iW_{k,i}) =  Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i)\\\\\n&\\text{where},\\\\\n&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n&W_k = [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n&\\mu_{k,i} = \\text{i-th 관측치의 $k$번째 모수}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#xw의-선형조합을-포함한-모수의-표현-유도하기",
    "href": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#xw의-선형조합을-포함한-모수의-표현-유도하기",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "X,W의 선형조합을 포함한 모수의 표현 유도하기",
    "text": "X,W의 선형조합을 포함한 모수의 표현 유도하기\n위에서 언급했듯이 대부분의 회귀에서 모델링의 핵심아이디어는 추정하고자 하는 대상을 설명변수와 가중치의 일차결합(선형조합)이 포함되도록 표현하는 것입니다. 다항로지스틱회귀도 추정하고자 하는 모수\\(\\mu_i = (\\mu_{1,i},\\mu_{2,i},\\dots,\\mu_{K,i})\\)를 각각을 설명변수와 가중치의 일차결합으로 표현해야 합니다.이진로지스틱회귀와에서도 이렇게 모수를 표현했었는데 다항로지스틱회귀에서는 일차결합으로 표현해야할 모수가 좀 더 많습니다. -_-;;\n차근차근 한번 유도해보겠습니다. 일단 K개의 모수를 표현하는 일차결합을 만들어줍니다. 이러한 일차결합에서 x는 관측치마다 존재하는 설명변수의 값에 따라서 회귀계수(가중치)인 W는 관측치에 따라서 변하지 않는 일정한 값입니다.\n\\[\\begin{aligned}\n&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i-\\text{ln}Z \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\\n&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i {\\text{ (임의의 k번째 항)}}\\\\  \n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n&where,\\\\\n&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n\\end{aligned}\\]\n 한 가지 유의해야 할 점은 마지막 모수는 일차결합으로 표현하지 않는다는 것입니다. 카테고리분포에서 모수의 총합은 1이기 때문에 마지막 \\(K\\)번째 모수는 1에서 전부 빼면 되기 때문입니다.\n그런데 섣불리 일차결합을 만들다보니 … 좌변에 있는 모수는 \\([0,1]\\)의 범위이고 우변은 \\([-\\infty,\\infty]\\)의 범위이므로 가지므로 양변의 범위가 전혀 맞지 않습니다. 그러므로 좌변을 Odds Ratio(엄밀히 Odds Ratio는 아니지만 통일성을 위해 Odds Ratio라고 하겠습니다.) + Logit transform을 취하여 좌변이 우변과 같은 범위를 가질 수 있도록 확장하여 줍니다. (로그안에 있는 분모가 K번째 클래스에 대한 항임을 유의합니다.) \\[\\text{ln}\\frac{\\mu_{k,i}}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = \\text{ln}\\frac{Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i;W_k)}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = W_k^TX_i\\]\n원래의 목적은 모수에 대한 일차결합이 포함된 항을 얻는 것이었습니다. 그러므로 정리하여 모수에 대한 표현을 얻습니다.\n\\[\\begin{aligned}\n\\mu_{k,i} = Pr(Y_i = (0,\\dots,0,1_{k-th},0,\\dots,0|X_i;W_k) = Pr(Y_i = K|X_i)e^{X_iW_k}\n\\end{aligned}\\]\n여기까지 해서 모수에 대한 표현을 얻었습니다. 다만 \\(Y_i\\)가 \\(K\\)번째 클래스에 대한 확률은 카테고리분포에서의 모수에 대한 제약조건을 활용하여 더 간단하게 바꿀 수 있습니다.\n\\[\\begin{aligned}\n&Pr(Y_i = K|X_i) = 1- \\sum_{k=1}^{K-1}Pr(Y_i = K|X_i)e^{X_iW_k} = 1-Pr(Y_i = K|X_i)\\sum_{k=1}^{K-1}e^{X_iW_k} \\\\\n&\\Longleftrightarrow Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{k=1}^{K-1}e^{X_iW_k}}\n\\end{aligned}\\]\n더 간단하게 표현된 항으로 다시 정리하여 쓰면 다음과 같습니다.\n\\[\\begin{aligned}\n&\\mu_{k,i}=Pr(Y_i = k|X_i) = Pr(Y_i = K|X_i)e^{X_iW_k} = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}}\\\\\n&\\text{인덱스 겹치므로 시그마의 $k \\rightarrow j$}\n\\end{aligned}\\]\n 최종적으로 카테고리 분포의 모수는 다음과 같습니다. 전개하는 과정이 마지막 \\(K\\)번째 항은 제외한채 진행되었으므로 K번째 항에대한 확률은 따로 써줍니다.\n\\[\\begin{aligned}\n&\\mu_{k,i}=Pr(Y_i = k|X_i) = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}} \\text{(단, $k != K$)}\\\\\n&\\mu_{K,i}=Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{j=1}^{K-1}e^{X_iW_k}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#estimation",
    "href": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#estimation",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "Estimation",
    "text": "Estimation\n더 공부해 오겠습니다 ^__^;;"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#가정",
    "href": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#가정",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "가정",
    "text": "가정\n소프트맥스회귀의 가정은 로지스틱회귀의 가정과 습니다. 각 datapoint에서의 종속변수의 값은 카테고리분포를 따르는 확률변수에서 샘플링되었으며 카테고리분포의 모수는 각 datapoint마다 변하는 설명변수와 회귀계수(가중치)의 일차결합으로 표현됩니다."
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#xw의-선형조합을-포함한-모수의-표현-유도하기-1",
    "href": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#xw의-선형조합을-포함한-모수의-표현-유도하기-1",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "X,W의 선형조합을 포함한 모수의 표현 유도하기",
    "text": "X,W의 선형조합을 포함한 모수의 표현 유도하기\n소프트맥스 회귀마찬가지로 추정하고자 하는 모수를 설명변수와 가중치의 일차결합이 포함된 항으로 표현합니다.\n먼저 설명변수와 가중치의 일차결합형태로 모수를 나타냅니다. 임의의 i번째 관측치가 각각의 범주에 \\((1,2,...,K)\\) 속할 확률을 의미하는 모수는 다음과 같습니다.\n\\[\\begin{aligned}\n&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i-\\text{ln}Z \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\\n&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i \\\\  \n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n&where,\\\\\n&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n\\end{aligned}\\]\n이렇게 나타내고 보니 좌변과 0~1사이의 수만 갖지만 우변은 어떤 수던지 나올 수 있습니다. 범위를 맞춰 주기 위해서 좌변에 로그를 씌워 로그확률로 만들어줍니다. 추가적으로 우변에 \\(-lnZ\\)라는 normalizating factor를 더해줍니다. 다음과정에서 카테고리분포의 모수의 합이 1이되도록 하는 확률질량함수의 특징을 유지하기 위해서 사용합니다.\n\\[\\begin{aligned}\n&\\text{ln}\\mu_{1,i} = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\text{ln}\\mu_{2,i} = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\text{ln}\\mu_{k,i} = w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{M,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\text{ln}\\mu_{K-1,i} = w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i-\\text{ln}Z \\\\\n\\\\\n\\end{aligned}\\]\n따라서,임의의 \\(k\\)번째 모수는 다음과 같습니다. \\[\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th}|X_i;W_k) = \\frac{1}{Z}e^{W_k^TX_i}\\]\n카테고리분포의 제약조건 즉,모수는 각각의 범주에 속할 확률을 나타내므로 총합이 1임을 활용합니다. 이를 활용하여 Z를 표현하면 다음과 같습니다.\n\\[\\begin{aligned}\n&\\sum_{k=1}^{K}{\\mu_{k,i}} =\\sum_{k=1}^{K}{Pr(Y_i=k)}= \\frac{1}{Z}\\sum_{k=1}^{K}e^{W_k^TX_i} = 1\\\\\n&\\Longleftrightarrow Z = \\sum_{k=1}^{K}e^{W_k^TX_i}\n\\end{aligned}\\]\n최종적으로, 결과를 정리하면 다음과 같습니다. - 추정하고자하는 카테고리분포의 모수는 \\(\\mu_k\\)는 \\(W_k\\)와 \\(X_i\\)의 일차결합으로 표현되었습니다. 이는 소프트맥스 함수이므로 소프트맥스 회귀라는 이름이 붙었습니다. \\[\\mu_{c,i}(X_i;W) = Pr(Y_i = (0,0,\\dots,1_{c-th},0,\\dots,0)|X_i;W_k) = \\frac{e^{W_c^TX_i}}{\\sum_{k=1}^{K}e^{W_k^TX_i}} = softmax(c,W_1^TX_i,W_2^TX_i,\\dots,W_K^TX_i)\\] - 카테고리분포의 위에서 구한 모수로 다시 정리하면 확률질량 함수는 새로운 모수 \\(W_1,W_2,\\dots,W_K\\)를 가집니다.(인덱스 \\(k->j,c->k\\)) \\[Y_i \\sim Cat(y|X_i;W_1,W_2,\\dots,W_K) = \\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\\]"
  },
  {
    "objectID": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#mle",
    "href": "posts/Deep learning/Deep learning theory/(3) Multinomial Logistic Regression & Softmax Regression/Multinomial Logistic Regression,Softmatx Regression.html#mle",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "MLE",
    "text": "MLE\n여기까지의 과정으로부터 카테고리분포의 모수는 설명변수와 가중치(회귀계수)의 일차결합으로 표현되며 또한 확률질량함수가 새로운 모수 \\(W = (W_1,W_2,\\dots,W_K)\\)로 표현되었습니다.만약 카테고리분포의 모수만 추정할 수 있다면 우리는 데이터포인트가 어떤 범주에 속할 확률이 가장 높은지 알 수 있으며 범주를 분류할 수 있습니다. 여기서는 카테고리분포의 모수\\(W\\)를 MLE로 추정합니다.\n확률분포에서 임의의 모수\\(W = (W_1,W_2,\\dots,W_K)\\)를 가정할 때, 확률변수 \\(Y_1,Y_2,\\dots,Y_N\\)으로부터 realization인 \\(y_1,y_2,\\dots,y_N\\)이 나올 가능도는 다음과 같습니다.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\nL({W};X_i|y_1,y_2,\\dots,y_n) &= Pr_{Y_1,Y_2,\\dots,Y_N}(y1,y2,\\dots,y_n|X_i;W)\\\\\n&= \\prod_{i=1}^{N}Pr_{Y_i}(Y_i=y_i|X_i;W) \\\\\n&= \\prod_{i=1}^{N}\\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\\\\\n\\end{aligned}\n\\\\\n&\\text{where } \\{W\\} = \\{W1,W2,\\dots,W_N\\}\n\\end{aligned}\\]\n위와 같은 가능도를 최소화 하는 \\(W\\)를 찾는 것이 목적입니다.다음과 같습니다\n\\[\\begin{aligned}\n\\overset{*}{\\{W\\}} = \\underset{\\{W\\}}{\\text{argmax}} \\prod_{i=1}^{N}\\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Deep learning/pytorch 구현/Pytorch Rnn 구현.html",
    "href": "posts/Deep learning/pytorch 구현/Pytorch Rnn 구현.html",
    "title": "pytorch로 Rnn구현하기",
    "section": "",
    "text": "hi?hi!가 반복되는 텍스트 데이터에서 다음 문자가 뭐가 나올지 예측하는 RNN모형 만들기"
  },
  {
    "objectID": "posts/Deep learning/pytorch 구현/Pytorch Rnn 구현.html#vectorization",
    "href": "posts/Deep learning/pytorch 구현/Pytorch Rnn 구현.html#vectorization",
    "title": "pytorch로 Rnn구현하기",
    "section": "vectorization",
    "text": "vectorization\n\n여러가지 방법이 있으나(tf-idf,dense vector,one-hot encoding 등등…) 여기서는 원핫인코딩 사용\n\n\ndef mapping(txt,map_dict):\n    return [map_dict[chr]for chr in txt]\ntxt_mapped = mapping(txt,map_dict)\nprint(txt_mapped[:10])\n\ndef onehot_encoding(txt_mapped):\n    seq_encoded = torch.nn.functional.one_hot(torch.tensor(txt_mapped))\n    return seq_encoded.float()\nsequence_data_encoded = onehot_encoding(txt_mapped)\nprint(sequence_data_encoded[:10])\n\n[2, 3, 0, 2, 3, 1, 2, 3, 0, 2]\ntensor([[0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.]])\n\n\n데이터 살짝 변형 하나의 긴 sequence data를 RNN의 입력으로 해도 되지만 처리속도,성능을 고려했을 때 자그마한 sequencedata로 분리하여 입력해주는게 더 좋은 방법임. 분리하는 방법도 여러가지가 있을 수 있겠는데 여기서는 다음과 같이 분리함 raw sequence data : hi?hi!hi?hi!hi?hi! ……….. sequence1 : (x,y) = (hi?,h) sequence2 : (x,y) = (i?h,i) sequence3 : (x,y) = (?hi,!) …\n\ndef create_seqdataset(seq_data,seq_length):\n    #x = seq_data[:-1]\n    #y = seq_data[1:]\n    seqs_x = []\n    seqs_y = []\n    for idx in range(0,len(seq_data)-seq_length):\n        seqs_x.append(seq_data[idx:idx+seq_length])\n        seqs_y.append(seq_data[idx+seq_length])\n    return torch.stack(seqs_x),torch.stack(seqs_y)\n    #return seq_x,seq_y\n\nx_data,y_data = create_seqdataset(sequence_data_encoded,3)\nprint(x_data.shape,y_data.shape)\n\ntorch.Size([57, 3, 4]) torch.Size([57, 4])\n\n\n\n왜 저런 shape을 맞춰 주는가?\n여기서 나오는 x_data.shape = \\((57,3,4)\\)가 살짝 난해함.  파이토치 공식문서에 따르면 batch_first = True로 설정할 경우,rnn계열의 모델에 넣어줘야 하는 텐서의 shape은 \\((N,L,H_{in})\\) = (batch size,sequnce length,input_size)이고 dataloader라는 일종의 데이터 중간관리자?를 한 번 거쳐서 모델에 입력됨. dataloader에서 나오는 output.shape = \\((N,L,H_{in})\\)이 되기 위해서는 input.shape = \\((D,L,H_{in}\\)(D는 분리된 시퀀스의 갯수)이어야 함(즉 입력텐서의 차원이 3개여야 출력텐서의 차원도3개이고 차원이 나오는 순서도 저런식이 되어야 함). 따라서 저렇게 설정함.\n\n\n파라미터 잠깐 설명\nbatch size는 배치의 총 갯수(배치안에 있는 원소의 갯수 아님!), sequnce length는 시퀀스데이터의 길이이자 timestemp(시점)의 총 갯수(길이), \\(H_{in}\\)은 each timestep(각 시점)마다 입력되는 벡터의 길이라고 볼 수 있음. 위처럼 원핫인코딩을 한 경우 \\(H_{in}\\)은 시퀀스데이터에 있는 문자의 갯수로 결정되므로 4이고 L은 create_seqdataset함수에서 인수로 넣어준 3(sequnce_length)이고 마지막으로 N(batch_size)은 torch.utils.data.DataLoader안에 인수로 넣어주는 batch_size로 인해서 일정한 갯수로 배치를 나누었을때 나오는 배치들의 총 숫자임.rnn 문서에서 설명하는 batch_size는 torch.utils.dada.DataLoader에서 설정한 batch_size의 갯수만큼 데이터를 모아서 여러개의 배치로 만들었을때 나오는 배치의 총 갯수라고 보면됨.(헷갈리는 부분….)"
  },
  {
    "objectID": "posts/Deep learning/pytorch 구현/Pytorch Rnn 구현.html#학습-준비하기",
    "href": "posts/Deep learning/pytorch 구현/Pytorch Rnn 구현.html#학습-준비하기",
    "title": "pytorch로 Rnn구현하기",
    "section": "학습 준비하기",
    "text": "학습 준비하기\n\ndefine architecture,loss,optimizer\ndata check\n\n\n#architecture,loss,optimizer \ntorch.manual_seed(2022)\nrnn = torch.nn.RNN(4,20,batch_first = True)\nlinr = torch.nn.Linear(20,4)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=1e-3)\n\n\nds = torch.utils.data.TensorDataset(x_data,y_data)\ndl = torch.utils.data.DataLoader(ds,batch_size=8,drop_last=True)\n\nfor idx,(x,y) in enumerate(dl):\n    if idx ==5:\n        break\n    print(x.shape,y.shape)\n\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\n\n\n위에서 언급했듯이 데이터로더를 거쳐서 나오는 텐서는 RNN에 바로 입력될 것임. input.shape = \\((N,L,H_{in}) = (8,3,4)\\)"
  },
  {
    "objectID": "posts/Deep learning/pytorch 구현/Pytorch Rnn 구현.html#모형학습",
    "href": "posts/Deep learning/pytorch 구현/Pytorch Rnn 구현.html#모형학습",
    "title": "pytorch로 Rnn구현하기",
    "section": "모형학습",
    "text": "모형학습\n\nfor epoch in range(0,101):\n    for tr_x,tr_y in dl:\n        #1 output\n        hidden,hT = rnn(tr_x)\n        #print(hidden.shape)\n        output = linr(hT[-1])\n        #2 loss\n        loss = loss_fn(output,tr_y)\n        #3 derivative\n        loss.backward()\n        #4 update & clean\n        optimizer.step()\n        optimizer.zero_grad()\n    if epoch % 10 == 0:\n        print(f'epoch : {epoch},loss : {round(loss.tolist(),5)}')\n\nepoch : 0,loss : 1.31779\nepoch : 10,loss : 0.69453\nepoch : 20,loss : 0.19338\nepoch : 30,loss : 0.05891\nepoch : 40,loss : 0.02861\nepoch : 50,loss : 0.01791\nepoch : 60,loss : 0.0126\nepoch : 70,loss : 0.00947\nepoch : 80,loss : 0.00744\nepoch : 90,loss : 0.00602\nepoch : 100,loss : 0.00499\n\n\npytorch의 rnn을 거쳐서 나오는 output은 두 가지임. - hidden : 가장 깊이 위치한 히든레이어의 각각의 시점에서의 출력값을 모아놓은 텐서 - hT : 모든 히든레이어에의 마지막 시점(시점T)에서의 출력값을 모아놓은 텐서 - 외우기! 위치 : 가장깊은 <=> 모든 , 시점 : 각각의 <=> 마지막\n위와같은 설정에서는 가장 깊이 위치한 히든레이어의 마지막시점에서의 출력값만이 우리는 다음에올 문자열을 예측할 때 필요하므로 hT[-1]을 하여 그 값을 가져옴."
  },
  {
    "objectID": "posts/Linear Algebra/Ax=b의 해의 갯수 알아내기/Ax = b의 해석.html",
    "href": "posts/Linear Algebra/Ax=b의 해의 갯수 알아내기/Ax = b의 해석.html",
    "title": "Ax=b의 해의 갯수 알아내기",
    "section": "",
    "text": "유투브 - 혁펜하임님의 선형대수학강의를 정리하기 위해 작성한 글입니다.Rank와 Null space로 Ax=b의 해의 갯수를 파악합니다.\n연립일차방정식은 행렬 Ax=b로 나타내고 행렬의 랭크와 열공간을 기반으로 해의 갯수를 나타낼 수 있습니다.\n\n열공간을 기반으로 한 Ax=b의 해석\n행렬\\(A = \\begin{bmatrix}a_1 & a_2 & \\dots & a_n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\)와 벡터\\(x = \\begin{bmatrix}x_1,x_2,\\dots,x_n\\end{bmatrix}^T\\in \\mathbb{R}^{n \\times 1}\\)의 곱을 생각해보자. 행렬과 벡터의 곱은 다음과 같이 열벡터의 일차결합으로 바라볼 수 있다.\n\\[\\begin{aligned}\nAx = \\begin{bmatrix}a_1 & a_2 & \\dots & a_n\\end{bmatrix}\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots\\\\\nx_n\n\\end{bmatrix}\n= a_1x_1 + a_2x_2 + \\dots a_nx_n\n\\end{aligned}\\]\n행렬 A의 열공간은 열벡터들의 선형생성이다.\n\\[\\begin{aligned}\n&\\text{C}(A) = \\text{span}(a_1,a_2,\\dots,a_n) = \\{c_1a_1 + c_2a_2 + \\dots c_na_n|c_1,c_2,\\dots,c_n \\in K\\}\n\\end{aligned}\\]\n그러므로, 행렬 \\(A\\)의 열공간은 임의의 \\(x\\)에 대하여 가능한 \\(Ax\\)의 모든 집합과 같다. 가능한 모든 스칼라\\(x_1,x_2,\\dots,x_n\\)로 벡터의 일차결합이 이루는 집합은 생성(span)과 같기 때문이다 \\[\\text{C}(A) = \\text{span}(a_1,a_2,\\dots,a_n) = \\{Ax |x\\in K^n\\}\\]\n만약 \\(Ax = b\\)인 방정식의 해 \\(x\\)를 구하려 한다 하자. \\(\\text{C}(A)\\)는 가능한 \\(Ax\\)의 모든 집합이였으므로 만약 \\(\\text{C}(A)\\)가 \\(b\\)를 포함한다면(즉,\\(b\\)가 열공간의 원소라면) \\(Ax=b\\)인 \\(x\\)가 존재하여 방정식의 해가 존재하고 \\(\\text{C}(A)\\)가 \\(b\\)를 포함하지 않는다면 \\(Ax = b\\)인 \\(x\\)값이 존재하지 않고 따라서 방정식의 해가 존재하지 않는다.\n\n\nCase1 : A가 full column rank일 때\n문제 : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) 일때, \\(Ax = b\\)를 만족하는 x의 갯수는?\n 위의 문제에서 A는 full column rank이다. full column rank일 경우 \\(Ax\\)의 모양은 위와 같고 열공간은 m차원 벡터공간의 부분공간이자 n차원 벡터공간이다.앞에서 “m차원 벡터공간의 부분공간인 n차원 벡터공간” 이라 했는데 그 이유는 (열)벡터가 m차원 벡터공간의 원소(벡터)이기 때문이다. m차원 벡터공간에 있는 (열)벡터 n개를 기저로 생성된 공간이기 때문에 m차원 벡터공간의 부분공간이면서 동시에 n차원 벡터공간이 된다. \n위에서 언급했듯이 열공간은 임의의 x에 대하여 가능한 \\(Ax\\)의 집합이기에 방정식의 해가 존재하려면 b가 열공간안에 있으면 되고 아니면 바깥에 있으면 된다. 그렇다면 이 경우 b의 위치는 어떻게 될까?\n\n가능한 b의 위치는 2가지이다. 1의 경우는 열공간안에 b가 없는 경우다. 이 경우 b는 m차원 벡터공간에는 있으면서(\\(b \\in R^{m \\times 1}\\)이기에 당연하다) 부분공간인 n차원 열공간안에는 없게된다. 따라서 이 경우 해는 존재하지 않는다. 2의 경우는 열공간안에 b가 있는 경우다. 이 경우 b는 m차원 벡터공간에 속하면서 동시에 부분공간인 n차원 벡터공간에도 속한다.\n이렇게 생각하면 끝난 것 같은데 한가지 더 생각해야 할 것이 있다. 바로 null space이다. 만약 Ax = b를 만족하는 하나의 해인 \\(x_{particular}\\)가 있다고 해보자. 이때 \\(Ax=0\\)을 만족하는 널공간의 임의의 원소인 \\(x\\)를 \\(x_{null}\\in N(A)\\)이라고 하면 다음이 성립한다.\n\\[A(x_{particular} + x_{null}) = Ax_{particular} + Ax_{null} = b\\]\n윗식에 의해서 null space의 원소인 \\(x_{null}\\)과 \\(x_{particular}\\)의 합도 방정식의 해이므로 해를 구할때 null space도 확인해야 한다. null space에 대한 x를 추가한 완전해는 다음과 같다. \\[x_{complete} = x_{particular} + x_{null}\\]\n그렇다면 x_{null}을 어떻게 확인할 수 있을까? 랭크-널리티 정리로 확인할 수 있는데 위와같이 full column rank인 경우는 다음과 같다.\n\\[\\begin{aligned}\n&\\text{rank}(A) + \\text{nulity(A)} = n\\\\\n&\\Longleftrightarrow \\text{nulity(A)} = n - rank(A) = n - n = 0\n\\end{aligned}\\]\n랭크-널리티 정리에 의하여 null space의 차원이 0임을 확인했다. 차원이 0인 널공간(벡터공간)은 \\(\\{\\bf{0}\\}\\)이므로 \\(x_{null} \\in N(A)\\)인 \\(x_{null} ={\\bf 0}\\)이다. 따라서 2번의 경우, \\(x_{particular}\\)가 존재하여 \\(Ax = 0\\)일 경우의 완전해는 다음과 같다. \\[\\therefore  x_{total} = x_{particular} + x_{null} = x_{particular} + {\\bf 0} = x_{particular}\\]\n결론적으로, full column rank일 경우 해가 존재하지 않거나 단 하나 존재한다.\n\n\nCase2 : A가 full row rank일 때\n문제 : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = m<n\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) 일때, \\(Ax = b\\)를 만족하는 x의 갯수는?\n 위의 문제에서 A는 full row rank이다. 위해서 한 것처럼 먼저 \\(C(A)\\)와 \\(b\\)가 어떻게 위치하고 있을지 파악하고 null space를 따져 완전해를 구하는 것이다. 먼저 \\(C(A)\\)를 생각해보자. \\(C(A)\\)는 행렬의 랭크가 m이기에 n개의 (열)벡터 중 선형독립인 column vector는 m개 뿐이다. 따라서 벡터의 span인 열공간은 m차원 벡터공간이다. (n개의 열벡터가 존재하지만,선형종속이기떄문이다.)\n이전문제와 다른점도 존재하는데 바로 열공간이 (열)벡터가 존재하는 벡터공간의 부분공간이 아니라는 점이다. 이 문제에서 각각의 열벡터는 m차원 공간의 벡터이고 열벡터의 생성도 m차원 벡터공간이기때문에 열벡터가 존재하는 바로 그 공간이 열공간이다.\n위와 같이 열공간에 대해서 생각했으면 이제 b에 대해서 생각해볼 차례다. b의 위치는 어떻게 될까? b는 A의 열공간에 속하는 벡터일까 아닐까?\n\nb의 경우 \\(b \\in \\mathbb{R}^{m \\times 1}\\)이기에 열공간에 속하는 벡터이다. 그러므로 full row rank인 경우는 반드시 해가 존재한다.\n여기서 완전해를 구하기 위해서 null space도 생각해야한다. 랭크-널리티 정리에 의해 다음과 같다.\n\\[\\begin{aligned}\n&rank(A) + nulity(A) = n \\\\\n&\\Longleftrightarrow nulity(A) = n - rank(A) =  n - m\n\\end{aligned}\\]\n랭크정리에 의해서 null space는 n-m차원의 벡터공간이다. 이 경우 가능한 \\(x_{null}\\)은 무한히 많이 존재하므로 해가 무수히 많다. 완전해는 다음과 같다.\n\\[\\begin{aligned}\n\\therefore\\,\\,  &x_{complete} = x_{particular} + x_{null}\\\\\n&\\text{where, } x_{null} \\in N(A) = \\mathbb{R}^{n-m}\n\\end{aligned}\\]\n결론적으로, full row rank의 경우 해는 무수히 많다.\n\n\nCase3 : A가 full rank일 때\n문제 : \\(A \\in \\mathbb{R}^{m \\times m},\\text{rank}(A) = m\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) 일때, \\(Ax = b\\)를 만족하는 x의 갯수는?\n\n사실 이 문제의 해는 다음과 같다. \\[x = A^{-1}b\\] 그러므로,full rank인 경우 해의 갯수가 1개이다.\n위처럼 간단하게 해를 구할 수 있지만 … 그래도 기하학적으로 생각해보기 위에서 했던 것처럼 따져보자. column space는 m차원 벡터공간이다. column vector는 column space 그 자체인 m차원 벡터공간의 원소이다. \\(b\\in \\mathbb{R}^{m \\times 1}\\)의 경우 m차원 벡터공간의 원소이다. 그러므로, column space는 반드시 b를 포함하며 그림으로 표현하면 다음과 같다.\n\n널공간을 따지기 위해 랭크-널리티 정리를 사용해 보면 다음과 같다.\n\\[\\begin{aligned}\n&\\text{rank}(A)+\\text{nulity}(A) = m\\\\\n&\\Longleftrightarrow \\text{nulity}(A) = m - \\text{rank}(A) = m - m = 0\n\\end{aligned}\\]\nnull space는 차원이 0이므로 \\(N(A) = \\{{\\bf 0}\\}\\)이고 다음과 같다. \\[x_{complete} = x_{particular} + x_{null} = x_{particular} + {\\bf 0} = x_{particular}\\]\n결과적으로, full rank인 경우 해는 반드시 존재하며 갯수는 1개이다.\n\n\nCase4 : A가 rank deficient 일 때\n문제 : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = \\alpha < \\text{min}(m,n)\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) 일때, \\(Ax = b\\)를 만족하는 x의 갯수는?\n\n위의 예시에서는 A가 rank deficient인 경우 중, \\(m<n\\) 인 경우를 생각해보자. 행렬 A의 column space는 m차원 공간의 부분공간이자 \\(\\alpha\\)차원의 벡터공간이다.\\(b \\in \\mathbb{R}^{m \\times 1}\\)이므로 가능한 경우는 아래와 같다.\n\n1번의 경우라면 b는 column space의 원소가 아니므로 방정식의 해는 존재하지 않는다. 만약 2번의 경우라면 해가 존재한다. 이때에는 null space를 고려한 완전해를 따져야하므로 랭크-널리티 정리를 확인한다.\n\\[\\begin{aligned}\n&\\text{rank}(A) + \\text{nulity}(A) = \\alpha + \\text{nulity}(A) = n \\\\\n&\\Longleftrightarrow \\text{nulity}(A) = n - \\alpha > 0\n\\end{aligned}\\]\n랭크정리에 의해서 null space는 \\(n - \\alpha\\)차원의 벡터공간이다. 이 경우 null space의 임의의 원소인 \\(x_{null}\\)는 무한히 많이 존재하므로 해가 무수히 많다. 완전해는 다음과 같다.\n\\[\\begin{aligned}\n\\therefore\\,\\,  &x_{complete} = x_{particular} + x_{null}\\\\\n&\\text{where, } x_{null} \\in N(A) = \\mathbb{R}^{n-\\alpha}\n\\end{aligned}\\]\n결론적으로, rank deficient의 경우 해는 무수히 많거나 해는 존재하지 않는다.\n\n\n정리\n\n\n\n\n\n\n\n\nrank type\nexpression\n해의 갯수\n\n\n\n\nfull column rank\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) = n<m\\)\n해가 없거나 해가 한개만 존재한다.\n\n\nfull row rank\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) = m<n\\)\n해가 무수히 많다.\n\n\nfull rank\n\\(A \\in \\mathbb{R}^{m \\times m}\\),\\(\\text{rank}(A) = m\\)\n해가 한개만 존재한다.\n\n\nrank deficient\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) < \\text{min}(m,n)\\)\n해가 없거나 해가 무수히 많다.\n\n\n\n\n\n참고자료\n혁펜하임 - [선대] 2-11강. Ax=b 의 해의 수 알아내기 프린키피아"
  },
  {
    "objectID": "posts/Linear Algebra/Least Squares/Least Squares.html",
    "href": "posts/Linear Algebra/Least Squares/Least Squares.html",
    "title": "Least Squares",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m,x \\in \\mathbb{R}^{n \\times 1},b \\in \\mathbb{R}^{m \\times 1}\\) 가 주어지고 방정식 \\(Ax = b\\)를 만족하는 해인 \\(x\\)를 구할 수 없을 때, \\(Ax\\)가 \\(b\\)와 가장 비슷하게 하는 \\(x\\)를 찾는 것이 목적입니다."
  },
  {
    "objectID": "posts/Linear Algebra/Least Squares/Least Squares.html#projection-matrix",
    "href": "posts/Linear Algebra/Least Squares/Least Squares.html#projection-matrix",
    "title": "Least Squares",
    "section": "projection matrix",
    "text": "projection matrix\n위해서 구한 \\(\\hat{x}\\)를 \\(A\\hat{x}\\)에 대입하면 다음과 같습니다. \\[A\\hat{x} = (A^TA)^{-1}A^Tb\\]\n위 식은 우변의 \\(b\\)에 \\(A(A^TA)^{-1}A^T\\)를 곱하여 \\(C(A)\\)에서 \\(b\\)와 가장 비슷하면서(거리가 가장 가까우면서) \\(b\\)를 \\(C(A)\\)에 정사영(projection) 한 벡터 \\(A\\hat{x}\\)을 얻음을 의미합니다. 따라서 \\(A(A^TA)^{-1}A^T\\)를 projection matrix라 부르고 \\(p_A\\)로 표기합니다."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html",
    "title": "rank & null space(kernel)",
    "section": "",
    "text": "유투브 - 혁펜하임님의 선형대수학강의를 정리하기 위해 작성한 글입니다."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#예제",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#예제",
    "title": "rank & null space(kernel)",
    "section": "예제",
    "text": "예제\n\\[\\begin{pmatrix}\n1&2&3\\\\\n0&0&0\n\\end{pmatrix}\\]\n위에 있는 행렬에서 선형독립인 열벡터의 갯수 = 1이다. 또한 선형독립인 열벡터의 갯수 = 선형독립인 행벡터의 갯수 = 열공간의 차원 = 행공간의 차원이므로 랭크정리도 성립한다.\n위와 같은 \\(2 \\times 3\\) 행렬은 랭크보다 행,열의 갯수가 많이 부족하므로 rank-deficient 라고 한다.\n\\[\\begin{pmatrix}\n1&0&1\\\\\n0&1&1\n\\end{pmatrix}\\]\n위에 있는 행렬에서 선형독립인 열벡터의 갯수 = 2이다. 또한 선형독립인 열벡터의 갯수 = 선형독립인 행벡터의 갯수 = 열공간의 차원 = 행공간의 차원이므로 랭크정리도 성립한다.\n위와 같은 행렬은 행의 갯수만큼 랭크가 다 차있으므로 full row rank라 한다."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#용어정리",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#용어정리",
    "title": "rank & null space(kernel)",
    "section": "용어정리",
    "text": "용어정리\n\\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m \\Rightarrow\\) 열벡터가 모두 선형독립,full column rank,위아래로 길쭉하고 양옆은 좁은 직사각행렬 \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n>m \\Rightarrow\\) 행벡터가 모두 선형독립,full row rank,위아래가 좁고 좌우 양옆으로 길쭉한 직사각행렬 \\(A \\in \\mathbb{R}^{n \\times n},\\text{rank}(A) = n \\Rightarrow\\) 행백터,열벡터가 모두 선형독립,full rank,위,아래,양,옆의 길이가 모두 같은 정사각행렬 \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) < \\text{min}(n,m)\\) = 선형종속인 행벡터,열벡터 반드시 존재,rank deficient"
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#예제1",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#예제1",
    "title": "rank & null space(kernel)",
    "section": "예제1)",
    "text": "예제1)\n행렬 A = \\(\\begin{pmatrix}1&0&1 \\\\0&1&1\\end{pmatrix}\\)일 때, 행렬A의 영공간은?\n먼저 만족하는 \\(x\\)를 찾아보자. 어떤 방법이던 사용가능하지만 문제가 간단하므로 직관적으로 풀이한다.\n\\[\\begin{aligned}\n&Ax = x_1\\begin{pmatrix}1\\\\0\\end{pmatrix} + x_2\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + x_3\\begin{pmatrix}1\\\\1\\end{pmatrix} = {\\bf 0} \\\\\n&\\Longleftrightarrow\n\\begin{cases}\nx_1 + x_3 = 0 \\\\\nx_2 + x_3 = 0\n\\end{cases}\n\\\\\n&\\Longleftrightarrow x_3 = t,x_2 = -t,x_1 = t \\\\\n&\\Longleftrightarrow x = t\\begin{pmatrix}1\\\\-1\\\\1\\end{pmatrix} \\\\\n&\\therefore \\text{N}(A) = \\Bigg\\{t\\begin{pmatrix}1\\\\-1\\\\1 \\end{pmatrix}|t \\in \\mathbb{R}\\Bigg\\}\n\\end{aligned}\\]\n방정식의 해를 구해보니 1)\\([1,-1,1]^T\\)의 span이 영공간이며 2)영공간은 행렬곱에 의해서(중간에서 차원일치 \\(2 \\times 3 과 3 \\times 1\\)) 행벡터가 존재하는 차원인 3차원 벡터공간의 부분공간인 1차원 벡터공간을 생성함을 알 수 있다.\n영공간의 원소(방정식의 해)는 무수히 많음이 자명한데 왜냐하면 \\(Ax = 0\\)을 만족하는 임의의 x벡터에 대한 스칼라배에 대해 다음이 성립하기 때문이다.\n\\[\\begin{aligned}\n&Ax = {\\bf 0} \\\\\n&\\Longleftrightarrow cAx = c{\\bf 0} = {\\bf 0} \\\\\n&\\Longleftrightarrow A(cx) ={\\bf 0} \\\\\n\\end{aligned}\\]\n따라서 \\(x\\)의 스칼라배인 \\(cx\\)도 \\(A\\)와 곱해져서 \\({\\bf 0}\\) 만들기 \\(x\\)의 스칼라배도 영공간의 원소이다. \nrank-nulity theoreom도 성립함을 확인할 수 있다. 영공간의 차원은 영공간의 기저의 갯수인데 \\([1,-1,1]^T\\)이 기저의 조건인 1)선형생성 = 벡터공간 2)선형독립 이라는 두 조건을 만족하므로 차원은 1이다.랭크는 다른방식으로도 구할 수 있지만 행렬이 간단해서 바로2임을 확인할 수 있으므로 다음과 같다 \\[\\text{nulity}(A) + \\text{rank}(A)= n   \\Longleftrightarrow  1 + 2 = 3\\]"
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#예제2",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#예제2",
    "title": "rank & null space(kernel)",
    "section": "예제2)",
    "text": "예제2)\n행렬 A = \\(\\begin{pmatrix}1&2&3 \\\\0&0&0\\end{pmatrix}\\)일 때, 행렬A의 영공간은?\n마찬가지로 만족하는 \\(x\\)를 먼저 찾아보자.\n\\[\\begin{aligned}\n&Ax = x_1\\begin{pmatrix}1\\\\0\\end{pmatrix} + x_2\\begin{pmatrix}2 \\\\ 0\\end{pmatrix} + x_3\\begin{pmatrix}3\\\\0\\end{pmatrix} = {\\bf 0} \\\\\n&\\Longleftrightarrow\nx_1 + 2x_2 + 3x_3 = 0 \\\\\n&\\Longleftrightarrow x_3 = t,x_2 = q,x_1 = -2q -3t \\\\\n&\\Longleftrightarrow x = \\begin{pmatrix}-2q-3t\\\\q\\\\t\\end{pmatrix} = q\\begin{pmatrix}-2\\\\1\\\\0\\end{pmatrix} + t\\begin{pmatrix}-3\\\\0\\\\1\\end{pmatrix} \\\\\n&\\therefore \\text{N}(A) = \\Bigg\\{q\\begin{pmatrix}-2\\\\1\\\\0 \\end{pmatrix} + t\\begin{pmatrix}-3\\\\0\\\\1\\end{pmatrix}|t,q \\in \\mathbb{R}\\Bigg\\}\n\\end{aligned}\\]\n방정식의 해를 구해보니 1)두 벡터의 span이 영공간이며 2)생성된 영공간은 행렬곱에 의해서(중간에서 차원일치 \\(2 \\times 3\\) 과 \\(3 \\times 1\\)) 행벡터가 존재하는 차원인 3차원 벡터공간안에서 부분공간인 1차원 벡터공간을 생성함을 알 수 있다.\n\\(Ax = 0\\)을 만족하는 2개의 x의 선형조합이 모두 영공간의 원소임을 확인해보자.\n\\[\\begin{aligned}\n&Ax_1 = {\\bf 0},Ax_2 = {\\bf 0} \\\\\n&\\Longleftrightarrow cAx_1 = c{\\bf 0}={\\bf 0},cAx_2 = c{\\bf 0}={\\bf 0} \\\\\n&\\Longleftrightarrow A(cx_1) + A(cx_2) = A(cx_1 + cx_2) = {\\bf 0} \\\\\n\\end{aligned}\\]\n따라서 \\(x\\) 방정식을 만족하는 두 벡터의 선형조합인 \\(c(x_1 + cx_2)\\)도 \\(A\\)와 곱해져서 영공간의 원소임을 알 수 있다. \n마찬가지로 rank-nulity theoreom도 성립함을 확인할 수 있다. 영공간의 차원은 영공간의 기저의 갯수이고 \\([-2,1,0]^T,[-3,0,1]^T\\)가 기저의 조건인 1)선형생성 = 벡터공간(여기서 영공간) 2)선형독립 이라는 두 조건을 만족하므로 영공간의 기저는 \\([-2,1,0]^T,[-3,0,1]^T\\)이고 차원은 2이다.랭크는 다른방식으로도 구할 수 있지만 행렬이 간단해서 바로1임을 확인할 수 있으므로 다음과 같다 \\[\\text{nulity}(A) + \\text{rank}(A)= n   \\Longleftrightarrow  2 + 1 = 3\\]"
  },
  {
    "objectID": "posts/Linear Algebra/span과 columns space.html",
    "href": "posts/Linear Algebra/span과 columns space.html",
    "title": "Linear Combination & Span",
    "section": "",
    "text": "유투브 - 혁펜하임님의 선형대수학강의를 정리하기 위해 작성한 글입니다.\n\nlinear Combination\n벡터 \\(\\bf{v_1,v_2,\\dots,v_n}\\)의 선형결합(또는 일차결합)은 다음과 같다. \\[w_1{\\bf v_1} + w_2{\\bf v_2} + \\dots + w_n{\\bf v_n}\\] 선형결합의 결과는 여러가지 벡터들의 결합이다. 여기서 각각의 \\(w_1,w_2,\\dots,w_n\\)은 스칼라이며 대응하는 \\(x_1,x_2,\\dots,x_n\\)를 결합에 사용하는 정도를 의미한다. 만약 \\(w_1 = 0.0001\\)이면 여러 벡터들을 결합하지만 그 결합 중 \\(\\bf v_1\\)이 아주 사용하여 결합하는 것이고 \\(w_2 = 120\\)이라면 결합에서 \\(\\bf v_2\\)를 아주 많이 사용하는 것이다.\n\n\nspan\n벡터 \\(v_1,v_2,\\dots,v_n\\)의 선형결합에서 \\(w_1,w_2,\\dots,w_n\\)(스칼라,결합에 사용하는 정도)를 바꿨을때 가능한 모든 벡터들의 집합이며 즉,벡터의 선형결합으로 가능한 모든 집합들이며 정의는 다음과 같다. \\[\\text{span}({\\bf{v_1,v_2,\\dots,v_n}}) := \\{w_1{\\bf{v_1}} + w_2{\\bf{v_2}} + \\dots + w_n{\\bf{v_n}}:w_1,w_2,\\dots,w_n \\in K\\}\\]  span은 선형결합으로 만들어지는 또다른 벡터공간이다. 여기서 K는 field를 의미하는데 스칼라는 field라는 또다른 집합으로부터 가져온 원소이기때문에 그렇다.임의의 벡터들의 span은 어떨까? 아래의 그림을 확인해보자.\n<참고> span은 동사로도 사용한다. ex : 벡터공간을 생성한다. 열공간은 열벡터들이 생성하는 공간이다.(=열공간은 열벡터의 생성이다.).기저들이 벡터공간을 생성한다.\n:는 조건을 의미합니다.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nplt.style.use(\"ggplot\")\n\ndef linrcmb(v1,v2):\n    linrcomb_x= []\n    linrcomb_y= []\n\n    _w = np.linspace(-50,50,300).tolist()\n    for i in range(2000):\n    #w1,w2를 -100~100사이의 임의의 숫자로\n        w1 = random.sample(_w,1)\n        w2 = random.sample(_w,1)\n        #print(w1,w2)\n    #선형결합 계산\n        linrcomb = w1 * v1 + w2 * v2\n    #시각화를 위해 선형결합의 x값 y값 따로 모아놓기\n        linrcomb_x.append(linrcomb[0][0])\n        linrcomb_y.append(linrcomb[1][0])\n    return linrcomb_x,linrcomb_y\n\nv1 = np.array([[0],[0]])\nv2 = np.array([[0],[0]])\nx,y = linrcmb(v1,v2)\nfig,ax = plt.subplots(figsize=(30,5))\nplt.subplot(1,5,1)\nplt.title(\"$v_1 = [0,0]^{T},v_2 = [0,0]^T$\")\nplt.scatter(x,y,color=\"black\")\n\nv1 = np.array([[1],[0]])\nv2 = np.array([[-3],[0]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,2)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [-3,0]^T$\")\nplt.scatter(x,y)\n\nv1 = np.array([[1],[1]])\nv2 = np.array([[0],[0]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,3)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [0,0]^T$\")\nplt.scatter(x,y,color=\"green\",alpha=1)\n\n\nv1 = np.array([[1],[0]])\nv2 = np.array([[0],[1]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,4)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [0,1]^T$\")\nplt.scatter(x,y,color=\"purple\",alpha=1)\n\ndef linrcmb2(v1,v2):\n    linrcomb_x= []\n    linrcomb_y= []\n\n    _w = (np.linspace(-250,250,300)).tolist()\n    for i in range(2000):\n        w1 = random.sample(_w,1)[0] * 100 #그래프를 그리기 위한 값 조절\n        w2 = random.sample(_w,1)[0] \n        #선형결합 계산\n        linrcomb = w1 * v1 + w2 * v2\n        #시각화를 위해 선형결합의 x값 y값 따로 모아놓기\n        linrcomb_x.append(linrcomb[0][0])\n        linrcomb_y.append(linrcomb[1][0])\n    \n    return linrcomb_x,linrcomb_y\nlinrcmb2(v1,v2)\n\nv1 = np.array([[-1],[0]])\nv2 = np.array([[2],[2]])\nx,y = linrcmb2(v1,v2)\nplt.subplot(1,5,5)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [2,2]^T$\")\nplt.scatter(x,y,color=\"blue\",alpha=1)\nplt.subplots_adjust(wspace=0.4,hspace=0.5)\nplt.suptitle(\"span$(v_1,v_2)$\",y=1.02,fontsize=20)\n\n\nText(0.5, 1.02, 'span$(v_1,v_2)$')\n\n\n\n\n\n점 하나는 벡터 하나를 나타낸다. 1번째 그림에서 벡터들의 생성(span)은 2차원 벡터공간의 부분공간이자 0차원 벡터공간(점)이며 2,3번째 그림에서 벡터들의 생성(span)은 2차원 벡터공간의 부분공간이자 1차원 벡터공간(직선)이다. 이와는 다르게 3번째 그림에서의 벡터들의 생성(span)은 3차원 벡터공간 그 자체인데 그 이유는 3차원 벡터공간의 모든 점을 표현할 수 있기 때문이다.\n\n\ncolumn Space\n열공간(columns space)은 행렬의 열벡터들의 span 즉, 행렬의 열벡터들로 가능한 모든 선형조합(벡터)의 집합입니다. \\(v_1,v_2,\\dots,v_n\\)이 행렬A의 열벡터들이라고 할 때, 열공간은 다음과 같습니다. \\[\\text{C}(A) = \\text{span}(v_1,v_2,\\dots,v_n) = \\{w_1{\\bf v_1} + w_2{\\bf v_2} + \\dots + w_n{\\bf v_n}:w_1,w_2,\\dots,w_n \\in K\\}\\] 열공간은 방정식 \\(Ax = b\\)의 해의 갯수를 파악하는데 쓰입니다.\n\n\n참고자료\n[선형대수] 벡터공간(vector space), 벡터 부분공간(vector subspace), 생성공간(span), 차원(dimension) 위키피디아-Linear span 위키피디아-Row and columns spaces 혁펜하임 - [선대] 2-6강. span 과 column space (열공간) 직관적 설명"
  },
  {
    "objectID": "posts/Linear Algebra/행렬곱에 대한 여러가지 시각.html",
    "href": "posts/Linear Algebra/행렬곱에 대한 여러가지 시각.html",
    "title": "행렬곱에 대한 여러가지 관점",
    "section": "",
    "text": "유튜브 - 혁펜하임님의 선형대수학 강의 정리용 입니다.\n\n행렬곱은 내적이다.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},B \\in \\mathbb{R}^{n \\times p} \\\\\n&AB =\n\\begin{bmatrix}\na_1^T\\\\\na_2^T\\\\\n\\vdots\\\\\na_m^T\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 & b_2 & \\dots & b_p\n\\end{bmatrix}=\n\\begin{bmatrix}\na_1^Tb_1 & a_1^Tb_2 & \\dots & a_1^Tb_p \\\\\na_2^Tb_1 & a_2^Tb_2 & \\dots & a_2^Tb_p \\\\\n\\vdots & \\vdots  & \\vdots & \\vdots \\\\\na_{m}^Tb_1 & a_m^Tb_2 & \\dots & a_m^Tb_p \\\\\n\\end{bmatrix}\n\\end{aligned}\\]\n\n\n행렬곱은 rank-1 matrix의 합이다.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},B \\in \\mathbb{R}^{n \\times p} \\\\\n&AB =\n\\begin{bmatrix}\na_1 & a_2 & \\dots & a_n\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1^T \\\\\nb_2^T \\\\\n\\vdots \\\\\nb_n^T\n\\end{bmatrix}\n= a_1b_1^T + a_2b_2^T + \\dots + a_nb_n^T\n\\end{aligned}\\]\n\n\n행렬과 벡터의 곱은 열공간에 속한 임의의 벡터이다.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},x \\in \\mathbb{R}^{n \\times 1} \\\\\n&Ax =\n\\begin{bmatrix}\na_1 & a_2 & \\dots & a_n\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n= a_1x_1 + a_2x_2 + \\dots + a_nx_n\n\\end{aligned}\\]\n\\(a_1,a_2,\\dots,a_n\\)은 벡터 \\(x_1,x_2,\\dots,x_n\\)은 스칼라이다. 행렬곱은 위와같이 열공간의 기저(행렬\\(A\\)의 열벡터)와 스칼라(미지수벡터\\(x\\)의 원소)와의 일차결합이므로 기저인 열벡터가 생성(span)하는 열공간의 원소이다. 이는 방정식 \\(Ax=b\\)의 해의 갯수를 알아내는데에 사용하는 중요한 개념이다.(참고 : 방정식 Ax = b의 해의 갯수 알아내기)\n열공간(column space) : 행렬에서 (열)벡터의 일차결합으로 생성되는 벡터공간. 열벡터의 span\n\n\n행벡터와 행렬의 곱은 row space(행공간)에 속한 임의의 벡터이다.\n\\[\\begin{aligned}\n&\\text{Let }x \\in \\mathbb{R}^{1 \\times n},X \\in \\mathbb{R}^{n \\times p} \\\\\n&xA =\n\\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\n\\vdots \\\\\na_n\n\\end{bmatrix}\n= x_1a_1^T + x_2a_2^T + \\dots + x_na_n^T\n\\end{aligned}\\]\n\\(a_1^T,a_2^T,\\dots,a_n^T\\)은 벡터 \\(x_1,x_2,\\dots,x_n\\)은 스칼라이다. 행렬곱은 위와같이 행공간의 기저(행렬\\(A\\)의 행벡터)와 스칼라(\\(x\\)의 원소)와의 일차결합으로 기저인 행벡터가 생성하는 행공간의 원소이다.\n행공간 : 행렬에서 행벡터의 일차결합으로 생성되는 벡터공간. 행벡터의 span\n\n\n참고문헌\n혁펜하임 - 선대 2-5강. 행렬의 곱셈과 네 가지 관점 (열공간 (column space) 등)"
  },
  {
    "objectID": "posts/numpy/np.meshgrid.html",
    "href": "posts/numpy/np.meshgrid.html",
    "title": "np.meshgrid",
    "section": "",
    "text": "np.meshgrid"
  },
  {
    "objectID": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html",
    "href": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html",
    "title": "Finite Difference Method with np.gradient",
    "section": "",
    "text": "\\({\\bf x} =\\begin{bmatrix}x_1&x_2&\\dots&x_m\\end{bmatrix}^T\\)일 때, \\(x\\)에 대한 다변수함수 \\(f({\\bf x})\\)의 gradient는 다음과 같다.\n\\[\\begin{aligned}\n&\\text{gradient of }f({\\bf{x}}) = \\frac{\\partial f}{\\partial {\\bf x}} = \\nabla f(\\bf{x}) =\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_m}\n\\end{pmatrix}\n\\end{aligned}\\]\n함수가 가지는 모든 변수에 대해서 편미분 한 뒤 모아놓은 벡터라고 생각하면 된다. 함수의 수식을 알고 미분이 가능하면 우리는 해석적으로 미분해서(미분공식써서) 그레디언트를 구하고 각각의 어떤 point에서의 편미분계수들도 구할 수 있다. 그러나 우리가 주어진 데이터는 함수f의 함숫값들이 주어진다. 예를 들면 다음과 같다.\n\nimport numpy as np\nf = np.array([1,2,4,7,11,17],dtype = float)\nprint(\"f(x)\")\nprint(f)\n\nf(x)\n[ 1.  2.  4.  7. 11. 17.]\n\n\n위와 같은 함숫값들만 주어질때에는 원래의 함수를 알기는 불가능하다. 따라서 도함수를 통한 정확한 미분계수를 구하기가 불가능하므로 주어진 데이터로 \\(\\bf x\\)에서의 미분계수의 값을 근사적으로 구할 수 있는데 이를 수치미분이라 한다."
  },
  {
    "objectID": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#차원-배열의-경우",
    "href": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#차원-배열의-경우",
    "title": "Finite Difference Method with np.gradient",
    "section": "1차원 배열의 경우",
    "text": "1차원 배열의 경우\n\nEx) \\(dx\\) = 1\n넘파이 1차원 배열이 다음과 같이 주어져 있다고 하자.\n\nfrom IPython.display import display, Markdown\nf = np.array([1,2,4,7,11,16],dtype=float)\nprint(f)\n\n[ 1.  2.  4.  7. 11. 16.]\n\n\nnp.gradient함수는 1차원 배열의 내부에 있는 각각의 값들은 \\(x\\)값이 거리가 \\(dx\\)=1씩 변화할때마다의 함숫값\\(f(x)\\)들로 이해한다. 즉,다음과 같다.\n\nfor i in range(len(f)):\n    display(Markdown(rf'$x_{i+1}$에서의 함숫값 $f(x_{i+1})$ = {f[i]}'))\n\n\\(x_1\\)에서의 함숫값 \\(f(x_1)\\) = 1.0\n\n\n\\(x_2\\)에서의 함숫값 \\(f(x_2)\\) = 2.0\n\n\n\\(x_3\\)에서의 함숫값 \\(f(x_3)\\) = 4.0\n\n\n\\(x_4\\)에서의 함숫값 \\(f(x_4)\\) = 7.0\n\n\n\\(x_5\\)에서의 함숫값 \\(f(x_5)\\) = 11.0\n\n\n\\(x_6\\)에서의 함숫값 \\(f(x_6)\\) = 16.0\n\n\n위에서 넘파이의 그래디언트는 끝값을 제외한 내부의 요소에는 중앙차분근사 양끝값에 대해서는 후향차분 또는 전향차분을 사용한다고 언급했었다. 계산한,\\(x_2,x_5\\)에서 미분계수의 2차중앙차분근사는 다음과 같다.\n\\[\\begin{aligned}\n&f^{'}(x_2) \\overset{\\sim}{=} \\frac{f(x_3)-f(x_1)}{2h} = \\frac{4-1}{2} = 1.5 \\\\\n&f^{'}(x_5) \\overset{\\sim}{=} \\frac{f(x_6)-f(x_4)}{2h} = \\frac{16-7}{2} = 4.5 \\\\\n&\\text{where, } h = x_3-x_2 = x_2-x_1 = 1\n\\end{aligned}\\]\n1차원 배열의 가장 처음에 오는 값에 전향차분근사를 사용하고 가장 마지막에 오는 값에서는 후향차분근사를 사용한다.\n\\[\\begin{aligned}\n&f^{'}(x_1) = \\frac{f(x_2) - f(x_1)}{h} = \\frac{2-1}{1} = 1 \\\\\n&f^{'}(x_6) = \\frac{f(x_6) - f(x_5)}{h} = \\frac{16-11}{1} = 5\n\\end{aligned}\\]\n계산한 값과 실제로 일치하는지 확인.\n\nnum_diff = np.gradient(f)\nprint(\"np.gradient의 출력값\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$에서의 도함수의 근삿값 $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradient의 출력값\n[1.  1.5 2.5 3.5 4.5 5. ]\n\n\n\\(x_1\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 1.0\n\n\n\\(x_2\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 1.5\n\n\n\\(x_3\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 2.5\n\n\n\\(x_4\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 3.5\n\n\n\\(x_5\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 4.5\n\n\n\\(x_6\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 5.0\n\n\n\n\nEx) \\(dx \\not = 1\\) (default가 아닐 경우)\n거리\\(dx=2\\)일때 계산한,\\(x_2,x_5\\)에서 미분계수의 2차중앙차분근사는 다음과 같다.\n\\[\\begin{aligned}\n&f^{'}(x_2) \\overset{\\sim}{=} \\frac{f(x_3)-f(x_1)}{2h} = \\frac{4-1}{4} = 0.75 \\\\\n&f^{'}(x_5) \\overset{\\sim}{=} \\frac{f(x_6)-f(x_4)}{2h} = \\frac{16-7}{4} = 2.25 \\\\\n&\\text{where, } h = x_3-x_1 = x_6-x_4 = 2\n\\end{aligned}\\]\n거리가 \\(dx=2\\)일때 배열의 양 끝값에서 전향,후향차분근사를 통한 미분계수의 값은 다음과 같다.\n\\[\\begin{aligned}\n&f^{'}(x_1) = \\frac{f(x_2) - f(x_1)}{h} = \\frac{2-1}{2} = 0.5 \\\\\n&f^{'}(x_6) = \\frac{f(x_6) - f(x_5)}{h} = \\frac{16-11}{2} = 2.5\n\\end{aligned}\\]\nx값 사이의 거리\\(dx\\)를 바꾸고 싶다면? => 두번째 인수에 스칼라 대입하면 된다.\n\ndx = 2\nnum_diff = np.gradient(f,dx)\nprint(\"np.gradient의 출력값\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$에서의 도함수의 근삿값 $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradient의 출력값\n[0.5  0.75 1.25 1.75 2.25 2.5 ]\n\n\n\\(x_1\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 0.5\n\n\n\\(x_2\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 0.75\n\n\n\\(x_3\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 1.25\n\n\n\\(x_4\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 1.75\n\n\n\\(x_5\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 2.25\n\n\n\\(x_6\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 2.5\n\n\n\n\nEx) x값의 좌표를 직접 정해주는 경우\n이전에는 각각의 인덱스간의 거리는 모두 동일하게 기본값 1이거나 다른값을 사용했다. 그러지 않고 \\(x_1,x_2,\\dots,x_6\\)의 좌표를 직접 지정해주는 것도 가능하다. 함수의 2번재 인수에 좌표를 직접 넣어주면 된다.\n먼저 x값의 좌표를 다음과 같다고 해보자.\n\nx = np.array([0., 1., 1.5, 3.5, 4., 6.], dtype=float)\nf = np.array([1,2,4,7,11,16],dtype=float)\nprint(\"각각의 좌표와 함숫값\")\nfor i in range(len(x)):\n    display(Markdown(rf'$x_{i+1}$ = {x[i]}, $f(x_{i+1})$ = {f[i]}'))\n\n각각의 좌표와 함숫값\n\n\n\\(x_1\\) = 0.0, \\(f(x_1)\\) = 1.0\n\n\n\\(x_2\\) = 1.0, \\(f(x_2)\\) = 2.0\n\n\n\\(x_3\\) = 1.5, \\(f(x_3)\\) = 4.0\n\n\n\\(x_4\\) = 3.5, \\(f(x_4)\\) = 7.0\n\n\n\\(x_5\\) = 4.0, \\(f(x_5)\\) = 11.0\n\n\n\\(x_6\\) = 6.0, \\(f(x_6)\\) = 16.0\n\n\n각각의 좌표에서 도함수의 근삿값을 구하면 아래와 같다.(수식 계산은 잘 모르겠네요 … 추후에 더 공부하겠습니다!)\n\nnum_diff = np.gradient(f,x)\nprint(\"np.gradient의 출력값\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$에서의 도함수의 근삿값 $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradient의 출력값\n[1.  3.  3.5 6.7 6.9 2.5]\n\n\n\\(x_1\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 1.0\n\n\n\\(x_2\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 2.9999999999999996\n\n\n\\(x_3\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 3.5\n\n\n\\(x_4\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 6.700000000000001\n\n\n\\(x_5\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 6.899999999999999\n\n\n\\(x_6\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 2.5"
  },
  {
    "objectID": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#차원-배열의-경우-1",
    "href": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#차원-배열의-경우-1",
    "title": "Finite Difference Method with np.gradient",
    "section": "2차원 배열의 경우",
    "text": "2차원 배열의 경우\n2차원 배열의 경우 axis=0(세로축)과 axis=1(가로축) 두 축방향으로 계산한 도함수의 근삿값을 반환한다.axis=0일 경우 각각의 열마다 따로따로 독립적으로 \\(x_1,x_2...\\)에 대한 함숫값\\(f(x_1),f(x_2),\\dots\\)이 있다고 생각하면 되고 axis=1일 경우 각각의 행마다 따로따로 독립적으로 \\(x_1,x_2...\\)에 대한 함숫값\\(f(x_1),f(x_2),\\dots\\)이 있다고 생각하면 된다.또한 1차원 배열과 유사하게 각각의 행,열의 끝값에는 전향or후향차분근사를 행,열의 내부에 있는 값은 중앙차분근사를 사용한다.\n\nEx) \\(dx=1,dy=1\\)\n2차원 배열은 다음과 같다.\n\nnp.array([[1, 2, 6], [3, 4, 5]], dtype=float)\n\narray([[1., 2., 6.],\n       [3., 4., 5.]])\n\n\n\nax0_difcoef,ax1_difcoef= np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float))\nprint(f'axis = 0 방향으로 도함수의 근삿값 계산 \\n{ax0_difcoef}')\nprint(f'axis = 1 방향으로 도함수의 근삿값 계산 \\n{ax1_difcoef}')\n\naxis = 0 방향으로 도함수의 근삿값 계산 \n[[ 2.  2. -1.]\n [ 2.  2. -1.]]\naxis = 1 방향으로 도함수의 근삿값 계산 \n[[1.  2.5 4. ]\n [1.  1.  1. ]]\n\n\n\n\nEx) \\(dx \\not = 1,dy \\not = 1\\) (default가 아닌 경우)\n각각의 행,열마다 거리를 따로 설정해주고 싶은 경우? => 스칼라 2개 인수로 전달.\n\ndx = 2;dy = 2\nax0_difcoef,ax1_difcoef= np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float),dx,dy)\nprint(f'axis = 0 방향으로 도함수의 근삿값 계산 \\n{ax0_difcoef}')\nprint(f'axis = 1 방향으로 도함수의 근삿값 계산 \\n{ax1_difcoef}')\n\naxis = 0 방향으로 도함수의 근삿값 계산 \n[[ 1.   1.  -0.5]\n [ 1.   1.  -0.5]]\naxis = 1 방향으로 도함수의 근삿값 계산 \n[[0.5  1.25 2.  ]\n [0.5  0.5  0.5 ]]"
  },
  {
    "objectID": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#전향차분근사-유도",
    "href": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#전향차분근사-유도",
    "title": "Finite Difference Method with np.gradient",
    "section": "전향차분근사 유도",
    "text": "전향차분근사 유도\n\\(x_1,x_2,\\dots,x_{i-1},x_i,x_{i+1},\\dots,x_n\\)과 각각에 대응하는 함숫값 \\(f(x_1),f(x_2),\\dots,f(x_{i-1}),f(x_i),f(x_{i+1}),\\dots,f(x_n)\\) 주어진 데이터라고 가정하자. 목적은 x_i에서의 미분계수를 구하는 것이다. \\(a = x_i\\)에서 함수\\(f(x)\\)의 테일러 급수 근사는 다음과 같다. \\[f(x) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x-x_i) + \\frac{f^{''}(x_i)}{2!}(x-x_i)^2 + \\dots \\]\n\\(x=x_{i+1}\\)에서의 함숫값은 다음과 같다. \\[f(x_{i+1}) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x_{i+1}-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x_{i+1}-x_i) + \\frac{f^{''}(x_i)}{2!}(x_{i+1}-x_i)^2 + \\dots \\]\n\\(f'(x_i)\\)가 포함된항만 남겨두고 나머지는 이항하면 다음과 같다. \\[f^{'}(x_i)(x_{i+1}-x_i) = f(x_{i+1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}(x_{i+1}-x_i)^2 + \\dots\\]\n\\(h = x_{i+1}-x_i\\)로 두고 양변을 h로 나누면 다음과 같다. \\[f'(x_i) = \\frac{f(x_{i+1})}{h} - \\frac{f(x_i)}{h} - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!}\\]\n여기서 우변의 두개의 항만 남겨두고 \\(O(h) = - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\)라 하면 다음과 같다. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i+1})-f(x_i)}{h}\\\\\n&O(h) = - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\\\\n&\\text{where, } h = x_{i+1} - x_i \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#후향차분근사-유도",
    "href": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#후향차분근사-유도",
    "title": "Finite Difference Method with np.gradient",
    "section": "후향차분근사 유도",
    "text": "후향차분근사 유도\n\\(x_1,x_2,\\dots,x_{i-1},x_i,x_{i+1},\\dots,x_n\\)과 각각에 대응하는 함숫값 \\(f(x_1),f(x_2),\\dots,f(x_{i-1}),f(x_i),f(x_{i+1}),\\dots,f(x_n)\\) 주어진 데이터라고 가정하자. 목적은 x_i에서의 미분계수를 구하는 것이다. \\(a = x_i\\)에서 함수\\(f(x)\\)의 테일러 급수 근사는 다음과 같다. \\[f(x) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x-x_i) + \\frac{f^{''}(x_i)}{2!}(x-x_i)^2 + \\dots \\]\n\\(f(x_i)\\)는 다음과 같다. \\[f(x_{i-1}) = \\sum_{n=0}^{\\infty}\\frac{f^n(x_i)}{n!}(x_{i-1}-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x_{i-1}-x_i) + \\frac{f^{''}(x_i)}{2!}(x_{i-1}-x_i)^2+\\dots \\]\n1차미분이 포함된 항만 남기고 나머지는 이항하면 다음과 같다. \\[f^{'}(x_i)(x_{i-1}-x_i) = f(x_{i-1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}(x_{i-1}-x_i)^2-\\frac{f^{'''}(x_i)}{3!}(x_{i-1}-x_i)^3-\\dots \\]\n\\(h = x_{i} - x_{i-1}\\)로 놓으면 다음과 같다. \\[f^{'}(x_i)(-h) = f(x_{i-1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}h^2+\\frac{f^{'''}(x_i)}{3!}h^3-\\dots \\]\n양변을 \\(-h\\)로 나누면 다음과 같다.\n\\[\\begin{aligned}\nf^{'}(x_i) &= \\frac{f(x_{i-1})}{-h} + \\frac{f(x_i)}{h} + \\frac{f^{''}(x_i)}{2!}h-\\frac{f^{'''}(x_i)}{3!}h^2+\\dots \\\\\n&=\\frac{f(x_i)-f(x_{i-1}) }{h} +  \\frac{f^{''}(x_i)}{2!}h-\\frac{f^{'''}(x_i)}{3!}h^2+\\dots\n\\end{aligned}\\]\n마찬가지로 우변의 두개 항만 남겨두고 \\(O(h) = \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} + \\dots\\)라 하면 다음과 같다. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i})-f(x_{i-1})}{h}\\\\\n&O(h) = \\frac{h}{2!}f^{''}(x_i) - \\frac{h^2}{3!}f{'''}(x_i)+\\dots \\\\\n&\\text{where, } h = x_{i} - x_{i-1}, \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#중앙차분근사-유도",
    "href": "posts/numpy/수치미분 & np.gradient/수치미분 & np.gradient.html#중앙차분근사-유도",
    "title": "Finite Difference Method with np.gradient",
    "section": "중앙차분근사 유도",
    "text": "중앙차분근사 유도\n전향차분근사와 후향차분근사의 유도과정에서의 테일러 전개식은 다음과 같다. \\[f'(x_i) = \\frac{f(x_{i+1})}{h} - \\frac{f(x_i)}{h} - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - ...\\] \\[f'(x_i) = \\frac{f(x_{i})}{h} - \\frac{f(x_{i-1})}{h} + \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} + ...\\]\n두 식을 더해주면 다음과 같다.\n\\[\\begin{aligned}\n&2f^{'}(x_i) = \\frac{f(x_{i+1})-f(x_{i-1})}{h} - \\frac{2h^2f^{'''}(x_i)}{3!} \\\\\n&\\Leftrightarrow f^{'}(x_i) = \\frac{f(x_{i+1})-f(x_{i-1})}{2h} - \\frac{h^2f^{'''}(x_i)}{3!} - ...\n\\end{aligned}\\]\n마찬가지로 우변의 두개 항만 남겨두고 절단오차\\(O(h^2) = -\\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\)라 하면 다음과 같다. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i+1})-f(x_{i-1})}{2h}\\\\\n&O(h) = - \\frac{h^2}{3!}f{'''}(x_i)+\\dots \\\\\n&\\text{where, } h = x_{i} - x_{i-1}\\,\\,\\text{or}\\,\\, h = x_{i+1} - x_i\\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html",
    "href": "posts/open/Dakon competetion chisquare test.html",
    "title": "Untitled",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import chisquare\nimport warnings\nwarnings.filterwarnings('ignore')\ntest_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/test.csv\"\ntrain_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/train.csv\"\n\n\nclass CFG:\n    SEED = 42\n\n\ntrain = pd.read_csv(train_path)\ntrain_len = len(train)\ntest = pd.read_csv(test_path)\nid_test = test[\"id\"]\ntest = pd.read_csv(test_path)\n\n\ndataset = pd.concat([train,test],axis=0).reset_index(drop=True)\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      id\n      father\n      mother\n      gender\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      ...\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      0\n      0\n      0\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      ...\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      0\n      0\n      0\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      ...\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      0\n      0\n      0\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      ...\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      TRAIN_003\n      0\n      0\n      0\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      ...\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      TRAIN_004\n      0\n      0\n      0\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      ...\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      0\n      0\n      0\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      ...\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      0\n      0\n      0\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      ...\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      0\n      0\n      0\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      ...\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      0\n      0\n      0\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      ...\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      0\n      0\n      0\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      ...\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 21 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#fathermothergender",
    "href": "posts/open/Dakon competetion chisquare test.html#fathermothergender",
    "title": "Untitled",
    "section": "father,mother,gender",
    "text": "father,mother,gender\n\nfather,mother,gender = 0 => drop\n\n\ndataset = dataset.drop(columns = [\"father\",\"mother\",\"gender\"])\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      id\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      TRAIN_003\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      TRAIN_004\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 18 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_01",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_01",
    "title": "Untitled",
    "section": "SNP_01",
    "text": "SNP_01\n\nclass = B 인 경우, AA는 아예 없음 = > feature extraction hasGG 추가\n\n추후 고려사항 - class = A 인 경우, AA가 좀 높음 - class = B 인 경우, GG가 압도적으로 높음 - class = C 인 경우, GG가 좀 높음\n\ncol = \"SNP_01\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_02",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_02",
    "title": "Untitled",
    "section": "SNP_02",
    "text": "SNP_02\n\nclass = A인 경우, AA는 없음 => f.e has02AA\n\n추후 고려 - B,의 경우 AG->GG-AA 순 - C,의 경우 AG->GG->AA 순\n\ncol = \"SNP_02\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_03",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_03",
    "title": "Untitled",
    "section": "SNP_03",
    "text": "SNP_03\n\nA인 경우 , 무조건 AA만 존재 => feature extraction\n\n추후 고려 나머지는 뭐 대충 고르게\n\ncol = \"SNP_03\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    plt.ylim(20,60)\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_04",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_04",
    "title": "Untitled",
    "section": "SNP_04",
    "text": "SNP_04\n\nC인 경우 , GG는 없음 => feature extraction has04GG\n\n추후 고려 나머지는 뭐 대충 고르게 - class A인 경우,AA가 압도적으로 낮음 => feature extraction\n\ncol = \"SNP_04\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_05",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_05",
    "title": "Untitled",
    "section": "SNP_05",
    "text": "SNP_05\n\nclass = A인 경우, CC는 없음\n\n\ncol = \"SNP_05\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_06",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_06",
    "title": "Untitled",
    "section": "SNP_06",
    "text": "SNP_06\n\nclass = A인 경우, AA는 없음 => feature extraction has06AA\n\n\ncol = \"SNP_06\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_07",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_07",
    "title": "Untitled",
    "section": "SNP_07",
    "text": "SNP_07\n\nA의 경우 => AA는 없음 =>f.e has07AA\nB,C의 경우 => GG는 없음=>f.e has07GG\n\n\ncol = \"SNP_07\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_08",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_08",
    "title": "Untitled",
    "section": "SNP_08",
    "text": "SNP_08\n\nA의 경우 => GG는 없음 =>f.e has08GG\n\n\ncol = \"SNP_08\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_09",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_09",
    "title": "Untitled",
    "section": "SNP_09",
    "text": "SNP_09\n\nC의 경우 => 대부분이 AA이고 GA와 GG는 사실 없다고 봐도 무방=> f.e has09AA\nB의 경우 => GG는 거의 없음 => f.e has09GG\n\n앞선 경우들은 빈도수가 낮은 경우네는 따로 feature extraction을 안해줬는데 왜 여기서는 해? => 여기서는 낮은 것들의 빈도수가 1로 너무 낮음,그래서 여기는 함.\n\ncol = \"SNP_09\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset.loc[dataset[\"class\"] == \"B\",col].value_counts()\n\nA A    91\nG A    22\nG G     1\nName: SNP_09, dtype: int64\n\n\n\ndataset.loc[dataset[\"class\"] == \"C\",col].value_counts()\n\nA A    78\nG A     1\nName: SNP_09, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_10",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_10",
    "title": "Untitled",
    "section": "SNP_10",
    "text": "SNP_10\n\nA와 B에서 낮은값들이 있긴 한데 … 그래도 특이점 1인 정도는 아님\n\n\ncol = \"SNP_10\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"B\",col]\nx.value_counts()\n\nG G    110\nA G      4\nName: SNP_10, dtype: int64\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"A\",col]\nx.value_counts()\n\nA G    34\nA A    32\nG G     3\nName: SNP_10, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_11",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_11",
    "title": "Untitled",
    "section": "SNP_11",
    "text": "SNP_11\n\nA의 경우 => AA는 없음 =>f.e has11AA\n\n\ncol = \"SNP_11\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_12",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_12",
    "title": "Untitled",
    "section": "SNP_12",
    "text": "SNP_12\n\nA의 경우 => AA가 있는 관측치가 거의 없음,GG가 있는 관측치는 많음\nB,C의 경우 => GG가 있는 관측치가 거의 없음,AA가 있는 관측치는 많음\n\n=>f.e has12AA =>f.e has12GG\n\ncol = \"SNP_12\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset.loc[dataset[\"class\"] == \"A\",col].value_counts()\n\nG G    48\nG A    20\nA A     1\nName: SNP_12, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_13",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_13",
    "title": "Untitled",
    "section": "SNP_13",
    "text": "SNP_13\n\nA의 경우 => AA는 없음 => f.e has13AA\n\n\ncol = \"SNP_13\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"A\",col]\nx.value_counts()\n\nG G    66\nA G     3\nName: SNP_13, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_14",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_14",
    "title": "Untitled",
    "section": "SNP_14",
    "text": "SNP_14\n\nB의 경우 => AA만 존재 =>f.e has14AA 를 추가해서 AA인것들의 계수를 결정하도록\nC의 경우 CC가 있긴 한데 .. 엄청낮긴함(그래도 1은 아니니까 f.e는 안함)\n\n\ncol = \"SNP_14\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"C\",col]\nx.value_counts()\n\nA A    60\nC A    17\nC C     2\nName: SNP_14, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_15",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_15",
    "title": "Untitled",
    "section": "SNP_15",
    "text": "SNP_15\n\n별다른 특징 없음\n\n\ncol = \"SNP_15\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      id\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      TRAIN_003\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      TRAIN_004\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 18 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#class별-관측치의-숫자",
    "href": "posts/open/Dakon competetion chisquare test.html#class별-관측치의-숫자",
    "title": "Untitled",
    "section": "class별 관측치의 숫자",
    "text": "class별 관측치의 숫자\n\nclass imbalance? => No\n\n\nx = dataset[\"class\"].value_counts().index\ny = dataset[\"class\"].value_counts().values\nplt.bar(x,y)\n\n<BarContainer object of 3 artists>"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#chi2-test-between-abc",
    "href": "posts/open/Dakon competetion chisquare test.html#chi2-test-between-abc",
    "title": "Untitled",
    "section": "chi^2 test, between [“A”,“B”,“C”]",
    "text": "chi^2 test, between [“A”,“B”,“C”]\n\n목적 : 각각의 클래스에서 각각의 SNP변수에서 나오는 G G or A g 등등.. 의 값의 빈도수가 같은지 아니면 다른지 카이제곱검정을 통해서 파악한 후, 클래스별로 빈도수 차이있으면 그 행만 추가\n\n\n\"\"\"\n#1 각각의 독립변수(검정에서는 class)별로 SNP_01의 특정값(예를 들면 GG)가 나오는 관측치 세어보기\n\n#컬럼,클래스 지정\nsnp_name = [col_name for col_name in dataset.columns if \"SNP\" in col_name]\nclass_name = [\"A\",\"B\",\"C\"]\n\nsignif_col = []\nunsignif_col = []\nfor snp in snp_name:\n    _snp_unique = dataset.loc[:,snp].unique().tolist()\n    for unq_vl in _snp_unique:\n        _condition = (dataset.loc[:,snp] == unq_vl)\n        _data = dataset.loc[_condition,[snp,\"class\"]].dropna(axis=0).value_counts().droplevel(axis=0,level=0).copy()\n        _data_class = _data.index\n        for cl_name in class_name:\n            if cl_name not in _data_class:\n                #print(\"존재하지 않는 class : \",cl_name)\n                #print(\"존재하지 않는 클래스 추가후 df\")\n                _data = _data.append(pd.Series({cl_name:0}))\n\n        _data = _data[class_name]\n        #특정컬럼의 특정값에 대해서 카이제곱검정 수행\n        #예를 들어서 각각의 클래스 A,B,C에서 SNP_01의 A A값을 가지는 빈도가 같은지 다른지 수행\n        f_obs = _data.values.tolist()\n        p_value = chisquare(f_obs)[1].round(5)\n        if p_value < 0.01: #유의확률 0.01\n            #print(f\"{snp}={unq_vl}\")\n            #print(\"p_value :\",p_value)\n            name = snp+\"=\"+unq_vl\n            signif_col.append(name)\n        else:\n            name = snp+\"=\"+unq_vl\n            unsignif_col.append(name)\nlen(signif_col),len(unsignif_col)\n\"\"\"\n\n'\\n#1 각각의 독립변수(검정에서는 class)별로 SNP_01의 특정값(예를 들면 GG)가 나오는 관측치 세어보기\\n\\n#컬럼,클래스 지정\\nsnp_name = [col_name for col_name in dataset.columns if \"SNP\" in col_name]\\nclass_name = [\"A\",\"B\",\"C\"]\\n\\nsignif_col = []\\nunsignif_col = []\\nfor snp in snp_name:\\n    _snp_unique = dataset.loc[:,snp].unique().tolist()\\n    for unq_vl in _snp_unique:\\n        _condition = (dataset.loc[:,snp] == unq_vl)\\n        _data = dataset.loc[_condition,[snp,\"class\"]].dropna(axis=0).value_counts().droplevel(axis=0,level=0).copy()\\n        _data_class = _data.index\\n        for cl_name in class_name:\\n            if cl_name not in _data_class:\\n                #print(\"존재하지 않는 class : \",cl_name)\\n                #print(\"존재하지 않는 클래스 추가후 df\")\\n                _data = _data.append(pd.Series({cl_name:0}))\\n\\n        _data = _data[class_name]\\n        #특정컬럼의 특정값에 대해서 카이제곱검정 수행\\n        #예를 들어서 각각의 클래스 A,B,C에서 SNP_01의 A A값을 가지는 빈도가 같은지 다른지 수행\\n        f_obs = _data.values.tolist()\\n        p_value = chisquare(f_obs)[1].round(5)\\n        if p_value < 0.01: #유의확률 0.01\\n            #print(f\"{snp}={unq_vl}\")\\n            #print(\"p_value :\",p_value)\\n            name = snp+\"=\"+unq_vl\\n            signif_col.append(name)\\n        else:\\n            name = snp+\"=\"+unq_vl\\n            unsignif_col.append(name)\\nlen(signif_col),len(unsignif_col)\\n'"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#chi2-test-between-bc",
    "href": "posts/open/Dakon competetion chisquare test.html#chi2-test-between-bc",
    "title": "Untitled",
    "section": "chi^2 test, between “B”,“C”",
    "text": "chi^2 test, between “B”,“C”\n\n목적 : 각각의 클래스에서 각각의 SNP변수에서 나오는 G G or A g 등등.. 의 값의 빈도수가 같은지 아니면 다른지 카이제곱검정을 통해서 파악한 후, 클래스별로 빈도수 차이있으면 그 행만 추가\n\n\n# EDA과정에서 trait == 1 이면 반드시 A였음,따라서 trait 변수는 제거하고 나중에 trait == 1이면 반드시 1로 제출\nidx = dataset[dataset.trait == 1].index\n_dt = dataset.drop(index=idx)\n\n\n_dt\n\n\n\n\n\n  \n    \n      \n      id\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      4\n      TRAIN_004\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      5\n      TRAIN_005\n      2\n      G G\n      G G\n      C A\n      A A\n      C C\n      A A\n      A A\n      G A\n      A A\n      G G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n317 rows × 18 columns\n\n\n\n\n#1 각각의 독립변수(검정에서는 class)별로 SNP_01의 특정값(예를 들면 GG)가 나오는 관측치 세어보기\n\n#컬럼,클래스 지정\nsnp_name = [col_name for col_name in _dt.columns if \"SNP\" in col_name]\nclass_name = [\"B\",\"C\"]\n\nsignif_col = []\nunsignif_col = []\nfor snp in snp_name:\n    _snp_unique = _dt.loc[:,snp].unique().tolist()\n    for unq_vl in _snp_unique:\n        _condition = (_dt.loc[:,snp] == unq_vl)\n        _data = _dt.loc[_condition,[snp,\"class\"]].dropna(axis=0).value_counts().droplevel(axis=0,level=0).copy()\n        _data_class = _data.index\n        for cl_name in class_name:\n            if cl_name not in _data_class:\n                #print(\"존재하지 않는 class : \",cl_name)\n                #print(\"존재하지 않는 클래스 추가후 df\")\n                _data = _data.append(pd.Series({cl_name:0}))\n\n        _data = _data[class_name]\n        #특정컬럼의 특정값에 대해서 카이제곱검정 수행\n        #예를 들어서 각각의 클래스 A,B,C에서 SNP_01의 A A값을 가지는 빈도가 같은지 다른지 수행\n        f_obs = _data.values.tolist()\n        p_value = chisquare(f_obs)[1].round(5)\n        if p_value < 0.05: #유의확률 0.01\n            #print(f\"{snp}={unq_vl}\")\n            #print(\"p_value :\",p_value)\n            name = snp+\"_\"+unq_vl\n            signif_col.append(name)\n        else:\n            name = snp+\"_\"+unq_vl\n            unsignif_col.append(name)\nlen(signif_col),len(unsignif_col)\n\n(27, 17)\n\n\n\nsignif_col\n\n['SNP_01_G G',\n 'SNP_01_A A',\n 'SNP_02_A G',\n 'SNP_02_G G',\n 'SNP_02_A A',\n 'SNP_03_C A',\n 'SNP_03_C C',\n 'SNP_04_G A',\n 'SNP_04_A A',\n 'SNP_04_G G',\n 'SNP_05_A A',\n 'SNP_05_C C',\n 'SNP_06_A G',\n 'SNP_07_A A',\n 'SNP_07_G A',\n 'SNP_08_G G',\n 'SNP_08_A A',\n 'SNP_09_G A',\n 'SNP_10_G G',\n 'SNP_10_A G',\n 'SNP_10_A A',\n 'SNP_11_A G',\n 'SNP_11_G G',\n 'SNP_13_A A',\n 'SNP_14_A A',\n 'SNP_14_C A',\n 'SNP_15_A A']\n\n\n\nunsignif_col\n\n['SNP_01_A G',\n 'SNP_03_A A',\n 'SNP_05_C A',\n 'SNP_06_A A',\n 'SNP_06_G G',\n 'SNP_08_G A',\n 'SNP_09_A A',\n 'SNP_09_G G',\n 'SNP_11_A A',\n 'SNP_12_A A',\n 'SNP_12_G A',\n 'SNP_12_G G',\n 'SNP_13_G G',\n 'SNP_13_A G',\n 'SNP_14_C C',\n 'SNP_15_G A',\n 'SNP_15_G G']\n\n\n\n\"\"\"\nimport scipy.stats as stats\nimport numpy as np\n  \n# Make a 3 x 3 table\ndataset = np.array([[13, 17, 11], [4, 6, 9],\n                    [20, 31, 42]])\n  \n# Finding Chi-squared test statistic,\n# sample size, and minimum of rows\n# and columns\nX2 = stats.chi2_contingency(dataset, correction=False)\nN = np.sum(dataset)\nminimum_dimension = min(dataset.shape)-1\nX2\n\"\"\"\n\n'\\nimport scipy.stats as stats\\nimport numpy as np\\n  \\n# Make a 3 x 3 table\\ndataset = np.array([[13, 17, 11], [4, 6, 9],\\n                    [20, 31, 42]])\\n  \\n# Finding Chi-squared test statistic,\\n# sample size, and minimum of rows\\n# and columns\\nX2 = stats.chi2_contingency(dataset, correction=False)\\nN = np.sum(dataset)\\nminimum_dimension = min(dataset.shape)-1\\nX2\\n'"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#add-column-delete-column",
    "href": "posts/open/Dakon competetion chisquare test.html#add-column-delete-column",
    "title": "Untitled",
    "section": "add column & delete column",
    "text": "add column & delete column\n\ndef create_col(dataset,col,value):\n    _t = []\n    for val in dataset[col] == value:\n        if val == True:\n            _t.append(1)\n        else:\n            _t.append(0)\n    \n    col_name_base = \"has\"+col[-2:]\n    value_name = \"\"\n    for chr in value:\n        if chr != \" \":\n            value_name+=chr\n    col_name = col_name_base+value_name\n    print(col_name)\n    dataset[col_name] = _t\n\n    return dataset\n\n\"\"\"\ndataset = create_col(dataset,\"SNP_01\",\"G G\")\ndataset = create_col(dataset,\"SNP_02\",\"A A\")\ndataset = create_col(dataset,\"SNP_03\",\"A A\")\ndataset = create_col(dataset,\"SNP_04\",\"G G\")\ndataset = create_col(dataset,\"SNP_05\",\"C C\")\ndataset = create_col(dataset,\"SNP_06\",\"A A\")\ndataset = create_col(dataset,\"SNP_07\",\"A A\")\ndataset = create_col(dataset,\"SNP_07\",\"G G\")\ndataset = create_col(dataset,\"SNP_08\",\"G G\")\ndataset = create_col(dataset,\"SNP_09\",\"A A\")\ndataset = create_col(dataset,\"SNP_09\",\"G G\")\ndataset = create_col(dataset,\"SNP_11\",\"A A\")\ndataset = create_col(dataset,\"SNP_12\",\"A A\")\ndataset = create_col(dataset,\"SNP_12\",\"G G\")\ndataset = create_col(dataset,\"SNP_13\",\"A A\")\ndataset = create_col(dataset,\"SNP_14\",\"A A\")\n\"\"\"\n#dataset = dataset.drop(columns = [\"SNP_03\",\"SNP_04\",\"SNP_05\",\"SNP_06\",\"SNP_07\",\"SNP_08\",\"SNP_09\",\"SNP_11\",\"SNP_12\",\"SNP_13\",\"SNP_14\"])\n\n'\\ndataset = create_col(dataset,\"SNP_01\",\"G G\")\\ndataset = create_col(dataset,\"SNP_02\",\"A A\")\\ndataset = create_col(dataset,\"SNP_03\",\"A A\")\\ndataset = create_col(dataset,\"SNP_04\",\"G G\")\\ndataset = create_col(dataset,\"SNP_05\",\"C C\")\\ndataset = create_col(dataset,\"SNP_06\",\"A A\")\\ndataset = create_col(dataset,\"SNP_07\",\"A A\")\\ndataset = create_col(dataset,\"SNP_07\",\"G G\")\\ndataset = create_col(dataset,\"SNP_08\",\"G G\")\\ndataset = create_col(dataset,\"SNP_09\",\"A A\")\\ndataset = create_col(dataset,\"SNP_09\",\"G G\")\\ndataset = create_col(dataset,\"SNP_11\",\"A A\")\\ndataset = create_col(dataset,\"SNP_12\",\"A A\")\\ndataset = create_col(dataset,\"SNP_12\",\"G G\")\\ndataset = create_col(dataset,\"SNP_13\",\"A A\")\\ndataset = create_col(dataset,\"SNP_14\",\"A A\")\\n'"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#encoding",
    "href": "posts/open/Dakon competetion chisquare test.html#encoding",
    "title": "Untitled",
    "section": "encoding",
    "text": "encoding\n\n_dataset = dataset\n_dataset\n\n\n\n\n\n  \n    \n      \n      id\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      TRAIN_003\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      TRAIN_004\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 18 columns\n\n\n\n\nonehot-encoding trait != 1 (A클래스 빼고)\n\n_dt = _dataset.loc[_dataset.trait != 1,:].copy()\ncl = _dt[\"class\"]\n#print(cl)\none_hot_label = [col_name for col_name in _dt.columns if \"SNP\" in col_name]\nget_class = [\"id\"]+signif_col + [\"trait\",\"class\"]\ntrait_map = {1:0,2:1}\n\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\n_dt = pd.get_dummies(_dt,columns = one_hot_label)\n_dt[\"class\"] = cl.map(class_map)\ncond = ~pd.isna(_dt[\"class\"])\ntrain_ohe = _dt.loc[cond,:]\ntrain_ohe = train_ohe.loc[:,get_class]\ntrain_ohe[\"trait\"] = train_ohe[\"trait\"].map(trait_map)\ntrain_ohe[\"class\"] = train_ohe[\"class\"].astype(int)\n\ntest_ohe = _dt.loc[~cond,:][get_class]\ntest_ohe = test_ohe.drop(columns = \"class\")\ntest_ohe[\"trait\"] = test_ohe[\"trait\"].map(trait_map)\n\n\ntrain_id = train_ohe[\"id\"]\nX_train_ohe = train_ohe.drop(columns = [\"class\",\"id\"])\nY_train_ohe = train_ohe[\"class\"]\n\n\nX_train_ohe\n\n\n\n\n\n  \n    \n      \n      SNP_01_G G\n      SNP_01_A A\n      SNP_02_A G\n      SNP_02_G G\n      SNP_02_A A\n      SNP_03_C A\n      SNP_03_C C\n      SNP_04_G A\n      SNP_04_A A\n      SNP_04_G G\n      ...\n      SNP_10_G G\n      SNP_10_A G\n      SNP_10_A A\n      SNP_11_A G\n      SNP_11_G G\n      SNP_13_A A\n      SNP_14_A A\n      SNP_14_C A\n      SNP_15_A A\n      trait\n    \n  \n  \n    \n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      2\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      4\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      5\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      255\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      0\n      1\n      1\n      1\n      0\n      0\n      1\n    \n    \n      256\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      257\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      258\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      261\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n  \n\n193 rows × 28 columns\n\n\n\n\n\nlabel-encoding trait != 0 (A클래스 빼고)\n\n_dt = _dataset.loc[_dataset.trait != 1,:].copy()\ncl = _dt[\"class\"]\n#print(cl)\none_hot_label = [col_name for col_name in _dt.columns if \"SNP\" in col_name]\nget_class = [\"id\"]+signif_col + [\"trait\",\"class\"]\ntrait_map = {1:0,2:1}\n\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\n_dt = pd.get_dummies(_dt,columns = one_hot_label)\n_dt[\"class\"] = cl.map(class_map)\ncond = ~pd.isna(_dt[\"class\"])\ntrain_ohe = _dt.loc[cond,:]\ntrain_ohe = train_ohe.loc[:,get_class]\ntrain_ohe[\"trait\"] = train_ohe[\"trait\"].map(trait_map)\ntrain_ohe[\"class\"] = train_ohe[\"class\"].astype(int)\n\ntest_ohe = _dt.loc[~cond,:][get_class]\ntest_ohe = test_ohe.drop(columns = \"class\")\ntest_ohe[\"trait\"] = test_ohe[\"trait\"].map(trait_map)\n\n\n\nOne-hot encoding\n\n\"\"\"\n#one-hot encoding for distance base algorithm\ndataset_ohe = pd.get_dummies(_dataset,columns = _dataset.columns.drop(\"class\"),drop_first=True) #multicollinearity를 막기위한 drop_first 옵션\ntrain_ohe = dataset_ohe[:train_len].copy()\ntest_ohe = dataset_ohe[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain_ohe[\"class\"]=train_ohe[\"class\"].map(class_map).astype(int)\nX_train_ohe = train_ohe.drop(columns = \"class\")\nY_train_ohe = train_ohe[\"class\"]\n\"\"\"\n\n'\\n#one-hot encoding for distance base algorithm\\ndataset_ohe = pd.get_dummies(_dataset,columns = _dataset.columns.drop(\"class\"),drop_first=True) #multicollinearity를 막기위한 drop_first 옵션\\ntrain_ohe = dataset_ohe[:train_len].copy()\\ntest_ohe = dataset_ohe[train_len:].copy().drop(columns=\"class\")\\n\\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\\ntrain_ohe[\"class\"]=train_ohe[\"class\"].map(class_map).astype(int)\\nX_train_ohe = train_ohe.drop(columns = \"class\")\\nY_train_ohe = train_ohe[\"class\"]\\n'\n\n\n\n\nLabel encoding\n\n\"\"\"\nle_col = []\nfor col_name in dataset.columns.tolist():\n    if \"SNP\" in col_name:\n        le_col.append(col_name)\nle_col.append(\"trait\")\nfrom sklearn import preprocessing\nfor col in le_col:\n    le = preprocessing.LabelEncoder()\n    _col = dataset[col].tolist()\n    le.fit(_col)\n    dataset[col] = le.transform(_col)\ndataset\n\"\"\"\n\n'\\nle_col = []\\nfor col_name in dataset.columns.tolist():\\n    if \"SNP\" in col_name:\\n        le_col.append(col_name)\\nle_col.append(\"trait\")\\nfrom sklearn import preprocessing\\nfor col in le_col:\\n    le = preprocessing.LabelEncoder()\\n    _col = dataset[col].tolist()\\n    le.fit(_col)\\n    dataset[col] = le.transform(_col)\\ndataset\\n'\n\n\n\n\"\"\"\ntrain = dataset[:train_len].copy()\ntest = dataset[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain[\"class\"]=train[\"class\"].map(class_map).astype(int)\nX_train = train.drop(columns = \"class\")\nY_train = train[\"class\"]\n\"\"\"\n\n'\\ntrain = dataset[:train_len].copy()\\ntest = dataset[train_len:].copy().drop(columns=\"class\")\\n\\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\\ntrain[\"class\"]=train[\"class\"].map(class_map).astype(int)\\nX_train = train.drop(columns = \"class\")\\nY_train = train[\"class\"]\\n'"
  },
  {
    "objectID": "posts/open/Dakon competetion.html",
    "href": "posts/open/Dakon competetion.html",
    "title": "Untitled",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\ntest_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/test.csv\"\ntrain_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/train.csv\"\n\n\nclass CFG:\n    SEED = 42\n\n\ntrain = pd.read_csv(train_path).drop(columns = [\"id\"])\ntrain_len = len(train)\ntest = pd.read_csv(test_path)\nid_test = test[\"id\"]\ntest = pd.read_csv(test_path).drop(columns = [\"id\"])\n\n\ndataset = pd.concat([train,test],axis=0)\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      father\n      mother\n      gender\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      0\n      0\n      0\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      0\n      0\n      0\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      0\n      0\n      0\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      0\n      0\n      0\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      170\n      0\n      0\n      0\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      171\n      0\n      0\n      0\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      172\n      0\n      0\n      0\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      173\n      0\n      0\n      0\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      174\n      0\n      0\n      0\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 20 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#fathermothergender",
    "href": "posts/open/Dakon competetion.html#fathermothergender",
    "title": "Untitled",
    "section": "father,mother,gender",
    "text": "father,mother,gender\n\nfather,mother,gender = 0 => drop\n\n\ndataset = dataset.drop(columns = [\"father\",\"mother\",\"gender\"])\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 17 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_01",
    "href": "posts/open/Dakon competetion.html#snp_01",
    "title": "Untitled",
    "section": "SNP_01",
    "text": "SNP_01\n\nclass = B 인 경우, AA는 아예 없음 = > feature extraction hasGG 추가\n\n추후 고려사항 - class = A 인 경우, AA가 좀 높음 - class = B 인 경우, GG가 압도적으로 높음 - class = C 인 경우, GG가 좀 높음\n\ncol = \"SNP_01\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_02",
    "href": "posts/open/Dakon competetion.html#snp_02",
    "title": "Untitled",
    "section": "SNP_02",
    "text": "SNP_02\n\nclass = A인 경우, AA는 없음 => f.e has02AA\n\n추후 고려 - B,의 경우 AG->GG-AA 순 - C,의 경우 AG->GG->AA 순\n\ncol = \"SNP_02\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_03",
    "href": "posts/open/Dakon competetion.html#snp_03",
    "title": "Untitled",
    "section": "SNP_03",
    "text": "SNP_03\n\nA인 경우 , 무조건 AA만 존재 => feature extraction\n\n추후 고려 나머지는 뭐 대충 고르게\n\ncol = \"SNP_03\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_04",
    "href": "posts/open/Dakon competetion.html#snp_04",
    "title": "Untitled",
    "section": "SNP_04",
    "text": "SNP_04\n\nC인 경우 , GG는 없음 => feature extraction has04GG\n\n추후 고려 나머지는 뭐 대충 고르게 - class A인 경우,AA가 압도적으로 낮음 => feature extraction\n\ncol = \"SNP_04\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_05",
    "href": "posts/open/Dakon competetion.html#snp_05",
    "title": "Untitled",
    "section": "SNP_05",
    "text": "SNP_05\n\nclass = A인 경우, CC는 없음\n\n\ncol = \"SNP_05\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_06",
    "href": "posts/open/Dakon competetion.html#snp_06",
    "title": "Untitled",
    "section": "SNP_06",
    "text": "SNP_06\n\nclass = A인 경우, AA는 없음 => feature extraction has06AA\n\n\ncol = \"SNP_06\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_07",
    "href": "posts/open/Dakon competetion.html#snp_07",
    "title": "Untitled",
    "section": "SNP_07",
    "text": "SNP_07\n\nA의 경우 => AA는 없음 =>f.e has07AA\nB,C의 경우 => GG는 없음=>f.e has07GG\n\n\ncol = \"SNP_07\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_08",
    "href": "posts/open/Dakon competetion.html#snp_08",
    "title": "Untitled",
    "section": "SNP_08",
    "text": "SNP_08\n\nA의 경우 => GG는 없음 =>f.e has08GG\n\n\ncol = \"SNP_08\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_09",
    "href": "posts/open/Dakon competetion.html#snp_09",
    "title": "Untitled",
    "section": "SNP_09",
    "text": "SNP_09\n\nC의 경우 => 대부분이 AA이고 GA와 GG는 사실 없다고 봐도 무방=> f.e has09AA\nB의 경우 => GG는 거의 없음 => f.e has09GG\n\n앞선 경우들은 빈도수가 낮은 경우네는 따로 feature extraction을 안해줬는데 왜 여기서는 해? => 여기서는 낮은 것들의 빈도수가 1로 너무 낮음,그래서 여기는 함.\n\ncol = \"SNP_09\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset.loc[dataset[\"class\"] == \"B\",col].value_counts()\n\nA A    91\nG A    22\nG G     1\nName: SNP_09, dtype: int64\n\n\n\ndataset.loc[dataset[\"class\"] == \"C\",col].value_counts()\n\nA A    78\nG A     1\nName: SNP_09, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_10",
    "href": "posts/open/Dakon competetion.html#snp_10",
    "title": "Untitled",
    "section": "SNP_10",
    "text": "SNP_10\n\nA와 B에서 낮은값들이 있긴 한데 … 그래도 특이점 1인 정도는 아님\n\n\ncol = \"SNP_10\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"B\",col]\nx.value_counts()\n\nG G    110\nA G      4\nName: SNP_10, dtype: int64\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"A\",col]\nx.value_counts()\n\nA G    34\nA A    32\nG G     3\nName: SNP_10, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_11",
    "href": "posts/open/Dakon competetion.html#snp_11",
    "title": "Untitled",
    "section": "SNP_11",
    "text": "SNP_11\n\nA의 경우 => AA는 없음 =>f.e has11AA\n\n\ncol = \"SNP_11\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_12",
    "href": "posts/open/Dakon competetion.html#snp_12",
    "title": "Untitled",
    "section": "SNP_12",
    "text": "SNP_12\n\nA의 경우 => AA가 있는 관측치가 거의 없음,GG가 있는 관측치는 많음\nB,C의 경우 => GG가 있는 관측치가 거의 없음,AA가 있는 관측치는 많음\n\n=>f.e has12AA =>f.e has12GG\n\ncol = \"SNP_12\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset.loc[dataset[\"class\"] == \"A\",col].value_counts()\n\nG G    48\nG A    20\nA A     1\nName: SNP_12, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_13",
    "href": "posts/open/Dakon competetion.html#snp_13",
    "title": "Untitled",
    "section": "SNP_13",
    "text": "SNP_13\n\nA의 경우 => AA는 없음 => f.e has13AA\n\n\ncol = \"SNP_13\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"A\",col]\nx.value_counts()\n\nG G    66\nA G     3\nName: SNP_13, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_14",
    "href": "posts/open/Dakon competetion.html#snp_14",
    "title": "Untitled",
    "section": "SNP_14",
    "text": "SNP_14\n\nB의 경우 => AA만 존재 =>f.e has14AA 를 추가해서 AA인것들의 계수를 결정하도록\nC의 경우 CC가 있긴 한데 .. 엄청낮긴함(그래도 1은 아니니까 f.e는 안함)\n\n\ncol = \"SNP_14\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"C\",col]\nx.value_counts()\n\nA A    60\nC A    17\nC C     2\nName: SNP_14, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_15",
    "href": "posts/open/Dakon competetion.html#snp_15",
    "title": "Untitled",
    "section": "SNP_15",
    "text": "SNP_15\n\n별다른 특징 없음\n\n\ncol = \"SNP_15\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#class별-관측치의-숫자",
    "href": "posts/open/Dakon competetion.html#class별-관측치의-숫자",
    "title": "Untitled",
    "section": "class별 관측치의 숫자",
    "text": "class별 관측치의 숫자\n\nclass imbalance? => No\n\n\nx = dataset[\"class\"].value_counts().index\ny = dataset[\"class\"].value_counts().values\nplt.bar(x,y)\n\n<BarContainer object of 3 artists>"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#one-hot-encoding",
    "href": "posts/open/Dakon competetion.html#one-hot-encoding",
    "title": "Untitled",
    "section": "One-hot encoding",
    "text": "One-hot encoding\n\n#one-hot encoding for distance base algorithm\ndataset_ohe = pd.get_dummies(dataset,columns = dataset.columns.drop(\"class\"),drop_first=True) #multicollinearity를 막기위한 drop_first 옵션\ntrain_ohe = dataset_ohe[:train_len].copy()\ntest_ohe = dataset_ohe[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain_ohe[\"class\"]=train_ohe[\"class\"].map(class_map).astype(int)\nX_train_ohe = train_ohe.drop(columns = \"class\")\nY_train_ohe = train_ohe[\"class\"]\n\n\nX_train_ohe\n\n\n\n\n\n  \n    \n      \n      trait_1\n      SNP_01_1\n      SNP_01_2\n      SNP_02_1\n      SNP_02_2\n      SNP_03_1\n      SNP_03_2\n      SNP_04_1\n      SNP_04_2\n      SNP_05_1\n      ...\n      has07AA_1\n      has07GG_1\n      has08GG_1\n      has09AA_1\n      has09GG_1\n      has11AA_1\n      has12AA_1\n      has12GG_1\n      has13AA_1\n      has14AA_1\n    \n  \n  \n    \n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n      ...\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      1\n      1\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      3\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n    \n    \n      4\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      257\n      1\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      258\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      259\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      260\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      261\n      1\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n  \n\n262 rows × 47 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#label-encoding",
    "href": "posts/open/Dakon competetion.html#label-encoding",
    "title": "Untitled",
    "section": "Label encoding",
    "text": "Label encoding\n\nle_col = []\nfor col_name in dataset.columns.tolist():\n    if \"SNP\" in col_name:\n        le_col.append(col_name)\nle_col.append(\"trait\")\nle_col\n\n['SNP_01',\n 'SNP_02',\n 'SNP_03',\n 'SNP_04',\n 'SNP_05',\n 'SNP_06',\n 'SNP_07',\n 'SNP_08',\n 'SNP_09',\n 'SNP_10',\n 'SNP_11',\n 'SNP_12',\n 'SNP_13',\n 'SNP_14',\n 'SNP_15',\n 'trait']\n\n\n\nfrom sklearn import preprocessing\nfor col in le_col:\n    le = preprocessing.LabelEncoder()\n    _col = dataset[col].tolist()\n    le.fit(_col)\n    dataset[col] = le.transform(_col)\ndataset\n\n\n\n\n\n  \n    \n      \n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      ...\n      has07AA\n      has07GG\n      has08GG\n      has09AA\n      has09GG\n      has11AA\n      has12AA\n      has12GG\n      has13AA\n      has14AA\n    \n  \n  \n    \n      0\n      1\n      2\n      1\n      0\n      1\n      1\n      0\n      0\n      2\n      0\n      ...\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      1\n      1\n      1\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      2\n      2\n      0\n      1\n      2\n      2\n      0\n      1\n      1\n      ...\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      3\n      0\n      0\n      2\n      0\n      1\n      0\n      2\n      2\n      0\n      2\n      ...\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n    \n    \n      4\n      1\n      2\n      2\n      2\n      0\n      2\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      170\n      1\n      1\n      2\n      2\n      0\n      1\n      1\n      0\n      2\n      0\n      ...\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      171\n      1\n      2\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      172\n      1\n      2\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      173\n      1\n      1\n      2\n      1\n      1\n      2\n      2\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      174\n      1\n      2\n      2\n      2\n      1\n      1\n      0\n      1\n      2\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n  \n\n437 rows × 33 columns\n\n\n\n\ntrain = dataset[:train_len].copy()\ntest = dataset[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain[\"class\"]=train[\"class\"].map(class_map).astype(int)\nX_train = train.drop(columns = \"class\")\nY_train = train[\"class\"]\n\n\n#원핫인코딩 vs 레이블인코딩\nlen(X_train_ohe.columns),len(X_train.columns)\n\n(47, 32)\n\n\n\nlen(Y_train),len(Y_train_ohe)\n\n(262, 262)"
  },
  {
    "objectID": "posts/open/Dakon competition AutoEncoder.html",
    "href": "posts/open/Dakon competition AutoEncoder.html",
    "title": "차원축소를 위한 AutoEncoder",
    "section": "",
    "text": "데이콘 competetion - 데이터 차원축소하기"
  },
  {
    "objectID": "posts/open/Dakon competition AutoEncoder.html#encoding-dimension3",
    "href": "posts/open/Dakon competition AutoEncoder.html#encoding-dimension3",
    "title": "차원축소를 위한 AutoEncoder",
    "section": "encoding dimension=3",
    "text": "encoding dimension=3\n\ntraining autoencoder\n\nautoencoder = AutoEncoder(47,3)\nloss_fn = torch.nn.MSELoss()\nrelu = torch.nn.LeakyReLU()\noptimizer = torch.optim.Adam(autoencoder.parameters(),lr=0.001)\n\n\nfor epoch in range(50000):\n    #1.yhat\n    out = autoencoder(X_train_ohe)\n    #2\n    loss = loss_fn(out,X_train_ohe)\n    #3\n    loss.backward()\n    if epoch % 10000 == 0:\n        print(f\"epoch:{epoch} loss:{loss.tolist()}\")\n    #4\n    optimizer.step()\n    optimizer.zero_grad()\n\nepoch:0 loss:0.42684122920036316\nepoch:10000 loss:0.1271684169769287\nepoch:20000 loss:0.12717127799987793\nepoch:30000 loss:0.1271684169769287\nepoch:40000 loss:0.1271684169769287\n\n\n\n\nvisualization\n\nclass_map_inv = {}\nfor key,value in class_map.items():\n    class_map_inv[value] = key\nclass_map_inv\n\n{0: 'A', 1: 'B', 2: 'C'}\n\n\n\ndt_visual = pd.DataFrame({\"class\":Y_train_ohe})\ndt_visual = pd.concat([pd.DataFrame(np.array(autoencoder.encoder(X_train_ohe).tolist())),dt_visual],axis=1)\ndt_visual = dt_visual.rename(columns = {0:\"x\",1:\"y\",2:\"z\"})\n\n\ncount = 0\ndata = []\nfor cl in dt_visual[\"class\"].unique():\n    cond = dt_visual[\"class\"] == cl\n    _data = dt_visual.loc[cond,:]\n    x = _data.x.tolist()\n    y = _data.y.tolist()\n    z = _data.z.tolist()\n    if count == 0:\n        color = \"red\"\n    elif count == 1:\n        color = \"blue\"\n    else:\n        color = \"black\"\n    trace=go.Scatter3d(\n        x=x,\n        y=y,\n        z=z,\n        mode=\"markers\",\n        marker = dict(color = color,size=5),\n        name = str(class_map_inv[cl])\n        )\n    data.append(trace)\n    count+=1\n\nlayout = go.Layout(title=dict(text = \"vectors\"))\n\n#4. figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()"
  },
  {
    "objectID": "posts/open/Dakon competition AutoEncoder.html#encoding-dimension-10",
    "href": "posts/open/Dakon competition AutoEncoder.html#encoding-dimension-10",
    "title": "차원축소를 위한 AutoEncoder",
    "section": "encoding dimension = 10",
    "text": "encoding dimension = 10\n\ntraining autoencoder\n\nautoencoder = AutoEncoder(47,10)\nloss_fn = torch.nn.MSELoss()\nrelu = torch.nn.LeakyReLU()\noptimizer = torch.optim.Adam(autoencoder.parameters(),lr=0.001)\n\n\nfor epoch in range(50000):\n    #1.yhat\n    out = autoencoder(X_train_ohe)\n    #2\n    loss = loss_fn(out,X_train_ohe)\n    #3\n    loss.backward()\n    if epoch % 10000 == 0:\n        print(f\"epoch:{epoch} loss:{loss.tolist()}\")\n    #4\n    optimizer.step()\n    optimizer.zero_grad()\n\nepoch:0 loss:0.06043163314461708\nepoch:10000 loss:0.06043798848986626\nepoch:20000 loss:0.06043170019984245\nepoch:30000 loss:0.060431692749261856\nepoch:40000 loss:0.060431867837905884"
  },
  {
    "objectID": "posts/open/Dakon competition torch.html",
    "href": "posts/open/Dakon competition torch.html",
    "title": "Untitled",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nimport torch\ntest_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/test.csv\"\ntrain_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/train.csv\"\n\n\ntrain = pd.read_csv(train_path).drop(columns = [\"id\"])\ntrain_len = len(train)\ntest = pd.read_csv(test_path)\nid_test = test[\"id\"]\ntest = pd.read_csv(test_path).drop(columns = [\"id\"])\n\n\ndataset = pd.concat([train,test],axis=0)\ndataset = dataset.drop(columns = [\"father\",\"mother\",\"gender\"])\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows × 17 columns\n\n\n\n\n_t = []\nfor val in dataset.SNP_01 == \"G G\":\n    if val == True:\n        _t.append(1)\n    else:\n        _t.append(0)\ndataset[\"has01GG\"] = _t\n\n_t = []\nfor val in dataset.SNP_02 == \"A A\":\n    if val == True:\n        _t.append(1)\n    else:\n        _t.append(0)\ndataset[\"has02AA\"] = _t\n\n\ndef create_col(dataset,col,value):\n    _t = []\n    for val in dataset[col] == value:\n        if val == True:\n            _t.append(1)\n        else:\n            _t.append(0)\n    \n    col_name_base = \"has\"+col[-2:]\n    value_name = \"\"\n    for chr in value:\n        if chr != \" \":\n            value_name+=chr\n    col_name = col_name_base+value_name\n    #print(col_name)\n    dataset[col_name] = _t\n\n    return dataset\n\ndataset = create_col(dataset,\"SNP_03\",\"A A\")\ndataset = create_col(dataset,\"SNP_04\",\"G G\")\ndataset = create_col(dataset,\"SNP_05\",\"C C\")\ndataset = create_col(dataset,\"SNP_06\",\"A A\")\ndataset = create_col(dataset,\"SNP_07\",\"A A\")\ndataset = create_col(dataset,\"SNP_07\",\"G G\")\ndataset = create_col(dataset,\"SNP_08\",\"G G\")\n\ndataset = create_col(dataset,\"SNP_09\",\"A A\")\ndataset = create_col(dataset,\"SNP_09\",\"G G\")\ndataset = create_col(dataset,\"SNP_11\",\"A A\")\n\ndataset = create_col(dataset,\"SNP_12\",\"A A\")\ndataset = create_col(dataset,\"SNP_12\",\"G G\")\n\ndataset = create_col(dataset,\"SNP_13\",\"A A\")\ndataset = create_col(dataset,\"SNP_14\",\"A A\")\n\nhas03AA\nhas04GG\nhas05CC\nhas06AA\nhas07AA\nhas07GG\nhas08GG\nhas09AA\nhas09GG\nhas11AA\nhas12AA\nhas12GG\nhas13AA\nhas14AA\n\n\n\ndataset = dataset.drop(columns = \"SNP_03\")\n\n\n#one-hot encoding for distance base algorithm\ndataset_ohe = pd.get_dummies(dataset,columns = dataset.columns.drop(\"class\"),drop_first=True) #multicollinearity를 막기위한 drop_first 옵션\ntrain_ohe = dataset_ohe[:train_len].copy()\ntest_ohe = dataset_ohe[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain_ohe[\"class\"]=train_ohe[\"class\"].map(class_map).astype(int)\nX_train_ohe = train_ohe.drop(columns = \"class\")\nY_train_ohe = train_ohe[\"class\"]\n\n\nY_train_ohe\n\n0      1\n1      2\n2      1\n3      0\n4      2\n      ..\n257    1\n258    2\n259    0\n260    0\n261    1\nName: class, Length: 262, dtype: int32\n\n\n\nimport torch\nimport torch.nn as nn\n\n\nX_train_ohe = torch.from_numpy(X_train_ohe.values).float()\n#y_train_ohe = torch.from_numpy(pd.get_dummies(Y_train_ohe).values).float()\nY_train_ohe = torch.from_numpy(Y_train_ohe.values).long()\n\n\nclass mynet(nn.Module):\n    def __init__(self,in_features,dropout_p,l1_out=64):\n        super().__init__()\n        self.linr1 = torch.nn.Linear(in_features,l1_out)\n        self.relu1 = torch.nn.ReLU()\n        self.d1 = torch.nn.Dropout(p=dropout_p)\n        self.b1 = torch.nn.BatchNorm1d(l1_out)\n        # 256\n        \n        self.linr2 = torch.nn.Linear(l1_out,l1_out//2)\n        self.relu2 = torch.nn.ReLU()\n        self.d2 = torch.nn.Dropout(p=dropout_p)\n        self.b2 = torch.nn.BatchNorm1d(l1_out//2)\n        # 128\n\n        self.linr3 = torch.nn.Linear(l1_out//2,l1_out//4)\n        self.relu3 = torch.nn.ReLU()\n        self.d3 = torch.nn.Dropout(p=dropout_p)\n        self.b3 = torch.nn.BatchNorm1d(l1_out//4)\n        # 64\n        self.linr4 = torch.nn.Linear(l1_out//4,l1_out//8)\n        self.relu4 = torch.nn.ReLU()\n        self.d4 = torch.nn.Dropout(p=dropout_p)\n        self.b4 = torch.nn.BatchNorm1d(l1_out//8)\n        # 32\n        self.linr5 = torch.nn.Linear(l1_out//8,l1_out//16)\n        self.relu5 = torch.nn.ReLU()\n        self.d5 = torch.nn.Dropout(p=dropout_p)\n        self.b5 = torch.nn.BatchNorm1d(l1_out//16)\n        #16\n        self.linr6 = torch.nn.Linear(l1_out//16,3)\n\n    def forward(self,x):\n        out = self.b1(self.d1(self.relu1(self.linr1(x))))\n        out = self.b2(self.d2(self.relu2(self.linr2(out))))\n        out = self.b3(self.d3(self.relu3(self.linr3(out))))\n        out = self.b4(self.d4(self.relu4(self.linr4(out))))\n        out = self.b5(self.d5(self.relu5(self.linr5(out))))\n        out = self.linr6(out)\n        #out = self.b3(self.d3(self.relu3(self.linr3(x))))\n        return out\n\n\nfrom sklearn.model_selection import StratifiedKFold\nimport random\nepochs_list = [i for i in range(1400,2000,20)]\nweight_decay = np.linspace(0.001,0.0001,500).tolist()\nlr = np.linspace(1e-2,1e-5,500).tolist()\nhidden_nodes = [i for i in range(1300,1800)]\ndropout_p = np.linspace(0.6,0.8,1000).tolist()\nrs = [i for i in range(0,500)]\n\n\ndef dl_cv(epochs,weight_decay,learning_rate,hidden1_nodes,dropout_p,rs):\n\n    try_number=0\n    skf = StratifiedKFold(n_splits=5,shuffle=True)\n    skf.get_n_splits(X_train_ohe,Y_train_ohe)\n    \n    while True:\n        try_number+=1\n        print(f'try:{try_number}...')\n        train_accs = []\n        val_accs = []\n        hdly1 = random.sample(hidden1_nodes,1)[0]\n        lr = random.sample(learning_rate,1)[0]\n        wght_decay = random.sample(weight_decay,1)[0]\n        epoch = random.sample(epochs,1)[0]\n        drop_p = random.sample(dropout_p,1)[0]\n        r_seed = random.sample(rs,1)[0]\n        print(f'lr:{lr} wght_decay:{wght_decay} epochs:{epoch} hidden_l1nodes:{hdly1} dropout_prob:{drop_p}')\n        for train_index,valid_index in skf.split(X_train_ohe,Y_train_ohe): \n            torch.manual_seed(r_seed)\n            net = mynet(45,drop_p,l1_out=hdly1)\n            optimizer = torch.optim.Adam(net.parameters(),lr=lr,weight_decay = wght_decay)        \n            loss_fn = torch.nn.CrossEntropyLoss()\n            #KFold\n            train_index = train_index.tolist();valid_index = valid_index.tolist()\n            X_tr = X_train_ohe[train_index,:];y_tr = Y_train_ohe[train_index]\n            X_tst = X_train_ohe[valid_index,:];y_valid = Y_train_ohe[valid_index]\n\n            #fitting\n            for ep in range(epoch):\n                net.train()\n                #1 yhat\n                yhat = net(X_tr)\n                #2 loss\n                loss = loss_fn(yhat,y_tr)\n                if ep % 50 == 0:\n                    pass\n                    #print(loss)\n                #3 derivative\n                loss.backward()\n                #4 update\n                optimizer.step()\n                optimizer.zero_grad()\n            train_yhat = torch.argmax(net(X_tr),dim=1)\n            train_acc = torch.mean((train_yhat==y_tr).float())\n            print(\"trainacc : \",train_acc)\n            train_accs.append(train_acc)\n            net.eval()\n            with torch.no_grad():\n                val_yhat = torch.argmax(net(X_tst),dim=1)\n                val_acc = torch.mean((val_yhat == y_valid).float()).tolist()\n                print(\"validacc : \",val_acc)\n                val_accs.append(val_acc)\n            if val_acc < 0.97:\n                break\n\n        valid_accuracy = torch.mean(torch.tensor(val_accs))\n        print(f\"K-Fold train accuracy {torch.mean(torch.tensor(train_accs))}\")\n        print(f\"K-Fold valid accuracy {torch.mean(torch.tensor(val_accs))}\")\n        print(\"==========================================================\")\n        if valid_accuracy > 0.99:\n            path = \"./model{}.pth\".format(try_number)\n            torch.save(net,path)\n\n\nt = dl_cv(epochs_list,weight_decay,lr,hidden_nodes,dropout_p,rs)\n\ntry:1...\nlr:0.007377374749498998 wght_decay:0.0003597194388777555 epochs:1480 hidden_l1nodes:1609 dropout_prob:0.6712712712712713\ntrainacc :  tensor(0.9952)\nvalidacc :  0.9433962106704712\nK-Fold train accuracy 0.9952152967453003\nK-Fold valid accuracy 0.9433962106704712\n==========================================================\ntry:2...\nlr:0.0031931863727454905 wght_decay:0.00048597194388777553 epochs:1900 hidden_l1nodes:1639 dropout_prob:0.786986986986987\ntrainacc :  tensor(0.9952)\nvalidacc :  0.9433962106704712\nK-Fold train accuracy 0.9952152967453003\nK-Fold valid accuracy 0.9433962106704712\n==========================================================\ntry:3...\nlr:0.00439438877755511 wght_decay:0.0006266533066132265 epochs:1920 hidden_l1nodes:1583 dropout_prob:0.7737737737737738\ntrainacc :  tensor(0.9952)\nvalidacc :  0.9245283007621765\nK-Fold train accuracy 0.9952152967453003\nK-Fold valid accuracy 0.9245283007621765\n==========================================================\ntry:4...\nlr:0.0018118036072144288 wght_decay:0.00023166332665330663 epochs:1960 hidden_l1nodes:1427 dropout_prob:0.6944944944944945\ntrainacc :  tensor(1.)\nvalidacc :  0.9056603908538818\nK-Fold train accuracy 1.0\nK-Fold valid accuracy 0.9056603908538818\n==========================================================\ntry:5...\nlr:0.0040340280561122245 wght_decay:0.0002983967935871743 epochs:1740 hidden_l1nodes:1544 dropout_prob:0.6948948948948949\ntrainacc :  tensor(1.)\nvalidacc :  0.9622641801834106\nK-Fold train accuracy 1.0\nK-Fold valid accuracy 0.9622641801834106\n==========================================================\ntry:6...\nlr:0.0032332264529058114 wght_decay:0.0007889779559118236 epochs:1860 hidden_l1nodes:1774 dropout_prob:0.7327327327327328\ntrainacc :  tensor(0.9856)\nvalidacc :  0.9056603908538818\nK-Fold train accuracy 0.9856459498405457\nK-Fold valid accuracy 0.9056603908538818\n==========================================================\ntry:7...\nlr:0.00995995991983968 wght_decay:0.0009585170340681363 epochs:1740 hidden_l1nodes:1363 dropout_prob:0.6546546546546547\ntrainacc :  tensor(0.9809)\nvalidacc :  0.9433962106704712\nK-Fold train accuracy 0.980861246585846\nK-Fold valid accuracy 0.9433962106704712\n==========================================================\ntry:8...\nlr:0.002092084168336672 wght_decay:0.0009314629258517034 epochs:1880 hidden_l1nodes:1740 dropout_prob:0.6902902902902903\ntrainacc :  tensor(0.9952)\nvalidacc :  0.9245283007621765\nK-Fold train accuracy 0.9952152967453003\nK-Fold valid accuracy 0.9245283007621765\n==========================================================\ntry:9...\nlr:0.009759759519038076 wght_decay:0.0007312625250501003 epochs:1780 hidden_l1nodes:1395 dropout_prob:0.6788788788788789\ntrainacc :  tensor(0.9809)\nvalidacc :  0.9056603908538818\nK-Fold train accuracy 0.980861246585846\nK-Fold valid accuracy 0.9056603908538818\n==========================================================\ntry:10...\nlr:0.006116112224448898 wght_decay:0.0004535070140280561 epochs:1820 hidden_l1nodes:1308 dropout_prob:0.6164164164164164\ntrainacc :  tensor(0.9904)\nvalidacc :  0.9433962106704712\nK-Fold train accuracy 0.9904305934906006\nK-Fold valid accuracy 0.9433962106704712\n==========================================================\ntry:11...\nlr:0.004634629258517034 wght_decay:0.0008935871743486974 epochs:1460 hidden_l1nodes:1605 dropout_prob:0.7099099099099099\ntrainacc :  tensor(0.9952)\nvalidacc :  0.9433962106704712\nK-Fold train accuracy 0.9952152967453003\nK-Fold valid accuracy 0.9433962106704712\n==========================================================\ntry:12...\nlr:0.006956953907815632 wght_decay:0.0006248496993987976 epochs:1620 hidden_l1nodes:1616 dropout_prob:0.7661661661661662\ntrainacc :  tensor(0.9761)\nvalidacc :  0.9245283007621765\nK-Fold train accuracy 0.9760765433311462\nK-Fold valid accuracy 0.9245283007621765\n==========================================================\ntry:13...\nlr:0.005035030060120241 wght_decay:0.0006825651302605211 epochs:1460 hidden_l1nodes:1382 dropout_prob:0.6962962962962963\ntrainacc :  tensor(0.9904)\nvalidacc :  0.9622641801834106\nK-Fold train accuracy 0.9904305934906006\nK-Fold valid accuracy 0.9622641801834106\n==========================================================\ntry:14...\nlr:0.006596593186372745 wght_decay:0.00027314629258517033 epochs:1780 hidden_l1nodes:1640 dropout_prob:0.7631631631631632\ntrainacc :  tensor(0.9904)\nvalidacc :  0.9433962106704712\nK-Fold train accuracy 0.9904305934906006\nK-Fold valid accuracy 0.9433962106704712\n==========================================================\ntry:15...\nlr:0.004814809619238477 wght_decay:0.0003002004008016032 epochs:1420 hidden_l1nodes:1746 dropout_prob:0.7347347347347347\ntrainacc :  tensor(0.9904)\nvalidacc :  0.9433962106704712\nK-Fold train accuracy 0.9904305934906006\nK-Fold valid accuracy 0.9433962106704712\n==========================================================\ntry:16...\nlr:0.006356352705410821 wght_decay:0.0008809619238476954 epochs:1700 hidden_l1nodes:1498 dropout_prob:0.7955955955955957\ntrainacc :  tensor(0.9474)\nvalidacc :  1.0\ntrainacc :  tensor(0.9665)\nvalidacc :  0.9622641801834106\nK-Fold train accuracy 0.9569377899169922\nK-Fold valid accuracy 0.9811320900917053\n==========================================================\ntry:17...\nlr:0.003933927855711422 wght_decay:0.0009440881763527055 epochs:1620 hidden_l1nodes:1533 dropout_prob:0.7239239239239239\n\n\nKeyboardInterrupt: \n\n\n\nAssemble\n\ntest_ohe = torch.from_numpy(test_ohe.values).float()\n\n\nimport os\npath = \"C:/Users/22668/Desktop/새 폴더\"\nmodels = []\nfor model in os.listdir(path):\n    md_path = os.path.join(path,model)\n    models.append(torch.load(md_path))\n\n\nassemble = torch.zeros(test_ohe.shape[0],3)\nassemble.shape\n\ntorch.Size([175, 3])\n\n\n\nsoft = torch.nn.Softmax(dim=1)\n\n\nfor model in models:\n    net = model\n    yhat = soft(model(test_ohe))\n    assemble +=yhat\n\n\ntest_predict = torch.argmax(assemble,dim=1)\nresult = pd.concat([pd.Series(id_test),pd.Series(test_predict)],axis=1)\nresult.columns = [\"id\",\"class\"]\nclass_map_inv = {0:\"A\",1:\"B\",2:\"C\"}\nresult[\"class\"] = result[\"class\"].map(class_map_inv)\n\n\nresult\n\n\n\n\n\n  \n    \n      \n      id\n      class\n    \n  \n  \n    \n      0\n      TEST_000\n      A\n    \n    \n      1\n      TEST_001\n      B\n    \n    \n      2\n      TEST_002\n      C\n    \n    \n      3\n      TEST_003\n      B\n    \n    \n      4\n      TEST_004\n      A\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      170\n      TEST_170\n      B\n    \n    \n      171\n      TEST_171\n      C\n    \n    \n      172\n      TEST_172\n      C\n    \n    \n      173\n      TEST_173\n      B\n    \n    \n      174\n      TEST_174\n      B\n    \n  \n\n175 rows × 2 columns\n\n\n\n\nresult.to_csv(\"./submission_dl.csv\",index=False)"
  },
  {
    "objectID": "posts/Probability&Statistics/categori distribution.html",
    "href": "posts/Probability&Statistics/categori distribution.html",
    "title": "카테고리 분포",
    "section": "",
    "text": "카테고리 분포에 대한 정리\n\n카테고리 분포\n카테고리분포는 시행의 한번의 시행(또는 실험)으로부터 나올 수 있는 사건이 K개인 확률분포를 모델링할때 쓰이며 다음과 같습니다.\n\\[\\begin{aligned}\n&Cat({\\bf{x};\\bf{\\mu}}) =\n\\begin{cases}\n\\mu_1\\, (\\text{if } x = (1,0,0,0,\\dots,1)) \\\\\n\\mu_2\\, (\\text{if } x = (0,1,0,0,\\dots,1)) \\\\\n\\mu_3\\, (\\text{if } x = (0,0,1,0,\\dots,1)) \\\\\n\\vdots \\\\\n\\mu_k\\, (\\text{if } x = (0,0,0,0,\\dots,1)) \\\\\n\\end{cases}\n\\\\\n&\\text{where, }x = (x_1,x_2,\\dots,x_K),\\mu = (\\mu_1,\\mu_2,\\dots,\\mu_k)\n\\end{aligned}\\]\n카테고리분포의 변수\\(\\bf{X}\\)는 K개의 원소를 가지는 원핫인코딩(one-hot encoded)된 벡터이며 각원소는 indicate number(어떤 클래스에 속하는지 나타내는)인 1또는0입니다. 모수(벡터)\\(\\mu\\)도 K개의 원소를 가지며 각각의 원소는 카테고리 확률분포로부터 대응하는 결과값(원핫벡터)에 대한 확률입니다. 즉,각각의 원핫벡터가 표본추출될 가능성(확률)을 알려줍니다.\n위와 같은 사실로부터 다음과 같은 4가지의 제약조건이 존재합니다.\n\n\\(\\mu_i\\)는 원핫벡터가 나올 확률입니다.\n\n\\[0\\leq\\mu_i\\leq1\\]\n\n확률의 합은 1입니다.\n\n\\[\\sum_{i=1}^{K}\\mu_i = 1\\]\n\n원핫벡터의 각 원소는 indicate number인 1또는 0입니다.\n\n\\[\\begin{aligned}\nx_i =\n\\begin{cases}\n0\\\\\n1\n\\end{cases}\n\\end{aligned}\\]\n\n원핫벡터의 모든 원소의 합은 1입니다. \\[\\sum_{i=1}^{K}x_i = 1\\]"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html",
    "title": "Maximum likelyhood estimation",
    "section": "",
    "text": "최대가능도 추정법(MLE)에 대한 정리"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#problem-setting",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#problem-setting",
    "title": "Maximum likelyhood estimation",
    "section": "Problem Setting",
    "text": "Problem Setting\n\\[\\begin{aligned}\n&\\text{Given}\\,x_1,x_2,...,x_N\\ \\text{which are realizations of a random sample }X_1,X_2,\\dots,X_N \\\\\n&\\text{where,}\\, X_1 \\sim f_1(x_1;\\theta),X_2 \\sim f_2(x_1;\\theta),\\dots,X_N \\sim f_N(x_N;\\theta) \\\\ \\\\\n\n&\\text{Goal : 확률분포의 모수 $\\theta$를 점추정하기}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#what-is-estimation",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#what-is-estimation",
    "title": "Maximum likelyhood estimation",
    "section": " What is estimation? ",
    "text": "What is estimation? \n\nimport scipy as sp\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n\nplt.figure(figsize=(10,5))\nrv = sp.stats.norm(175,10)\nx = np.linspace(150,200,500)\nplt.plot(x,rv.pdf(x),\"b\")\nplt.title(\"$N(175,10)$\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nText(0, 0.5, 'y')\n\n\n\n\n\n대한민국 20대 남성들의 키가 다음과 확률분포를 따른다고 가정해보겠습니다. 우리는 “키가 175근처인 사람이 대부분이네??” 또는 “키가 160이하이거나 190이상인 사람은 거의 없겠네?” 등의 해석을 할 수 있습니다. 이러한 해석을 가능하게 하는것은 우리가 확률분포의 모양(정규분포)과 모수(평균,분산)을 가정했기 때문입니다. 극단적으로,같은 평균이라는 모수를 가지지만 전혀다른 분포인 베르누이분포라고 가정을 하면 확률변수의 값이 딱 떨어지는 2개의 이산적인 값만을 가지므로 “이하”라는 표현이나 “이상”이라는 표현은 쓸 수 없습니다.(물론 베르누이분포는 분산은 모수가 아니긴합니다.) 모양이 같지만 모수가 다른 경우도 마찬가지입니다. 같은 정규분포라도 평균이나 분산이 다르면 전혀 다른 해석이 가능합니다.\n만약 위와 같은 해석을 하고싶으나 20대 남성들의 키가 따르는 분포자체를 모른다면 해야할까요? 위와 같은 해석을 하기위해서는 확률분포 더 상세히말하자면 확률분포의 모양과 확률분포의 모수를 알아야 합니다. 이를 확률분포의 추정이라고 합니다.\n확률분포의 추정에서 보통 분포의 모양은 어떠한 분포로 가정합니다(ex 정규분포,베타분포,베르누이분포,다항분포 등등…). 그 다음은 확률분포의 모수를 추정해야 합니다. 이때 모수를 추정하는 방법 중 하나가 바로 최대가능도 추정법(Maximum likelihood estimation)입니다.\n아까전 상황을 다시 생각해봅시다. 우리는 확률분포로부터 몇 가지의 해석을 할 수 있었습니다. 이는 확률분포의 모양과 모수(평균,분산)를 먼저 가정을 했기 때문이었고,이로부터 특정한 표본이 나올 수 있는 가능성을 알 수 있었습니다.이와는 반대로,최대가능도 추정법에서는 역으로 샘플이 주어져있다고 가정하고 가정한 확률분포의 모양에 대해서 가장 가능성이 높은 모수를 이 모수를 모수에 대한 추정량(estimated value)로 합니다."
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#likelyhood-function",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#likelyhood-function",
    "title": "Maximum likelyhood estimation",
    "section": "Likelyhood function ",
    "text": "Likelyhood function \n위에서 샘플이 주어져있다고 가정하고 가장 가능성이 높은 모수가 모수에 대한 추정량이라고 했습니다. 그렇다면 가능성이 가장 높은 모수를 정하기 위해서 모수의 가능성이라는 지표를 정의할 필요가 있습니다. 모수의 가능성은 어떻게 정의해야 할까요?\n\nrv1 = sp.stats.norm(175,10)\nrv2 = sp.stats.norm(185,10)\nrv3 = sp.stats.norm(160,10)\nx = np.linspace(150,200,500)\nplt.figure(figsize=(10,5))\nplt.plot(x,rv1.pdf(x),\"b\")\nplt.plot(x,rv2.pdf(x),\"g\")\nplt.plot(x,rv3.pdf(x),\"purple\")\nplt.plot([177],[0],\"o\",color=\"black\",ms=\"10\")\nplt.plot \nplt.axvline(177,color = \"black\",linewidth = 1,linestyle = \"--\",ymin=0.07)\nplt.title(\"Normal distributions\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend([\"$N(175,10)$\",\"$N(170,10)$\",\"$N(165,10)$\"],loc=\"upper right\")\n\n<matplotlib.legend.Legend at 0x21d4e82fb20>\n\n\n\n\n\n그림과 같이 크기가1인 표본을 177 얻었고 확률분포의 모양은 정규분포로 가정했다고 해봅시다. 샘플이 주어져 있을때 가능성이 가장 높은 모수는 무엇일까요? 그래프를 들여다보니 보라색 확률분포에서는 뭔가 표본을 얻기는 힘들 것 같습니다. 표본을 뽑을 확률(엄밀히는 확률밀도이지만 확률로 적겠습니다.)이 너무 낮아서 가능성이 너무 낮기 때문입니다. 초록색의 경우는 보라색보다는 확률이 더 크기때문에 가능성이 더 커보입니다. 파랑색의 경우는 확률이 초록색보다도 더 큽니다. 그러므로 3개의 분포중에서는 파랑색 확률분포의 모수인 175가 가능성이 가장 커보이고 이를 실제 모수에 대한 추정값으로 하는 것이 합리적입니다.\n위의 예시에서 가능성을 확인하기 위해서 확률분포의 값(y축,확률,확률밀도)을 확인했습니다.결국 우리가 정의하기로 지표인 모수의 가능성은 결국은 확률(또는 확률밀도) 그 자체임을 알 수 있습니다. 그러나 확률과는 약간 다르게 확률(또는 확률밀도)은 모수를 가정하고 확률분포의 값을 읽는 반면 모수의 가능성은 샘플을 가정하고 모수를 바꿔가며 서로다른 확률분포에서 값을 파악함에 차이가 있습니다. 모수의 가능성에는 가능도(likelyhood)라는 이름을 붙여줍니다.\n\nrv1 = sp.stats.norm(175,10)\nrv2 = sp.stats.norm(185,10)\nrv3 = sp.stats.norm(160,10)\nx = np.linspace(150,200,500)\nplt.figure(figsize=(10,5))\nplt.plot(x,rv1.pdf(x),\"b\")\nplt.plot(x,rv2.pdf(x),\"g\")\nplt.plot(x,rv3.pdf(x),\"purple\")\ns = [173,177,175,180]\nplt.plot(s,[0,0,0,0],\"o\",color=\"black\",ms=\"10\")\nfor i in s:\n    temp = [i]\n    plt.axvline(temp,linestyle = \"--\",color=\"black\",linewidth=1,ymin=0.07,ymax = 0.96)\nplt.title(\"Normal distributions\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend([\"$N(175,10)$\",\"$N(170,10)$\",\"$N(165,10)$\"],loc=\"upper right\")\n\n<matplotlib.legend.Legend at 0x21d4e89d160>\n\n\n\n\n\n이번에는 크기가 4인 표본을 얻었다고 해봅시다. 분포의 모양은 마찬가지로 정규분포로 가정했습니다. 이번에는 샘플의 크기가4이므로 이전과는 다르게 하나의 데이터포인트만 반영하는 것이 아니라 4개의 데이터포인트를 모두 반영하여 가능성이 가장 높은 모수를 고려해야 합니다. \n위에서 1개의 데이터포인트(표본)를 고려할때에는 여러가지 모수를 따르는 분포에서 1개의 확률값만 읽고 어떤 모수가 가능성이 가장 높은지 확인하고 가능도라는 것을 정의했습니다. 4개일때도 다르지 않습니다. 4개일때에도 확률값을 읽는데 다만 달라진 것은 4개의 확률값을 동시에 함께 반영해야 한다는 점입니다. 4개일 경우에는 4개의 표본 모두에 대하여 동시에 함께 그 가능성을 고려해야 합니다. 그러므로 여기서는 가능도인 확률을 각각의 데이터포인트를 뽑을 동시에 함께 뽑을 확률이자 확률의 곱인 확률인 결합확률(joint distribution)로 계산해야 합니다.\n정리 모수의 가능도는 샘플과 확률분포의 모양이 주어져있을 때,어떤 모수가 주어진 샘플을 취할 가능성을 의미하며 이는 곧 (결합)확률 입니다. 다만 확률(또는 확률분포)의 경우에는 목적이 모수가 정해진 확률분포로부터 읽는 그 값을 읽어 어떤 샘플 x값을 취할 가능성을 파악하는 것이고 반면 가능도는 샘플이 이미 뽑혀서 정해져있을때, 서로다른 모수를 가지는 확률분포의 값을 읽어서 어떤 모수가 가장 가능성이 높은지 파악하는 것입니다. 수학적으로 표기하면 다음과 같습니다.\n\\[\\begin{align}\nL(\\theta|D) = L(\\theta|x_1,x_2,\\dots,x_N) &= f_{X_1,X_2,\\dots,X_N}(x_1,x_2,\\dots,x_N;\\theta) \\\\\n&= \\prod_{i=1}^{N}f_{X_i}(x_i;\\theta)\n\\end{align}\\]\n식(1)은 가능도 = 결합확률임을 의미합니다. 식(2)는 확률변수가 독립일 경우의 결합확률 = 확률의 곱입니다. 각각의 샘플에 대한 확률변수는 모두 독립이라고 가정합니다."
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#mle최대가능도-추정법",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#mle최대가능도-추정법",
    "title": "Maximum likelyhood estimation",
    "section": "MLE(최대가능도 추정법)",
    "text": "MLE(최대가능도 추정법)\n최대가능도추정법(MLE)는 확률분포의 모수를 추정하는 점추정방법 중 하나로 가능도를 가장크게하는 모숫값을 모수에 대한 추정값 하는 방법입니다.다음과 같습니다.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MLE} &= \\underset{\\theta}{\\text{argmax}}\\,L(\\theta|x_1,x_2 \\dots x_n) \\\\\n&=  \\underset{\\theta}{\\text{argmax}},f_{X_1,X_2,\\dots,X_n}(x_1,x_2,\\dots,x_n|\\theta) \\\\\n&= \\underset{\\theta}{\\text{argmax}}\\,\\prod_{i=1}^{N}f_{X_i}(x_i;\\theta)\n\\end{aligned}\\]\n\n\n예시\n대한민국 20대 남성들의 키의 분포가 정규분포를 따른다고 가정해보겠습니다. 샘플링하여 크기가 4인표본[173,177,175,180]을 얻은 상태입니다. 목적은 정규분포의 모수인 \\(\\mu\\)를 MLE로 추정하는 것입니다.\n다음을 계산해야 합니다\n\\[\\begin{aligned}\n\\underset{\\theta}{\\text{argmax }}L(\\theta;x_1,x_2,x_3) &= \\prod_{i=1}^{4}f_{X_i}(x_i;\\theta)\\\\\n&= \\underset{\\theta}{\\text{argmax }}f_{X_1}(x_1;\\theta)f_{X_2}(x_2;\\theta)f_{X_2}(x_2;\\theta)f_{X_2}(x_2;\\theta)\\\\\n&= \\underset{\\theta}{\\text{argmax }}\\mathcal{N}(x_1;\\mu,\\sigma^2)\\mathcal{N}(x_2;\\mu,\\sigma^2)\\mathcal{N}(x_3;\\mu,\\sigma^2)\\mathcal{N}(x_4;\\mu,\\sigma^2) \\\\\n&= \\underset{\\theta}{\\text{argmax }}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left({-\\frac{(173-\\mu)^2}{2\\sigma^2}}\\right) \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left({-\\frac{(177-\\mu)^2}{2\\sigma^2}}\\right) \\\\ \\cdot &\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left({-\\frac{(175-\\mu)^2}{2\\sigma^2}}\\right) \\cdot\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left({-\\frac{(178-\\mu)^2}{2\\sigma^2}}\\right)\\\\\n&= \\underset{\\theta}{\\text{argmax }}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(-\\frac{(173-\\mu)^2 + (177-\\mu)^2 + (175-\\mu)^2 + (178-\\mu)^2}{\\sigma^2})\n\\end{aligned}\\]\n위의 가능도 함수가 최대가 되려면 exp의 지수의 분자인 \\(-[(173-\\mu)^2 + (177-\\mu)^2 + (175-\\mu)^2 + (178-\\mu)^2]\\)가 최대가 되면 됩니다.(분산은 고려하지 않겠습니다.)분자는 2차함수이므로 최대가 되는 지점을 구할 수 있습니다. 여기서는 넘파이를 활용하여 계산하겠습니다.\n\nx = np.linspace(120,232,50000)\ny = -((173-x)**2 +(177-x)**2+(175-x)**2+(178-x)**2)\nmax_idx = np.where(y == np.max(y))\nprint(x[max_idx])\nplt.axvline(x[max_idx],color=\"black\",linestyle=\"--\")\nplt.plot(x,y,\"b\")\nplt.scatter(x[max_idx],y[max_idx],s=100,c=\"black\",marker=\"s\")\nplt.title(f\"max(y) = {round(np.max(y),3)} if $\\mu$ = {round(x[max_idx][0],3)}, \")\n\n[175.750235]\n\n\nText(0.5, 1.0, 'max(y) = -14.75 if $\\\\mu$ = 175.75, ')\n\n\n\n\n\n가능도함수를 최대로 하는 모숫값은 175.75입니다. 따라서 MLE에 의한 모수에 대한 추정값은 175.75입니다. \\[\\hat{\\theta}_{MLE} = 175.75\\]\n\n\nLL\nLL은 log likelyhood의 약자로 likelyhood에 log를 취해준 값입니다. 로그함수를 취해도 함수가 MLE의 계산결과(가능도가 최대인 모수에 대한 추정값)의 위치가 변하지 않고 계산을 곱셈을 더하기로 바꿔서 계산하기에 더 편리하기 때문에 LL을 사용할 수도 있습니다. \\[LL = \\text{ln }L(\\theta|x_1,x_2 \\dots,x_N)\\]\n\n\nNLL\nNLL은 LL에 -(negative)를 곱해준 값입니다. 함수가 최대인 지점을 찾는 문제를 최소인 지점을 찾는 문제로 바꿀 수 있습니다. \\[NLL = -\\text{ln }L(\\theta|x_1,X_2 = x_2 \\dots x_N)\\]"
  },
  {
    "objectID": "posts/Probability&Statistics/notation.html",
    "href": "posts/Probability&Statistics/notation.html",
    "title": "확률분포에서 ;와|의 사용",
    "section": "",
    "text": "확률분포에서 ;와|에 관한 notation 정리"
  },
  {
    "objectID": "posts/Probability&Statistics/notation.html#언제-를-사용하지",
    "href": "posts/Probability&Statistics/notation.html#언제-를-사용하지",
    "title": "확률분포에서 ;와|의 사용",
    "section": "언제 ;를 사용하지?",
    "text": "언제 ;를 사용하지?\n;의 뒤에는 다변수 함수에서 특정 변수를 시켜놓았다라는 의미로 쓰인다. 다음과 같은 이변수함수를 생각해보자 \\[f(x,y)\\] 독립변수 x와 y를 갖는 함수이며 두 변수 모두에 대해서 미분가능하다. 여기서 변수 y가 어떤 어떤 값으로 고정된것이 알려지거나 또는 고정된상황을 가정한 후 x에 대해서만 관심이 있다고 해보자(미분,극한등을 아마도?취하고 싶다..아마도…) 이때는 이변수함수로 표기하는것이 아닌 일변수함수로 표기해야한다. 그때는 다음과 같이 표기하면 된다. \\[\nf(x;y)\n\\]\n이렇게 표기하면 y값은 이제 어떤 값으로 이미 설정(setting)되거나 고정(fixed)되어있음을 의미한다."
  },
  {
    "objectID": "posts/Probability&Statistics/notation.html#확률함수에서-의-사용",
    "href": "posts/Probability&Statistics/notation.html#확률함수에서-의-사용",
    "title": "확률분포에서 ;와|의 사용",
    "section": "확률함수에서 ;의 사용",
    "text": "확률함수에서 ;의 사용\n베르누이분포의 확률질량함수는 다음과 같다 \\[\nf_X(x) = p^x(1-p)^{1-x}\n\\]\n이식을 해석해보면 다음과 같다. - 좌변 : 확률변수 X에 관한 확률함수이며 실험 또는 시행으로부터 확률변수X가 취할 수 있는 값인 변수x를 입력으로 갖는다. - 우변 : 구체적인 확률질량함수(또는 확률밀도함수)이다.\n베르누이분포 확률질량함수에서 이미 setting,fixed된 모수(parameter)를 명시적으로 표현하고자 할때에는 다음과 같은 표기를 사용한다. \\[f_X(x;p) = p^x(1-p)^{1-x}\\]"
  },
  {
    "objectID": "posts/Probability&Statistics/notation.html#의-사용-1",
    "href": "posts/Probability&Statistics/notation.html#의-사용-1",
    "title": "확률분포에서 ;와|의 사용",
    "section": "|의 사용",
    "text": "|의 사용\n|는 given that이라는 의미이며 |의 오른쪽에는 조건 또는 주어지는 상황이 나온다. 확률,통계 분야에서만 쓰이며 조건부 확률이나,베이지안에서 많이 쓰이는 기호라고 한다. 여기서 조건은 확률변수의 어떤 값이므로 마찬가지로 미리주어지는 값이다. 변수가 아니라 모수(parameter)로 생각할 수 있으며 결국에는 설정된,고정된 값이다.\n조건부 확률분포(조건부확률밀도함수,조건부 확률함수)는 다음과 같다. \\[\np_{X|Y}(x|y) = \\frac{p_{XY}(x,y)}{p_Y(y)}\\\\\np_{X|Y}(y|x) = \\frac{p_{XY}(x,y)}{p_X(x)}\n\\]  가장 윗식을 해석해보면 다음과 같다. - 좌변 : 확률변수 Y가 주어진(given)상황에서 확률변수X의 확률(또는 확률밀도)를 나타내는 함수이며 시행으로부터 확률변수 X가 취할 수 있는 값 x를 입력으로 받는다. 조건부 확률분포에서 Y의 값은 y로 주어져 있다고 가정하므로 입력하는 변수가 아니다. 따라서 확률변수 Y는 변수가 아니라 모수(parameter)이다. - 우변 : 조건부확률분포 공식."
  },
  {
    "objectID": "posts/Probability&Statistics/notation.html#결론-과",
    "href": "posts/Probability&Statistics/notation.html#결론-과",
    "title": "확률분포에서 ;와|의 사용",
    "section": "결론 ; 과 |",
    "text": "결론 ; 과 |\n\\[p_{\\theta}(x) = p(x;\\theta) = p(x|\\theta)\\]  ;는 수학의 모든 분야에서 쓰이지만 |는 확률통계분야에서만 쓰인다. ;다음에는 이미 설정되거나 고정된 값이 나오며 |다음에는 이미 주어진 조건이 나온다. 확률,통계 분야에서는 |다음에 나오는 조건은 조건부확률분포에서의 주어진 조건이자 설정된,고정된 값인 경우가 많다. 따라서 비공식적으로 혼용해서 쓰며 경험상 ;다음에는 그래도 설정된 값을 의미하는 모수(parameter)가 |는 조건부 확률이나 확률분포를 쓰는 경우가 많은 것 같다.\n참고링크 링크1(;의 사용법) 링크2(조건부확률분포) 링크3(표기법의 혼용)"
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "",
    "text": "파이썬 변수,할당문,인터닝\n전북대학교 최규빈 교수님의 딥러닝 deepcopy-shallowcopy 특강 중,변수와 할당문 부분을 재구성한 글입니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-변수",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-변수",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "파이썬에서의 변수",
    "text": "파이썬에서의 변수\n\n파이썬의 변수는 메모리상에 저장된 객체를 참조(reference)합니다.\n따라서 변수는 자바에서 참조변수와 같습니다.\n변수는 객체(object)를 부르는 별칭(다른이름),객체에 붙여진 포스트잇,또다른 레이블 이라고 생각하는 것이 비유적으로 맞습니다.\n앞으로 객체에 접근하기 위해서는 a라는 변수(별칭,객체)를 찾으면 됩니다.\n마치 C언어의 포인터와 유사하게 동작합니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-할당문",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-할당문",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "파이썬에서의 할당문(=)",
    "text": "파이썬에서의 할당문(=)\n\n=은 파이썬(뿐만아니라 대부분의 프로그래밍언어)에서 할당문입니다.\n할당문을 실행하면 = 오른쪽에 있는 객체를 먼저 메모리 상에 생성하거나 가져옵니다.\n=의 왼쪽에는 변수가 존재하며 파이썬은 변수에 생성된 객체를 할당합니다.\n변수는 객체에 붙여지는 별칭,레이블로 표현할 수 있습니다. 이렇게 객체에 변수가 할당되고나면 할당된 변수와 객체에 대해서 “변수가 객체에 바인딩(묶이다)되어있다”고 표현합니다.\n\n\na = [1,2,3]\nprint(id(a))\n\n1819154486912\n\n\n위 코드에서 객체[1,2,3]에 변수 a가 바인딩 되었습니다.내부동작은 다음과 같습니다. 1. 메모리 주소(1819161042880)에 리스트 객체[1,2,3]을 생성합니다 2. 생성된 리스트객체를 변수 a에 할당합니다. a는 객체[1,2,3]에 붙여지는 별칭,레이블이라고 할 수 있습니다.\n\n에일리어싱\n에일리어싱은 하나의 객체를 여러개의 변수가 참조하게 하는 것입니다. 하나의 객체에 여러개의 별칭,별명,레이블을 붙이는 것이라고도 할 수 있습니다.\n\nb = a\nprint(id(a));print(id(b))\nprint(a);print(b)\n\n1819154486912\n1819154486912\n[1, 2, 3]\n[1, 2, 3]"
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#idvalue",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#idvalue",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "id,value",
    "text": "id,value\nid는 객체가 가지는 메모리상의 고유한 주소입니다. 서로다른 객체는 다른 값을 가질수도 같은값을 가질수도 있습니다.\n\na=[1,2,3]\nb=a\na.append(4)\nc=[1,2,3,4]\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\n\na:[1, 2, 3, 4] b:[1, 2, 3, 4] c:[1, 2, 3, 4]\nid(a):1819154849280,id(b):1819154849280,id(c):1819155097408\n\n\n변수a,b,c는 모두 같은 value(값)을 가집니다.a와b는 같은 객체에 바인딩되어있지만 c는 또다른 객체에 바인딩되어 있습니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#is-vs",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#is-vs",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "is vs ==",
    "text": "is vs ==\n- is는 객체비교연산자로 두 변수가 동일한 객체를 참조하는지 아니면 다른객체를 참조하는지 확인한 후 True or False를 반환합니다. 파이썬은 내부적으로 동일한 객체인지 아닌지를 판단할때에는 메모리주소를 확인한다고 합니다. - ==는 값비교연산자로 두 변수가 참조하는 객체의 값이 같은지 아니면 값이 다른지를 확인한 후 True or False를 반환합니다. - 참조하다는 뭔가 잘 와닿는데 참조`라는 용어는 잘 와닿지가 않습니다…변수가 참조하는(또는 가리키는,지칭하는)객체(object) 그 자체입니다.\n\ncode1\n\na=[1,2,3] #1\nprint(\"append하기 전 id(a):\",id(a))\nb=a #2 에일리어싱,동일한 객체를 가리키도록 함.\na.append(4) #3\nc=[1,2,3,4] #4\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\nprint(\"a의 참조(reference)와 b의 참조는 동일한 객체인가요??\",a is b)\nprint(\"a의 참조와 b의 참조는 값(value)이 같나요?\",a == b)\nprint(\"a의 참조(reference)와 c의 참조는 동일한 객체인가요??\",a is c)\nprint(\"a의 참조와 c의 참조는 값(value)이 같나요?\",a == c)\n\nappend하기 전 id(a): 1819155097408\na:[1, 2, 3, 4] b:[1, 2, 3, 4] c:[1, 2, 3, 4]\nid(a):1819155097408,id(b):1819155097408,id(c):1819155103296\na의 참조(reference)와 b의 참조는 동일한 객체인가요?? True\na의 참조와 b의 참조는 값(value)이 같나요? True\na의 참조(reference)와 c의 참조는 동일한 객체인가요?? False\na의 참조와 c의 참조는 값(value)이 같나요? True\n\n\n코드설명 1. 변수a에 [1,2,3]을 할당합니다.a는 [1,2,3]을 참조합니다. 2. 에일리어싱으로 변수b도 [1,2,3]을 참조합니다. 3. 변수a가 참조하는 리스트객체[1,2,3]에 4를 추가합니다. 4. 변수c에 [1,2,3,4]를 할당합니다.\n\n\ncode2 - 살짝 심화\n\na=[1,2,3] #1\nprint(\"재할당 하기 전 id(a):\",id(a))\nb=a #2에일리어싱,동일한 객체를 가리키도록 함\na=[1,2,3]+[4] #3재할당\nc=[1,2,3,4] #4\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\nprint(\"a의 참조(reference)와 b의 참조는 동일한 객체인가요??\",a is b)\nprint(\"a의 참조와 b의 참조는 값(value)이 같나요?\",a == b)\nprint(\"a의 참조(reference)와 c의 참조는 동일한 객체인가요??\",a is c)\nprint(\"a의 참조와 c의 참조는 값(value)이 같나요?\",a == c)\n\n재할당 하기 전 id(a): 1819155102592\na:[1, 2, 3, 4] b:[1, 2, 3] c:[1, 2, 3, 4]\nid(a):1819184303168,id(b):1819155102592,id(c):1819184334272\na의 참조(reference)와 b의 참조는 동일한 객체인가요?? False\na의 참조와 b의 참조는 값(value)이 같나요? False\na의 참조(reference)와 c의 참조는 동일한 객체인가요?? False\na의 참조와 c의 참조는 값(value)이 같나요? True\n\n\n코드설명 1. 변수a에 [1,2,3]을 할당합니다.a는 [1,2,3]을 참조합니다. 2. 에일리어싱으로 변수b도 [1,2,3]을 참조합니다. 3. 변수a에 리스트객체[1,2,3,4]를 재할당합니다. 4. 변수c에 [1,2,3,4]를 할당합니다.\na,b,c 각각의 값은 code1과 code2에서 모두 같습니다. 차이점은 code1에서는 a가 참조하는 리스트[1,2,3]에 4를 추가하고 code2에서는 a에 리스트[1,2,3,4]를 재할당한다는 점입니다.중요한 차이점은 다음과 같습니다.\n- code1에 append전 후의 a가 참조하는 객체는 주소는 변하지 않은 것으로 보아 동일한 객체에 원소만 추가되었음을 알 수 있습니다. - 반면 code2에서 할당문 전 후의 a가 참조하는 객체의 주소가 변합니다. - 이전에 없었던 1)객체가 메모리에 생성되고 2)변수a는 이전의 [1,2,3]을 더 이상 참조하지 않고 생성된 객체[1,2,3,4]를 참조**하는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#인터닝",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#인터닝",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "인터닝",
    "text": "인터닝\n인터닝이란 이미 생성된 객체를 재사용하는 것을 말합니다. 객체의 빠른 재사용을 가능하게 하며 메모리를 절약한다고 합니다. 내부적으로는 다음과 같이 구현됩니다. 1. 임의의 할당문을 실행합니다. 2. = 오른쪽에 있는 객체가 Intern 컬렉션에 등록되어 있는지 아닌지 확인합니다. 3. 등록되어 있는 객체의 경우 그 객체를 그대로 참조합니다. 등록되지 않은 경우 메모리에 객체를 생성하며 변수는 생성된 객체의 주소를 참조합니다\n자주 사용하는 객체의 경우 직접 Intern 컬렉션에 등록할 수 있고 빠르게 재사용할 수 있습니다. -5~256사이의 정수이거나 20자 미만의 문자열은 할당문을 실행하면 자동으로 Inter 컬렉션에 등록됩니다. 따라서 해당하는 정수나 문자열을 또다른 할당문에 실행하면 변수는 같은 객체를 참조합니다.\n예제1-인터닝 X\n\na=1+2021\nb=2023-1\nc=2022\nprint(id(a),id(b),id(c))\nprint(a,b,c)\n\n1819155040400 1819155039600 1819155040688\n2022 2022 2022\n\n\na,b,c는 서로다른 객체를 참조하며 객체들은 모두 같은 값을 가집니다.\n예제2-인터닝 O\n\na=1+2 \nb=4-1\nc=3\nprint(id(a),id(b),id(c))\nprint(a,b,c)\n\n1819075897712 1819075897712 1819075897712\n3 3 3\n\n\na,b,c는 모두같은 객체를 참조합니다. 내부적인 동작은 다음과 같습니다. 1. a=1+2 할당문을 실행합니다. 2. 할당되는 객체가 -5~256사이의 정수이므로 자동으로 Intern 컬렉션에 등록됩니다. 3. b와c에도 정수 3을 할당합니다. 3은 Intern 컬렉션 등록되어있는 객체이며 메모리상에 생성되어있는 객체이므로 새로운 3객체가 생성되지 b,c는 이미 생성된 3객체를 가리킵니다.\n참고링크1 : https://guebin.github.io/DL2022/posts/Appendix/2022-12-14-A1.html 참고링크2 : http://pythonstudy.xyz/python/article/512-%ED%8C%8C%EC%9D%B4%EC%8D%AC-Object-Interning"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html",
    "title": "plotly 시각화 모음",
    "section": "",
    "text": "Code\n# %pip install plotly (jupyter notebook)\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\npio.renderers.default = \"plotly_mimetype+notebook\"\n\n\n\n\nCode\nimport pandas as pd\ntimesData = pd.read_csv(\"./timesData.csv\")\ntimesData.head(5)\n\n\n\n\n\n\n  \n    \n      \n      world_rank\n      university_name\n      country\n      teaching\n      international\n      research\n      citations\n      income\n      total_score\n      num_students\n      student_staff_ratio\n      international_students\n      female_male_ratio\n      year\n    \n  \n  \n    \n      0\n      1\n      Harvard University\n      United States of America\n      99.7\n      72.4\n      98.7\n      98.8\n      34.5\n      96.1\n      20,152\n      8.9\n      25%\n      NaN\n      2011\n    \n    \n      1\n      2\n      California Institute of Technology\n      United States of America\n      97.7\n      54.6\n      98.0\n      99.9\n      83.7\n      96.0\n      2,243\n      6.9\n      27%\n      33 : 67\n      2011\n    \n    \n      2\n      3\n      Massachusetts Institute of Technology\n      United States of America\n      97.8\n      82.3\n      91.4\n      99.9\n      87.5\n      95.6\n      11,074\n      9.0\n      33%\n      37 : 63\n      2011\n    \n    \n      3\n      4\n      Stanford University\n      United States of America\n      98.3\n      29.5\n      98.1\n      99.2\n      64.3\n      94.3\n      15,596\n      7.8\n      22%\n      42 : 58\n      2011\n    \n    \n      4\n      5\n      Princeton University\n      United States of America\n      90.9\n      70.3\n      95.4\n      99.9\n      -\n      94.2\n      7,929\n      8.4\n      27%\n      45 : 55\n      2011\n    \n  \n\n\n\n\n\n\nCode\ntimesData.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2603 entries, 0 to 2602\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   world_rank              2603 non-null   object \n 1   university_name         2603 non-null   object \n 2   country                 2603 non-null   object \n 3   teaching                2603 non-null   float64\n 4   international           2603 non-null   object \n 5   research                2603 non-null   float64\n 6   citations               2603 non-null   float64\n 7   income                  2603 non-null   object \n 8   total_score             2603 non-null   object \n 9   num_students            2544 non-null   object \n 10  student_staff_ratio     2544 non-null   float64\n 11  international_students  2536 non-null   object \n 12  female_male_ratio       2370 non-null   object \n 13  year                    2603 non-null   int64  \ndtypes: float64(4), int64(1), object(9)\nmemory usage: 284.8+ KB"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#add-markers-and-text",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#add-markers-and-text",
    "title": "plotly 시각화 모음",
    "section": "add markers and text",
    "text": "add markers and text\n\n\nCode\n#1 data frame\ndf = timesData.iloc[:100]\n\n#2 trace and data\ntrace = go.Scatter(\n    x = df.world_rank,\n    y = df.citations,\n    mode = \"lines+markers\", #add marker,\n    marker = dict(color = \"rgba(16,112,2,0.8)\"),\n    text = df.university_name #add text\n)\ndata = [trace]\n#3 layout and data\nlayout = go.Layout(\n    title = \"citation\",\n    xaxis = dict(title = \"World Rank\",ticklen = 5)\n)\n\n#4 create figure\nfig = go.Figure(data = data,layout = layout)\n\n#5 plot figure\nfig.show()\n\n\n\n                                                \n\n\nversion2가 뭔가 더 좋을듯?"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#add-markers-and-text-1",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#add-markers-and-text-1",
    "title": "plotly 시각화 모음",
    "section": "add markers and text",
    "text": "add markers and text\n\n\nCode\n#1 data frame\ndf2014 = timesData[timesData.year == 2014].iloc[:100,:]\n\n#2 trace,data\ntrace = go.Scatter(\n    x = df2014.world_rank,\n    y = df2014.citations,\n    mode = \"markers\",\n    #marker = dict(color = \"green\",opacity=0.8), #alpha(불투명도) 조절 vs1\n    marker = dict(color = \"rgba(255,128,2,0.8)\"), #alpha(불투명도) 조절 vs2\n    text = df2014.university_name,\n\n)\ndata = [trace]\n\n#3 layout\nlayout = go.Layout(xaxis = dict(title = \"World Rank\"),yaxis = dict(title = \"Citation\"))\n\n#4 create figure\nfig = go.Figure(data=data,layout=layout)\n\n#5 plot\nfig.show()"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#여러개의-차트-겹처-그리기",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#여러개의-차트-겹처-그리기",
    "title": "plotly 시각화 모음",
    "section": "여러개의 차트 겹처 그리기",
    "text": "여러개의 차트 겹처 그리기\n\n여기서는 histogram으로 했으나 다른차트들도 가능\n\n\n\nCode\n#1.dataframe\nx2011 = timesData.student_staff_ratio[timesData.year == 2011]\nx2012 = timesData.student_staff_ratio[timesData.year == 2012]\n#2.trace&data\ntrace1 = go.Histogram(\n    x=x2011,\n    #opacity=0.7, #불투명도 조절\n    name=\"2011\", #범례(legend)를 설정하기 위한 이름 설정\n    marker=dict(color=\"rgb(171,50,96)\",opacity=0.7)\n)\n\ntrace2 = go.Histogram(\n    x=x2012,\n    name=\"2012\",\n    marker=dict(color=\"blue\",opacity=0.7)\n)\ndata=[trace1,trace2]\n#3.layout\nlayout = go.Layout(\n    barmode = \"overlay\", #trace 겹쳐 그리기\n    xaxis=dict(title=\"students-staff ratio\"),\n    yaxis=dict(title=\"count\"),\n    title = dict(text = \"histogram\",x = 0.5)\n)\n#4 figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()\n\n\n\n                                                \n\n\n참고자료 - Opacity와 alpha? : Opacity는 marker안팎에서 모두 쓰일 수 있으며 alpha는 rgba와 쓸때만 입력,같은 역할을 함. 단,Opacity를 marker의 밖에서 입력하면 trace안에서 밀도를 표현 하지 못함. 다른 trace끼리 겹칠때에는 밀도표현됨.(같은 trace에서만 안됨.)\n\n\nCode\n# 1.data frame\ndataframe = timesData[timesData.year == 2015]\n\n#2.trace and data\ndata = []\nfor col in [\"world_rank\",\"citations\",\"income\",\"total_score\"]:\n    _trace = go.Scatter(\n        x = dataframe[\"world_rank\"],\n        y = dataframe[col],\n        mode = \"lines\"\n    )\n    data.append(_trace)\n\n#3. layout\nlayout = go.Layout(\n    xaxis=dict(\n        domain=[0, 0.45]\n    ),\n    yaxis=dict(\n        domain=[0, 0.45]\n    ),\n    xaxis2=dict(\n        domain=[0.55, 1]\n    ),\n    xaxis3=dict(\n        domain=[0, 0.45],\n        anchor='y3'\n    ),\n    xaxis4=dict(\n        domain=[0.55, 1],\n        anchor='y4'\n    ),\n    yaxis2=dict(\n        domain=[0, 0.45],\n        anchor='x2'\n    ),\n    yaxis3=dict(\n        domain=[0.55, 1]\n    ),\n    yaxis4=dict(\n        domain=[0.55, 1],\n        anchor='x4'\n    ),\n    title = 'Research, citation, income and total score VS World Rank of Universities'\n)\n\n#4. fig\nfig = make_subplots(rows=2,cols=2)\n#5. plot\nrow = 1\ncol = 1\nfor trace in data:\n    fig.append_trace(trace,row=row,col=col)\n    col+=1\n    if col > 2:\n        col = 1\n        row+=1\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{\"type\": \"xy\"}, {\"type\": \"polar\"}],\n           [{\"type\": \"domain\"}, {\"type\": \"scene\"}]],\n)\n\nfig.add_trace(go.Bar(y=[2, 3, 1]),\n              row=1, col=1)\n\nfig.add_trace(go.Barpolar(theta=[0, 45, 90], r=[2, 3, 1]),\n              row=1, col=2)\n\nfig.add_trace(go.Pie(values=[2, 3, 1]),\n              row=2, col=1)\n\nfig.add_trace(go.Scatter3d(x=[2, 3, 1], y=[0, 0, 0],\n                           z=[0.5, 1, 2], mode=\"lines\"),\n              row=2, col=2)\n\nfig.update_layout(height=700, showlegend=False)\n\nfig.show()"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#vector-fieldquiver-plot",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#vector-fieldquiver-plot",
    "title": "plotly 시각화 모음",
    "section": "Vector field(quiver plot)",
    "text": "Vector field(quiver plot)\n\n사전준비\n\nnp.meshgrid : x좌표,y좌표를 가지는 벡터를 입력했을때, 두 벡터로 만들 수 있는 격자의 좌표(x,y)를 출력\n\n\n\nCode\nimport numpy as np\nx_coord = np.arange(0,2,.2)\ny_coord = np.arange(0,2,.2)\nx,y = np.meshgrid(np.arange(0,2,.2),np.arange(0,2,.2))\nprint(x_coord.shape,y_coord.shape)\nprint(x.shape,y.shape)\n\n\n(10,) (10,)\n(10, 10) (10, 10)\n\n\n\n격자(grid,matrix)에 함수 적용하면? => matrix(x,y 각각의 좌표)의 모든 요소에 함수가 적용됨\n\n\n\nCode\nprint(np.cos(x).shape,np.sin(x).shape)\n\n\n(10, 10) (10, 10)\n\n\n\n배열의 요소 값 차례대로 읽어보기 …\n\n(0,0),(0.2,0),(0.4,0) … (1.8,0) => (0,0.2),(0.2,0.2),(0.4,0.2)… x좌표 다 읽고 y좌표증가 그 다음 x좌표 다 읽고 y좌표 증가 …\n\n\nCode\nx,y\n\n\n(array([[0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8]]),\n array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n        [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n        [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n        [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6],\n        [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],\n        [1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],\n        [1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2],\n        [1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4],\n        [1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6],\n        [1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8]]))\n\n\n\n\nGradient Vector Field\n\n\\(\\nabla f = xe^{-x^2-y^2}\\)\n\n\nCode\n#1.prepare data\nx,y = np.meshgrid(np.arange(-2,2,0.2),np.arange(-2,2,.25)) #좌표\nz = x*np.exp(-x**2-y**2) #함수\n\ndx=0.2;dy=0.25 #dx,dy\nv,u = np.gradient(z,dx,dy) #함수의 그레디언트(각좌표에서의 미분계수)\n\n\n\n\nCode\n#2.trace and data => 생략\n#3.fig\nfig = ff.create_quiver(x,y,u,v,scale=.25,arrow_scale=.4,name=\"quiver\",line_width=1)\nfig.add_trace(go.Scatter(x=[-.7,.75],y=[0,0],\n                         mode=\"markers\",\n                         marker_size=12,\n                         name=\"points\"))\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nx = np.linspace(-1,1,100)\ny = np.linspace(-1,1,100)\nxx,yy = np.meshgrid(x,y)\nfor i in range()\n\n\n(array([[-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        ...,\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ]]),\n array([[-1.        , -1.        , -1.        , ..., -1.        ,\n         -1.        , -1.        ],\n        [-0.97979798, -0.97979798, -0.97979798, ..., -0.97979798,\n         -0.97979798, -0.97979798],\n        [-0.95959596, -0.95959596, -0.95959596, ..., -0.95959596,\n         -0.95959596, -0.95959596],\n        ...,\n        [ 0.95959596,  0.95959596,  0.95959596, ...,  0.95959596,\n          0.95959596,  0.95959596],\n        [ 0.97979798,  0.97979798,  0.97979798, ...,  0.97979798,\n          0.97979798,  0.97979798],\n        [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n          1.        ,  1.        ]]))\n\n\n\n\n\n1. 시점\n\n종점은 화살표로 표시해야 하므로 시점만 만들기\n\n\n\nCode\nimport plotly.graph_objs as go\n\n\n\n\nCode\n#1. prepare data\n\n#첫번째 벡터의 시점 x[0],y[0],z[0] 종점 x[1],y[1],z[1]\n#두번째 벡터의 시점 x[2],y[2],z[2] 종점 x[2],y[2],z[2]\n#두 개씩 묶임\nx = [10.1219, 10.42579, 15.21396, 15.42468, 20.29639,20.46268, 25.36298, 25.49156]\ny = [5.0545,  5.180104, 5.0545,   5.20337,  5.0545,  5.194271, 5.0545,   5.231627]\nz = [5.2713,  5.231409, 5.2713,   5.231409, 5.2713 ,  5.235852,  5.2713, 5.231627]\n#pairs = [(0,1),(2,3),(4,5),(6,7)]\n[coord for coord in range(0,len(x),2)]\n\n\n[0, 2, 4, 6]\n\n\n\n\nCode\n#2. trace,data(trace set)\ntrace1 = go.Scatter3d(\n    x=[x[coord] for coord in range(0,len(x),2)],\n    y=[y[coord] for coord in range(0,len(y),2)],\n    z=[z[coord] for coord in range(0,len(z),2)],\n    mode = \"markers\",\n    line=dict(color=\"red\")\n)\ndata = [trace1]\n\n#3. Layout\nlayout = go.Layout(title=dict(text = \"vectors\"))\n\n#4. figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()\n\n\n\n                                                \n\n\n\n\n2. 선 만들기\n\n\nCode\n#1.prepare data\nx_lines = list()\ny_lines = list()\nz_lines = list()\n\nfor i in range(len(x)):\n    x_lines.append(x[i])\n    y_lines.append(y[i])\n    z_lines.append(z[i])\n    #plotly에서 Scatter의 line mode는 점과 점 사이에 선을 만듦\n    #0,1번째 자리의 좌표에는 시점,종점을 넣고 3번째 자리에 None을 추가하여 점을 만들지 않음 \n    #따라서, 선이 생기지 않음\n    if i % 2 == 1:    \n        x_lines.append(None)\n        y_lines.append(None)\n        z_lines.append(None)\n\n#2.trace and tr_set(=data)\ntrace2 = go.Scatter3d(\n    x=x_lines,\n    y=y_lines,\n    z=z_lines,\n    mode = \"lines\",\n    line = dict(width = 2, color = 'rgb(255, 0,0)')\n)\ndata = [trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\n#중간체크\ndata = [trace1,trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\n3.종점 만들기\n\n\nCode\ndata = [trace1,trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nimport plotly.graph_objs as go\n# plotly.offline.init_notebook_mode()\n\nx = [10.1219, 10.42579, 15.21396, 15.42468, 20.29639,20.46268, 25.36298, 25.49156]\ny = [5.0545,  5.180104, 5.0545,   5.20337,  5.0545,  5.194271, 5.0545,   5.231627]\nz = [5.2713,  5.231409, 5.2713,   5.231409, 5.2713 ,  5.235852,  5.2713, 5.231627]\n\npairs = [(0,1), (2,3),(4,5), (6,7)]\n\n## plot ONLY the first ball in each pair of balls\ntrace1 = go.Scatter3d(\n    x=[x[p[0]] for p in pairs],\n    y=[y[p[0]] for p in pairs],\n    z=[z[p[0]] for p in pairs],\n    mode='markers',\n    name='markers',\n    line=dict(color='red')\n)\n\ngo.Figure(data=trace1)"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html",
    "href": "python code/IAB Deep learning/DL 12-01.html",
    "title": "Hoyeon's Blog",
    "section": "",
    "text": "def f(txt,mapping):\n    return [mapping[chr] for chr in txt]\n\n\nimport torch\ntxt = list(\"hi?hello!!\") * 100\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {\"!\" : 0 ,\"?\" : 1,\"h\" : 2, \"i\" : 3,\"e\" : 4,\"l\" : 5, \"o\" : 6}\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\nprint(len(mapping))\n\n7\n\n\n\n\n\n- 코드1 : 정석코드\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\n_h0 = _c0 = torch.zeros(1,4).to(\"cuda:0\")\nlstm(x,(_h0,_c0))\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))\n\n\n- 코드2 : 초깃값이 없는 코드(c0,h0는 사실 없어도 무방함) - 알아서 차원맞춰서 잘 초기화 해줘요.\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\nlstm(x)\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))\n\n\n\n\n\n- timeseiries와 RNN timeseries 데이터는 시점(timestep)에 따라서 그 값들이 정렬된 데이터입니다. 각 시점에서의 값들은 서로 연관되어 있습니다. 하나의 timeseries 데이터를 RNN계열의 모형에 입력하면 각 시점에서 값을 재귀적으로 읽어서 \\(\\hat{y}\\)를 출력합니다. - 파라미터 설명 - \\(L\\) : sequnce length = timestep(시점)의 총 갯수 = timestp의 길이 = 시계열 데이터의 길이 - \\(H_{in}\\) : each timestep(시점)에 들어오는 입력백터의 길이, 입력시계열이 시점별로 몇개의 변수로 나타내어 지는가?,만약에 원핫인코딩으로 정리하면 단어수를 의미함,시점마다 길이는 모두 같음(전처리 과정에서 길이를 같게해주기 때문) - \\(N\\) : 전체 데이터를 몇개의 시계열(묶음)데이터 인지? = 전체데이터 안에 있는 시계열데이터 묶음의 갯수 = 전체데이터의 미니배치의 갯수, 예를들어 (1000,7)의 shape을 가진 시계열데이터를 2개의 묶음으로 나눈다면 N = batch size = 2,왜 쪼개는지는 나중에 … 공부\n- 코드3 : x의 차원은 사실 엄밀하게는 (\\(L\\),\\(N\\),\\(H_{in}\\))이다.\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda\")\n\n\nlstm(x.reshape(999,1,7)) #전체데이터를 쪼개지 않았으므로 즉 묶음은 1개뿐이므로 N = 1\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563]],\n \n         [[-0.0786, -0.1430, -0.0250,  0.1189]],\n \n         [[-0.0300, -0.2256, -0.1324,  0.1439]],\n \n         ...,\n \n         [[-0.0723,  0.0620,  0.1913,  0.2015]],\n \n         [[-0.1155,  0.0746,  0.1747,  0.2938]],\n \n         [[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))\n\n\n- 코드4 : batch_first = True일 경우 차원은 (\\(N\\),\\(L\\),\\(H_{in}\\))이다.\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4,batch_first = True).to(\"cuda\")\nlstm(x.reshape(1,999,7)) #전체데이터를 쪼개지 않았으므로 즉 묶음은 1개뿐이므로 N = 1\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563],\n          [-0.0786, -0.1430, -0.0250,  0.1189],\n          [-0.0300, -0.2256, -0.1324,  0.1439],\n          ...,\n          [-0.0723,  0.0620,  0.1913,  0.2015],\n          [-0.1155,  0.0746,  0.1747,  0.2938],\n          [-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))\n\n\n\n\n\n네트워크에 우리는 h_0,c_0를 넣었었다.그때 h_0.shape = c_0.shape = (1,\\(H_{out}\\))이었는데 여기서 사실은 1은 \\(D \\times `num_layers`\\)입니다. 우리는 1개의 히든레이어를 가진 LSTM이므로 num_layers= 1 따라서 1이었습니다.\n-파라미터 설명 - \\(D\\) = 2 if bidirectional = True otherwise = 1 (양방향이면2 단방향이면1,우리는 단방향만 써왔으므로 \\(D\\) = 1) - num_layers = 히든레이어가 1개 이상인 경우(중첩된 RNN) - \\(H_{out}\\) = 히든노드의 수 - \\(N\\) : 전체시계열 데이터안에 있는 묶음의 갯수 = 전체데이터 안에 있는 시계열데이터 묶음의 갯수 = 전체데이터의 미니배치의 갯수, 미니배치안에 있는 원소들의 갯수가 아님 !! 예를들어 (1000,7)의 shape을 가진 시계열데이터를 2개의 묶음으로 나눈다면 N = batch size = 2,왜 쪼개는지는 나중에 … 공부\n-코드5 - x.shape = $(1,H_{in}) $ x.shape = \\((L,1,H_{in})\\) - h.shape = $(1,H_{out}) $ h.shape = \\((1,1,H_{out})\\)\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\n#엄밀한 방식\n_h0,_c0 = torch.zeros(1,4).to(\"cuda:0\"),torch.zeros(1,4).to(\"cuda:0\")\nlstm(x.reshape(999,1,7),(_h0.reshape(1,1,4),_c0.reshape(1,1,4)))\n\n#엄밀한 방식2\n_h0 = _c0 = torch.zeros(1,1,4).to(\"cuda:0\")\nlstm(x.reshape(999,1,7),(_h0,_c0))\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563]],\n \n         [[-0.0786, -0.1430, -0.0250,  0.1189]],\n \n         [[-0.0300, -0.2256, -0.1324,  0.1439]],\n \n         ...,\n \n         [[-0.0723,  0.0620,  0.1913,  0.2015]],\n \n         [[-0.1155,  0.0746,  0.1747,  0.2938]],\n \n         [[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))\n\n\n- 사실 _h0.shape= _c0.shape = (\\(1,H_{out}\\))에서 1은 \\(D \\times\\) numlayers입니다. - 만약 은닉계층의 수가 1개인 lstm이라면 _h0.shape = _c0.shape = \\((1,H_{out})\\) - 만약 은닉계층의 수는 1개,양방향 lstm이라면 _h0.shape = _c0.shape = \\((2,H_{out})\\) - 만약 은닉계층의 수가 3개인 (단방향)lstm이라면 _h0.shape = _c0.shape = \\((3,H_{out})\\) - 만약 은닉계층의 수가 3개,양방향 lstm이라면 _h0.shape = _c0.shape = \\((6,H_{out})\\)"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#세트2-xt.shape-nh_in-or-h_in",
    "href": "python code/IAB Deep learning/DL 12-01.html#세트2-xt.shape-nh_in-or-h_in",
    "title": "Hoyeon's Blog",
    "section": "세트2 : xt.shape = \\((N,H_{in}) or (H_{in})\\)",
    "text": "세트2 : xt.shape = \\((N,H_{in}) or (H_{in})\\)\n-코드2:초깃값 생략\n\ntorch.manual_seed(43052)\nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda\")\n\n\nxt = x[[1]]\nxt.shape\n\ntorch.Size([1, 7])\n\n\n\nlstmcell(xt)\n\n(tensor([[-0.0290, -0.1758, -0.0537,  0.0598]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>),\n tensor([[-0.0582, -0.4566, -0.1256,  0.1922]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>))\n\n\n-코드3:간단한 shape 사용\n\ntorch.manual_seed(43052)\nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\")\n\n\nxt = x[1]\nxt.shape #위의 shape과 다른것을 확인\n\ntorch.Size([7])\n\n\n\nlstmcell(xt)\n\n(tensor([-0.0290, -0.1758, -0.0537,  0.0598], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n tensor([-0.0582, -0.4566, -0.1256,  0.1922], device='cuda:0',\n        grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#세트3-hidden.shape-nh_out-or-h_out",
    "href": "python code/IAB Deep learning/DL 12-01.html#세트3-hidden.shape-nh_out-or-h_out",
    "title": "Hoyeon's Blog",
    "section": "세트3 : hidden.shape = \\((N,H_{out})\\) or \\((H_{out})\\)",
    "text": "세트3 : hidden.shape = \\((N,H_{out})\\) or \\((H_{out})\\)\n- 코드4 xt.shape = \\((1,H_{in}) \\to (H_{in})\\) ht.shape = \\((1,H_{out}) \\to (H_{out})\\) 더 간단한 차원을 가진 텐서를 입력으로 해도 무방하다.\n\ntorch.manual_seed(43052)\nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\")\n\n\nxt = x[1]\n_h0,_c0 = torch.zeros(4).to(\"cuda\"),torch.zeros(4).to(\"cuda\")\nxt.shape,_h0.shape,_c0.shape\n\n(torch.Size([7]), torch.Size([4]), torch.Size([4]))\n\n\n\nlstmcell(xt,(_h0,_c0))\n\n(tensor([-0.0290, -0.1758, -0.0537,  0.0598], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n tensor([-0.0582, -0.4566, -0.1256,  0.1922], device='cuda:0',\n        grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#똑같은-코드들-정리",
    "href": "python code/IAB Deep learning/DL 12-01.html#똑같은-코드들-정리",
    "title": "Hoyeon's Blog",
    "section": "똑같은 코드들 정리",
    "text": "똑같은 코드들 정리\n- 1은 단순한 observation의 차원이 아니다. - 네트워크가(1) 단방향 (2)배치가없는(조각이 없는) (3)중첩하지 않은(다계층이 아닌) 순환망에 한해서는 observation 처럼 생각해도 무방하다. –> 엄밀하게는 위험한 생각입니다."
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#실제구현시",
    "href": "python code/IAB Deep learning/DL 12-01.html#실제구현시",
    "title": "Hoyeon's Blog",
    "section": "실제구현시",
    "text": "실제구현시\n- 현실적으로 (1)-(3)이 아닌 조건에서는 Cell 단위로 연산을 이용할 일이 없다. (느립니다.단지 이해용) - torch.nn.RNN or torch.nn.LSTM으로 네트워크를 구성할 시 _h0,_c0의 dim을 명시할 일도 없다. - 입력시계열데이터를 배치로 나누는 경우에 대한 개념만 명확하게 잡으면 된다.배치로 나눌지 말지 나누면 어떻게 되고 안나누면 어떻게 되는지 명확하게 알자."
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#하나의-긴-시계열데이터를-배치로-나누지-않고-학습",
    "href": "python code/IAB Deep learning/DL 12-01.html#하나의-긴-시계열데이터를-배치로-나누지-않고-학습",
    "title": "Hoyeon's Blog",
    "section": "하나의 긴 시계열데이터를 배치로 나누지 않고 학습",
    "text": "하나의 긴 시계열데이터를 배치로 나누지 않고 학습\n\n배치로 나눴을 경우와 비교하기 위해 먼저 나누지 않고 학습진행.\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\nmapping = {\"!\": 0 ,\"?\":1 , \"h\":2 ,\"i\" : 3}\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda\")\nprint(x.shape)\n\ntorch.Size([17, 4])\n\n\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(4,10).to(\"cuda\")\nlinr = torch.nn.Linear(10,4).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstm.parameters()) + list(linr.parameters()))\n\n\nfor epoch in range(1000):\n    ## 1\n    hidden,(h_T,c_T) = lstm(x)\n    #hidden은 가장 깊이 있는 은닉계층의 모든 시점에서 output이므로 hidden.shape = (17,10)\n    #h_T,c_T는 모든 은닉계층에서 마지막 시점에서 output이므로 h_T(c_T).shape = c_T.shape = (1,10)\n    print(hidden.shape,h_T.shape,c_T.shape)\n    output = linr(hidden)\n    ## 2\n    loss = loss_fn(output,y)\n    ## 3\n    loss.backward()\n    ## 4\n    optimizer.step()\n    optimizer.zero_grad()\n\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\n\n\n\nimport matplotlib.pyplot as plt\nsoft = torch.nn.Softmax(dim=1)\nplt.matshow(soft(output).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x165602a6d30>"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#하나의-긴-시계열-데이터를-배치로-나눠서-학습",
    "href": "python code/IAB Deep learning/DL 12-01.html#하나의-긴-시계열-데이터를-배치로-나눠서-학습",
    "title": "Hoyeon's Blog",
    "section": "하나의 긴 시계열 데이터를 배치로 나눠서 학습",
    "text": "하나의 긴 시계열 데이터를 배치로 나눠서 학습\n시계열 데이터셋이 배치로 나뉘어져 있을 경우,파이토치에서는 \\((L,N,H_{in})\\)의 shape을 요구한다. 다음의 과정은 shape을 맞추기 위해 진행하는 과정이다.\n\ntxt1 = txt[:9]\ntxt2 = txt[9:]\n\n\ntxt1_x = txt1[:-1]\ntxt1_y = txt1[1:]\ntxt2_x = txt2[:-1]\ntxt2_y = txt2[1:]\n\n\nmapping = {\"!\":0,\"?\":1,\"h\":2,\"i\":3}\nx1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_x,mapping))).float().to(\"cuda\")\ny1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_y,mapping))).float().to(\"cuda\")\nx2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_x,mapping))).float().to(\"cuda\")\ny2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_y,mapping))).float().to(\"cuda\")\n\n\nx1.shape,y1.shape,x2.shape,y2.shape\n\n(torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]))\n\n\n\nxx = torch.stack([x1,x2],axis=1)\nyy = torch.stack([y1,y2],axis=1)\nxx.shape,yy.shape\n\n(torch.Size([8, 2, 4]), torch.Size([8, 2, 4]))\n\n\n\nxx,yy\n\n(tensor([[[0., 0., 1., 0.],\n          [0., 0., 1., 0.]],\n \n         [[0., 0., 0., 1.],\n          [0., 0., 0., 1.]],\n \n         [[1., 0., 0., 0.],\n          [0., 1., 0., 0.]],\n \n         [[0., 0., 1., 0.],\n          [0., 0., 1., 0.]],\n \n         [[0., 0., 0., 1.],\n          [0., 0., 0., 1.]],\n \n         [[1., 0., 0., 0.],\n          [0., 1., 0., 0.]],\n \n         [[0., 0., 1., 0.],\n          [0., 0., 1., 0.]],\n \n         [[0., 0., 0., 1.],\n          [0., 0., 0., 1.]]], device='cuda:0'),\n tensor([[[0., 0., 0., 1.],\n          [0., 0., 0., 1.]],\n \n         [[1., 0., 0., 0.],\n          [0., 1., 0., 0.]],\n \n         [[0., 0., 1., 0.],\n          [0., 0., 1., 0.]],\n \n         [[0., 0., 0., 1.],\n          [0., 0., 0., 1.]],\n \n         [[1., 0., 0., 0.],\n          [0., 1., 0., 0.]],\n \n         [[0., 0., 1., 0.],\n          [0., 0., 1., 0.]],\n \n         [[0., 0., 0., 1.],\n          [0., 0., 0., 1.]],\n \n         [[1., 0., 0., 0.],\n          [0., 1., 0., 0.]]], device='cuda:0'))\n\n\ntorch.stack에서 axis의 인자로 1을 넣어 \\(batch size = 2\\)인 \\((8,2,4)\\)의 shape을 가지는 텐서를 만듦\n준비단계\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(4,10).to(\"cuda\")\nlinr = torch.nn.Linear(10,4).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr = 0.1)\n\n학습단계 - 먼저 관찰하기위해서 ’hi!’가 3번반복되는 배치와 hi?가 3번반복되는 배치를 따로 분리하고 네트워크에 입력하여 학습.\n\nfor epoch in range(100):\n    ## 1\n    hidden,(h_T,c_T) = lstm(xx)\n    #hidden은 가장 깊이 있는 은닉계층의 모든 시점에서 output이므로 hidden.shape = (17,10)\n    #h_T,c_T는 모든 은닉계층에서 마지막 시점에서 output이므로 h_T(c_T).shape = c_T.shape = (1,10)\n    #print(hidden.shape,h_T.shape,c_T.shape)\n    output = linr(hidden)\n    ## 2\n    loss = loss_fn(output[:,0,:],yy[:,0,:]) + loss_fn(output[:,1,:],yy[:,1,:])\n    if epoch % 20 == 0:\n        print(epoch,loss)\n    ## 3\n    loss.backward()\n    ## 4\n    optimizer.step()\n    optimizer.zero_grad()\n\n0 tensor(2.7541, device='cuda:0', grad_fn=<AddBackward0>)\n20 tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)\n40 tensor(0.1739, device='cuda:0', grad_fn=<AddBackward0>)\n60 tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\n80 tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\nfig,ax = plt.subplots(1,2)\nax[0].matshow(soft(output[:,0,:]).to(\"cpu\").data,cmap=\"bwr\",vmin=-1,vmax=1)\nax[1].matshow(soft(output[:,1,:]).to(\"cpu\").data,cmap=\"bwr\",vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x16563cda370>\n\n\n\n\n\n\nhidden,_ = lstm(x)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap=\"bwr\",vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x165645e1df0>\n\n\n\n\n\nhi?와 hi!인 경우를 분리하여 각각 네트워크를 학습시킬 경우, 분리된 데이터에 대해 잘 예측함\n그런데 hi!와 hi?가 분리되어 있지 않은 전체데이터를 입력으로 줬을때는 전혀 예측을 하지 못하는 모습… 이는 hi!와 hi?를 따로따로 분리하여 학습해서 !가 나오면 hi!만 반복되고 ?가 나오면 hi?만 반복되는 것으로 학습했기 때문이다. 이렇게 학습을 시키면 hi!가 3번나온 뒤 hi?가 3번나오는 데이터를 모델링하지 못한다.연결,흐름,경향은 학습하지 못했기 때문이다.\n\nyy.shape,y.shape,xx.shape,x.shape\n\n(torch.Size([8, 2, 4]),\n torch.Size([17, 4]),\n torch.Size([8, 2, 4]),\n torch.Size([17, 4]))"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#재미있는-실험",
    "href": "python code/IAB Deep learning/DL 12-01.html#재미있는-실험",
    "title": "Hoyeon's Blog",
    "section": "재미있는 실험",
    "text": "재미있는 실험\n- x1만 배운네트워크에 x2를 넣는다면? - 즉, hi!만 배운 네트워크에 hi?데이터에 대해서 예측하라 하면?\n준비단계\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(4,10).to(\"cuda\")\nlinr = torch.nn.Linear(10,4).to(\"cuda\")\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer= torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoch in range(100):\n    ##1 output\n    hidden,(h_T,c_T) = lstm(x1)\n    output = linr(hidden)\n    ##2 loss\n    loss = loss_fn(output,y1)\n    ##3 derivative\n    loss.backward()\n    ##4 update & clean\n    optimizer.step()\n    optimizer.zero_grad()\n\n\nhidden,(h_T,c_T) = lstm(x2)\nplt.matshow(soft(linr(hidden)).data.to(\"cpu\"),cmap=\"bwr\",vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x16566df0760>\n\n\n\n\n\nhi?를 넣어도 hi!로 예측함! 만약 x2만 넣은 경우도 마찬가지 결과가 나온다(생략)\n정리 : 하나의 긴 시계열 데이터를 n개의 배치들로 나누어서 학습할 수 있다. 다만 이러한 경우에 전체시계열데이터의를 네트워크에 학습시키는 것이 아니기 때문에 전체적인 패턴,흐름,연결은 학습하지 못한다."
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#왜-배치로-나눠서-시계열-데이터를-학습해야-하는가",
    "href": "python code/IAB Deep learning/DL 12-01.html#왜-배치로-나눠서-시계열-데이터를-학습해야-하는가",
    "title": "Hoyeon's Blog",
    "section": "왜 배치로 나눠서 시계열 데이터를 학습해야 하는가?",
    "text": "왜 배치로 나눠서 시계열 데이터를 학습해야 하는가?\n\n속도가 빠르다.\n연결,흐름을 파악하지 않아도 되는 독립적인 시계열 데이터셋이 있는 경우도 있다.(예를들어,독립적인 여러개의 댓글이 있는 시계열 데이터셋의 경우)\n이해하지 못했다 … ㅜㅜ\n\n참고 배치사이즈는 배치가 몇개인가? or 배치안에 있는 원소가 몇개인가? 두가지 서로다른 의미가 있다. document별로 알아서 해석해야 한다."
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#똑같은-코드들-fastaipytorch",
    "href": "python code/IAB Deep learning/DL 12-01.html#똑같은-코드들-fastaipytorch",
    "title": "Hoyeon's Blog",
    "section": "똑같은 코드들 fastai,pytorch",
    "text": "똑같은 코드들 fastai,pytorch\n\ntxt = (['one',',','two',',','three',',','four',',','five',',']*100)[:-1]\n\n\nmapping = {',':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5} \nmapping\n\n{',': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5}\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[0:5], txt_y[0:5]\n\n(['one', ',', 'two', ',', 'three'], [',', 'two', ',', 'three', ','])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda\")"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#fastai-사용",
    "href": "python code/IAB Deep learning/DL 12-01.html#fastai-사용",
    "title": "Hoyeon's Blog",
    "section": "fastai 사용",
    "text": "fastai 사용\n\nfrom fastai.text.all import *\n#import pytorch_lightning as pl \ntr_ds = torch.utils.data.TensorDataset(x,y) \nval_ds = torch.utils.data.TensorDataset(x,y) #dummy,validation이 있다고 가정\ntr_ldr = torch.utils.data.DataLoader(tr_ds,batch_size=998) #한개의 배치안에 998개의 원소가 있도록\nval_ldr = torch.utils.data.DataLoader(val_ds,batch_size=998)\ndls = DataLoaders(tr_ldr,val_ldr)\n\n\nclass MyLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(43052)\n        self.lstm = torch.nn.LSTM(6,20)\n        self.linr = torch.nn.Linear(20,6) \n    def forward(self,x):\n        _water = torch.zeros(1,20).to(\"cuda:0\")\n        hidden, (hT,cT) =self.lstm(x,(_water,_water))\n        output = self.linr(hidden)\n        return output         \n\n\nnet = MyLSTM().to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\n#optimizer 생략(fastai 구현시)\nlrnr = Learner(dls,net,loss_fn,lr = 0.1)\n\n\nlrnr.fit(30)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.762846\n      1.502211\n      00:00\n    \n    \n      1\n      1.631212\n      1.620583\n      00:00\n    \n    \n      2\n      1.627597\n      1.443686\n      00:00\n    \n    \n      3\n      1.580216\n      1.368762\n      00:00\n    \n    \n      4\n      1.536200\n      1.307310\n      00:00\n    \n    \n      5\n      1.496099\n      1.216339\n      00:00\n    \n    \n      6\n      1.453670\n      1.113821\n      00:00\n    \n    \n      7\n      1.408125\n      1.019931\n      00:00\n    \n    \n      8\n      1.361426\n      0.941434\n      00:00\n    \n    \n      9\n      1.315507\n      0.884033\n      00:00\n    \n    \n      10\n      1.272201\n      0.848489\n      00:00\n    \n    \n      11\n      1.232838\n      0.826641\n      00:00\n    \n    \n      12\n      1.197666\n      0.814630\n      00:00\n    \n    \n      13\n      1.166570\n      0.809186\n      00:00\n    \n    \n      14\n      1.139229\n      0.805969\n      00:00\n    \n    \n      15\n      1.115098\n      0.798611\n      00:00\n    \n    \n      16\n      1.093322\n      0.779956\n      00:00\n    \n    \n      17\n      1.072764\n      0.746476\n      00:00\n    \n    \n      18\n      1.052292\n      0.703937\n      00:00\n    \n    \n      19\n      1.031332\n      0.643301\n      00:00\n    \n    \n      20\n      1.008886\n      0.574666\n      00:00\n    \n    \n      21\n      0.984684\n      0.496858\n      00:00\n    \n    \n      22\n      0.958432\n      0.423367\n      00:00\n    \n    \n      23\n      0.930580\n      0.360229\n      00:00\n    \n    \n      24\n      0.901813\n      0.301070\n      00:00\n    \n    \n      25\n      0.872408\n      0.251317\n      00:00\n    \n    \n      26\n      0.842863\n      0.199397\n      00:00\n    \n    \n      27\n      0.813075\n      0.151757\n      00:00\n    \n    \n      28\n      0.783244\n      0.109921\n      00:00\n    \n    \n      29\n      0.753616\n      0.079872\n      00:00\n    \n  \n\n\n\n\nsoft(lrnr.model(x)).data.to(\"cpu\").numpy().round(3)\n\narray([[0.998, 0.   , 0.002, 0.   , 0.   , 0.   ],\n       [0.   , 0.018, 0.514, 0.097, 0.096, 0.275],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       ...,\n       [0.001, 0.002, 0.025, 0.001, 0.938, 0.033],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.066, 0.049, 0.005, 0.079, 0.8  ]], dtype=float32)"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#torch사용",
    "href": "python code/IAB Deep learning/DL 12-01.html#torch사용",
    "title": "Hoyeon's Blog",
    "section": "torch사용",
    "text": "torch사용\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(6,20).to(\"cuda:0\") \nlinr = torch.nn.Linear(20,6).to(\"cuda:0\") \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n#동일함을 보이기 위해 fastai의 Adam사용\n\n\nfor epoc in range(10):\n    ## 1 \n    hidden, _ = lstm(x)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()     \n\n\nhidden, _ = lstm(x)\noutput = linr(hidden) \nsoft(output).data.to(\"cpu\").numpy().round(3)\n\narray([[0.935, 0.009, 0.015, 0.011, 0.016, 0.014],\n       [0.133, 0.164, 0.242, 0.172, 0.141, 0.147],\n       [0.982, 0.003, 0.004, 0.003, 0.004, 0.003],\n       ...,\n       [0.122, 0.171, 0.242, 0.174, 0.146, 0.144],\n       [0.984, 0.003, 0.004, 0.002, 0.004, 0.003],\n       [0.119, 0.172, 0.244, 0.175, 0.145, 0.145]], dtype=float32)"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#human-numbers-100",
    "href": "python code/IAB Deep learning/DL 12-01.html#human-numbers-100",
    "title": "Hoyeon's Blog",
    "section": "human numbers 100",
    "text": "human numbers 100\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/IV.%20RNN/2022-11-25-human_numbers_100.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      text\n    \n  \n  \n    \n      0\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      2\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      3\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      4\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      ...\n      ...\n    \n    \n      1995\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1996\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1997\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1998\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1999\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n  \n\n2000 rows × 1 columns\n\n\n\n\n생략 .. 귀찮음\n\ndls = TextDataLoaders.from_df(df,is_lm=True,seq_len=5,text_col='text')\ndls.show_batch()\n\nDue to IPython and Windows limitation, python multiprocessing isn't available now.\nSo `n_workers` has to be changed to 0 to avoid getting stuck\n\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos one , two ,\n      one , two , three\n    \n    \n      1\n      hundred xxbos one , two\n      xxbos one , two ,\n    \n    \n      2\n      one hundred xxbos one ,\n      hundred xxbos one , two\n    \n    \n      3\n      , one hundred xxbos one\n      one hundred xxbos one ,\n    \n    \n      4\n      nine , one hundred xxbos\n      , one hundred xxbos one\n    \n    \n      5\n      ninety nine , one hundred\n      nine , one hundred xxbos\n    \n    \n      6\n      , ninety nine , one\n      ninety nine , one hundred\n    \n    \n      7\n      eight , ninety nine ,\n      , ninety nine , one\n    \n    \n      8\n      ninety eight , ninety nine\n      eight , ninety nine ,"
  },
  {
    "objectID": "python code/pytorch 구현/pytorch lstm hi느낌표hi물음표 모델링.html",
    "href": "python code/pytorch 구현/pytorch lstm hi느낌표hi물음표 모델링.html",
    "title": "pytorch lstm 구현하기",
    "section": "",
    "text": "pytorch-RNN으로 hi?hi!가 계속해서 반복되는 sequencedata를 모델링했습니다."
  },
  {
    "objectID": "python code/pytorch 구현/pytorch lstm hi느낌표hi물음표 모델링.html#setting",
    "href": "python code/pytorch 구현/pytorch lstm hi느낌표hi물음표 모델링.html#setting",
    "title": "pytorch lstm 구현하기",
    "section": "setting",
    "text": "setting\n\nimport torch\n\n\ndef f(txt,mapping):\n    return [mapping[chr] for chr in txt]"
  },
  {
    "objectID": "python code/pytorch 구현/pytorch lstm hi느낌표hi물음표 모델링.html#data",
    "href": "python code/pytorch 구현/pytorch lstm hi느낌표hi물음표 모델링.html#data",
    "title": "pytorch lstm 구현하기",
    "section": "data",
    "text": "data\n\ntxt = list('hi!')*3 + list('hi?')*3 \ntxt_x = txt[:-1] \ntxt_y = txt[1:] \n\n\nmapping = {'!':0, '?':1, 'h':2, 'i':3} \nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\n\n\ntxt1= txt[:9]\ntxt2= txt[9:]\n\n\ntxt1_x = txt1[:-1] \ntxt1_y = txt1[1:] \ntxt2_x = txt2[:-1] \ntxt2_y = txt2[1:] \n\n\nmapping = {'!':0, '?':1, 'h':2, 'i':3} \nx1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_x,mapping))).float().to(\"cuda:0\")\ny1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_y,mapping))).float().to(\"cuda:0\")\nx2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_x,mapping))).float().to(\"cuda:0\")\ny2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_y,mapping))).float().to(\"cuda:0\")\nx1.shape,y1.shape,x2.shape,y2.shape\n\n(torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]))\n\n\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(4,10).to(\"cuda\")\nlinr = torch.nn.Linear(10,4).to(\"cuda\")\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstm.parameters()) + list(linr.parameters()),lr = 0.1)\n\n\nxx = torch.stack([x1,x2],axis=1)\nyy = torch.stack([y1,y2],axis=1)\nxx.shape, yy.shape\n\n(torch.Size([8, 2, 4]), torch.Size([8, 2, 4]))\n\n\n\nfor epoc in range(100):\n    ##1 output\n    hidden,_ = lstm(xx)\n    output = linr(hidden)\n    \n    ##2 loss\n    loss = 0\n    \"\"\"\n    한참해맨 부분 ... 그냥 무지성 loss(output,yy)해서 틀림 ...\n    파이토치 cross entropy 부분 정리\n    \"\"\"\n    for batch_num in range(output.shape[1]):\n        batch_out = output[:,batch_num,:]\n        batch_y = yy[:,batch_num,:]\n        #print(batch_out.shape,batch_y.shape)\n        loss += loss_fn(batch_out,batch_y)\n    ##3 derivative\n    loss.backward()\n    if epoc % 10 == 0:\n        print(loss)\n    ##4 \n    optimizer.step()\n    optimizer.zero_grad()\n    \n\ntensor(2.7541, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.4173, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1739, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1737, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\nimport matplotlib.pyplot as plt\nsoft = torch.nn.Softmax(dim=1)\n\nfig , ax = plt.subplots(1,2) \nax[0].matshow(soft(output[:,0,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\nax[1].matshow(soft(output[:,1,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x1f29cd1f400>\n\n\n\n\n\n\nhidden,_ = lstm(x)\nplt.matshow(soft(linr(hidden)).data.to(\"cpu\"),cmap=\"bwr\",vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x1f29e4188e0>\n\n\n\n\n\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(4,10).to(\"cuda\")\nlinr = torch.nn.Linear(10,4).to(\"cuda\")\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstm.parameters()) + list(linr.parameters()),lr = 0.1)\n\n\nfor epoc in range(200):\n    ##1 output\n    hidden,_ = lstm(xx)\n    output = linr(hidden)\n    #print(output.shape,yy.shape)\n    #print(output[:,0,:].shape,yy[:,0,:].shape)\n    ##2 loss\n    loss = loss_fn(output[:,0,:],yy[:,0,:]) + loss_fn(output[:,1,:],yy[:,1,:])\n    ##3\n    loss.backward()\n    if epoc % 10 == 0:\n        print(loss)\n    ##4\n    optimizer.step()\n    optimizer.zero_grad()\n\ntensor(2.7541, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.4173, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1739, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1737, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\nimport matplotlib.pyplot as plt\nsoft = torch.nn.Softmax(dim=1)\n\nfig , ax = plt.subplots(1,2) \nax[0].matshow(soft(output[:,0,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\nax[1].matshow(soft(output[:,1,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x1f2a16e8ee0>\n\n\n\n\n\n\nhidden,_ = lstm(x)\nplt.matshow(soft(linr(hidden)).data.to(\"cpu\"),cmap=\"bwr\",vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x1f2a18ec9d0>"
  },
  {
    "objectID": "python code/quarto/Authoring.html",
    "href": "python code/quarto/Authoring.html",
    "title": "Quarto Document",
    "section": "",
    "text": "모든 형식을 작성할때 yaml format을 가장 상단에 작성한다. 들어갈 수 있는 기본적인 것들은 아래와 같다.\n\n\ntitle,author : 모든 문서에 다 들어가는 메타데이타\n\n\n\ntoc(table of contents) : 목차 number-sections : 섹션마다 번호 부여 highlight-style : ??\n\n\n\n다양한 포맷으로 한번에 랜더링할 수 있도록 형식지정 가능 밑에 코드 참조 pdf에 마진을 변경함 docx 옵션은 기본값들만 사용\n\n\n\nquarto render Authoring.ipynb format에 정해놓은 형식 모두 랜더링\n아래는 랜더링한 것들을 보여주는 코드 quarto preview Authoring.ipynb –to html quarto preview Authoring.ipynb –to pdf quarto preview Authoring.ipynb –to docx"
  },
  {
    "objectID": "python code/quarto/Authoring.html#colors",
    "href": "python code/quarto/Authoring.html#colors",
    "title": "Quarto Document",
    "section": "0.2 Colors",
    "text": "0.2 Colors\n\nRed\nGreen\nBlue"
  },
  {
    "objectID": "python code/quarto/Authoring.html#shapes",
    "href": "python code/quarto/Authoring.html#shapes",
    "title": "Quarto Document",
    "section": "0.3 Shapes",
    "text": "0.3 Shapes\n\nSquare\nCircle\nTriangle"
  },
  {
    "objectID": "python code/quarto/Authoring.html#textures",
    "href": "python code/quarto/Authoring.html#textures",
    "title": "Quarto Document",
    "section": "0.4 Textures",
    "text": "0.4 Textures\n\nSmooth\nBumpy\nFuzzy"
  },
  {
    "objectID": "python code/quarto/Authoring.html#수식도-적을-수-있음",
    "href": "python code/quarto/Authoring.html#수식도-적을-수-있음",
    "title": "Quarto Document",
    "section": "0.5 수식도 적을 수 있음",
    "text": "0.5 수식도 적을 수 있음\nE = mc^{2} E = mc^{2}"
  },
  {
    "objectID": "python code/quarto/Authoring.html#cross-references",
    "href": "python code/quarto/Authoring.html#cross-references",
    "title": "Quarto Document",
    "section": "1.1 Cross References",
    "text": "1.1 Cross References"
  },
  {
    "objectID": "python code/quarto/Authoring.html#overview",
    "href": "python code/quarto/Authoring.html#overview",
    "title": "Quarto Document",
    "section": "1.2 Overview",
    "text": "1.2 Overview\nSee Figure 1 in Section 1.3 for a demonstration of a simple plot See Equation 1 to better understand std"
  },
  {
    "objectID": "python code/quarto/Authoring.html#sec-plot",
    "href": "python code/quarto/Authoring.html#sec-plot",
    "title": "Quarto Document",
    "section": "1.3 Plot",
    "text": "1.3 Plot\n\n@sec-plot을 누르면 {#sec-plot}이 있는 위치로 화면이동(참조)\n@fig-simple을 누르면 #| label: fig-simple이 있는 화면이동(참조)\n단순히 #만 사용하면 피규어나 수식과 같은 것들 끝에 인덱스를 생성해줌.std참고\n\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n\n\n\n\nFigure 1: Simple plot"
  },
  {
    "objectID": "python code/quarto/Authoring.html#sec-equation",
    "href": "python code/quarto/Authoring.html#sec-equation",
    "title": "Quarto Document",
    "section": "1.4 Equation",
    "text": "1.4 Equation\n\nstd = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N(x_i - \\overline{x})^2}\n\\tag{1}\n\n\n\n\n\n\nNote\n\n\n\ntest note"
  },
  {
    "objectID": "python code/quarto/Computations.html",
    "href": "python code/quarto/Computations.html",
    "title": "Tutorial: Computations",
    "section": "",
    "text": "코드가 표시되는 것을 막음 execute: echo: false\n기본적으로 표시 안되게 하고 특정 코드만 표시하려면? 특정코드셀 맨 윗줄에 밑에꺼 붙이면 됨 #| echo: true\n코드 접기기능 echo옵션먼저 지우고 시작 밑에 코드 yaml 포맷에 입력 format: html: code-fold: true\n\nNumpy\n\n\nCode\nimport numpy as np\na = np.arange(15).reshape(3,5)\na\n\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\n\n\n\n\nMatplotlib\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nfig.set_size_inches(12,7)\nx = np.arange(10)\ny = 2.5 * np.sin(x / 20 * np.pi)\nyerr = np.linspace(0.05, 0.2, 10)\n\nplt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')\nplt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')\nplt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,\n             label='uplims=True, lolims=True')\n\nupperlimits = [True, False] * 5\nlowerlimits = [False, True] * 5\nplt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,\n             label='subsets of uplims and lolims')\n\nplt.legend(loc='lower right')\nplt.show(fig)\n\n\n\n\n\nFigure 1: Errorbar limit selection\n\n\n\n\n\n\nPlotly\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\ngapminder = px.data.gapminder()\ngapminder2007 = gapminder.query(\"year == 2007\")\nfig = px.scatter(gapminder2007, \n                 x=\"gdpPercap\", y=\"lifeExp\", color=\"continent\", \n                 size=\"pop\", size_max=60,\n                 hover_name=\"country\")\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n이 외에 Multiple Figure에 관한 Tutorial이 있지만 생략"
  },
  {
    "objectID": "python code/quarto/Tutorial Hello, Quarto.html",
    "href": "python code/quarto/Tutorial Hello, Quarto.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "Polar Axis\nFor a demonstration of a line plot on a polar axis,see Figure 1  direct change\n\n#YAML로 쓰인 셀옵션은 #| 라는 접두어를 통해서 활용됨. 여기서는 피규어를 가리키는 기능을 함.\n#label : 이 코드셀에 fig-polar라는 이름을 붙임. 위의 마크다운의 @과 연동되어 이 피규어를 가리키게 함. 랜더링 시에는 @fig-pollar는 figure1으로 표시됨\n#fig-cap : figure의 하단에 제목을 달아줌\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0,2,0.01)\ntheta = 2 * np.pi * r\nfig,ax = plt.subplots(\n    subplot_kw = {'projection' : 'polar'}\n)\nax.plot(theta,r)\nax.set_rticks([0.5,1,1.5,2])\nax.grid(True)\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis"
  }
]