[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "전북대학교 IT응용시스템 공학과 신호연 sinhoyeon0514@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Hoyeon's Blog",
    "section": "",
    "text": "최대가능도 추정법(MLE)\n\n\n\n\n\n\n\nprobability&statistics\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2022\n\n\n신호연\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n확률분포에서 ;와|의 사용\n\n\n\n\n\n\n\nprobability&statistics\n\n\n\n\n\n\n\n\n\n\n\nDec 27, 2022\n\n\n신호연\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n딥러닝(2) - 로지스틱회귀\n\n\n\n\n\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\n\nDec 26, 2022\n\n\n신호연\n\n\n\n\n\n\n  \n\n\n\n\n파이썬 - 변수,할당문,인터닝\n\n\n\n\n\n\n\nPython\n\n\n\n\n\n\n\n\n\n\n\nDec 25, 2022\n\n\n신호연\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\npytorch로 Rnn구현하기\n\n\n\n\n\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\n\nDec 24, 2022\n\n\n신호연\n\n\n\n\n\n\n  \n\n\n\n\n딥러닝(1) - 선형회귀\n\n\n\n\n\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\n신호연\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html",
    "href": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html",
    "title": "딥러닝(1) - 선형회귀",
    "section": "",
    "text": "선형회귀에 대해서 정리한 글입니다."
  },
  {
    "objectID": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#population-regression-model",
    "href": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#population-regression-model",
    "title": "딥러닝(1) - 선형회귀",
    "section": "population regression model",
    "text": "population regression model\n\\[\nY = w_0 + w_1X_1  \\dots + w_mX_m + \\epsilon\n\\tag{1}\\] 위와 모집단의 확률변수사이의 관계를 선형이라고 가정한 모형을 모형을 population regression model이라고 합니다. population regression model은 아주 작은 오차(error)를 포함합니다."
  },
  {
    "objectID": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#sample-regression-modelvector-form",
    "href": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#sample-regression-modelvector-form",
    "title": "딥러닝(1) - 선형회귀",
    "section": "sample regression model(vector form)",
    "text": "sample regression model(vector form)\n위와 같은 regression model로부터 sampling n개의 표본을 얻으면 다음과 같습니다. \\[\ny_1 = w_0 + w_1x_{11} + \\dots + w_mx_{1m} + \\epsilon_1\n\\] \\[\ny_2 = w_0 + w_1x_{21} + \\dots + w_mx_{2m} + \\epsilon_2\n\\] \\[\n\\vdots\n\\] \\[\ny_n = w_0 + w_1x_{n1} + \\dots +w_mx_{nm} + \\epsilon_n\n\\] \n샘플을 벡터-행렬로 표현해서 좀 더 간단히 나타낼 수 있습니다. \\[\n\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\dots &x_{1m}\\\\\n1 & x_{21} & x_{22} & \\dots &x_{2m}\\\\\n1 & x_{31} & x_{32} & \\dots &x_{3m}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n1 & x_{n1} & x_{n2} & \\dots & x_{nm}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nw_0\\\\\nw_1\\\\\nw_2\\\\\n\\vdots\\\\\nw_m\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\epsilon_0\\\\\n\\epsilon_1\\\\\n\\epsilon_2\\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n\\] \\[\n\\Longleftrightarrow\n\\bf{y} = \\bf{X}\\bf{W} + \\bf{\\epsilon}\n\\tag{2}\\]  위와 같이 샘플링한 표본을 나타내는 모형을 sample regression model이라고 합니다.\n\n\nText(0.5, 1.0, 'n=200')\n\n\n\n\n\n\\(w_1=1,w_0=0\\)인 population regression model로부터 200개의 표본을 추출하여 시각화하면 다음과 같습니다. 선형회귀는 위와 같이 표본만 주어질때(given), 우리가 모르는 \\(w_1,w_0\\)를 추정하는 것입니다. 표본으로부터 \\(\\bf{W}\\)를 추정할 수 있다면 두 변수사이의 관계가 이렇겠구나라고 알 수 있게 됩니다.\n\n\n\n\n\n\nNote\n\n\n\n실제문제에서 population regression model의 가중치 \\(\\bf{w} = (w_0,w_,1,...,w_m)\\)는 정확히 알 수 없습니다. 우리는 표본을 통해서 가중치를 추정할 뿐입니다. 위에서는 샘플데이터를 만들기 위해 어쩔 수 없이 알게되었지만 실제문제에서는 알 수 없습니다."
  },
  {
    "objectID": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#선형회귀의-loss-function",
    "href": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#선형회귀의-loss-function",
    "title": "딥러닝(1) - 선형회귀",
    "section": "선형회귀의 Loss function",
    "text": "선형회귀의 Loss function\n2번에서 구한 추정값 \\(\\hat{\\bf{W}}\\)이 얼마나 틀린지,부정확한지 알려주는 함수를 Loss function 또는 Cost function이라고 합니다. 선형회귀에서의 Loss function은 일반적으로 MSE를 사용하며 주어진 샘플에서 잔차(residual,\\(\\hat{y}_i-y\\))들을 전부 제곱하여 더한 값입니다.\n(Loss function) \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2} = \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}}) = \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\)\nMSE와 같은 Loss function은 우리의 추정이 얼마나 틀렸는지를 나타내는 \\(\\hat{\\bf{W}}\\)에 대한 함수입니다. 그러므로, loss function을 가장 최소화 하는 \\(\\bf{\\hat{W}}\\)을 찾아내면 확률변수사이의 선형관계인 \\(\\bf{W}\\)를 알아낼 수 있습니다.\n\n\nText(110, 15, 'residual')"
  },
  {
    "objectID": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#parameter-update",
    "href": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#parameter-update",
    "title": "딥러닝(1) - 선형회귀",
    "section": "Parameter update",
    "text": "Parameter update\nn개의 독립변수를 가지는 다변수 스칼라 함수에 대한 Gradient는 수학적으로 다음과 같습니다.\n\\(\\nabla_{X}{f(x_1,x_2,...,x_n)} = (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\dots,\\frac{\\partial f}{\\partial x_n})\\) 다변수 스칼라 함수에 그레디언트를 취하면 벡터입니다.그러므로,그레디언트를 벡터(다변수)를 입력했을 때,벡터를 출력으로 하는 벡터함수라고 생각해도 무방합니다.중요한 사실은 임의의 공간상의 임의의 point \\(X\\)에서 스칼라함수에 대한 gradient of f = \\(-\\nabla_{X}{f}\\) 방향은 스칼라함수가 가장 급격하게 감소하는 방향이라는 사실입니다.(증명생략)\n위의 사실에 의하면,우리는 임의의 \\(\\hat{\\bf{W}}\\)에서 Loss function이 가장 급격하게 감소하는 방향을 찾을 수 있습니다. 그러므로 감소하는 방향을 찾고 이동하고 감소하는 방향을 찾고 이동하고 반복하다보면… 궁극적인 목적인 틀린정도를 최소화하는 즉,Loss function값이 가장 작은 \\(\\hat{\\bf{W}}\\)를 찾을 수 있습니다. \\(\\bf\\hat{W}\\)를 수정하는 구체적인 수식은 다음과 같습니다.\n(Gradient descent parameter update) \\(\\hat{\\bf{W}}_{t} = \\hat{\\bf{W}}_{t-1} - \\alpha\\times\\nabla_{W}{L}\\)\n\\(\\hat{\\bf{W}}_{t-1}\\)은 수정되기전의 가중치(벡터)이며 \\(\\hat{\\bf{W}_{t}}\\)는 파라미터를 한번 업데이트 한 후의 가중치(벡터)입니다. \\(t-1\\)의 \\(\\hat{\\bf{W_{t-1}}}\\)에 \\(-\\alpha\\times\\nabla_{W}{L}\\)를 더해줌으로서 \\(\\hat{\\bf{W}}_{t-1}\\)은 loss function이 가장 급격히(많이)감소하는 방향으로 이동하며 \\(\\hat{\\bf{W}}_{t}\\)가 됩니다. \\(\\alpha\\)는 학습률(learning rate)입니다. \\(\\hat{\\bf{W}}_{t-1}\\)과 곱해져서 얼마나 많이 또는 적게 움직일지를 결정합니다. 한번에 얼마나 이동할지에 비유한 “보폭”으로 생각할 수 있습니다.\n요약하자면, 경사하강법을 통하여 위와 같이 가중치\\(\\hat{\\bf{W}}\\)를 재귀적으로 업데이트 하면 loss function \\(L\\)이 가장 최소가 되는 지점의 \\(\\hat{\\bf{W}}\\)를 찾을 수 있습니다."
  },
  {
    "objectID": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#mse에-대한-더-상세한-전개",
    "href": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#mse에-대한-더-상세한-전개",
    "title": "딥러닝(1) - 선형회귀",
    "section": "MSE에 대한 더 상세한 전개",
    "text": "MSE에 대한 더 상세한 전개\nMSE를 더 상세히 전개하면 다음과 같습니다. \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2}\\) \\(= \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}})\\) \\(= \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\) \\(= \\frac{1}{n}(\\bf{y^T - \\hat{\\bf{W}}^{T}\\bf{X}^{T})(\\bf{y} - \\bf{X}\\bf{\\hat{W}}})\\) \\(= \\frac{1}{n}(\\bf{y^Ty-y^TX\\hat{W}} - \\hat{W}X^Ty + \\hat{W}^TX^TX\\hat{W})\\)\n여기서 \\(\\bf{y^TX\\hat{W}} \\in \\bf{R}^{1 \\times 1}\\) 이므로 \\(\\bf{y^TX\\hat{W}} = (\\bf{y^TX\\hat{W}})^T = (\\bf{\\hat{W}X^Ty})\\)가 성립합니다. 그러므로 MSE를 정리하면 다음과 같습니다. (MSE) \\(MSE = \\frac{1}{n}(\\bf{y^Ty -2\\hat{W}X^Ty + \\hat{W}^TX^TX\\hat{W}})\\)"
  },
  {
    "objectID": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#gradient-descent에-대한-더-상세한-전개loss-mse일-경우",
    "href": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#gradient-descent에-대한-더-상세한-전개loss-mse일-경우",
    "title": "딥러닝(1) - 선형회귀",
    "section": "Gradient Descent에 대한 더 상세한 전개(\\(Loss\\) = MSE일 경우)",
    "text": "Gradient Descent에 대한 더 상세한 전개(\\(Loss\\) = MSE일 경우)\n(Gradient of MSE) \\(\\nabla{L} = MSE\\) \\(= \\bf{\\frac{1}{n}\\frac{\\partial}{\\partial \\hat{W}}(\\bf{y^Ty - 2\\hat{W}^TX^T + \\hat{W}^TX^TX\\hat{W}})}\\) \\(= \\bf{\\frac{1}{n}}(\\bf{\\frac{\\partial}{\\partial \\hat{W}}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2\\hat{W}^{T}X^{T}y + \\frac{\\partial}{\\partial\\hat{W}}\\hat{W}^{T}X^{T}X\\hat{W})\\) \\(= \\bf{\\frac{1}{n}(\\frac{\\partial}{\\partial \\hat{W}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2y^TX\\hat{W} + \\frac{\\partial}{\\partial\\hat{W}}\\hat{W}^TX^TX\\hat{W})}\\) \\(= \\bf{\\frac{1}{n}[0 - 2X^Ty + (X^TX + X^TX)\\hat{W}]}\\) \\(= \\bf{\\frac{2}{n}X^T(X\\hat{W} - y)}\\)\n(parameter update) \\(\\bf{\\hat{W}_{t} = \\hat{W}_{t-1} - \\alpha \\times \\frac{2}{n}X^T(X\\hat{W} - y)}\\)"
  },
  {
    "objectID": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#결과해석",
    "href": "posts/deep learning/Deep learning theory/(1) Linear Regression/Linear Regression.html#결과해석",
    "title": "딥러닝(1) - 선형회귀",
    "section": "결과해석",
    "text": "결과해석\n200개의 샘플로부터 \\(\\bf{w}\\)를 추정하여 \\(\\hat{\\bf{w}}= (0.125,0.969)\\)를 얻었습니다. population regression model의 \\({\\bf{w}} = (w_0,w_1) = (0,1)\\)을 올바르게 추정했음을 알 수 있습니다. 아주 약간의 차이가 존재하는데 이 차이는 모집단에서 샘플을 더 얻거나 더 세밀하게 업데이트하면 최소화할 수 있습니다.\n\n#plt.title(\"w_1 : {} // w_0: {}\".format(round(W_hat[1].tolist()[0],3),round(W_hat[0].tolist()[0],3)))\nplt.title(\"Linear Regression\")\ntext=f\"What = ({round(t[0].tolist()[0],3)},{round(t[1].tolist()[0],3)})\"\nplt.plot(X[:,1],y,\"bo\",alpha=0.5)\nplt.plot(X[:,1],X@W_hat,\"r--\")\nplt.gca().axes.xaxis.set_visible(False)\nplt.gca().axes.yaxis.set_visible(False)\nplt.title(text)\n\nText(0.5, 1.0, 'What = (-0.125,0.969)')"
  },
  {
    "objectID": "posts/deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html",
    "href": "posts/deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html",
    "title": "딥러닝(2) - 로지스틱회귀",
    "section": "",
    "text": "로지스틱회귀에 대해서 정리한 글입니다."
  },
  {
    "objectID": "posts/deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#기댓값에-대한-고찰",
    "href": "posts/deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#기댓값에-대한-고찰",
    "title": "딥러닝(2) - 로지스틱회귀",
    "section": "기댓값에 대한 고찰",
    "text": "기댓값에 대한 고찰\n기댓값은 실험 또는 시행을 무한히 반복했을때 확률변수가 취하는 값의 평균으로(또는 샘플링된 값의 평균) 기대되는 값입니다. 확률변수가 베르누이 분포를 따르는 경우 확률변수에 대한 기댓값과 베르누이분포의 모수가 같은 값을 가집니다. 그러므로,만약에 주어진 샘플데이터로부터 베르누이분포의 모수를 적절히 추정할 수 있다면 모수는 기댓값이므로 주어진 조건하에서 실험 또는 시행을 무한히 반복할 경우 확률변수가 1인사건과 0인사건중 어떤 사건이 더 많이 발생할지 아는것이므로 많이 발생한 사건 또는 분류된 클래스로 종속변수 Y의 값을 결정하는 것은 타당해보입니다. - e.g.\n\n\\(E\\,[y_i]\\, = \\hat{p_i}<0.5\\) => 무한히 실행했을때 0인 경우가 더 많을 것임 => 관측치를 0으로 예측 \n\\(E\\,[y_i]\\, = \\hat{p_i}\\geq0.5\\)=>무한히 실행했을때 1인 경우가 더 많을 것임 => 관측치를 1로 예측"
  },
  {
    "objectID": "posts/deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#concept",
    "href": "posts/deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#concept",
    "title": "딥러닝(2) - 로지스틱회귀",
    "section": "concept",
    "text": "concept\n선형회귀에서 추정하고자하는 변수\\(y\\)는 \\(x_0,x_1,...,x_m\\)과 \\(w_0,w_1,...,w_m\\)과의 linear combination표현되었으며 각각의 \\(w\\)는 독립변수 각각의 독립변수\\(x\\)가 종속변수\\(y\\)에 영향을 미치는 정도로 해석할 수 있었습니다. 로지스틱회귀에서도 선형회귀에서의 아이디어를 핵심아이디어를 가지고와서 추정하고자 하는 모수\\(p_i\\)를 \\(x_0,x_1,...,x_m\\)과 \\(w_0,w_1,...,w_m\\)의 linear combination로 표현하고자 합니다.\n선형회귀의 아이디어(linear combination) + 모수에 대한 표현이라는 조건을 만족하기 위해서 최종적인 식은 다음과 조건을 만족해야 것입니다. - \\((x_0,x_1,...,x_m)\\,,(w_0,w_1,..,w_m)\\)의 linear combination 식에 있어야 함. - linearcombination = 모수(추정하고자하는값)여야 함."
  },
  {
    "objectID": "posts/deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#본격적인-유도",
    "href": "posts/deep learning/Deep learning theory/(2) Logistic Regression/Logistic Regression.html#본격적인-유도",
    "title": "딥러닝(2) - 로지스틱회귀",
    "section": "본격적인 유도",
    "text": "본격적인 유도\n\n모수를 로지스틱함수로 바꾸기\n\n\\(\\bf{X},\\bf{W}\\)의 선형조합이 식에 존재해야 합니다. 그러므로 선형방정식을 하나 만듭니다. \\[\\begin{align}\nf(i) = x_{1,i}w_0 + x_{2,i}w_1 + x_2w_2 + ... + x_{m,i}w_m = X_iW \\nonumber \\\\\nwhere,X_i = \\,[x_{1,i},x_{2,i},\\dots,x_{m,i}]\\, ,W = \\,[w_0,w_1,\\dots,w_m]^\\text{T} \\nonumber \\\\\n\\end{align}\\]\n좌변은 예측하고자 하는 값인 모수여야 합니다. 좌변을 바꿔봅니다. \\[p_i = WX_i\\]\n좌변의 베르누이 분포의 모수 \\(p_i\\)는 확률변수 \\(Y_i = 1\\)인 사건이 일어날 확률입니다. 그러므로 \\([0,1]\\)이라는 범위를 가지는 반면 우변의 값\\(WX_i\\)은 \\(\\,[-\\infty,\\infty]\\,\\)에 범위를 가집니다. 여기서 Odss Ratio를 써서 모수 \\(p_i\\)를 포함하며 더 넓은 range를 갖도록 좌변을 수정합니다. \\[\\text{Odds Ratio} = \\frac{p_i}{1-p_i} = WX_i\\]\n좌변을 Odds Ratio로 수정했지만 여전히 좌변의 범위는\\(\\,[0,\\infty]\\,\\)으로 우변에 비해 좁습니다. 따라서 Odds Ratio에 로짓변환을 취하여 좌변의 범위를 \\(\\,[-\\infty,\\infty]\\)로 넓혀줍니다. \\[\\text{logit}(p) = \\text{ln}\\frac{p_i}{1-p_i} = WX_i\\]\n\n위 식을 해석하기 위해 \\(X\\)의 첫번째 요소인 \\(x_1\\)에 대응하는 회귀계수 \\(w_1\\)이 학습결과 3으로 정해졌다고 가정해봅시다.만약 \\(x_1\\)의 값이 1증가한다면 로그오즈비가 3증가합니다.\n\n이제 양변의 범위는 맞춰졌으므로 추정하고자 하는 변수 \\(p_i\\)가 좌변에 오도록 정리해봅시다. \\[p_i = \\frac{1}{\\,(1 + e^{-WX_i})\\, }\\] (전개) \\(\\frac{p_i}{1-p_i} = e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\,(1-p_i)\\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\,e^{WX_i}-p_ie^{WX_i}\\) \\(\\Longleftrightarrow p_i + p_ie^{WX_i} = \\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i\\,(1 + e^{WX_i})\\, = \\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\frac{\\,e^{WX_i}}{\\,(1 + e^{WX_i})\\, }\\) \\(\\Longleftrightarrow p_i = \\frac{1}{\\,(1 + e^{-WX_i})\\, }\\)\n\n최종적으로, 모수에대한 표현식은 다음과 같습니다. \\[p_i = Pr\\,(Y_i = 1|X_i)\\, = \\frac{1}{\\,1 + e^{-WX_i}\\,}\\] 앞서 목적이었던 X와 W의 선형조합이 수식내부에 존재합니다.\n모수 \\(p_i\\)가 새롭게 표현되었으므로 베르누이분포의 확률질량함수도 다르게 표현할 수 있습니다. \\[Pr\\,(Y_{i} = y|x_{1,i},x_{2,i},\\dots,x_{m,i},W) =\n\\begin{cases}\np_i   & \\text{if }y=1, \\\\\n1-p_i & \\text{if }y=0\n\\end{cases} = p_i^y(1-p_i)^{1-y} = \\,(\\frac{1}{\\,1 + e^{-WX_i}\\,})^y\\,\\,(1-\\frac{1}{\\,1 + e^{-WX_i}\\,})^{1-y}\\,\\]"
  },
  {
    "objectID": "posts/deep learning/pytorch 구현/Pytorch Rnn 구현.html",
    "href": "posts/deep learning/pytorch 구현/Pytorch Rnn 구현.html",
    "title": "pytorch로 Rnn구현하기",
    "section": "",
    "text": "hi?hi!가 반복되는 텍스트 데이터에서 다음 문자가 뭐가 나올지 예측하는 RNN모형 만들기"
  },
  {
    "objectID": "posts/deep learning/pytorch 구현/Pytorch Rnn 구현.html#vectorization",
    "href": "posts/deep learning/pytorch 구현/Pytorch Rnn 구현.html#vectorization",
    "title": "pytorch로 Rnn구현하기",
    "section": "vectorization",
    "text": "vectorization\n\n여러가지 방법이 있으나(tf-idf,dense vector,one-hot encoding 등등…) 여기서는 원핫인코딩 사용\n\n\ndef mapping(txt,map_dict):\n    return [map_dict[chr]for chr in txt]\ntxt_mapped = mapping(txt,map_dict)\nprint(txt_mapped[:10])\n\ndef onehot_encoding(txt_mapped):\n    seq_encoded = torch.nn.functional.one_hot(torch.tensor(txt_mapped))\n    return seq_encoded.float()\nsequence_data_encoded = onehot_encoding(txt_mapped)\nprint(sequence_data_encoded[:10])\n\n[2, 3, 0, 2, 3, 1, 2, 3, 0, 2]\ntensor([[0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.]])\n\n\n데이터 살짝 변형 하나의 긴 sequence data를 RNN의 입력으로 해도 되지만 처리속도,성능을 고려했을 때 자그마한 sequencedata로 분리하여 입력해주는게 더 좋은 방법임. 분리하는 방법도 여러가지가 있을 수 있겠는데 여기서는 다음과 같이 분리함 raw sequence data : hi?hi!hi?hi!hi?hi! ……….. sequence1 : (x,y) = (hi?,h) sequence2 : (x,y) = (i?h,i) sequence3 : (x,y) = (?hi,!) …\n\ndef create_seqdataset(seq_data,seq_length):\n    #x = seq_data[:-1]\n    #y = seq_data[1:]\n    seqs_x = []\n    seqs_y = []\n    for idx in range(0,len(seq_data)-seq_length):\n        seqs_x.append(seq_data[idx:idx+seq_length])\n        seqs_y.append(seq_data[idx+seq_length])\n    return torch.stack(seqs_x),torch.stack(seqs_y)\n    #return seq_x,seq_y\n\nx_data,y_data = create_seqdataset(sequence_data_encoded,3)\nprint(x_data.shape,y_data.shape)\n\ntorch.Size([57, 3, 4]) torch.Size([57, 4])\n\n\n\n왜 저런 shape을 맞춰 주는가?\n여기서 나오는 x_data.shape = \\((57,3,4)\\)가 살짝 난해함.  파이토치 공식문서에 따르면 batch_first = True로 설정할 경우,rnn계열의 모델에 넣어줘야 하는 텐서의 shape은 \\((N,L,H_{in})\\) = (batch size,sequnce length,input_size)이고 dataloader라는 일종의 데이터 중간관리자?를 한 번 거쳐서 모델에 입력됨. dataloader에서 나오는 output.shape = \\((N,L,H_{in})\\)이 되기 위해서는 input.shape = \\((D,L,H_{in}\\)(D는 분리된 시퀀스의 갯수)이어야 함(즉 입력텐서의 차원이 3개여야 출력텐서의 차원도3개이고 차원이 나오는 순서도 저런식이 되어야 함). 따라서 저렇게 설정함.\n\n\n파라미터 잠깐 설명\nbatch size는 배치의 총 갯수(배치안에 있는 원소의 갯수 아님!), sequnce length는 시퀀스데이터의 길이이자 timestemp(시점)의 총 갯수(길이), \\(H_{in}\\)은 each timestep(각 시점)마다 입력되는 벡터의 길이라고 볼 수 있음. 위처럼 원핫인코딩을 한 경우 \\(H_{in}\\)은 시퀀스데이터에 있는 문자의 갯수로 결정되므로 4이고 L은 create_seqdataset함수에서 인수로 넣어준 3(sequnce_length)이고 마지막으로 N(batch_size)은 torch.utils.data.DataLoader안에 인수로 넣어주는 batch_size로 인해서 일정한 갯수로 배치를 나누었을때 나오는 배치들의 총 숫자임.rnn 문서에서 설명하는 batch_size는 torch.utils.dada.DataLoader에서 설정한 batch_size의 갯수만큼 데이터를 모아서 여러개의 배치로 만들었을때 나오는 배치의 총 갯수라고 보면됨.(헷갈리는 부분….)"
  },
  {
    "objectID": "posts/deep learning/pytorch 구현/Pytorch Rnn 구현.html#학습-준비하기",
    "href": "posts/deep learning/pytorch 구현/Pytorch Rnn 구현.html#학습-준비하기",
    "title": "pytorch로 Rnn구현하기",
    "section": "학습 준비하기",
    "text": "학습 준비하기\n\ndefine architecture,loss,optimizer\ndata check\n\n\n#architecture,loss,optimizer \ntorch.manual_seed(2022)\nrnn = torch.nn.RNN(4,20,batch_first = True)\nlinr = torch.nn.Linear(20,4)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=1e-3)\n\n\nds = torch.utils.data.TensorDataset(x_data,y_data)\ndl = torch.utils.data.DataLoader(ds,batch_size=8,drop_last=True)\n\nfor idx,(x,y) in enumerate(dl):\n    if idx ==5:\n        break\n    print(x.shape,y.shape)\n\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\n\n\n위에서 언급했듯이 데이터로더를 거쳐서 나오는 텐서는 RNN에 바로 입력될 것임. input.shape = \\((N,L,H_{in}) = (8,3,4)\\)"
  },
  {
    "objectID": "posts/deep learning/pytorch 구현/Pytorch Rnn 구현.html#모형학습",
    "href": "posts/deep learning/pytorch 구현/Pytorch Rnn 구현.html#모형학습",
    "title": "pytorch로 Rnn구현하기",
    "section": "모형학습",
    "text": "모형학습\n\nfor epoch in range(0,101):\n    for tr_x,tr_y in dl:\n        #1 output\n        hidden,hT = rnn(tr_x)\n        #print(hidden.shape)\n        output = linr(hT[-1])\n        #2 loss\n        loss = loss_fn(output,tr_y)\n        #3 derivative\n        loss.backward()\n        #4 update & clean\n        optimizer.step()\n        optimizer.zero_grad()\n    if epoch % 10 == 0:\n        print(f'epoch : {epoch},loss : {round(loss.tolist(),5)}')\n\nepoch : 0,loss : 1.31779\nepoch : 10,loss : 0.69453\nepoch : 20,loss : 0.19338\nepoch : 30,loss : 0.05891\nepoch : 40,loss : 0.02861\nepoch : 50,loss : 0.01791\nepoch : 60,loss : 0.0126\nepoch : 70,loss : 0.00947\nepoch : 80,loss : 0.00744\nepoch : 90,loss : 0.00602\nepoch : 100,loss : 0.00499\n\n\npytorch의 rnn을 거쳐서 나오는 output은 두 가지임. - hidden : 가장 깊이 위치한 히든레이어의 각각의 시점에서의 출력값을 모아놓은 텐서 - hT : 모든 히든레이어에의 마지막 시점(시점T)에서의 출력값을 모아놓은 텐서 - 외우기! 위치 : 가장깊은 <=> 모든 , 시점 : 각각의 <=> 마지막\n위와같은 설정에서는 가장 깊이 위치한 히든레이어의 마지막시점에서의 출력값만이 우리는 다음에올 문자열을 예측할 때 필요하므로 hT[-1]을 하여 그 값을 가져옴."
  },
  {
    "objectID": "posts/probabilty,statistics/MLE.html",
    "href": "posts/probabilty,statistics/MLE.html",
    "title": "최대가능도 추정법(MLE)",
    "section": "",
    "text": "최대가능도 추정법(MLE)에 대한 정리"
  },
  {
    "objectID": "posts/probabilty,statistics/MLE.html#likelyhood가능도",
    "href": "posts/probabilty,statistics/MLE.html#likelyhood가능도",
    "title": "최대가능도 추정법(MLE)",
    "section": "Likelyhood(가능도)",
    "text": "Likelyhood(가능도)\n가능도(Likely hood) 샘플이 주어질 때, 임의의 모수(parameter)\\(\\theta\\)를 가지는 확률분포가 주어진 샘플을 취할 가능성,확률을 나타내며 그 값은 확률함수의 값과 같습니다.\n\\[\\begin{align}\n\\text{Given}\\, (x_1,x_2,...,x_n),\\,L(\\theta|X = (x_1,x_2,\\dots,x_n)) = Pr(X=(x_1,x_2,\\dots,x_n)|\\theta)\n\\end{align}\\]\n\n\n\n\n\n\nNote\n\n\n\n가능도 함수의 함숫값은 확률함수의 함숫값과 같지만 가능도함수가 확률함수는 아닙니다. 또한 확률분포는 확률변수에 대한 함수로서 확률변수가 특정값을 취하거나 또는 특정구간안에 값이 존재할 가능성을 알려주지만 가능도함수는 가정한 확률분포의 모수에 대한 함수로서 임의의 모수를 가지는 확률분포가 특정값을 가질 가능성을 알려줍니다."
  },
  {
    "objectID": "posts/probabilty,statistics/MLE.html#mle최대가능도-추정법",
    "href": "posts/probabilty,statistics/MLE.html#mle최대가능도-추정법",
    "title": "최대가능도 추정법(MLE)",
    "section": "MLE(최대가능도 추정법)",
    "text": "MLE(최대가능도 추정법)\n최대가능도추정법(MLE)는 확률분포의 모수를 추정하는 점추정방법 중 하나로 가능도함수의 함숫값(가능도)을 가장크게하는 모숫값 즉,주어진 샘플을 취할 가능성,확률이 가장 큰 모숫값을 모수에 대한 추정량으로 하는 방법입니다. \\[\\hat{\\theta} = \\underset{\\theta}{\\text{argmax}}\\,L(\\theta|X=x)\\]\n가능도함수의 함숫값(가능도)는 모수가 주어진 상황에서 확률분포의 값과 같으므로 식을 다음과 같이 쓸수도 있습니다. \\[\\hat{\\theta} = \\underset{\\theta}{\\text{argmax}}\\,L(\\theta|X=x) = \\underset{\\theta}{\\text{argmax}}\\,Pr(X=x|\\theta)\\]"
  },
  {
    "objectID": "posts/probabilty,statistics/MLE.html#nll",
    "href": "posts/probabilty,statistics/MLE.html#nll",
    "title": "최대가능도 추정법(MLE)",
    "section": "NLL",
    "text": "NLL\nLL은 log likelyhood의 약자로 likelyhood에 log를 취해준 값입니다. 로그함수를 취해도 함수가 최대값을 갖는 모수 \\(\\theta\\)위차가 변하지 않고 계산을 곱셈을 더하기로 바꿔서 계산하기에 더 편리하기 때문에 LL을 사용합니다.\nNLL은 LL에 -를 곱해준 값입니다. 함수가 최대인 지점을 찾는 문제를 최소인 지점을 찾는 문제로 바꿀 수 있으며 따라서 경사하강법을 활용할 수 있습니다.\n최종적으로 \\(\\hat{\\theta}\\)는 다음과 같습니다. \\[\\hat{\\theta} = \\underset{\\theta}{argmin}\\,\\text{NLL} = \\underset{\\theta}{argmin}\\,-\\text{ln}\\,L(\\theta|X=x)\\]"
  },
  {
    "objectID": "posts/probabilty,statistics/notation.html",
    "href": "posts/probabilty,statistics/notation.html",
    "title": "확률분포에서 ;와|의 사용",
    "section": "",
    "text": "확률분포에서 ;와|에 관한 notation 정리"
  },
  {
    "objectID": "posts/probabilty,statistics/notation.html#언제-를-사용하지",
    "href": "posts/probabilty,statistics/notation.html#언제-를-사용하지",
    "title": "확률분포에서 ;와|의 사용",
    "section": "언제 ;를 사용하지?",
    "text": "언제 ;를 사용하지?\n;의 뒤에는 다변수 함수에서 특정 변수를 시켜놓았다라는 의미로 쓰인다. 다음과 같은 이변수함수를 생각해보자 \\[f(x,y)\\] 독립변수 x와 y를 갖는 함수이며 두 변수 모두에 대해서 미분가능하다. 여기서 변수 y가 어떤 어떤 값으로 고정된것이 알려지거나 또는 고정된상황을 가정한 후 x에 대해서만 관심이 있다고 해보자(미분,극한등을 아마도?취하고 싶다..아마도…) 이때는 이변수함수로 표기하는것이 아닌 일변수함수로 표기해야한다. 그때는 다음과 같이 표기하면 된다. \\[\nf(x;y)\n\\]\n이렇게 표기하면 y값은 이제 어떤 값으로 이미 설정(setting)되거나 고정(fixed)되어있음을 의미한다."
  },
  {
    "objectID": "posts/probabilty,statistics/notation.html#확률함수에서-의-사용",
    "href": "posts/probabilty,statistics/notation.html#확률함수에서-의-사용",
    "title": "확률분포에서 ;와|의 사용",
    "section": "확률함수에서 ;의 사용",
    "text": "확률함수에서 ;의 사용\n베르누이분포의 확률질량함수는 다음과 같다 \\[\nf_X(x) = p^x(1-p)^{1-x}\n\\]\n이식을 해석해보면 다음과 같다. - 좌변 : 확률변수 X에 관한 확률함수이며 실험 또는 시행으로부터 확률변수X가 취할 수 있는 값인 변수x를 입력으로 갖는다. - 우변 : 구체적인 확률질량함수(또는 확률밀도함수)이다.\n베르누이분포 확률질량함수에서 이미 setting,fixed된 모수(parameter)를 명시적으로 표현하고자 할때에는 다음과 같은 표기를 사용한다. \\[f_X(x;p) = p^x(1-p)^{1-x}\\]"
  },
  {
    "objectID": "posts/probabilty,statistics/notation.html#의-사용-1",
    "href": "posts/probabilty,statistics/notation.html#의-사용-1",
    "title": "확률분포에서 ;와|의 사용",
    "section": "|의 사용",
    "text": "|의 사용\n|는 given that이라는 의미이며 |의 오른쪽에는 조건 또는 주어지는 상황이 나온다. 확률,통계 분야에서만 쓰이며 조건부 확률이나,베이지안에서 많이 쓰이는 기호라고 한다. 여기서 조건은 확률변수의 어떤 값이므로 마찬가지로 미리주어지는 값이다. 변수가 아니라 모수(parameter)로 생각할 수 있으며 결국에는 설정된,고정된 값이다.\n조건부 확률분포(조건부확률밀도함수,조건부 확률함수)는 다음과 같다. \\[\np_{X|Y}(x|y) = \\frac{p_{XY}(x,y)}{p_Y(y)}\\\\\np_{X|Y}(y|x) = \\frac{p_{XY}(x,y)}{p_X(x)}\n\\]  가장 윗식을 해석해보면 다음과 같다. - 좌변 : 확률변수 Y가 주어진(given)상황에서 확률변수X의 확률(또는 확률밀도)를 나타내는 함수이며 시행으로부터 확률변수 X가 취할 수 있는 값 x를 입력으로 받는다. 조건부 확률분포에서 Y의 값은 y로 주어져 있다고 가정하므로 입력하는 변수가 아니다. 따라서 확률변수 Y는 변수가 아니라 모수(parameter)이다. - 우변 : 조건부확률분포 공식."
  },
  {
    "objectID": "posts/probabilty,statistics/notation.html#결론-과",
    "href": "posts/probabilty,statistics/notation.html#결론-과",
    "title": "확률분포에서 ;와|의 사용",
    "section": "결론 ; 과 |",
    "text": "결론 ; 과 |\n\\[p_{\\theta}(x) = p(x;\\theta) = p(x|\\theta)\\]  ;는 수학의 모든 분야에서 쓰이지만 |는 확률통계분야에서만 쓰인다. ;다음에는 이미 설정되거나 고정된 값이 나오며 |다음에는 이미 주어진 조건이 나온다. 확률,통계 분야에서는 |다음에 나오는 조건은 조건부확률분포에서의 주어진 조건이자 설정된,고정된 값인 경우가 많다. 따라서 비공식적으로 혼용해서 쓰며 경험상 ;다음에는 그래도 설정된 값을 의미하는 모수(parameter)가 |는 조건부 확률이나 확률분포를 쓰는 경우가 많은 것 같다.\n참고링크 링크1(;의 사용법) 링크2(조건부확률분포) 링크3(표기법의 혼용)"
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "",
    "text": "파이썬 변수,할당문,인터닝\n전북대학교 최규빈 교수님의 딥러닝 deepcopy-shallowcopy 특강 중,변수와 할당문 부분을 재구성한 글입니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-변수",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-변수",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "파이썬에서의 변수",
    "text": "파이썬에서의 변수\n\n파이썬의 변수는 메모리상에 저장된 객체를 참조(reference)합니다.\n따라서 변수는 자바에서 참조변수와 같습니다.\n변수는 객체(object)를 부르는 별칭(다른이름),객체에 붙여진 포스트잇,또다른 레이블 이라고 생각하는 것이 비유적으로 맞습니다.\n앞으로 객체에 접근하기 위해서는 a라는 변수(별칭,객체)를 찾으면 됩니다.\n마치 C언어의 포인터와 유사하게 동작합니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-할당문",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-할당문",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "파이썬에서의 할당문(=)",
    "text": "파이썬에서의 할당문(=)\n\n=은 파이썬(뿐만아니라 대부분의 프로그래밍언어)에서 할당문입니다.\n할당문을 실행하면 = 오른쪽에 있는 객체를 먼저 메모리 상에 생성하거나 가져옵니다.\n=의 왼쪽에는 변수가 존재하며 파이썬은 변수에 생성된 객체를 할당합니다.\n변수는 객체에 붙여지는 별칭,레이블로 표현할 수 있습니다. 이렇게 객체에 변수가 할당되고나면 할당된 변수와 객체에 대해서 “변수가 객체에 바인딩(묶이다)되어있다”고 표현합니다.\n\n\na = [1,2,3]\nprint(id(a))\n\n1819154486912\n\n\n위 코드에서 객체[1,2,3]에 변수 a가 바인딩 되었습니다.내부동작은 다음과 같습니다. 1. 메모리 주소(1819161042880)에 리스트 객체[1,2,3]을 생성합니다 2. 생성된 리스트객체를 변수 a에 할당합니다. a는 객체[1,2,3]에 붙여지는 별칭,레이블이라고 할 수 있습니다.\n\n에일리어싱\n에일리어싱은 하나의 객체를 여러개의 변수가 참조하게 하는 것입니다. 하나의 객체에 여러개의 별칭,별명,레이블을 붙이는 것이라고도 할 수 있습니다.\n\nb = a\nprint(id(a));print(id(b))\nprint(a);print(b)\n\n1819154486912\n1819154486912\n[1, 2, 3]\n[1, 2, 3]"
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#idvalue",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#idvalue",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "id,value",
    "text": "id,value\nid는 객체가 가지는 메모리상의 고유한 주소입니다. 서로다른 객체는 다른 값을 가질수도 같은값을 가질수도 있습니다.\n\na=[1,2,3]\nb=a\na.append(4)\nc=[1,2,3,4]\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\n\na:[1, 2, 3, 4] b:[1, 2, 3, 4] c:[1, 2, 3, 4]\nid(a):1819154849280,id(b):1819154849280,id(c):1819155097408\n\n\n변수a,b,c는 모두 같은 value(값)을 가집니다.a와b는 같은 객체에 바인딩되어있지만 c는 또다른 객체에 바인딩되어 있습니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#is-vs",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#is-vs",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "is vs ==",
    "text": "is vs ==\n- is는 객체비교연산자로 두 변수가 동일한 객체를 참조하는지 아니면 다른객체를 참조하는지 확인한 후 True or False를 반환합니다. 파이썬은 내부적으로 동일한 객체인지 아닌지를 판단할때에는 메모리주소를 확인한다고 합니다. - ==는 값비교연산자로 두 변수가 참조하는 객체의 값이 같은지 아니면 값이 다른지를 확인한 후 True or False를 반환합니다. - 참조하다는 뭔가 잘 와닿는데 참조`라는 용어는 잘 와닿지가 않습니다…변수가 참조하는(또는 가리키는,지칭하는)객체(object) 그 자체입니다.\n\ncode1\n\na=[1,2,3] #1\nprint(\"append하기 전 id(a):\",id(a))\nb=a #2 에일리어싱,동일한 객체를 가리키도록 함.\na.append(4) #3\nc=[1,2,3,4] #4\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\nprint(\"a의 참조(reference)와 b의 참조는 동일한 객체인가요??\",a is b)\nprint(\"a의 참조와 b의 참조는 값(value)이 같나요?\",a == b)\nprint(\"a의 참조(reference)와 c의 참조는 동일한 객체인가요??\",a is c)\nprint(\"a의 참조와 c의 참조는 값(value)이 같나요?\",a == c)\n\nappend하기 전 id(a): 1819155097408\na:[1, 2, 3, 4] b:[1, 2, 3, 4] c:[1, 2, 3, 4]\nid(a):1819155097408,id(b):1819155097408,id(c):1819155103296\na의 참조(reference)와 b의 참조는 동일한 객체인가요?? True\na의 참조와 b의 참조는 값(value)이 같나요? True\na의 참조(reference)와 c의 참조는 동일한 객체인가요?? False\na의 참조와 c의 참조는 값(value)이 같나요? True\n\n\n코드설명 1. 변수a에 [1,2,3]을 할당합니다.a는 [1,2,3]을 참조합니다. 2. 에일리어싱으로 변수b도 [1,2,3]을 참조합니다. 3. 변수a가 참조하는 리스트객체[1,2,3]에 4를 추가합니다. 4. 변수c에 [1,2,3,4]를 할당합니다.\n\n\ncode2 - 살짝 심화\n\na=[1,2,3] #1\nprint(\"재할당 하기 전 id(a):\",id(a))\nb=a #2에일리어싱,동일한 객체를 가리키도록 함\na=[1,2,3]+[4] #3재할당\nc=[1,2,3,4] #4\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\nprint(\"a의 참조(reference)와 b의 참조는 동일한 객체인가요??\",a is b)\nprint(\"a의 참조와 b의 참조는 값(value)이 같나요?\",a == b)\nprint(\"a의 참조(reference)와 c의 참조는 동일한 객체인가요??\",a is c)\nprint(\"a의 참조와 c의 참조는 값(value)이 같나요?\",a == c)\n\n재할당 하기 전 id(a): 1819155102592\na:[1, 2, 3, 4] b:[1, 2, 3] c:[1, 2, 3, 4]\nid(a):1819184303168,id(b):1819155102592,id(c):1819184334272\na의 참조(reference)와 b의 참조는 동일한 객체인가요?? False\na의 참조와 b의 참조는 값(value)이 같나요? False\na의 참조(reference)와 c의 참조는 동일한 객체인가요?? False\na의 참조와 c의 참조는 값(value)이 같나요? True\n\n\n코드설명 1. 변수a에 [1,2,3]을 할당합니다.a는 [1,2,3]을 참조합니다. 2. 에일리어싱으로 변수b도 [1,2,3]을 참조합니다. 3. 변수a에 리스트객체[1,2,3,4]를 재할당합니다. 4. 변수c에 [1,2,3,4]를 할당합니다.\na,b,c 각각의 값은 code1과 code2에서 모두 같습니다. 차이점은 code1에서는 a가 참조하는 리스트[1,2,3]에 4를 추가하고 code2에서는 a에 리스트[1,2,3,4]를 재할당한다는 점입니다.중요한 차이점은 다음과 같습니다.\n- code1에 append전 후의 a가 참조하는 객체는 주소는 변하지 않은 것으로 보아 동일한 객체에 원소만 추가되었음을 알 수 있습니다. - 반면 code2에서 할당문 전 후의 a가 참조하는 객체의 주소가 변합니다. - 이전에 없었던 1)객체가 메모리에 생성되고 2)변수a는 이전의 [1,2,3]을 더 이상 참조하지 않고 생성된 객체[1,2,3,4]를 참조**하는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#인터닝",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#인터닝",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "인터닝",
    "text": "인터닝\n인터닝이란 이미 생성된 객체를 재사용하는 것을 말합니다. 객체의 빠른 재사용을 가능하게 하며 메모리를 절약한다고 합니다. 내부적으로는 다음과 같이 구현됩니다. 1. 임의의 할당문을 실행합니다. 2. = 오른쪽에 있는 객체가 Intern 컬렉션에 등록되어 있는지 아닌지 확인합니다. 3. 등록되어 있는 객체의 경우 그 객체를 그대로 참조합니다. 등록되지 않은 경우 메모리에 객체를 생성하며 변수는 생성된 객체의 주소를 참조합니다\n자주 사용하는 객체의 경우 직접 Intern 컬렉션에 등록할 수 있고 빠르게 재사용할 수 있습니다. -5~256사이의 정수이거나 20자 미만의 문자열은 할당문을 실행하면 자동으로 Inter 컬렉션에 등록됩니다. 따라서 해당하는 정수나 문자열을 또다른 할당문에 실행하면 변수는 같은 객체를 참조합니다.\n예제1-인터닝 X\n\na=1+2021\nb=2023-1\nc=2022\nprint(id(a),id(b),id(c))\nprint(a,b,c)\n\n1819155040400 1819155039600 1819155040688\n2022 2022 2022\n\n\na,b,c는 서로다른 객체를 참조하며 객체들은 모두 같은 값을 가집니다.\n예제2-인터닝 O\n\na=1+2 \nb=4-1\nc=3\nprint(id(a),id(b),id(c))\nprint(a,b,c)\n\n1819075897712 1819075897712 1819075897712\n3 3 3\n\n\na,b,c는 모두같은 객체를 참조합니다. 내부적인 동작은 다음과 같습니다. 1. a=1+2 할당문을 실행합니다. 2. 할당되는 객체가 -5~256사이의 정수이므로 자동으로 Intern 컬렉션에 등록됩니다. 3. b와c에도 정수 3을 할당합니다. 3은 Intern 컬렉션 등록되어있는 객체이며 메모리상에 생성되어있는 객체이므로 새로운 3객체가 생성되지 b,c는 이미 생성된 3객체를 가리킵니다.\n참고링크1 : https://guebin.github.io/DL2022/posts/Appendix/2022-12-14-A1.html 참고링크2 : http://pythonstudy.xyz/python/article/512-%ED%8C%8C%EC%9D%B4%EC%8D%AC-Object-Interning"
  },
  {
    "objectID": "posts example/deep learning final test/dl final.html",
    "href": "posts example/deep learning final test/dl final.html",
    "title": "딥러닝 기말고사",
    "section": "",
    "text": "This is a post of 2022 DL final test\n\nDL Final\n\nimport torch \nimport matplotlib.pyplot as plt\nimport torch.nn as nn\n\n\nsoft = torch.nn.Softmax(dim=1)\nsig = torch.nn.Sigmoid()\ntanh= torch.nn.Tanh()\n\n\nhihello\n\n\ntxt = list('hi?hello!!')*100 \ntxt_x = txt[:-1]\ntxt_y = txt[1:]\ntxt_x[:5], txt_y[:5]\n\n(['h', 'i', '?', 'h', 'e'], ['i', '?', 'h', 'e', 'l'])\n\n\n\ntorch.nn.RNN()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n\n\n교수님이 올려주신 것과 매핑이 다릅니다..!\n\n\ndef make_mapping(txt):\n    mapping = {}\n    idx = 0\n    for chr in txt:\n        if chr not in mapping.keys():\n            mapping[chr] = idx\n            idx+=1\n    return mapping\nmapping = make_mapping(txt)\nmapping\n\n{'h': 0, 'i': 1, '?': 2, 'e': 3, 'l': 4, 'o': 5, '!': 6}\n\n\n\ndef int_encoding(txt,mapping):\n    return [mapping[chr] for chr in txt]\nx = torch.nn.functional.one_hot(torch.tensor(int_encoding(txt_x,mapping))).float().to(\"cuda\")\ny = torch.nn.functional.one_hot(torch.tensor(int_encoding(txt_y,mapping))).float().to(\"cuda\")\nprint(x[:5],y[:5],x.dtype,y.dtype,x.shape,y.shape)\n\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.]], device='cuda:0') tensor([[0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.]], device='cuda:0') torch.float32 torch.float32 torch.Size([999, 7]) torch.Size([999, 7])\n\n\n\ntorch.manual_seed(5)\nnet = torch.nn.RNN(7,4).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(4,7).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(net.parameters())+list(linr_h2o.parameters()),lr=0.1)\n\n\nh_0 = torch.zeros((1,4)).to(\"cuda\")\nfor epoch in range(500):\n    #1 hidden,out\n    hidden,h_T = net(x,h_0)\n    #print(hidden.shape,h_T.shape)\n    out = linr_h2o(hidden)\n    #2 loss\n    loss = loss_fn(out,y)\n    if epoch % 100 == 0:\n        print(loss)\n    #3 derivative\n    loss.backward()\n    #4 update\n    optimizer.step()\n    optimizer.zero_grad()\n\ntensor(2.0413, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0055, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0025, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0014, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0010, device='cuda:0', grad_fn=<DivBackward1>)\n\n\n\nh_0 = torch.zeros((1,4)).to(\"cuda\")\nhidden,h_T = net(x,h_0)\nout = linr_h2o(hidden)\nyhat = soft(out)\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x1a949f603a0>,\n  <matplotlib.axis.XTick at 0x1a949f60370>,\n  <matplotlib.axis.XTick at 0x1a949f68ac0>,\n  <matplotlib.axis.XTick at 0x1a94ff82040>,\n  <matplotlib.axis.XTick at 0x1a94ff82640>,\n  <matplotlib.axis.XTick at 0x1a94ff82d90>,\n  <matplotlib.axis.XTick at 0x1a94ff87520>],\n [Text(0, 1, 'h'),\n  Text(1, 1, 'i'),\n  Text(2, 1, '?'),\n  Text(3, 1, 'e'),\n  Text(4, 1, 'l'),\n  Text(5, 1, 'o'),\n  Text(6, 1, '!')])\n\n\n\n\n\n\ntorch.nn.RNNCell()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n\n\ntorch.manual_seed(5)\nrnncell = torch.nn.RNNCell(7,4).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(4,7).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(rnncell.parameters()) + list(linr_h2o.parameters()),lr=0.1)\n\n\nT = len(x)\nfor epoch in range(500):\n    h_t = torch.zeros((1,4)).to(\"cuda\")\n    loss = 0.0\n    for t in range(T):\n        x_t,y_t = x[[t]],y[[t]]\n        #1 hidden,out\n        h_t = rnncell(x_t,h_t)\n        o_t = linr_h2o(h_t)\n        #2 loss at timestep t\n        l_t = loss_fn(o_t,y_t)\n        loss += l_t\n    loss = loss/T\n    if epoch % 100 == 0:\n        print(loss)\n    #3 derivative\n    loss.backward()\n    #4 update and clean\n    optimizer.step()\n    optimizer.zero_grad()\n\ntensor(2.0413, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0055, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0025, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0014, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0010, device='cuda:0', grad_fn=<DivBackward0>)\n\n\n\nT = len(x)\nhidden = torch.zeros((T,4)).to(\"cuda\")\nhidden[[0]] = rnncell(x[[0]],torch.zeros((1,4)).to(\"cuda\"))\nfor t in range(1,T):\n    x_t = x[[t]]\n    #print(x_t.shape)\n    hidden[[t]] = rnncell(x_t,hidden[[t-1]])\n\n\nyhat = soft(linr_h2o(hidden))\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x1a94ffb9a90>,\n  <matplotlib.axis.XTick at 0x1a94ffb9a60>,\n  <matplotlib.axis.XTick at 0x1a94ffcc220>,\n  <matplotlib.axis.XTick at 0x1a94ffc1610>,\n  <matplotlib.axis.XTick at 0x1a94ffc1d60>,\n  <matplotlib.axis.XTick at 0x1a94ffc74f0>,\n  <matplotlib.axis.XTick at 0x1a94ffc1af0>],\n [Text(0, 1, 'h'),\n  Text(1, 1, 'i'),\n  Text(2, 1, '?'),\n  Text(3, 1, 'e'),\n  Text(4, 1, 'l'),\n  Text(5, 1, 'o'),\n  Text(6, 1, '!')])\n\n\n\n\n\n\ntorch.nn.Module을 상속받은 클래스를 정의하고 (2)의 결과와 동일한 적합값이 나오는 신경망을 설계한 뒤 학습하라. (초기값을 적절하게 설정할 것)\n\n\nclass Rnn_cell(nn.Module):\n    def __init__(self,input_size,hidden_size):\n        super().__init__()\n        self.linri2h = torch.nn.Linear(input_size,hidden_size)\n        self.linrh2h = torch.nn.Linear(hidden_size,hidden_size) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,x,h_t):\n        return self.tanh(self.linri2h(x) + self.linrh2h(h_t))\n\n\n#동일한 적합값이 나오도록 신경망을 학습시키기 위해서 RNNCell의 파라미터를 가져왔습니다.\ntorch.manual_seed(5)\nrnncell = torch.nn.RNNCell(7,4).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(4,7).to(\"cuda\")\nrnncell.weight_hh.data\n\ntensor([[-0.2361, -0.1094, -0.3339, -0.2364],\n        [-0.4558, -0.0116,  0.2965,  0.2432],\n        [ 0.4697, -0.4391, -0.0615,  0.4868],\n        [ 0.0819, -0.1410, -0.4503,  0.2327]], device='cuda:0')\n\n\n\ntorch.manual_seed(5)\nrnn_cell = Rnn_cell(7,4).to(\"cuda\")\n#파라미터 초기화(from torch.nn.RNNCell)\nrnn_cell.linri2h.weight.data = rnncell.weight_ih.data\nrnn_cell.linri2h.bias.data = rnncell.bias_ih.data\nrnn_cell.linrh2h.weight.data = rnncell.weight_hh.data\nrnn_cell.linrh2h.bias.data = rnncell.bias_hh.data\nlinr_h2o = torch.nn.Linear(4,7).to(\"cuda\")\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(rnn_cell.parameters())+list(linr_h2o.parameters()),lr=0.1)\n\n\nT = len(x)\nfor epoch in range(500):\n    h_t = torch.zeros((1,4)).to(\"cuda\")\n    loss = 0.0\n    for t in range(T):\n        x_t,y_t = x[[t]],y[[t]]\n        #1 hidden,out\n        h_t = rnn_cell(x_t,h_t) #rnncell => rnn_cell\n        o_t = linr_h2o(h_t)\n        #2 loss at timestep t\n        l_t = loss_fn(o_t,y_t)\n        loss += l_t\n    loss = loss/T\n    if epoch % 100 == 0:\n        print(loss)\n    #3 derivative\n    loss.backward()\n    #4 update and clean\n    optimizer.step()\n    optimizer.zero_grad()\n\ntensor(2.0413, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0055, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0025, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0014, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0010, device='cuda:0', grad_fn=<DivBackward0>)\n\n\n\nT = len(x)\nhidden = torch.zeros((T,4)).to(\"cuda\")\nhidden[[0]] = rnn_cell(x[[0]],torch.zeros((1,4)).to(\"cuda\")) #rnncell => rnn_cell\nfor t in range(1,T):\n    x_t = x[[t]]\n    #print(x_t.shape)\n    hidden[[t]] = rnn_cell(x_t,hidden[[t-1]]) #rnncell => rnn_cell\n\n\nyhat = soft(linr_h2o(hidden))\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x1a954939c70>,\n  <matplotlib.axis.XTick at 0x1a9549396d0>,\n  <matplotlib.axis.XTick at 0x1a95b902fd0>,\n  <matplotlib.axis.XTick at 0x1a95b937520>,\n  <matplotlib.axis.XTick at 0x1a95b937d00>,\n  <matplotlib.axis.XTick at 0x1a95b93e430>,\n  <matplotlib.axis.XTick at 0x1a95b93eb80>],\n [Text(0, 1, 'h'),\n  Text(1, 1, 'i'),\n  Text(2, 1, '?'),\n  Text(3, 1, 'e'),\n  Text(4, 1, 'l'),\n  Text(5, 1, 'o'),\n  Text(6, 1, '!')])\n\n\n\n\n\n\ntorch.nn.LSTM()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n\n\ntorch.manual_seed(5)\nlstm = torch.nn.LSTM(7,8).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(8,7).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstm.parameters()) + list(linr_h2o.parameters()),lr=0.1)\n\n\nh_0 = torch.zeros((1,8)).to(\"cuda\")\nc_0 = torch.zeros((1,8)).to(\"cuda\")\nfor epoch in range(500):\n    #1 hidden,output\n    hidden,(h_T,c_T)= lstm(x,(h_0,c_0))\n    out = linr_h2o(hidden)\n    #2 loss\n    loss = loss_fn(out,y)\n    if epoch % 100 == 0:\n      print(loss)\n    #3 derivative\n    loss.backward()\n    #4 update & clean\n    optimizer.step()\n    optimizer.zero_grad()\n\ntensor(1.8939, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0009, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0005, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0003, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0002, device='cuda:0', grad_fn=<DivBackward1>)\n\n\n\nh_0 = c_0 = torch.zeros((1,8)).to(\"cuda\")\nhidden,(_,_) = lstm(x,(h_0,c_0))\nout = linr_h2o(hidden)\nyhat = soft(out)\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x7f68d01dd510>,\n  <matplotlib.axis.XTick at 0x7f68d01dd3d0>,\n  <matplotlib.axis.XTick at 0x7f68d0198590>,\n  <matplotlib.axis.XTick at 0x7f68d0198f50>,\n  <matplotlib.axis.XTick at 0x7f68d01a84d0>,\n  <matplotlib.axis.XTick at 0x7f68d01a8a10>,\n  <matplotlib.axis.XTick at 0x7f68d01a88d0>],\n [Text(0, 1, 'h'),\n  Text(0, 1, 'i'),\n  Text(0, 1, '?'),\n  Text(0, 1, 'e'),\n  Text(0, 1, 'l'),\n  Text(0, 1, 'o'),\n  Text(0, 1, '!')])\n\n\n\n\n\n\ntorch.nn.LSTMCell()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n\n\ntorch.manual_seed(5)\nlstmcell = torch.nn.LSTMCell(7,8).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(8,7).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstmcell.parameters()) + list(linr_h2o.parameters()),lr=0.1)\n\n\nT = len(x)\nfor epoch in range(500):\n  h_t = c_t = torch.zeros((1,8)).to(\"cuda\")\n  loss = 0.0\n  for t in range(T):\n    x_t,y_t = x[[t]],y[[t]]\n    #1 hidden,out\n    h_t,c_t = lstmcell(x_t,(h_t,c_t))\n    out = linr_h2o(h_t)\n    #2 loss\n    l_t = loss_fn(out,y_t)\n    loss += l_t\n  loss = loss/T\n  if epoch % 100 == 0:\n    print(loss)\n  #3derivative\n  loss.backward()\n  #4 update & clean\n  optimizer.step()\n  optimizer.zero_grad()\n\ntensor(1.8939, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0009, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)\n\n\n\nT = len(x)\nhidden = torch.zeros((T,8)).to(\"cuda\")\ncell = torch.zeros((T,8)).to(\"cuda\")\n\nh_0 = c_0 = torch.zeros((1,8)).to(\"cuda\") # t = 0일때의 h,c값 초기화\nhidden[[0]],cell[[0]] = lstmcell(x[[0]],(h_0,c_0)) # t = 1일때의 h_1,c_t먼저 기록\nfor t in range(1,T): #t = 2일때부터 기록\n  x_t = x[[t]]\n  hidden[[t]],cell[[t]] = lstmcell(x_t,(hidden[[t-1]],cell[[t-1]]))\n\n\nout = linr_h2o(hidden)\nyhat = soft(out)\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x7f68d010abd0>,\n  <matplotlib.axis.XTick at 0x7f68d010a710>,\n  <matplotlib.axis.XTick at 0x7f68d00bbc50>,\n  <matplotlib.axis.XTick at 0x7f68d00d5650>,\n  <matplotlib.axis.XTick at 0x7f68d00d5b90>,\n  <matplotlib.axis.XTick at 0x7f68d00d5750>,\n  <matplotlib.axis.XTick at 0x7f68d00cc690>],\n [Text(0, 1, 'h'),\n  Text(0, 1, 'i'),\n  Text(0, 1, '?'),\n  Text(0, 1, 'e'),\n  Text(0, 1, 'l'),\n  Text(0, 1, 'o'),\n  Text(0, 1, '!')])\n\n\n\n\n\n\n(5)의 결과와 동일한 적합값을 출력하는 신경망을 직접설계한 뒤 학습시켜라. (초기값을 적절하게 설정할 것)\n\nversion1 - class사용\n\nclass LSTM_cell(nn.Module):\n  def __init__(self,input_size,hidden_size):\n    super().__init__()\n    self.linri2h = torch.nn.Linear(input_size,hidden_size * 4)\n    self.linrh2h = torch.nn.Linear(hidden_size,hidden_size * 4)\n    self.tanh = torch.nn.Tanh()\n    self.sig = torch.nn.Sigmoid()\n    self.hidden_size = hidden_size\n  def forward(self,x,h_t,c_t):\n    ifgo = self.linri2h(x) + self.linrh2h(h_t)\n    i_t = self.sig(ifgo[:,0:self.hidden_size])\n    f_t = self.sig(ifgo[:,self.hidden_size:self.hidden_size*2])\n    g_t = self.tanh(ifgo[:,self.hidden_size*2:self.hidden_size*3])\n    o_t  = self.sig(ifgo[:,self.hidden_size*3:self.hidden_size*4])\n    #print(ifgo.shape,i_t.shape,f_t.shape,g_t.shape,o_t.shape)\n    c_t = f_t * c_t + i_t * g_t\n    h_t = o_t * self.tanh(c_t)\n    return h_t,c_t\n\n\n#동일한 적합값이 나오도록 신경망을 학습시키기 위해서  LSTMCell의 파라미터를 가져왔습니다.\ntorch.manual_seed(5)\nlstmcell = torch.nn.LSTMCell(7,8).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(8,7).to(\"cuda\")\n\n\ntorch.manual_seed(5)\nlstm_cell = LSTM_cell(7,8).to(\"cuda\")\n#파라미터 초기화(from torch.nn.LSTMCELL)\nlstm_cell.linri2h.weight.data = lstmcell.weight_ih.data\nlstm_cell.linri2h.bias.data = lstmcell.bias_ih.data\nlstm_cell.linrh2h.weight.data = lstmcell.weight_hh.data\nlstm_cell.linrh2h.bias.data = lstmcell.bias_hh.data\n\nlinr_h2o = torch.nn.Linear(8,7).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstm_cell.parameters()) + list(linr_h2o.parameters()),lr=0.1)\n\n\nT = len(x)\nfor epoch in range(500):\n    h_t = c_t = torch.zeros((1,8)).to(\"cuda\")\n    loss = 0\n    for t in range(T):\n        x_t,y_t = x[[t]],y[[t]]\n        #1\n        h_t,c_t = lstm_cell(x_t,h_t,c_t)\n        out = linr_h2o(h_t)\n        #2\n        l_t = loss_fn(out,y_t)\n        loss += l_t\n    loss = loss/T\n    if epoch % 100 == 0:\n        print(loss)\n    #3\n    loss.backward()\n    #4\n    optimizer.step()\n    optimizer.zero_grad()\n\ntensor(1.8939, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0009, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)\n\n\n\nT = len(x)\nhidden = torch.zeros((T,8)).to(\"cuda\")\ncell = torch.zeros((T,8)).to(\"cuda\")\n\nh_0 = c_0 = torch.zeros((1,8)).to(\"cuda\") # t = 0일때의 h,c값 초기화\nhidden[[0]],cell[[0]] = lstm_cell(x[[0]],h_0,c_0) # t = 1일때의 h_1,c_t먼저 기록\nfor t in range(1,T): #t = 2일때부터 기록\n  x_t = x[[t]]\n  hidden[[t]],cell[[t]] = lstm_cell(x_t,hidden[[t-1]],cell[[t-1]])\n\n\nout = linr_h2o(hidden)\nyhat = soft(out)\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x7f6948c6bc10>,\n  <matplotlib.axis.XTick at 0x7f6948c6b990>,\n  <matplotlib.axis.XTick at 0x7f687c01eb90>,\n  <matplotlib.axis.XTick at 0x7f686f8d8590>,\n  <matplotlib.axis.XTick at 0x7f686f8d8ad0>,\n  <matplotlib.axis.XTick at 0x7f686f8d8210>,\n  <matplotlib.axis.XTick at 0x7f686f8dc5d0>],\n [Text(0, 1, 'h'),\n  Text(0, 1, 'i'),\n  Text(0, 1, '?'),\n  Text(0, 1, 'e'),\n  Text(0, 1, 'l'),\n  Text(0, 1, 'o'),\n  Text(0, 1, '!')])\n\n\n\n\n\nversion2 - class안사용\n\ntorch.manual_seed(5) \nlstm_cell = torch.nn.LSTMCell(7,8).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(8,7).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr_h2o.parameters()),lr=0.1)\n\n\nT = len(x)\nfor epoch in range(500):\n    h_t = c_t = torch.zeros((1,8)).to(\"cuda\")\n    loss = 0 \n    ## 1~2\n    for t in range(T):\n        x_t,y_t = x[[t]].to(\"cuda\"), y[[t]].to(\"cuda\")\n        \n        ## lstm_cell step1: calculate _ifgo \n        ifgo = x_t @ lstm_cell.weight_ih.T + h_t @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n        ## lstm_cell step2: decompose _ifgo \n        i_t = sig(ifgo[:,0:8])\n        f_t = sig(ifgo[:,8:16])\n        g_t = tanh(ifgo[:,16:24])\n        o_t = sig(ifgo[:,24:32])\n        ## lstm_cell step3: calculate ht,ct \n        c_t = f_t * c_t + i_t * g_t\n        h_t = o_t * tanh(c_t)\n        \n        out = linr_h2o(h_t) \n        #print(o_t.shape,y_t.shape)\n        loss = loss + loss_fn(out,y_t)\n    loss = loss / T\n    if epoch % 100 == 0:\n        print(loss)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\ntensor(1.8939, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0009, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)\n\n\n\nT = len(x)\nhidden = torch.zeros((T,8)).to(\"cuda\")\ncell = torch.zeros((T,8)).to(\"cuda\")\n\nh_0 = c_0 = torch.zeros((1,8)).to(\"cuda\") # t = 0일때의 h,c값 초기화\nhidden[[0]],cell[[0]] = lstm_cell(x[[0]],(h_0,c_0)) # t = 1일때의 h_1,c_t먼저 기록\n\nfor t in range(1,T): #t = 2일때부터 기록\n  x_t = x[[t]]\n  hidden[[t]],cell[[t]] = lstm_cell(x_t,(hidden[[t-1]],cell[[t-1]]))\n\n\nout = linr_h2o(hidden)\nyhat = soft(out)\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x7f68d005dfd0>,\n  <matplotlib.axis.XTick at 0x7f68d005df90>,\n  <matplotlib.axis.XTick at 0x7f686f82bc10>,\n  <matplotlib.axis.XTick at 0x7f686f830610>,\n  <matplotlib.axis.XTick at 0x7f686f830b50>,\n  <matplotlib.axis.XTick at 0x7f686f830d90>,\n  <matplotlib.axis.XTick at 0x7f686f854650>],\n [Text(0, 1, 'h'),\n  Text(0, 1, 'i'),\n  Text(0, 1, '?'),\n  Text(0, 1, 'e'),\n  Text(0, 1, 'l'),\n  Text(0, 1, 'o'),\n  Text(0, 1, '!')])\n\n\n\n\n\n\n\n2. 다음을 읽고 참 거짓을 판단하여라. (10점)\n\nLSTM은 RNN보다 장기기억에 유리하다. O \ntorch.nn.Embedding(num_embeddings=2,embedding_dim=1)와 torch.nn.Linear(in_features=1,out_features=1)의 학습가능한 파라메터수는 같다. O \n아래와 같은 네트워크를 고려하자.차원이 (n,1) 인 임의의 텐서에 대하여 net(x)와 net.forword(x)의 출력결과는 같다. O\n아래와 같이 a,b,c,d 가 반복되는 문자열이 반복되는 자료에서 다음문자열을 맞추는 과업을 수행하기 위해서는 반드시 순환신경망의 형태로 설계해야만 한다. O (5)RNN 혹은 LSTM 으로 신경망을 설계할 시 손실함수는 항상 torch.nn.CrossEntropyLoss 를 사용해야 한다. X"
  },
  {
    "objectID": "posts example/post-with-code/index.html",
    "href": "posts example/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts example/welcome/index.html",
    "href": "posts example/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html",
    "href": "python code/IAB Deep learning/DL 12-01.html",
    "title": "Hoyeon's Blog",
    "section": "",
    "text": "def f(txt,mapping):\n    return [mapping[chr] for chr in txt]\n\n\nimport torch\ntxt = list(\"hi?hello!!\") * 100\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\nmapping = {\"!\" : 0 ,\"?\" : 1,\"h\" : 2, \"i\" : 3,\"e\" : 4,\"l\" : 5, \"o\" : 6}\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\nprint(len(mapping))\n\n7\n\n\n\n\n\n- 코드1 : 정석코드\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\n_h0 = _c0 = torch.zeros(1,4).to(\"cuda:0\")\nlstm(x,(_h0,_c0))\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))\n\n\n- 코드2 : 초깃값이 없는 코드(c0,h0는 사실 없어도 무방함) - 알아서 차원맞춰서 잘 초기화 해줘요.\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\nlstm(x)\n\n(tensor([[-0.1547,  0.0673,  0.0695,  0.1563],\n         [-0.0786, -0.1430, -0.0250,  0.1189],\n         [-0.0300, -0.2256, -0.1324,  0.1439],\n         ...,\n         [-0.0723,  0.0620,  0.1913,  0.2015],\n         [-0.1155,  0.0746,  0.1747,  0.2938],\n         [-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n (tensor([[-0.2350, -0.1559, -0.1093,  0.2682]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>),\n  tensor([[-0.4451, -0.2456, -0.1900,  0.6232]], device='cuda:0',\n         grad_fn=<SqueezeBackward1>)))\n\n\n\n\n\n- timeseiries와 RNN timeseries 데이터는 시점(timestep)에 따라서 그 값들이 정렬된 데이터입니다. 각 시점에서의 값들은 서로 연관되어 있습니다. 하나의 timeseries 데이터를 RNN계열의 모형에 입력하면 각 시점에서 값을 재귀적으로 읽어서 \\(\\hat{y}\\)를 출력합니다. - 파라미터 설명 - \\(L\\) : sequnce length = timestep(시점)의 총 갯수 = timestp의 길이 = 시계열 데이터의 길이 - \\(H_{in}\\) : each timestep(시점)에 들어오는 입력백터의 길이, 입력시계열이 시점별로 몇개의 변수로 나타내어 지는가?,만약에 원핫인코딩으로 정리하면 단어수를 의미함,시점마다 길이는 모두 같음(전처리 과정에서 길이를 같게해주기 때문) - \\(N\\) : 전체 데이터를 몇개의 시계열(묶음)데이터 인지? = 전체데이터 안에 있는 시계열데이터 묶음의 갯수 = 전체데이터의 미니배치의 갯수, 예를들어 (1000,7)의 shape을 가진 시계열데이터를 2개의 묶음으로 나눈다면 N = batch size = 2,왜 쪼개는지는 나중에 … 공부\n- 코드3 : x의 차원은 사실 엄밀하게는 (\\(L\\),\\(N\\),\\(H_{in}\\))이다.\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda\")\n\n\nlstm(x.reshape(999,1,7)) #전체데이터를 쪼개지 않았으므로 즉 묶음은 1개뿐이므로 N = 1\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563]],\n \n         [[-0.0786, -0.1430, -0.0250,  0.1189]],\n \n         [[-0.0300, -0.2256, -0.1324,  0.1439]],\n \n         ...,\n \n         [[-0.0723,  0.0620,  0.1913,  0.2015]],\n \n         [[-0.1155,  0.0746,  0.1747,  0.2938]],\n \n         [[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))\n\n\n- 코드4 : batch_first = True일 경우 차원은 (\\(N\\),\\(L\\),\\(H_{in}\\))이다.\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4,batch_first = True).to(\"cuda\")\nlstm(x.reshape(1,999,7)) #전체데이터를 쪼개지 않았으므로 즉 묶음은 1개뿐이므로 N = 1\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563],\n          [-0.0786, -0.1430, -0.0250,  0.1189],\n          [-0.0300, -0.2256, -0.1324,  0.1439],\n          ...,\n          [-0.0723,  0.0620,  0.1913,  0.2015],\n          [-0.1155,  0.0746,  0.1747,  0.2938],\n          [-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))\n\n\n\n\n\n네트워크에 우리는 h_0,c_0를 넣었었다.그때 h_0.shape = c_0.shape = (1,\\(H_{out}\\))이었는데 여기서 사실은 1은 \\(D \\times `num_layers`\\)입니다. 우리는 1개의 히든레이어를 가진 LSTM이므로 num_layers= 1 따라서 1이었습니다.\n-파라미터 설명 - \\(D\\) = 2 if bidirectional = True otherwise = 1 (양방향이면2 단방향이면1,우리는 단방향만 써왔으므로 \\(D\\) = 1) - num_layers = 히든레이어가 1개 이상인 경우(중첩된 RNN) - \\(H_{out}\\) = 히든노드의 수 - \\(N\\) : 전체시계열 데이터안에 있는 묶음의 갯수 = 전체데이터 안에 있는 시계열데이터 묶음의 갯수 = 전체데이터의 미니배치의 갯수, 미니배치안에 있는 원소들의 갯수가 아님 !! 예를들어 (1000,7)의 shape을 가진 시계열데이터를 2개의 묶음으로 나눈다면 N = batch size = 2,왜 쪼개는지는 나중에 … 공부\n-코드5 - x.shape = $(1,H_{in}) $ x.shape = \\((L,1,H_{in})\\) - h.shape = $(1,H_{out}) $ h.shape = \\((1,1,H_{out})\\)\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(7,4).to(\"cuda:0\")\n\n\n#엄밀한 방식\n_h0,_c0 = torch.zeros(1,4).to(\"cuda:0\"),torch.zeros(1,4).to(\"cuda:0\")\nlstm(x.reshape(999,1,7),(_h0.reshape(1,1,4),_c0.reshape(1,1,4)))\n\n#엄밀한 방식2\n_h0 = _c0 = torch.zeros(1,1,4).to(\"cuda:0\")\nlstm(x.reshape(999,1,7),(_h0,_c0))\n\n(tensor([[[-0.1547,  0.0673,  0.0695,  0.1563]],\n \n         [[-0.0786, -0.1430, -0.0250,  0.1189]],\n \n         [[-0.0300, -0.2256, -0.1324,  0.1439]],\n \n         ...,\n \n         [[-0.0723,  0.0620,  0.1913,  0.2015]],\n \n         [[-0.1155,  0.0746,  0.1747,  0.2938]],\n \n         [[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n        grad_fn=<CudnnRnnBackward0>),\n (tensor([[[-0.2350, -0.1559, -0.1093,  0.2682]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>),\n  tensor([[[-0.4451, -0.2456, -0.1900,  0.6232]]], device='cuda:0',\n         grad_fn=<CudnnRnnBackward0>)))\n\n\n- 사실 _h0.shape= _c0.shape = (\\(1,H_{out}\\))에서 1은 \\(D \\times\\) numlayers입니다. - 만약 은닉계층의 수가 1개인 lstm이라면 _h0.shape = _c0.shape = \\((1,H_{out})\\) - 만약 은닉계층의 수는 1개,양방향 lstm이라면 _h0.shape = _c0.shape = \\((2,H_{out})\\) - 만약 은닉계층의 수가 3개인 (단방향)lstm이라면 _h0.shape = _c0.shape = \\((3,H_{out})\\) - 만약 은닉계층의 수가 3개,양방향 lstm이라면 _h0.shape = _c0.shape = \\((6,H_{out})\\)"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#세트2-xt.shape-nh_in-or-h_in",
    "href": "python code/IAB Deep learning/DL 12-01.html#세트2-xt.shape-nh_in-or-h_in",
    "title": "Hoyeon's Blog",
    "section": "세트2 : xt.shape = \\((N,H_{in}) or (H_{in})\\)",
    "text": "세트2 : xt.shape = \\((N,H_{in}) or (H_{in})\\)\n-코드2:초깃값 생략\n\ntorch.manual_seed(43052)\nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda\")\n\n\nxt = x[[1]]\nxt.shape\n\ntorch.Size([1, 7])\n\n\n\nlstmcell(xt)\n\n(tensor([[-0.0290, -0.1758, -0.0537,  0.0598]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>),\n tensor([[-0.0582, -0.4566, -0.1256,  0.1922]], device='cuda:0',\n        grad_fn=<ThnnFusedLstmCellBackward0>))\n\n\n-코드3:간단한 shape 사용\n\ntorch.manual_seed(43052)\nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\")\n\n\nxt = x[1]\nxt.shape #위의 shape과 다른것을 확인\n\ntorch.Size([7])\n\n\n\nlstmcell(xt)\n\n(tensor([-0.0290, -0.1758, -0.0537,  0.0598], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n tensor([-0.0582, -0.4566, -0.1256,  0.1922], device='cuda:0',\n        grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#세트3-hidden.shape-nh_out-or-h_out",
    "href": "python code/IAB Deep learning/DL 12-01.html#세트3-hidden.shape-nh_out-or-h_out",
    "title": "Hoyeon's Blog",
    "section": "세트3 : hidden.shape = \\((N,H_{out})\\) or \\((H_{out})\\)",
    "text": "세트3 : hidden.shape = \\((N,H_{out})\\) or \\((H_{out})\\)\n- 코드4 xt.shape = \\((1,H_{in}) \\to (H_{in})\\) ht.shape = \\((1,H_{out}) \\to (H_{out})\\) 더 간단한 차원을 가진 텐서를 입력으로 해도 무방하다.\n\ntorch.manual_seed(43052)\nlstmcell = torch.nn.LSTMCell(7,4).to(\"cuda:0\")\n\n\nxt = x[1]\n_h0,_c0 = torch.zeros(4).to(\"cuda\"),torch.zeros(4).to(\"cuda\")\nxt.shape,_h0.shape,_c0.shape\n\n(torch.Size([7]), torch.Size([4]), torch.Size([4]))\n\n\n\nlstmcell(xt,(_h0,_c0))\n\n(tensor([-0.0290, -0.1758, -0.0537,  0.0598], device='cuda:0',\n        grad_fn=<SqueezeBackward1>),\n tensor([-0.0582, -0.4566, -0.1256,  0.1922], device='cuda:0',\n        grad_fn=<SqueezeBackward1>))"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#똑같은-코드들-정리",
    "href": "python code/IAB Deep learning/DL 12-01.html#똑같은-코드들-정리",
    "title": "Hoyeon's Blog",
    "section": "똑같은 코드들 정리",
    "text": "똑같은 코드들 정리\n- 1은 단순한 observation의 차원이 아니다. - 네트워크가(1) 단방향 (2)배치가없는(조각이 없는) (3)중첩하지 않은(다계층이 아닌) 순환망에 한해서는 observation 처럼 생각해도 무방하다. –> 엄밀하게는 위험한 생각입니다."
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#실제구현시",
    "href": "python code/IAB Deep learning/DL 12-01.html#실제구현시",
    "title": "Hoyeon's Blog",
    "section": "실제구현시",
    "text": "실제구현시\n- 현실적으로 (1)-(3)이 아닌 조건에서는 Cell 단위로 연산을 이용할 일이 없다. (느립니다.단지 이해용) - torch.nn.RNN or torch.nn.LSTM으로 네트워크를 구성할 시 _h0,_c0의 dim을 명시할 일도 없다. - 입력시계열데이터를 배치로 나누는 경우에 대한 개념만 명확하게 잡으면 된다.배치로 나눌지 말지 나누면 어떻게 되고 안나누면 어떻게 되는지 명확하게 알자."
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#하나의-긴-시계열데이터를-배치로-나누지-않고-학습",
    "href": "python code/IAB Deep learning/DL 12-01.html#하나의-긴-시계열데이터를-배치로-나누지-않고-학습",
    "title": "Hoyeon's Blog",
    "section": "하나의 긴 시계열데이터를 배치로 나누지 않고 학습",
    "text": "하나의 긴 시계열데이터를 배치로 나누지 않고 학습\n\n배치로 나눴을 경우와 비교하기 위해 먼저 나누지 않고 학습진행.\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\nmapping = {\"!\": 0 ,\"?\":1 , \"h\":2 ,\"i\" : 3}\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda\")\nprint(x.shape)\n\ntorch.Size([17, 4])\n\n\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(4,10).to(\"cuda\")\nlinr = torch.nn.Linear(10,4).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstm.parameters()) + list(linr.parameters()))\n\n\nfor epoch in range(1000):\n    ## 1\n    hidden,(h_T,c_T) = lstm(x)\n    #hidden은 가장 깊이 있는 은닉계층의 모든 시점에서 output이므로 hidden.shape = (17,10)\n    #h_T,c_T는 모든 은닉계층에서 마지막 시점에서 output이므로 h_T(c_T).shape = c_T.shape = (1,10)\n    print(hidden.shape,h_T.shape,c_T.shape)\n    output = linr(hidden)\n    ## 2\n    loss = loss_fn(output,y)\n    ## 3\n    loss.backward()\n    ## 4\n    optimizer.step()\n    optimizer.zero_grad()\n\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\ntorch.Size([17, 10]) torch.Size([1, 10]) torch.Size([1, 10])\n\n\n\nimport matplotlib.pyplot as plt\nsoft = torch.nn.Softmax(dim=1)\nplt.matshow(soft(output).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x165602a6d30>"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#하나의-긴-시계열-데이터를-배치로-나눠서-학습",
    "href": "python code/IAB Deep learning/DL 12-01.html#하나의-긴-시계열-데이터를-배치로-나눠서-학습",
    "title": "Hoyeon's Blog",
    "section": "하나의 긴 시계열 데이터를 배치로 나눠서 학습",
    "text": "하나의 긴 시계열 데이터를 배치로 나눠서 학습\n시계열 데이터셋이 배치로 나뉘어져 있을 경우,파이토치에서는 \\((L,N,H_{in})\\)의 shape을 요구한다. 다음의 과정은 shape을 맞추기 위해 진행하는 과정이다.\n\ntxt1 = txt[:9]\ntxt2 = txt[9:]\n\n\ntxt1_x = txt1[:-1]\ntxt1_y = txt1[1:]\ntxt2_x = txt2[:-1]\ntxt2_y = txt2[1:]\n\n\nmapping = {\"!\":0,\"?\":1,\"h\":2,\"i\":3}\nx1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_x,mapping))).float().to(\"cuda\")\ny1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_y,mapping))).float().to(\"cuda\")\nx2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_x,mapping))).float().to(\"cuda\")\ny2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_y,mapping))).float().to(\"cuda\")\n\n\nx1.shape,y1.shape,x2.shape,y2.shape\n\n(torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]))\n\n\n\nxx = torch.stack([x1,x2],axis=1)\nyy = torch.stack([y1,y2],axis=1)\nxx.shape,yy.shape\n\n(torch.Size([8, 2, 4]), torch.Size([8, 2, 4]))\n\n\n\nxx,yy\n\n(tensor([[[0., 0., 1., 0.],\n          [0., 0., 1., 0.]],\n \n         [[0., 0., 0., 1.],\n          [0., 0., 0., 1.]],\n \n         [[1., 0., 0., 0.],\n          [0., 1., 0., 0.]],\n \n         [[0., 0., 1., 0.],\n          [0., 0., 1., 0.]],\n \n         [[0., 0., 0., 1.],\n          [0., 0., 0., 1.]],\n \n         [[1., 0., 0., 0.],\n          [0., 1., 0., 0.]],\n \n         [[0., 0., 1., 0.],\n          [0., 0., 1., 0.]],\n \n         [[0., 0., 0., 1.],\n          [0., 0., 0., 1.]]], device='cuda:0'),\n tensor([[[0., 0., 0., 1.],\n          [0., 0., 0., 1.]],\n \n         [[1., 0., 0., 0.],\n          [0., 1., 0., 0.]],\n \n         [[0., 0., 1., 0.],\n          [0., 0., 1., 0.]],\n \n         [[0., 0., 0., 1.],\n          [0., 0., 0., 1.]],\n \n         [[1., 0., 0., 0.],\n          [0., 1., 0., 0.]],\n \n         [[0., 0., 1., 0.],\n          [0., 0., 1., 0.]],\n \n         [[0., 0., 0., 1.],\n          [0., 0., 0., 1.]],\n \n         [[1., 0., 0., 0.],\n          [0., 1., 0., 0.]]], device='cuda:0'))\n\n\ntorch.stack에서 axis의 인자로 1을 넣어 \\(batch size = 2\\)인 \\((8,2,4)\\)의 shape을 가지는 텐서를 만듦\n준비단계\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(4,10).to(\"cuda\")\nlinr = torch.nn.Linear(10,4).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr = 0.1)\n\n학습단계 - 먼저 관찰하기위해서 ’hi!’가 3번반복되는 배치와 hi?가 3번반복되는 배치를 따로 분리하고 네트워크에 입력하여 학습.\n\nfor epoch in range(100):\n    ## 1\n    hidden,(h_T,c_T) = lstm(xx)\n    #hidden은 가장 깊이 있는 은닉계층의 모든 시점에서 output이므로 hidden.shape = (17,10)\n    #h_T,c_T는 모든 은닉계층에서 마지막 시점에서 output이므로 h_T(c_T).shape = c_T.shape = (1,10)\n    #print(hidden.shape,h_T.shape,c_T.shape)\n    output = linr(hidden)\n    ## 2\n    loss = loss_fn(output[:,0,:],yy[:,0,:]) + loss_fn(output[:,1,:],yy[:,1,:])\n    if epoch % 20 == 0:\n        print(epoch,loss)\n    ## 3\n    loss.backward()\n    ## 4\n    optimizer.step()\n    optimizer.zero_grad()\n\n0 tensor(2.7541, device='cuda:0', grad_fn=<AddBackward0>)\n20 tensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)\n40 tensor(0.1739, device='cuda:0', grad_fn=<AddBackward0>)\n60 tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\n80 tensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\nfig,ax = plt.subplots(1,2)\nax[0].matshow(soft(output[:,0,:]).to(\"cpu\").data,cmap=\"bwr\",vmin=-1,vmax=1)\nax[1].matshow(soft(output[:,1,:]).to(\"cpu\").data,cmap=\"bwr\",vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x16563cda370>\n\n\n\n\n\n\nhidden,_ = lstm(x)\nplt.matshow(soft(linr(hidden)).to(\"cpu\").data,cmap=\"bwr\",vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x165645e1df0>\n\n\n\n\n\nhi?와 hi!인 경우를 분리하여 각각 네트워크를 학습시킬 경우, 분리된 데이터에 대해 잘 예측함\n그런데 hi!와 hi?가 분리되어 있지 않은 전체데이터를 입력으로 줬을때는 전혀 예측을 하지 못하는 모습… 이는 hi!와 hi?를 따로따로 분리하여 학습해서 !가 나오면 hi!만 반복되고 ?가 나오면 hi?만 반복되는 것으로 학습했기 때문이다. 이렇게 학습을 시키면 hi!가 3번나온 뒤 hi?가 3번나오는 데이터를 모델링하지 못한다.연결,흐름,경향은 학습하지 못했기 때문이다.\n\nyy.shape,y.shape,xx.shape,x.shape\n\n(torch.Size([8, 2, 4]),\n torch.Size([17, 4]),\n torch.Size([8, 2, 4]),\n torch.Size([17, 4]))"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#재미있는-실험",
    "href": "python code/IAB Deep learning/DL 12-01.html#재미있는-실험",
    "title": "Hoyeon's Blog",
    "section": "재미있는 실험",
    "text": "재미있는 실험\n- x1만 배운네트워크에 x2를 넣는다면? - 즉, hi!만 배운 네트워크에 hi?데이터에 대해서 예측하라 하면?\n준비단계\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(4,10).to(\"cuda\")\nlinr = torch.nn.Linear(10,4).to(\"cuda\")\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer= torch.optim.Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n\n\nfor epoch in range(100):\n    ##1 output\n    hidden,(h_T,c_T) = lstm(x1)\n    output = linr(hidden)\n    ##2 loss\n    loss = loss_fn(output,y1)\n    ##3 derivative\n    loss.backward()\n    ##4 update & clean\n    optimizer.step()\n    optimizer.zero_grad()\n\n\nhidden,(h_T,c_T) = lstm(x2)\nplt.matshow(soft(linr(hidden)).data.to(\"cpu\"),cmap=\"bwr\",vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x16566df0760>\n\n\n\n\n\nhi?를 넣어도 hi!로 예측함! 만약 x2만 넣은 경우도 마찬가지 결과가 나온다(생략)\n정리 : 하나의 긴 시계열 데이터를 n개의 배치들로 나누어서 학습할 수 있다. 다만 이러한 경우에 전체시계열데이터의를 네트워크에 학습시키는 것이 아니기 때문에 전체적인 패턴,흐름,연결은 학습하지 못한다."
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#왜-배치로-나눠서-시계열-데이터를-학습해야-하는가",
    "href": "python code/IAB Deep learning/DL 12-01.html#왜-배치로-나눠서-시계열-데이터를-학습해야-하는가",
    "title": "Hoyeon's Blog",
    "section": "왜 배치로 나눠서 시계열 데이터를 학습해야 하는가?",
    "text": "왜 배치로 나눠서 시계열 데이터를 학습해야 하는가?\n\n속도가 빠르다.\n연결,흐름을 파악하지 않아도 되는 독립적인 시계열 데이터셋이 있는 경우도 있다.(예를들어,독립적인 여러개의 댓글이 있는 시계열 데이터셋의 경우)\n이해하지 못했다 … ㅜㅜ\n\n참고 배치사이즈는 배치가 몇개인가? or 배치안에 있는 원소가 몇개인가? 두가지 서로다른 의미가 있다. document별로 알아서 해석해야 한다."
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#똑같은-코드들-fastaipytorch",
    "href": "python code/IAB Deep learning/DL 12-01.html#똑같은-코드들-fastaipytorch",
    "title": "Hoyeon's Blog",
    "section": "똑같은 코드들 fastai,pytorch",
    "text": "똑같은 코드들 fastai,pytorch\n\ntxt = (['one',',','two',',','three',',','four',',','five',',']*100)[:-1]\n\n\nmapping = {',':0, 'one':1, 'two':2, 'three':3, 'four':4, 'five':5} \nmapping\n\n{',': 0, 'one': 1, 'two': 2, 'three': 3, 'four': 4, 'five': 5}\n\n\n\ntxt_x = txt[:-1]\ntxt_y = txt[1:]\n\n\ntxt_x[0:5], txt_y[0:5]\n\n(['one', ',', 'two', ',', 'three'], [',', 'two', ',', 'three', ','])\n\n\n\nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda\")"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#fastai-사용",
    "href": "python code/IAB Deep learning/DL 12-01.html#fastai-사용",
    "title": "Hoyeon's Blog",
    "section": "fastai 사용",
    "text": "fastai 사용\n\nfrom fastai.text.all import *\n#import pytorch_lightning as pl \ntr_ds = torch.utils.data.TensorDataset(x,y) \nval_ds = torch.utils.data.TensorDataset(x,y) #dummy,validation이 있다고 가정\ntr_ldr = torch.utils.data.DataLoader(tr_ds,batch_size=998) #한개의 배치안에 998개의 원소가 있도록\nval_ldr = torch.utils.data.DataLoader(val_ds,batch_size=998)\ndls = DataLoaders(tr_ldr,val_ldr)\n\n\nclass MyLSTM(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        torch.manual_seed(43052)\n        self.lstm = torch.nn.LSTM(6,20)\n        self.linr = torch.nn.Linear(20,6) \n    def forward(self,x):\n        _water = torch.zeros(1,20).to(\"cuda:0\")\n        hidden, (hT,cT) =self.lstm(x,(_water,_water))\n        output = self.linr(hidden)\n        return output         \n\n\nnet = MyLSTM().to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\n#optimizer 생략(fastai 구현시)\nlrnr = Learner(dls,net,loss_fn,lr = 0.1)\n\n\nlrnr.fit(30)\n\n\n\n\n\n\n\n  \n    \n      epoch\n      train_loss\n      valid_loss\n      time\n    \n  \n  \n    \n      0\n      1.762846\n      1.502211\n      00:00\n    \n    \n      1\n      1.631212\n      1.620583\n      00:00\n    \n    \n      2\n      1.627597\n      1.443686\n      00:00\n    \n    \n      3\n      1.580216\n      1.368762\n      00:00\n    \n    \n      4\n      1.536200\n      1.307310\n      00:00\n    \n    \n      5\n      1.496099\n      1.216339\n      00:00\n    \n    \n      6\n      1.453670\n      1.113821\n      00:00\n    \n    \n      7\n      1.408125\n      1.019931\n      00:00\n    \n    \n      8\n      1.361426\n      0.941434\n      00:00\n    \n    \n      9\n      1.315507\n      0.884033\n      00:00\n    \n    \n      10\n      1.272201\n      0.848489\n      00:00\n    \n    \n      11\n      1.232838\n      0.826641\n      00:00\n    \n    \n      12\n      1.197666\n      0.814630\n      00:00\n    \n    \n      13\n      1.166570\n      0.809186\n      00:00\n    \n    \n      14\n      1.139229\n      0.805969\n      00:00\n    \n    \n      15\n      1.115098\n      0.798611\n      00:00\n    \n    \n      16\n      1.093322\n      0.779956\n      00:00\n    \n    \n      17\n      1.072764\n      0.746476\n      00:00\n    \n    \n      18\n      1.052292\n      0.703937\n      00:00\n    \n    \n      19\n      1.031332\n      0.643301\n      00:00\n    \n    \n      20\n      1.008886\n      0.574666\n      00:00\n    \n    \n      21\n      0.984684\n      0.496858\n      00:00\n    \n    \n      22\n      0.958432\n      0.423367\n      00:00\n    \n    \n      23\n      0.930580\n      0.360229\n      00:00\n    \n    \n      24\n      0.901813\n      0.301070\n      00:00\n    \n    \n      25\n      0.872408\n      0.251317\n      00:00\n    \n    \n      26\n      0.842863\n      0.199397\n      00:00\n    \n    \n      27\n      0.813075\n      0.151757\n      00:00\n    \n    \n      28\n      0.783244\n      0.109921\n      00:00\n    \n    \n      29\n      0.753616\n      0.079872\n      00:00\n    \n  \n\n\n\n\nsoft(lrnr.model(x)).data.to(\"cpu\").numpy().round(3)\n\narray([[0.998, 0.   , 0.002, 0.   , 0.   , 0.   ],\n       [0.   , 0.018, 0.514, 0.097, 0.096, 0.275],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       ...,\n       [0.001, 0.002, 0.025, 0.001, 0.938, 0.033],\n       [1.   , 0.   , 0.   , 0.   , 0.   , 0.   ],\n       [0.   , 0.066, 0.049, 0.005, 0.079, 0.8  ]], dtype=float32)"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#torch사용",
    "href": "python code/IAB Deep learning/DL 12-01.html#torch사용",
    "title": "Hoyeon's Blog",
    "section": "torch사용",
    "text": "torch사용\n\ntorch.manual_seed(43052) \nlstm = torch.nn.LSTM(6,20).to(\"cuda:0\") \nlinr = torch.nn.Linear(20,6).to(\"cuda:0\") \nloss_fn = torch.nn.CrossEntropyLoss()\noptimizr = Adam(list(lstm.parameters())+list(linr.parameters()),lr=0.1)\n#동일함을 보이기 위해 fastai의 Adam사용\n\n\nfor epoc in range(10):\n    ## 1 \n    hidden, _ = lstm(x)\n    output = linr(hidden) \n    ## 2 \n    loss = loss_fn(output,y) \n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()     \n\n\nhidden, _ = lstm(x)\noutput = linr(hidden) \nsoft(output).data.to(\"cpu\").numpy().round(3)\n\narray([[0.935, 0.009, 0.015, 0.011, 0.016, 0.014],\n       [0.133, 0.164, 0.242, 0.172, 0.141, 0.147],\n       [0.982, 0.003, 0.004, 0.003, 0.004, 0.003],\n       ...,\n       [0.122, 0.171, 0.242, 0.174, 0.146, 0.144],\n       [0.984, 0.003, 0.004, 0.002, 0.004, 0.003],\n       [0.119, 0.172, 0.244, 0.175, 0.145, 0.145]], dtype=float32)"
  },
  {
    "objectID": "python code/IAB Deep learning/DL 12-01.html#human-numbers-100",
    "href": "python code/IAB Deep learning/DL 12-01.html#human-numbers-100",
    "title": "Hoyeon's Blog",
    "section": "human numbers 100",
    "text": "human numbers 100\n\ndf = pd.read_csv('https://raw.githubusercontent.com/guebin/DL2022/main/posts/IV.%20RNN/2022-11-25-human_numbers_100.csv')\ndf\n\n\n\n\n\n  \n    \n      \n      text\n    \n  \n  \n    \n      0\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      2\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      3\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      4\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      ...\n      ...\n    \n    \n      1995\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1996\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1997\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1998\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n    \n      1999\n      one, two, three, four, five, six, seven, eight, nine, ten, eleven, twelve, thirteen, fourteen, fifteen, sixteen, seventeen, eighteen, nineteen, twenty, twenty one, twenty two, twenty three, twenty four, twenty five, twenty six, twenty seven, twenty eight, twenty nine, thirty, thirty one, thirty two, thirty three, thirty four, thirty five, thirty six, thirty seven, thirty eight, thirty nine, forty, forty one, forty two, forty three, forty four, forty five, forty six, forty seven, forty eight, forty nine, fifty, fifty one, fifty two, fifty three, fifty four, fifty five, fifty six, fifty seve...\n    \n  \n\n2000 rows × 1 columns\n\n\n\n\n생략 .. 귀찮음\n\ndls = TextDataLoaders.from_df(df,is_lm=True,seq_len=5,text_col='text')\ndls.show_batch()\n\nDue to IPython and Windows limitation, python multiprocessing isn't available now.\nSo `n_workers` has to be changed to 0 to avoid getting stuck\n\n\n\n\n  \n    \n      \n      text\n      text_\n    \n  \n  \n    \n      0\n      xxbos one , two ,\n      one , two , three\n    \n    \n      1\n      hundred xxbos one , two\n      xxbos one , two ,\n    \n    \n      2\n      one hundred xxbos one ,\n      hundred xxbos one , two\n    \n    \n      3\n      , one hundred xxbos one\n      one hundred xxbos one ,\n    \n    \n      4\n      nine , one hundred xxbos\n      , one hundred xxbos one\n    \n    \n      5\n      ninety nine , one hundred\n      nine , one hundred xxbos\n    \n    \n      6\n      , ninety nine , one\n      ninety nine , one hundred\n    \n    \n      7\n      eight , ninety nine ,\n      , ninety nine , one\n    \n    \n      8\n      ninety eight , ninety nine\n      eight , ninety nine ,"
  },
  {
    "objectID": "python code/pytorch 구현/pytorch lstm hi느낌표hi물음표 모델링.html",
    "href": "python code/pytorch 구현/pytorch lstm hi느낌표hi물음표 모델링.html",
    "title": "pytorch lstm 구현하기",
    "section": "",
    "text": "pytorch-RNN으로 hi?hi!가 계속해서 반복되는 sequencedata를 모델링했습니다."
  },
  {
    "objectID": "python code/pytorch 구현/pytorch lstm hi느낌표hi물음표 모델링.html#setting",
    "href": "python code/pytorch 구현/pytorch lstm hi느낌표hi물음표 모델링.html#setting",
    "title": "pytorch lstm 구현하기",
    "section": "setting",
    "text": "setting\n\nimport torch\n\n\ndef f(txt,mapping):\n    return [mapping[chr] for chr in txt]"
  },
  {
    "objectID": "python code/pytorch 구현/pytorch lstm hi느낌표hi물음표 모델링.html#data",
    "href": "python code/pytorch 구현/pytorch lstm hi느낌표hi물음표 모델링.html#data",
    "title": "pytorch lstm 구현하기",
    "section": "data",
    "text": "data\n\ntxt = list('hi!')*3 + list('hi?')*3 \ntxt_x = txt[:-1] \ntxt_y = txt[1:] \n\n\nmapping = {'!':0, '?':1, 'h':2, 'i':3} \nx = torch.nn.functional.one_hot(torch.tensor(f(txt_x,mapping))).float().to(\"cuda:0\")\ny = torch.nn.functional.one_hot(torch.tensor(f(txt_y,mapping))).float().to(\"cuda:0\")\n\n\ntxt1= txt[:9]\ntxt2= txt[9:]\n\n\ntxt1_x = txt1[:-1] \ntxt1_y = txt1[1:] \ntxt2_x = txt2[:-1] \ntxt2_y = txt2[1:] \n\n\nmapping = {'!':0, '?':1, 'h':2, 'i':3} \nx1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_x,mapping))).float().to(\"cuda:0\")\ny1 = torch.nn.functional.one_hot(torch.tensor(f(txt1_y,mapping))).float().to(\"cuda:0\")\nx2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_x,mapping))).float().to(\"cuda:0\")\ny2 = torch.nn.functional.one_hot(torch.tensor(f(txt2_y,mapping))).float().to(\"cuda:0\")\nx1.shape,y1.shape,x2.shape,y2.shape\n\n(torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]),\n torch.Size([8, 4]))\n\n\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(4,10).to(\"cuda\")\nlinr = torch.nn.Linear(10,4).to(\"cuda\")\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstm.parameters()) + list(linr.parameters()),lr = 0.1)\n\n\nxx = torch.stack([x1,x2],axis=1)\nyy = torch.stack([y1,y2],axis=1)\nxx.shape, yy.shape\n\n(torch.Size([8, 2, 4]), torch.Size([8, 2, 4]))\n\n\n\nfor epoc in range(100):\n    ##1 output\n    hidden,_ = lstm(xx)\n    output = linr(hidden)\n    \n    ##2 loss\n    loss = 0\n    \"\"\"\n    한참해맨 부분 ... 그냥 무지성 loss(output,yy)해서 틀림 ...\n    파이토치 cross entropy 부분 정리\n    \"\"\"\n    for batch_num in range(output.shape[1]):\n        batch_out = output[:,batch_num,:]\n        batch_y = yy[:,batch_num,:]\n        #print(batch_out.shape,batch_y.shape)\n        loss += loss_fn(batch_out,batch_y)\n    ##3 derivative\n    loss.backward()\n    if epoc % 10 == 0:\n        print(loss)\n    ##4 \n    optimizer.step()\n    optimizer.zero_grad()\n    \n\ntensor(2.7541, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.4173, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1739, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1737, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\nimport matplotlib.pyplot as plt\nsoft = torch.nn.Softmax(dim=1)\n\nfig , ax = plt.subplots(1,2) \nax[0].matshow(soft(output[:,0,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\nax[1].matshow(soft(output[:,1,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x1f29cd1f400>\n\n\n\n\n\n\nhidden,_ = lstm(x)\nplt.matshow(soft(linr(hidden)).data.to(\"cpu\"),cmap=\"bwr\",vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x1f29e4188e0>\n\n\n\n\n\n\ntorch.manual_seed(43052)\nlstm = torch.nn.LSTM(4,10).to(\"cuda\")\nlinr = torch.nn.Linear(10,4).to(\"cuda\")\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstm.parameters()) + list(linr.parameters()),lr = 0.1)\n\n\nfor epoc in range(200):\n    ##1 output\n    hidden,_ = lstm(xx)\n    output = linr(hidden)\n    #print(output.shape,yy.shape)\n    #print(output[:,0,:].shape,yy[:,0,:].shape)\n    ##2 loss\n    loss = loss_fn(output[:,0,:],yy[:,0,:]) + loss_fn(output[:,1,:],yy[:,1,:])\n    ##3\n    loss.backward()\n    if epoc % 10 == 0:\n        print(loss)\n    ##4\n    optimizer.step()\n    optimizer.zero_grad()\n\ntensor(2.7541, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.4173, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1803, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1744, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1739, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1737, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1735, device='cuda:0', grad_fn=<AddBackward0>)\ntensor(0.1734, device='cuda:0', grad_fn=<AddBackward0>)\n\n\n\nimport matplotlib.pyplot as plt\nsoft = torch.nn.Softmax(dim=1)\n\nfig , ax = plt.subplots(1,2) \nax[0].matshow(soft(output[:,0,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\nax[1].matshow(soft(output[:,1,:]).to(\"cpu\").data,cmap='bwr',vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x1f2a16e8ee0>\n\n\n\n\n\n\nhidden,_ = lstm(x)\nplt.matshow(soft(linr(hidden)).data.to(\"cpu\"),cmap=\"bwr\",vmin=-1,vmax=1)\n\n<matplotlib.image.AxesImage at 0x1f2a18ec9d0>"
  },
  {
    "objectID": "python code/quarto/Authoring.html",
    "href": "python code/quarto/Authoring.html",
    "title": "Quarto Document",
    "section": "",
    "text": "모든 형식을 작성할때 yaml format을 가장 상단에 작성한다. 들어갈 수 있는 기본적인 것들은 아래와 같다.\n\n\ntitle,author : 모든 문서에 다 들어가는 메타데이타\n\n\n\ntoc(table of contents) : 목차 number-sections : 섹션마다 번호 부여 highlight-style : ??\n\n\n\n다양한 포맷으로 한번에 랜더링할 수 있도록 형식지정 가능 밑에 코드 참조 pdf에 마진을 변경함 docx 옵션은 기본값들만 사용\n\n\n\nquarto render Authoring.ipynb format에 정해놓은 형식 모두 랜더링\n아래는 랜더링한 것들을 보여주는 코드 quarto preview Authoring.ipynb –to html quarto preview Authoring.ipynb –to pdf quarto preview Authoring.ipynb –to docx"
  },
  {
    "objectID": "python code/quarto/Authoring.html#colors",
    "href": "python code/quarto/Authoring.html#colors",
    "title": "Quarto Document",
    "section": "0.2 Colors",
    "text": "0.2 Colors\n\nRed\nGreen\nBlue"
  },
  {
    "objectID": "python code/quarto/Authoring.html#shapes",
    "href": "python code/quarto/Authoring.html#shapes",
    "title": "Quarto Document",
    "section": "0.3 Shapes",
    "text": "0.3 Shapes\n\nSquare\nCircle\nTriangle"
  },
  {
    "objectID": "python code/quarto/Authoring.html#textures",
    "href": "python code/quarto/Authoring.html#textures",
    "title": "Quarto Document",
    "section": "0.4 Textures",
    "text": "0.4 Textures\n\nSmooth\nBumpy\nFuzzy"
  },
  {
    "objectID": "python code/quarto/Authoring.html#수식도-적을-수-있음",
    "href": "python code/quarto/Authoring.html#수식도-적을-수-있음",
    "title": "Quarto Document",
    "section": "0.5 수식도 적을 수 있음",
    "text": "0.5 수식도 적을 수 있음\nE = mc^{2} E = mc^{2}"
  },
  {
    "objectID": "python code/quarto/Authoring.html#cross-references",
    "href": "python code/quarto/Authoring.html#cross-references",
    "title": "Quarto Document",
    "section": "1.1 Cross References",
    "text": "1.1 Cross References"
  },
  {
    "objectID": "python code/quarto/Authoring.html#overview",
    "href": "python code/quarto/Authoring.html#overview",
    "title": "Quarto Document",
    "section": "1.2 Overview",
    "text": "1.2 Overview\nSee Figure 1 in Section 1.3 for a demonstration of a simple plot See Equation 1 to better understand std"
  },
  {
    "objectID": "python code/quarto/Authoring.html#sec-plot",
    "href": "python code/quarto/Authoring.html#sec-plot",
    "title": "Quarto Document",
    "section": "1.3 Plot",
    "text": "1.3 Plot\n\n@sec-plot을 누르면 {#sec-plot}이 있는 위치로 화면이동(참조)\n@fig-simple을 누르면 #| label: fig-simple이 있는 화면이동(참조)\n단순히 #만 사용하면 피규어나 수식과 같은 것들 끝에 인덱스를 생성해줌.std참고\n\n\n\nCode\nimport matplotlib.pyplot as plt\nplt.plot([1,23,2,4])\nplt.show()\n\n\n\n\n\nFigure 1: Simple plot"
  },
  {
    "objectID": "python code/quarto/Authoring.html#sec-equation",
    "href": "python code/quarto/Authoring.html#sec-equation",
    "title": "Quarto Document",
    "section": "1.4 Equation",
    "text": "1.4 Equation\n\nstd = \\sqrt{\\frac{1}{N-1} \\sum_{i=1}^N(x_i - \\overline{x})^2}\n\\tag{1}\n\n\n\n\n\n\nNote\n\n\n\ntest note"
  },
  {
    "objectID": "python code/quarto/Computations.html",
    "href": "python code/quarto/Computations.html",
    "title": "Tutorial: Computations",
    "section": "",
    "text": "코드가 표시되는 것을 막음 execute: echo: false\n기본적으로 표시 안되게 하고 특정 코드만 표시하려면? 특정코드셀 맨 윗줄에 밑에꺼 붙이면 됨 #| echo: true\n코드 접기기능 echo옵션먼저 지우고 시작 밑에 코드 yaml 포맷에 입력 format: html: code-fold: true\n\nNumpy\n\n\nCode\nimport numpy as np\na = np.arange(15).reshape(3,5)\na\n\n\narray([[ 0,  1,  2,  3,  4],\n       [ 5,  6,  7,  8,  9],\n       [10, 11, 12, 13, 14]])\n\n\n\n\nMatplotlib\n\n\nCode\nimport matplotlib.pyplot as plt\n\nfig = plt.figure()\nfig.set_size_inches(12,7)\nx = np.arange(10)\ny = 2.5 * np.sin(x / 20 * np.pi)\nyerr = np.linspace(0.05, 0.2, 10)\n\nplt.errorbar(x, y + 3, yerr=yerr, label='both limits (default)')\nplt.errorbar(x, y + 2, yerr=yerr, uplims=True, label='uplims=True')\nplt.errorbar(x, y + 1, yerr=yerr, uplims=True, lolims=True,\n             label='uplims=True, lolims=True')\n\nupperlimits = [True, False] * 5\nlowerlimits = [False, True] * 5\nplt.errorbar(x, y, yerr=yerr, uplims=upperlimits, lolims=lowerlimits,\n             label='subsets of uplims and lolims')\n\nplt.legend(loc='lower right')\nplt.show(fig)\n\n\n\n\n\nFigure 1: Errorbar limit selection\n\n\n\n\n\n\nPlotly\n\n\nCode\nimport plotly.express as px\nimport plotly.io as pio\ngapminder = px.data.gapminder()\ngapminder2007 = gapminder.query(\"year == 2007\")\nfig = px.scatter(gapminder2007, \n                 x=\"gdpPercap\", y=\"lifeExp\", color=\"continent\", \n                 size=\"pop\", size_max=60,\n                 hover_name=\"country\")\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\n이 외에 Multiple Figure에 관한 Tutorial이 있지만 생략"
  },
  {
    "objectID": "python code/quarto/Tutorial Hello, Quarto.html",
    "href": "python code/quarto/Tutorial Hello, Quarto.html",
    "title": "Quarto Basics",
    "section": "",
    "text": "Polar Axis\nFor a demonstration of a line plot on a polar axis,see Figure 1  direct change\n\n#YAML로 쓰인 셀옵션은 #| 라는 접두어를 통해서 활용됨. 여기서는 피규어를 가리키는 기능을 함.\n#label : 이 코드셀에 fig-polar라는 이름을 붙임. 위의 마크다운의 @과 연동되어 이 피규어를 가리키게 함. 랜더링 시에는 @fig-pollar는 figure1으로 표시됨\n#fig-cap : figure의 하단에 제목을 달아줌\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nr = np.arange(0,2,0.01)\ntheta = 2 * np.pi * r\nfig,ax = plt.subplots(\n    subplot_kw = {'projection' : 'polar'}\n)\nax.plot(theta,r)\nax.set_rticks([0.5,1,1.5,2])\nax.grid(True)\nplt.show()\n\n\n\n\nFigure 1: A line plot on a polar axis"
  }
]