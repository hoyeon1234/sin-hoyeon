[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "ì „ë¶ëŒ€í•™êµ ITì‘ìš©ì‹œìŠ¤í…œ ê³µí•™ê³¼ ì‹ í˜¸ì—° sinhoyeon0514@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HIHO",
    "section": "",
    "text": "Untitled\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\n\n\n\n\nDQN review\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\n\n\n\n\n\nÂ \n\n\n\n\n\n\n\n\n\n\n\n\nDQN review\n\n\n\n1/20/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[PRML ì½ê¸°] 1 - í™•ë¥ ë¡  ê°œìš”\n\n\nSum rule, Product rule, Bayeâ€™s rule,random variable independence\n\n\n\n1/15/23\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\n[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)\n\n\nprobability density, expectation and covariance, Bayesian probabilities,Gausian distribution\n\n\n\n1/15/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimension Reduction using Auto Encoder with pytorch\n\n\n\n1/12/23\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nimport\n\n\n\n1/12/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[í”„ë¡œê·¸ë˜ë¨¸ìŠ¤]Lv.1ì‹œì €ì•”í˜¸\n\n\n\n1/9/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[í”„ë¡œê·¸ë˜ë¨¸ìŠ¤]Lv1.ì´ìƒí•œ ë¬¸ì ë§Œë“¤ê¸°\n\n\n\n1/9/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n[í”„ë¡œê·¸ë˜ë¨¸ìŠ¤]Lv1.í¬ê¸°ê°€ ì‘ì€ ë¶€ë¶„ ë¬¸ìì—´\n\n\n\n1/9/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLeast Squares\n\n\nì—´ê³µê°„(column space)ê³¼ ì •ì‚¬ì˜(projection)ìœ¼ë¡œ ì ‘ê·¼í•œ ìµœì†Œì œê³±ë²•(least squares)\n\n\n\n1/8/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nnp.meshgrid\n\n\n\n1/8/23\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\ní™•ë¥ ë¡  ìš©ì–´ì •ë¦¬\n\n\nrandom variable,event,sample space,probability distribution,randomsample,realization\n\n\n\n1/8/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinite Difference Method with np.gradient\n\n\n\n1/7/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotly ì‹œê°í™” ëª¨ìŒ\n\n\n\n1/6/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nAx=bì˜ í•´ì˜ ê°¯ìˆ˜ ì•Œì•„ë‚´ê¸°\n\n\n\n1/5/23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nrank & null space(kernel)\n\n\n\n1/5/23\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nLinear Combination & Span\n\n\n\n1/3/23\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\ní–‰ë ¬ê³±ì— ëŒ€í•œ ì—¬ëŸ¬ê°€ì§€ ê´€ì \n\n\n\n1/2/23\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nMultinomial Logistic Regression & Softmax Regression\n\n\n\n12/30/22\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nì¹´í…Œê³ ë¦¬ ë¶„í¬\n\n\n\n12/30/22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nMaximum likelyhood estimation\n\n\n\n12/29/22\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\ní™•ë¥ ë¶„í¬ì—ì„œ ;ì™€|ì˜ ì‚¬ìš©\n\n\n\n12/27/22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n12/26/22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\níŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹\n\n\n\n12/25/22\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n12/24/22\n\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\npytorchë¡œ Rnnêµ¬í˜„í•˜ê¸°\n\n\n\n12/24/22\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/coading test/programmersì‹œì €ì•”í˜¸.html",
    "href": "posts/coading test/programmersì‹œì €ì•”í˜¸.html",
    "title": "[í”„ë¡œê·¸ë˜ë¨¸ìŠ¤]Lv.1ì‹œì €ì•”í˜¸",
    "section": "",
    "text": "ë¬¸ì œ\n\n\n\në‚˜ì˜ í’€ì´\n\ndef solution(s, n):\n    upper_ch = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    answer = \"\"\n    for el_s in s:\n        #1.ë§¨ ë§ˆì§€ë§‰ ë¦¬í„´ì„ ìœ„í•´ ì›ë˜ ë¬¸ìê°€ ì†Œë¬¸ìì¸ì§€ ëŒ€ë¬¸ìì¸ì§€ ê¸°ì–µ and ëŒ€ë¬¸ìì—ì„œ ê²€ìƒ‰í• ê²ƒì´ê¸° ë•Œë¬¸ì— ëŒ€ë¬¸ìë¡œ ë³€í™˜\n        #ëŒ€ë¬¸ìë¡œ ë³€í™˜ëœ ë¬¸ìì—´ sì˜ ê°ê°ì˜ ë¬¸ì,ëŒ€ì†Œë¬¸ì ì—¬ë¶€\n        if el_s == \" \":\n            answer += \" \"\n        elif el_s.isupper() == True:\n            was_upper = True\n        else:\n            was_upper = False\n            el_s = el_s.upper()\n        #2.ì¸ë±ìŠ¤ ìˆ«ìê°€ upper_chì—ì„œ ë²—ì–´ë‚ ë•Œ ì•„ë‹ë•Œ ì²˜ë¦¬\n        for idx,up_ch in enumerate(upper_ch):\n            if el_s == up_ch and idx + n <= len(upper_ch)-1:\n                find_idx = idx+n\n                if was_upper == True:\n                    answer += upper_ch[find_idx]\n                else:\n                    answer += upper_ch[find_idx].lower()        \n            elif el_s == up_ch and idx + n > len(upper_ch)-1:\n                find_idx = n-len(upper_ch[idx:])\n                if was_upper == True:\n                    answer += upper_ch[find_idx]\n                else:\n                    answer += upper_ch[find_idx].lower()  \n    return answer\n\n\n\në‹¤ë¥¸ í’€ì´\n\ndef caesar(s, n):\n    lower_list = \"abcdefghijklmnopqrstuvwxyz\"  \n    #ì†Œë¬¸ìë„ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¦ \n    #ì¢‹ì€ì ->ëŒ€ì†Œë¬¸ì ì—¬ë¶€ë¥¼ ê¸°ì–µí•˜ëŠ” ì½”ë“œ ë¶ˆí•„ìš”(ì¡°ê±´ë¬¸)\n    #ì•ˆì¢‹ì€ì ->ì†Œë¬¸ìë¡œ ì´ë¤„ì§„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“œëŠ”ë° ê·¸ë§Œí¼ì˜ ë©”ëª¨ë¦¬ í•„ìš”\n    upper_list = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \n    \n    \n    \n    result = [] \n    #ë¬¸ìì—´ë“¤ì„ ì €ì¥í•  listë¥¼ ë§Œë“¦\n    #mutableë¡œ ë¶™ì´ëŠ” ê²ƒê³¼ immutableë¡œ ë¶™ì´ëŠ” ê²ƒ ì°¨ì´\n    #ì†ë„ -> \n    #mutableì´ ë” ë¹ ë¦„,\n    #ê·¸ëŸ¬ë‚˜ garbage collectorë„ ê³ ë ¤ì‹œ ìŠ¤ìº”ë²”ìœ„ê°€ ë„ˆë¬´ì»¤ì„œ ëŠë ¤ì§ˆìˆ˜ë„?\n    #ë©”ëª¨ë¦¬ -> immutableì´ ë” ì ê²Œë“¤ì„ ë“¯(ë¦¬ìŠ¤íŠ¸ëŠ” ì´ì¤‘í¬ì¸í„°ê°™ì€ êµ¬ì¡°ë¼ì„œ ì´ë ‡ê²Œ ì˜ˆìƒë¨)\n    for i in s:\n        if i == \" \":\n            result.append(\" \")\n        elif i.islower() is True:\n            new_ = lower_list.find(i) + n\n            result.append(lower_list[new_ % 26]) \n            #ë‚˜ë¨¸ì§€ë¡œ ê³„ì‚°í•˜ëŠ” ë°©ì‹,ì´ê²Œ ë” ê°„ë‹¨í•˜ê³  ì¢‹ì€ë“¯\n            #ë°˜ë³µë¬¸ì´ ë¬¸ìì—´ì— ëŒ€í•´ì„œ ëŒë‹¤ë³´ë‹ˆ ë¬¸ìì—´ê³¼ ë¬¸ìì—´ì˜ ì¸ë±ìŠ¤ ìœ„ì£¼ë¡œ ë„ˆë¬´ ìƒê°í•¨\n            #ì–´ë–¤ ìˆ«ì(ì—¬ê¸°ì„œëŠ” ì¸ë±ìŠ¤)ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì„ë•Œì— ë‹¤ì‹œ 0ë¶€í„° ì¤˜ì•¼í•˜ëŠ” ìƒí™©? -> ë‚˜ë¨¸ì§€ í™œìš©\n        else:\n            new_ = upper_list.find(i) + n\n            result.append(upper_list[new_ % 26])\n    return \"\".join(result)\n\në‚˜ì¤‘ì— ë³¼ ë§í¬ ë§í¬1 ë§í¬2 ë§í¬3"
  },
  {
    "objectID": "posts/coading test/programmersì´ìƒí•œë¬¸ìë§Œë“¤ê¸°.html",
    "href": "posts/coading test/programmersì´ìƒí•œë¬¸ìë§Œë“¤ê¸°.html",
    "title": "[í”„ë¡œê·¸ë˜ë¨¸ìŠ¤]Lv1.ì´ìƒí•œ ë¬¸ì ë§Œë“¤ê¸°",
    "section": "",
    "text": "ë¬¸ì œ\n\n\n\në‚˜ì˜ í’€ì´\n\ndef solution(s):\n    words = s.split(\" \") #ìŠ¤í”Œë¦¿ í•¨ìˆ˜ í—·ê°ˆë¦¬ëŠ” ë¶€ë¶„ì´ ìˆì—ˆìŒ - ì •ë¦¬\n    answer = \"\"\n    for wd in words:\n        for idx,chr in enumerate(wd):\n            print(idx,chr)\n            if idx % 2 == 0:\n                chr = chr.upper()\n            else:\n                chr = chr.lower()\n            answer += chr\n        answer+= \" \"\n    return answer[:-1]\n\n\n\nsplit ë©”ì„œë“œ(í•¨ìˆ˜)\n\nt=\"sdsa  sd\"\nhelp(t.split)\n\nHelp on built-in function split:\n\nsplit(sep=None, maxsplit=-1) method of builtins.str instance\n    Return a list of the words in the string, using sep as the delimiter string.\n    \n    sep\n      The delimiter according which to split the string.\n      None (the default value) means split according to any whitespace,\n      and discard empty strings from the result.\n    maxsplit\n      Maximum number of splits to do.\n      -1 (the default value) means no limit.\n\n\n\nstringê°ì²´ì˜ ì¸ìŠ¤í„´ìŠ¤ ì¦‰,ë¬¸ìì—´ì— ëŒ€í•´ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë©”ì„œë“œ sepíŒŒë¼ë¯¸í„°ì— ì „ë‹¬í•œ ì¸ìˆ˜ë¥¼ êµ¬ë¶„ìë¡œ ì‚¬ìš©í•˜ì—¬ ë¬¸ìì—´ì•ˆì— ìˆëŠ” ë‹¨ì–´ë“¤ì„ ê°€ì ¸ì˜´. ê°€ì ¸ì˜¨ ë‹¨ì–´ë“¤ì€ ë¦¬ìŠ¤íŠ¸ì˜ ì›ì†Œê°€ ë˜ì–´ ë°˜í™˜ë¨ Parameter - sep - â€¦\nReturn - list[word1,word2,â€¦] (êµ¬ë¶„ìë¥¼ í†µí•˜ì—¬ êµ¬ë¶„ëœ ë‹¨ì–´ë“¤,ê¸°ë³¸ê°’ì€ ê³µë°±(space))\n\n\nì˜ˆì‹œ\n\n#ë¬¸ìì—´ ë‚´ì— êµ¬ë¶„ê¸°ì¤€ì´ ì—†ëŠ” ê²½ìš°?\n#sep = \" \"ìœ¼ë¡œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìŒ. ê³µë°±ì´ ì—†ìœ¼ë¯€ë¡œ ê·¸ëƒ¥ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ì— ë„£ì–´ì„œ ë°˜í™˜\n_t = \"abcde\"\nanswer = _t.split()\nprint(answer)\n\n['abcde']\n\n\n\n#ë¬¸ìì—´ ë‚´ì— êµ¬ë¶„ê¸°ì¤€ì´ ìˆëŠ” ê²½ìš°?\n_t = \"ab cd esda dg\"\nanswer = _t.split()\nprint(answer)\n\n['ab', 'cd', 'esda', 'dg']\n\n\n\n_t = \"abtcdtesdatdg\"\nanswer = _t.split(\"t\")\nprint(answer)\n\n['ab', 'cd', 'esda', 'dg']\n\n\n\n_t = \"ab;;cd;;esd;;atdg\"\nanswer = _t.split(\";;\")\nprint(answer)\n\n['ab', 'cd', 'esd', 'atdg']"
  },
  {
    "objectID": "posts/coading test/programmersí¬ê¸°ê°€ì‘ì€ë¶€ë¶„ë¬¸ìì—´.html",
    "href": "posts/coading test/programmersí¬ê¸°ê°€ì‘ì€ë¶€ë¶„ë¬¸ìì—´.html",
    "title": "[í”„ë¡œê·¸ë˜ë¨¸ìŠ¤]Lv1.í¬ê¸°ê°€ ì‘ì€ ë¶€ë¶„ ë¬¸ìì—´",
    "section": "",
    "text": "ë¬¸ì œ\n\n\n\në‚˜ì˜ í’€ì´\n\ndef solution(t, p):\n    t_len = len(t);p_len = len(p)\n    answer = 0\n    p = int(p)\n    for idx in range(t_len-p_len+1):\n        num = int(t[idx:idx+p_len])\n        if num <= p:\n            answer+=1\n        else:\n            pass\n    return answer"
  },
  {
    "objectID": "posts/DL/Auto Encoder Dimension Reduction.html",
    "href": "posts/DL/Auto Encoder Dimension Reduction.html",
    "title": "Dimension Reduction using Auto Encoder with pytorch",
    "section": "",
    "text": "import torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nimport torch\ntest_path = \"./test.csv\"\ntrain_path = \"./train.csv\"\n\n\n# %pip install plotly (jupyter notebook)\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n#pio.renderers.default = 'iframe_connected'\n#pio.renderers.default = \"vscode\"\npio.renderers.default = \"plotly_mimetype+notebook\""
  },
  {
    "objectID": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension3",
    "href": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension3",
    "title": "Dimension Reduction using Auto Encoder with pytorch",
    "section": "encoding dimension=3",
    "text": "encoding dimension=3\n\ntraining autoencoder\n\ntorch.manual_seed(201711375)\nautoencoder_3 = AutoEncoder(47,3)\nloss_fn = torch.nn.MSELoss()\nrelu = torch.nn.LeakyReLU()\noptimizer = torch.optim.Adam(autoencoder_3.parameters(),lr=0.001)\n\n\nfor epoch in range(20000):\n    #1.yhat\n    out = autoencoder_3(X_train_ohe)\n    #2\n    loss = loss_fn(out,X_train_ohe)\n    #3\n    loss.backward()\n    if epoch % 10000 == 0:\n        print(f\"epoch:{epoch} loss:{loss.tolist()}\")\n    #4\n    optimizer.step()\n    optimizer.zero_grad()\n\nepoch:0 loss:0.5274774432182312\nepoch:10000 loss:0.11526338756084442\n\n\n\n\nvisualization\n\nclass_map_inv = {}\nfor key,value in class_map.items():\n    class_map_inv[value] = key\nclass_map_inv\n\n{0: 'A', 1: 'B', 2: 'C'}\n\n\n\ndt_dim3 = pd.DataFrame({\"class\":Y_train_ohe})\ndt_dim3 = pd.concat([pd.DataFrame(np.array(autoencoder_3.encoder(X_train_ohe).tolist())),dt_dim3],axis=1)\ndt_dim3 = dt_dim3.rename(columns = {0:\"x\",1:\"y\",2:\"z\"})\n\n\ncount = 0\ndata = []\nfor cl in dt_dim3[\"class\"].unique():\n    cond = dt_dim3[\"class\"] == cl\n    _data = dt_dim3.loc[cond,:]\n    x = _data.x.tolist()\n    y = _data.y.tolist()\n    z = _data.z.tolist()\n    if count == 0:\n        color = \"red\"\n    elif count == 1:\n        color = \"blue\"\n    else:\n        color = \"black\"\n    trace=go.Scatter3d(\n        x=x,\n        y=y,\n        z=z,\n        mode=\"markers\",\n        marker = dict(color = color,size=2),\n        name = str(class_map_inv[cl])\n        )\n    data.append(trace)\n    count+=1\n\nlayout = go.Layout(title=dict(text = \"3-dimension \"))\n\n#4. figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()"
  },
  {
    "objectID": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension10",
    "href": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension10",
    "title": "Dimension Reduction using Auto Encoder with pytorch",
    "section": "encoding dimension=10",
    "text": "encoding dimension=10\n\ntorch.manual_seed(201711375)\nautoencoder_3 = AutoEncoder(47,10)\nloss_fn = torch.nn.MSELoss()\nrelu = torch.nn.LeakyReLU()\noptimizer = torch.optim.Adam(autoencoder_3.parameters(),lr=0.001)\n\n\nfor epoch in range(40000):\n    #1.yhat\n    out = autoencoder_3(X_train_ohe)\n    #2\n    loss = loss_fn(out,X_train_ohe)\n    #3\n    loss.backward()\n    if epoch % 10000 == 0:\n        print(f\"epoch:{epoch} loss:{loss.tolist()}\")\n    #4\n    optimizer.step()\n    optimizer.zero_grad()\n\nepoch:0 loss:0.3828417658805847\nepoch:10000 loss:0.060432590544223785\nepoch:20000 loss:0.06043253839015961\nepoch:30000 loss:0.060432031750679016"
  },
  {
    "objectID": "posts/DL/Linear Regression.html",
    "href": "posts/DL/Linear Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "ì„ í˜•íšŒê·€ì— ëŒ€í•´ì„œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#ë°ì´í„°ì—-ëŒ€í•œ-ê°€ì •-ëª¨ë¸ë§",
    "href": "posts/DL/Linear Regression.html#ë°ì´í„°ì—-ëŒ€í•œ-ê°€ì •-ëª¨ë¸ë§",
    "title": "Linear Regression",
    "section": "ë°ì´í„°ì— ëŒ€í•œ ê°€ì • & ëª¨ë¸ë§",
    "text": "ë°ì´í„°ì— ëŒ€í•œ ê°€ì • & ëª¨ë¸ë§\n\n\nText(0.5, 1.0, 'n=200')\n\n\n\n\n\nìš°ë¦¬ê°€ ê°€ì§„ ë°ì´í„°ë¥¼ ê´€ì°°í•´ë´…ì‹œë‹¤. ê°€ì¥í¬ê²Œ ëˆˆì— ë„ëŠ” ì‚¬ì‹¤ì€ xì™€ yì‚¬ì´ì˜ ê´€ê³„ê°€ ì„ í˜•ì ì´ë¼ëŠ” ì ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ê°„ë‹¨í•œ ì„ í˜•ëª¨í˜•ìœ¼ë¡œ ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì¡°ê¸ˆ ë” ì„¸ë¶€ì ìœ¼ë¡œ ë“¤ì–´ê°€ì„œ ê°ê°ì˜ xê°’ì— ëŒ€í•´ì„œ yê°’ì„ ê´€ì°°í•´ ë´…ì‹œë‹¤. ì²«ë²ˆì§¸ë¡œ ì•Œ ìˆ˜ ìˆëŠ” ì ì€ ë™ì¼í•œ xê°’ì— ëŒ€í•´ì„œë„ ì„œë¡œë‹¤ë¥¸ yê°’ì„ ê°€ì§€ëŠ” ì ë“¤ ë°ì´í„°ê°€ ë§ì´ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¡œë¶€í„° \\(y\\)ê°€ \\(x\\)ë¿ë§Œì•„ë‹ˆë¼ ë˜ë‹¤ë¥¸ í™•ë¥ ë³€ìˆ˜ \\(\\epsilon\\)ì˜ ê°’ì— ì˜í•´ ê²°ì •ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘ë²ˆì§¸ë¡œ xê°’ì— ì˜í•´ì„œ ì°íˆëŠ” ì ì˜ ìœ„ì¹˜(y)ì˜ ì–‘ìƒì´ ë‹¤ë¥´ë‹¤ëŠ” ì ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. xê°€ ì‘ìœ¼ë©´ ì¼ë°˜ì ìœ¼ë¡œ yëŠ” ë‚®ì€ìœ„ì¹˜ì—ì„œ ì ì´ ì°íˆê³  xê°€ í¬ë©´í´ìˆ˜ë¡ ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ìœ„ì¹˜ì—ì„œ ì ì´ ì°íˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në°ì´í„°ë¥¼ ê´€ì°°í•˜ë©´ì„œ ì–»ì€ ì‚¬ì‹¤ë¡œë¶€í„° \\(x\\)ì™€ \\(y\\)ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§ í•´ë³´ê² ìŠµë‹ˆë‹¤. ì²«ë²ˆì§¸ ì‚¬ì‹¤ë¡œë¶€í„° ìš°ë¦¬ëŠ” ë™ì¼í•œ \\(x\\)ë¼ë„ ê°ê°ì˜ ë°ì´í„°ì— ëŒ€í•´ì„œ ì–´ë–¤ ë˜ë‹¤ë¥¸ ê°’ì´ ë”í•´ì§ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ë½‘íë•Œë§ˆë‹¤ ê·¸ ê°’ì´ ë‹¤ë¥¸ ë³€ìˆ˜ëŠ” í™•ë¥ ë³€ìˆ˜ \\(\\epsilon_i\\)ë¥¼ ë”í•´ì¤Œìœ¼ë¡œì„œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì˜¤ì°¨ê°€ ë”°ë¥´ëŠ” ë¶„í¬ì— ëŒ€í•´ì„œ ê°€ì •ì„ í•©ë‹ˆë‹¤. ì˜¤ì°¨ëŠ” í‰ê· ì´ 0ì´ê³  ë¶„ì‚°ì´ \\(\\sigma^2\\)ì¸ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.\në˜í•œ ë‘ë²ˆì§¸ ì‚¬ì‹¤ë¡œë¶€í„° ìš°ë¦¬ëŠ” \\(x\\)ì™€ \\(y\\)ëŠ” ì»¤ì§€ë©´ ì»¤ì§€ê³  ì‘ì•„ì§€ë©´ ì‘ì•„ì§€ëŠ” ê´€ê³„ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê´€ê³„ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì§€ë§Œ ì„ í˜•íšŒê·€ì—ì„œëŠ” ì„ í˜•ìœ¼ë¡œ ì´ ê´€ê³„ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&Y_i = w_0 + w_1x_1 + \\dots + w_Mx_M + \\epsilon_i=x_iw + \\epsilon_i\\\\\n&\\text{where , }x_i = \\begin {bmatrix}0,x_{i,1},\\dots,x_{i,M} \\end {bmatrix} \\in \\mathbb{R^{1 \\times (M+1)},w = \\begin {bmatrix} w_0,w_1,\\dots,w_M\\end{bmatrix}}^T \\in \\mathbb{R^{(M+1)\\times 1}}\\\\\n\\quad\\quad\\quad &\\epsilon_i \\overset{i.i.d}{\\sim} \\mathcal{N}(0,\\sigma^2)\\,\\,(\\text{for } i=1,2,\\dots,N)\n\\end{aligned}\\]\nì´ë ‡ê²Œ \\(x_i\\)ì™€ \\(Y_i\\)ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì •í•  ê²½ìš° ê°ê°ì˜ \\(Y_i\\)ì•ˆì— í™•ë¥ ë³€ìˆ˜ \\(\\epsilon_i\\)ê°€ \\(Y_i\\)ë„ ë§ˆì°¬ê°€ì§€ë¡œ ì •ê·œë¶„ë¡œë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜ë¡œ ê·¸ ê°’ì´ \\(\\epsilon\\)ì— ì˜í•´ ë…ë¦½ë³€ìˆ˜ì˜ ê°’ì´ ë™ì¼í•˜ë”ë¼ë„ ì¶”ì¶œí• ë•Œë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \\(Y_i\\)ë„ í™•ë¥ ë³€ìˆ˜ì´ë¯€ë¡œ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜ì´ë¯€ë¡œ ì¤‘ì‹¬ìœ„ì¹˜ì™€ ë³€ë™ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ê¸°ëŒ“ê°’ê³¼ ë¶„ì‚°ì„ êµ¬í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ \\(x\\)ëŠ” ì£¼ì–´ì ¸ ìˆìœ¼ë¯€ë¡œ ê·¸ë•Œì˜ í™•ë¥ ë¶„í¬ì— ëŒ€í•´ì„œ ê³„ì‚°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mathbb{E}[Y_i] = \\mathbb{E}[w_0 + \\dots + w_Mx_M+\\epsilon_i] = w_0 + w_1x_1 + \\dots + w_Mx_M=x_iw \\\\\n&\\text{var}[Y_i] = \\text{var}[w_0 + \\dots + w_Mx_M+\\epsilon_i] = \\text{var}[\\epsilon_i] = \\sigma^2\n\\end{aligned}\\]\nìœ„ìˆ˜ì‹ìœ¼ë¡œë¶€í„° ê°ê°ì˜ \\(Y_i\\)ì— ëŒ€í•œ í™•ë¥ ë¶„í¬ëŠ” ì •ê·œë¶„í¬ì´ë©°(\\(\\epsilon_i\\)ì— ì˜í•´) ê¸°ëŒ“ê°’ì€ \\(x\\)ì™€ \\(w\\)ì— ì˜í•´ ì •í•´ì§€ì§€ë§Œ ë¶„ì‚°ì€ \\(\\sigma^2\\)ìœ¼ë¡œ í•­ìƒ ë™ì¼í•˜ë‹¤ëŠ” ì ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë‘ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ë§Œ ê¸°ëŒ“ê°’ë§Œ \\(x\\)ì— ì˜í•˜ì—¬ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì¦‰ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&Y_i|w;x_i \\sim \\mathcal{N}(x_iw,\\sigma^2)\\, \\text{ for } i = 1,2,\\dots,N\\\\\n&p(y_i|w;x_i) = ì •ê·œë¶„í¬ì‹\n\\end{aligned}\\]\nì´ë¯¸ì§€\nì—¬ê¸°ê¹Œì§€ ìš°ë¦¬ê°€ ê°€ì§„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ \\(x\\)ì™€ \\(Y\\)ì‚¬ì´ì˜ ê´€ê³„ë¡œ ì„ í˜•ëª¨í˜•ì„ ë§Œë“¤ì–´ë´¤ìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ê¶ê·¹ì ì¸ ëª©ì ì¸ unseen dataì— ì ì ˆí•˜ê²Œ \\(Y\\)ê°’ì„ ì˜ˆì¸¡í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”?ë§Œì•½ í•™ìŠµë°ì´í„°ë¡œë¶€í„° ì ì ˆíˆ \\(w\\)ë¥¼ êµ¬í• ìˆ˜ë§Œ ìˆë‹¤ë©´ ì£¼ì–´ì§„ ì…ë ¥\\(x\\)ì— ëŒ€í•˜ì—¬ \\(Y\\)ê°€ ë”°ë¥´ëŠ” ì •ê·œë¶„í¬ë¥¼ ëŒ€ëµì ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆê³  ë¶„í¬ê°€ ìµœëŒ“ê°’ì„ ê°€ì§€ëŠ” ê³³ì˜ yê°’ì¸ xwë¥¼ ì˜ˆì¸¡ê°’ìœ¼ë¡œ í•˜ë©´ ë©ë‹ˆë‹¤. ê¸°í•˜í•™ì ìœ¼ë¡œ ìƒê°í–ˆì„ë•Œ \\(w\\)ë¥¼ êµ¬í•œë‹¤ë©´ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” ì§ì„ (í‰ë©´,ì´ˆí‰ë©´)ì„ ì–»ì€ê²ƒì´ë¯€ë¡œ ì…ë ¥xì— ëŒ€í•˜ì—¬ ì§ì„ ì˜ ê°’ì„ ì½ì–´ì„œ ì˜ˆì¸¡ê°’ìœ¼ë¡œ í•˜ë©´ ë©ë‹ˆë‹¤. ë‘ ê²½ìš° ëª¨ë‘ \\(w\\)ë¥¼ êµ¬í•´ì•¼í•˜ë¯€ë¡œ ìš°ë¦¬ì˜ ëª©ì ì€ ì´ì œ wë¥¼ êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#wì—-ëŒ€í•œ-ì¶”ì •---mle",
    "href": "posts/DL/Linear Regression.html#wì—-ëŒ€í•œ-ì¶”ì •---mle",
    "title": "Linear Regression",
    "section": "wì— ëŒ€í•œ ì¶”ì • - MLE",
    "text": "wì— ëŒ€í•œ ì¶”ì • - MLE\nê°€ì¤‘ì¹˜\\(w\\)ëŠ” ê·¸ë ‡ë‹¤ë©´ ì–´ë–»ê²Œ êµ¬í•´ì•¼ í• ê¹Œìš”? ì–´ë–»ê²Œ í•´ì•¼ ì ì ˆí•œ \\(w\\)ë¥¼ êµ¬í•  ìˆ˜ ìˆì„ê¹Œìš”? ìš°ë¦¬ê°€ ê°€ì§„ê²ƒì€ ë°ì´í„°ì…‹ \\(X,y\\)ì´ë¯€ë¡œ ì´ë¡œë¶€í„° \\(w\\)ë¥¼ êµ¬í•´ì•¼ í•©ë‹ˆë‹¤. í™•ë¥ ë¡ ì—ì„œ ë°ì´í„°ì…‹ìœ¼ë¡œë¶€í„° \\(w\\)ë¥¼ êµ¬í•œë‹¤ëŠ” ê²ƒì€ ì£¼ì–´ì§„ ë°ì´í„°ë¡œë¶€í„° ê°€ì¥ ë‚˜ì˜¬ ê°€ëŠ¥ì„±ì´ ë†’ì€ wë¥¼ ì°¾ìë¼ëŠ” ë§ê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ ë§ì€ ë‹¤ë¥¸ë§ë¡œ ë°ì´í„°ì…‹ì´ ì£¼ì–´ì¡Œë‹¤ëŠ” ì¡°ê±´í•˜ì—ì„œ ê°€ì¥ í™•ë¥ ë¶„í¬ë¥¼ ìµœëŒ€í™” í•˜ëŠ” \\(w\\)ë¥¼ ì°¾ìì™€ ê°™ìŠµë‹ˆë‹¤. ìµœëŒ€í™” í•´ì•¼í•˜ëŠ” ì¡°ê±´ë¶€í™•ë¥ ë¶„í¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[p(w|y;X)\\]\ní•˜ì§€ë§Œ ìš°ë¦¬ëŠ” ìœ„ì™€ê°™ì€ ì¡°ê±´ë¶€ í™•ë¥ ì„ í˜„ì¬ëŠ” ì•Œì§€ ëª»í•©ë‹ˆë‹¤. ê·¸ë ‡ê¸°ì— ë¨¼ì € ë² ì´ì¦ˆ ì •ë¦¬ì˜ ë„ì›€ì„ ë°›ì•„ì•¼ í•©ë‹ˆë‹¤. ë² ì´ì¦ˆ ì •ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&p(w|y,X) = \\frac{p(\\mathcal{D}|w)p(w)}{p(\\mathcal{D})} \\propto (\\mathcal{D}|w)p(w)\\\\\n&\\text{where, } \\mathcal{D} = (y_1,y_2,\\dots,y_n)\n\\end{aligned}\\]\n\\[\\begin{aligned}\n&p(w|\\mathcal{D}) = \\frac{p(\\mathcal{D}|w)p(w)}{p(\\mathcal{D})} \\propto (\\mathcal{D}|w)p(w)\\\\\n&\\text{where, } \\mathcal{D} = (y_1,y_2,\\dots,y_n)\n\\end{aligned}\\]\n\n\n\n\n\n\nNote\n\n\n\nì—¬ê¸°ì„œ ëŠ” ìš°ë¦¬ì—ê²Œ ì£¼ì–´ì§„ ë°ì´í„°ì…‹ì´ ì•„ë‹ˆë¼ ìœ„ì—ì„œ ë°ì´í„°ì…‹ì—ì„œì˜ yê°’ë“¤ë§Œì„ ë§í•©ë‹ˆë‹¤. MLEì—ì„œëŠ” êµ¬í•˜ê³ ì(ì¶”ì •í•˜ê³ ì)í•˜ëŠ” íŒŒë¼ë¯¸í„°\\(w\\)ëŠ” í™•ë¥ ë³€ìˆ˜ê°€ ë”°ë¥´ëŠ” ì–´ë– í•œ í™•ë¥ ë¶„í¬ì˜ íŒŒë¼ë¯¸í„°ì´ê³  ê·¸ ê°’ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œ í™•ë¥ ë³€ìˆ˜ì˜ ê°’(realizations)ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ í™•ë¥ ë¶„í¬ë¡œë¶€í„° ë‚˜ì˜¨ í™•ë¥ ë³€ìˆ˜ì˜ ê°’ì€ (y_1,y_2,,y_n)ì…ë‹ˆë‹¤.\n\n\nì˜¤ë¥¸ìª½ ìˆ˜ì‹ì—ì„œ ë¶„ëª¨ëŠ” normalization constantë¼ í•˜ëŠ” ìƒìˆ˜ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ë¶„ìë¥¼ ìµœëŒ€í™”í•˜ëŠ” \\(w\\)ë¥¼ êµ¬í•˜ë©´ ë˜ëŠ”ë° MLEëŠ” ë¶„ìì˜\\(p(\\mathcal{D}|w)\\)ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•©ë‹ˆë‹¤. \\(p(\\mathcal{D}|w)\\)ëŠ” likelyhood(function)ë¼ê³  í•˜ë©° ì´ë ‡ê²Œ \\(w\\)ì— ëŒ€í•´ ì¶”ì •í•˜ëŠ” ë°©ë²•ì„ maximum likelyhood estimationì´ë¼ê³  ë¶€ë¥´ëŠ” ì´ìœ ì…ë‹ˆë‹¤.\nê·¸ë ‡ë‹¤ë©´ ì¡°ê±´ë¶€í™•ë¥ ë¶„í¬ \\(p(\\mathcal{D}|w)\\)ëŠ” ì–´ë–»ê²Œ êµ¬í•´ì•¼ í• ê¹Œìš”? ë¨¼ì € \\(p(\\mathcal{D}|w)\\)ëŠ” wê°€ ì£¼ì–´ì ¸ìˆì„ë•Œ = (y_1,y_2,,y_n)ì— ëŒ€í•œ í™•ë¥ ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#sample-regression-modelvector-form",
    "href": "posts/DL/Linear Regression.html#sample-regression-modelvector-form",
    "title": "Linear Regression",
    "section": "sample regression model(vector form)",
    "text": "sample regression model(vector form)\nìœ„ì™€ ê°™ì€ regression modelë¡œë¶€í„° sampling nê°œì˜ í‘œë³¸ì„ ì–»ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\[\ny_1 = w_0 + w_1x_{11} + \\dots + w_mx_{1m} + \\epsilon_1\n\\] \\[\ny_2 = w_0 + w_1x_{21} + \\dots + w_mx_{2m} + \\epsilon_2\n\\] \\[\n\\vdots\n\\] \\[\ny_n = w_0 + w_1x_{n1} + \\dots +w_mx_{nm} + \\epsilon_n\n\\] \nìƒ˜í”Œì„ ë²¡í„°-í–‰ë ¬ë¡œ í‘œí˜„í•´ì„œ ì¢€ ë” ê°„ë‹¨íˆ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \\[\n\\begin{bmatrix} y_1 \\\\ y_2 \\\\ \\vdots \\\\y_n\\end{bmatrix} =\n\\begin{bmatrix}\n1 & x_{11} & x_{12} & \\dots &x_{1m}\\\\\n1 & x_{21} & x_{22} & \\dots &x_{2m}\\\\\n1 & x_{31} & x_{32} & \\dots &x_{3m}\\\\\n\\vdots & \\vdots & \\vdots & \\vdots & \\vdots\\\\\n1 & x_{n1} & x_{n2} & \\dots & x_{nm}\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nw_0\\\\\nw_1\\\\\nw_2\\\\\n\\vdots\\\\\nw_m\n\\end{bmatrix}\n+\n\\begin{bmatrix}\n\\epsilon_0\\\\\n\\epsilon_1\\\\\n\\epsilon_2\\\\\n\\vdots\\\\\n\\epsilon_n\n\\end{bmatrix}\n\\] \\[\n\\Longleftrightarrow\n\\bf{y} = \\bf{X}\\bf{W} + \\bf{\\epsilon}\n\\tag{1}\\]  ìœ„ì™€ ê°™ì´ ìƒ˜í”Œë§í•œ í‘œë³¸ì„ ë‚˜íƒ€ë‚´ëŠ” ëª¨í˜•ì„ sample regression modelì´ë¼ê³  í•©ë‹ˆë‹¤.\n\\(w_1=1,w_0=0\\)ì¸ population regression modelë¡œë¶€í„° 200ê°œì˜ í‘œë³¸ì„ ì¶”ì¶œí•˜ì—¬ ì‹œê°í™”í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì„ í˜•íšŒê·€ëŠ” ìœ„ì™€ ê°™ì´ í‘œë³¸ë§Œ ì£¼ì–´ì§ˆë•Œ(given), ìš°ë¦¬ê°€ ëª¨ë¥´ëŠ” \\(w_1,w_0\\)ë¥¼ ì¶”ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í‘œë³¸ìœ¼ë¡œë¶€í„° \\(\\bf{W}\\)ë¥¼ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ë©´ ë‘ ë³€ìˆ˜ì‚¬ì´ì˜ ê´€ê³„ê°€ ì´ë ‡ê² êµ¬ë‚˜ë¼ê³  ì•Œ ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤.\n\n\n\n\n\n\nNote\n\n\n\nì‹¤ì œë¬¸ì œì—ì„œ population regression modelì˜ ê°€ì¤‘ì¹˜ \\(\\bf{w} = (w_0,w_,1,...,w_m)\\)ëŠ” ì •í™•íˆ ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” í‘œë³¸ì„ í†µí•´ì„œ ê°€ì¤‘ì¹˜ë¥¼ ì¶”ì •í•  ë¿ì…ë‹ˆë‹¤. ìœ„ì—ì„œëŠ” ìƒ˜í”Œë°ì´í„°ë¥¼ ë§Œë“¤ê¸° ìœ„í•´ ì–´ì©” ìˆ˜ ì—†ì´ ì•Œê²Œë˜ì—ˆì§€ë§Œ ì‹¤ì œë¬¸ì œì—ì„œëŠ” ì•Œ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#ì„ í˜•íšŒê·€ì˜-loss-function",
    "href": "posts/DL/Linear Regression.html#ì„ í˜•íšŒê·€ì˜-loss-function",
    "title": "Linear Regression",
    "section": "ì„ í˜•íšŒê·€ì˜ Loss function",
    "text": "ì„ í˜•íšŒê·€ì˜ Loss function\n2ë²ˆì—ì„œ êµ¬í•œ ì¶”ì •ê°’ \\(\\hat{\\bf{W}}\\)ì´ ì–¼ë§ˆë‚˜ í‹€ë¦°ì§€,ë¶€ì •í™•í•œì§€ ì•Œë ¤ì£¼ëŠ” í•¨ìˆ˜ë¥¼ Loss function ë˜ëŠ” Cost functionì´ë¼ê³  í•©ë‹ˆë‹¤. ì„ í˜•íšŒê·€ì—ì„œì˜ Loss functionì€ ì¼ë°˜ì ìœ¼ë¡œ MSEë¥¼ ì‚¬ìš©í•˜ë©° ì£¼ì–´ì§„ ìƒ˜í”Œì—ì„œ ì”ì°¨(residual,\\(\\hat{y}_i-y\\))ë“¤ì„ ì „ë¶€ ì œê³±í•˜ì—¬ ë”í•œ ê°’ì…ë‹ˆë‹¤.\n(Loss function) \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2} = \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}}) = \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\)\nMSEì™€ ê°™ì€ Loss functionì€ ìš°ë¦¬ì˜ ì¶”ì •ì´ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” \\(\\hat{\\bf{W}}\\)ì— ëŒ€í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, loss functionì„ ê°€ì¥ ìµœì†Œí™” í•˜ëŠ” \\(\\bf{\\hat{W}}\\)ì„ ì°¾ì•„ë‚´ë©´ í™•ë¥ ë³€ìˆ˜ì‚¬ì´ì˜ ì„ í˜•ê´€ê³„ì¸ \\(\\bf{W}\\)ë¥¼ ì•Œì•„ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nText(110, 15, 'residual')"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#parameter-update",
    "href": "posts/DL/Linear Regression.html#parameter-update",
    "title": "Linear Regression",
    "section": "Parameter update",
    "text": "Parameter update\nnê°œì˜ ë…ë¦½ë³€ìˆ˜ë¥¼ ê°€ì§€ëŠ” ë‹¤ë³€ìˆ˜ ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ì— ëŒ€í•œ GradientëŠ” ìˆ˜í•™ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\(\\nabla_{X}{f(x_1,x_2,...,x_n)} = (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\dots,\\frac{\\partial f}{\\partial x_n})\\) ë‹¤ë³€ìˆ˜ ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ì— ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ì·¨í•˜ë©´ ë²¡í„°ì…ë‹ˆë‹¤.ê·¸ëŸ¬ë¯€ë¡œ,ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ë²¡í„°(ë‹¤ë³€ìˆ˜)ë¥¼ ì…ë ¥í–ˆì„ ë•Œ,ë²¡í„°ë¥¼ ì¶œë ¥ìœ¼ë¡œ í•˜ëŠ” ë²¡í„°í•¨ìˆ˜ë¼ê³  ìƒê°í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.ì¤‘ìš”í•œ ì‚¬ì‹¤ì€ ì„ì˜ì˜ ê³µê°„ìƒì˜ ì„ì˜ì˜ point \\(X\\)ì—ì„œ ìŠ¤ì¹¼ë¼í•¨ìˆ˜ì— ëŒ€í•œ gradient of f = \\(-\\nabla_{X}{f}\\) ë°©í–¥ì€ ìŠ¤ì¹¼ë¼í•¨ìˆ˜ê°€ ê°€ì¥ ê¸‰ê²©í•˜ê²Œ ê°ì†Œí•˜ëŠ” ë°©í–¥ì´ë¼ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤.(ì¦ëª…ìƒëµ)\nìœ„ì˜ ì‚¬ì‹¤ì— ì˜í•˜ë©´,ìš°ë¦¬ëŠ” ì„ì˜ì˜ \\(\\hat{\\bf{W}}\\)ì—ì„œ Loss functionì´ ê°€ì¥ ê¸‰ê²©í•˜ê²Œ ê°ì†Œí•˜ëŠ” ë°©í–¥ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ê°ì†Œí•˜ëŠ” ë°©í–¥ì„ ì°¾ê³  ì´ë™í•˜ê³  ê°ì†Œí•˜ëŠ” ë°©í–¥ì„ ì°¾ê³  ì´ë™í•˜ê³  ë°˜ë³µí•˜ë‹¤ë³´ë©´â€¦ ê¶ê·¹ì ì¸ ëª©ì ì¸ í‹€ë¦°ì •ë„ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì¦‰,Loss functionê°’ì´ ê°€ì¥ ì‘ì€ \\(\\hat{\\bf{W}}\\)ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \\(\\bf\\hat{W}\\)ë¥¼ ìˆ˜ì •í•˜ëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n(Gradient descent parameter update) \\(\\hat{\\bf{W}}_{t} = \\hat{\\bf{W}}_{t-1} - \\alpha\\times\\nabla_{W}{L}\\)\n\\(\\hat{\\bf{W}}_{t-1}\\)ì€ ìˆ˜ì •ë˜ê¸°ì „ì˜ ê°€ì¤‘ì¹˜(ë²¡í„°)ì´ë©° \\(\\hat{\\bf{W}_{t}}\\)ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ í•œë²ˆ ì—…ë°ì´íŠ¸ í•œ í›„ì˜ ê°€ì¤‘ì¹˜(ë²¡í„°)ì…ë‹ˆë‹¤. \\(t-1\\)ì˜ \\(\\hat{\\bf{W_{t-1}}}\\)ì— \\(-\\alpha\\times\\nabla_{W}{L}\\)ë¥¼ ë”í•´ì¤Œìœ¼ë¡œì„œ \\(\\hat{\\bf{W}}_{t-1}\\)ì€ loss functionì´ ê°€ì¥ ê¸‰ê²©íˆ(ë§ì´)ê°ì†Œí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ë©° \\(\\hat{\\bf{W}}_{t}\\)ê°€ ë©ë‹ˆë‹¤. \\(\\alpha\\)ëŠ” í•™ìŠµë¥ (learning rate)ì…ë‹ˆë‹¤. \\(\\hat{\\bf{W}}_{t-1}\\)ê³¼ ê³±í•´ì ¸ì„œ ì–¼ë§ˆë‚˜ ë§ì´ ë˜ëŠ” ì ê²Œ ì›€ì§ì¼ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. í•œë²ˆì— ì–¼ë§ˆë‚˜ ì´ë™í• ì§€ì— ë¹„ìœ í•œ â€œë³´í­â€ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nìš”ì•½í•˜ìë©´, ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•˜ì—¬ ìœ„ì™€ ê°™ì´ ê°€ì¤‘ì¹˜\\(\\hat{\\bf{W}}\\)ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•˜ë©´ loss function \\(L\\)ì´ ê°€ì¥ ìµœì†Œê°€ ë˜ëŠ” ì§€ì ì˜ \\(\\hat{\\bf{W}}\\)ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#mseì—-ëŒ€í•œ-ë”-ìƒì„¸í•œ-ì „ê°œ",
    "href": "posts/DL/Linear Regression.html#mseì—-ëŒ€í•œ-ë”-ìƒì„¸í•œ-ì „ê°œ",
    "title": "Linear Regression",
    "section": "MSEì— ëŒ€í•œ ë” ìƒì„¸í•œ ì „ê°œ",
    "text": "MSEì— ëŒ€í•œ ë” ìƒì„¸í•œ ì „ê°œ\nMSEë¥¼ ë” ìƒì„¸íˆ ì „ê°œí•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2}\\) \\(= \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}})\\) \\(= \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\) \\(= \\frac{1}{n}(\\bf{y^T - \\hat{\\bf{W}}^{T}\\bf{X}^{T})(\\bf{y} - \\bf{X}\\bf{\\hat{W}}})\\) \\(= \\frac{1}{n}(\\bf{y^Ty-y^TX\\hat{W}} - \\hat{W}X^Ty + \\hat{W}^TX^TX\\hat{W})\\)\nì—¬ê¸°ì„œ \\(\\bf{y^TX\\hat{W}} \\in \\bf{R}^{1 \\times 1}\\) ì´ë¯€ë¡œ \\(\\bf{y^TX\\hat{W}} = (\\bf{y^TX\\hat{W}})^T = (\\bf{\\hat{W}X^Ty})\\)ê°€ ì„±ë¦½í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ MSEë¥¼ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. (MSE) \\(MSE = \\frac{1}{n}(\\bf{y^Ty -2\\hat{W}X^Ty + \\hat{W}^TX^TX\\hat{W}})\\)"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#gradient-descentì—-ëŒ€í•œ-ë”-ìƒì„¸í•œ-ì „ê°œloss-mseì¼-ê²½ìš°",
    "href": "posts/DL/Linear Regression.html#gradient-descentì—-ëŒ€í•œ-ë”-ìƒì„¸í•œ-ì „ê°œloss-mseì¼-ê²½ìš°",
    "title": "Linear Regression",
    "section": "Gradient Descentì— ëŒ€í•œ ë” ìƒì„¸í•œ ì „ê°œ(\\(Loss\\) = MSEì¼ ê²½ìš°)",
    "text": "Gradient Descentì— ëŒ€í•œ ë” ìƒì„¸í•œ ì „ê°œ(\\(Loss\\) = MSEì¼ ê²½ìš°)\n(Gradient of MSE) \\(\\nabla{L} = MSE\\) \\(= \\bf{\\frac{1}{n}\\frac{\\partial}{\\partial \\hat{W}}(\\bf{y^Ty - 2\\hat{W}^TX^T + \\hat{W}^TX^TX\\hat{W}})}\\) \\(= \\bf{\\frac{1}{n}}(\\bf{\\frac{\\partial}{\\partial \\hat{W}}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2\\hat{W}^{T}X^{T}y + \\frac{\\partial}{\\partial\\hat{W}}\\hat{W}^{T}X^{T}X\\hat{W})\\) \\(= \\bf{\\frac{1}{n}(\\frac{\\partial}{\\partial \\hat{W}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2y^TX\\hat{W} + \\frac{\\partial}{\\partial\\hat{W}}\\hat{W}^TX^TX\\hat{W})}\\) \\(= \\bf{\\frac{1}{n}[0 - 2X^Ty + (X^TX + X^TX)\\hat{W}]}\\) \\(= \\bf{\\frac{2}{n}X^T(X\\hat{W} - y)}\\)\n(parameter update) \\(\\bf{\\hat{W}_{t} = \\hat{W}_{t-1} - \\alpha \\times \\frac{2}{n}X^T(X\\hat{W} - y)}\\)"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#ê²°ê³¼í•´ì„",
    "href": "posts/DL/Linear Regression.html#ê²°ê³¼í•´ì„",
    "title": "Linear Regression",
    "section": "ê²°ê³¼í•´ì„",
    "text": "ê²°ê³¼í•´ì„\n200ê°œì˜ ìƒ˜í”Œë¡œë¶€í„° \\(\\bf{w}\\)ë¥¼ ì¶”ì •í•˜ì—¬ \\(\\hat{\\bf{w}}= (0.125,0.969)\\)ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. population regression modelì˜ \\({\\bf{w}} = (w_0,w_1) = (0,1)\\)ì„ ì˜¬ë°”ë¥´ê²Œ ì¶”ì •í–ˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ì£¼ ì•½ê°„ì˜ ì°¨ì´ê°€ ì¡´ì¬í•˜ëŠ”ë° ì´ ì°¨ì´ëŠ” ëª¨ì§‘ë‹¨ì—ì„œ ìƒ˜í”Œì„ ë” ì–»ê±°ë‚˜ ë” ì„¸ë°€í•˜ê²Œ ì—…ë°ì´íŠ¸í•˜ë©´ ìµœì†Œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n#plt.title(\"w_1 : {} // w_0: {}\".format(round(W_hat[1].tolist()[0],3),round(W_hat[0].tolist()[0],3)))\nplt.title(\"Linear Regression\")\ntext=f\"What = ({round(t[0].tolist()[0],3)},{round(t[1].tolist()[0],3)})\"\nplt.plot(X[:,1],y,\"bo\",alpha=0.5)\nplt.plot(X[:,1],X@W_hat,\"r--\")\nplt.gca().axes.xaxis.set_visible(False)\nplt.gca().axes.yaxis.set_visible(False)\nplt.title(text)\n\nText(0.5, 1.0, 'What = (-0.125,0.969)')"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#ì°¸ê³ ìë£Œ",
    "href": "posts/DL/Linear Regression.html#ì°¸ê³ ìë£Œ",
    "title": "Linear Regression",
    "section": "ì°¸ê³ ìë£Œ",
    "text": "ì°¸ê³ ìë£Œ\nMaximum Likelihood Estimation(MLE) & Maximum A Posterior(MAP)"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html",
    "href": "posts/DL/Logistic Regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "ë¡œì§€ìŠ¤í‹±íšŒê·€ì— ëŒ€í•´ì„œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#ê¸°ëŒ“ê°’ì—-ëŒ€í•œ-ê³ ì°°",
    "href": "posts/DL/Logistic Regression.html#ê¸°ëŒ“ê°’ì—-ëŒ€í•œ-ê³ ì°°",
    "title": "Logistic Regression",
    "section": "ê¸°ëŒ“ê°’ì— ëŒ€í•œ ê³ ì°°",
    "text": "ê¸°ëŒ“ê°’ì— ëŒ€í•œ ê³ ì°°\nê¸°ëŒ“ê°’ì€ ì‹¤í—˜ ë˜ëŠ” ì‹œí–‰ì„ ë¬´í•œíˆ ë°˜ë³µí–ˆì„ë•Œ í™•ë¥ ë³€ìˆ˜ê°€ ì·¨í•˜ëŠ” ê°’ì˜ í‰ê· ìœ¼ë¡œ(ë˜ëŠ” ìƒ˜í”Œë§ëœ ê°’ì˜ í‰ê· ) ê¸°ëŒ€ë˜ëŠ” ê°’ì…ë‹ˆë‹¤. í™•ë¥ ë³€ìˆ˜ê°€ ë² ë¥´ëˆ„ì´ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ê²½ìš° í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•œ ê¸°ëŒ“ê°’(\\(E\\,[y|x_{1,i},x_{2,i}\\.,\\dots,x_{m,i}]\\))ê³¼ ëª¨ìˆ˜\\((p_i)\\)ê°€ ê°™ì€ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ,ë§Œì•½ì— ì£¼ì–´ì§„ ìƒ˜í”Œë°ì´í„°ë¡œë¶€í„° ë² ë¥´ëˆ„ì´ë¶„í¬ì˜ ëª¨ìˆ˜ë¥¼ ì ì ˆíˆ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ë©´ ì£¼ì–´ì§„ ì¡°ê±´í•˜ì—ì„œ ì‹¤í—˜ ë˜ëŠ” ì‹œí–‰ì„ ë¬´í•œíˆ ë°˜ë³µí•  ê²½ìš° í™•ë¥ ë³€ìˆ˜ê°€ 1ì¸ì‚¬ê±´ê³¼ 0ì¸ì‚¬ê±´ì¤‘ ì–´ë–¤ ì‚¬ê±´ì´ ë” ë§ì´ ë°œìƒí• ì§€ ì•Œ ìˆ˜ ìˆê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…ì†ë³€ìˆ˜ Yì˜ ê°’ì„ ê²°ì •í•˜ëŠ” ê²ƒì€ íƒ€ë‹¹í•©ë‹ˆë‹¤. - e.g.\n\n\\(E\\,[y]\\, = \\hat{p_i}<0.5\\) => ë¬´í•œíˆ ì‹¤í–‰í–ˆì„ë•Œ 0ì¸ ê²½ìš°ê°€ ë” ë§ì„ ê²ƒì„ => ê´€ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì˜ˆì¸¡ \n\\(E\\,[y]\\, = \\hat{p_i}\\geq0.5\\)=>ë¬´í•œíˆ ì‹¤í–‰í–ˆì„ë•Œ 1ì¸ ê²½ìš°ê°€ ë” ë§ì„ ê²ƒì„ => ê´€ì¸¡ì¹˜ë¥¼ 1ë¡œ ì˜ˆì¸¡"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#concept",
    "href": "posts/DL/Logistic Regression.html#concept",
    "title": "Logistic Regression",
    "section": "concept",
    "text": "concept\nì„ í˜•íšŒê·€ì—ì„œ ì¶”ì •í•˜ê³ ìí•˜ëŠ” ë³€ìˆ˜\\(y\\)ëŠ” \\(x_0,x_1,...,x_m\\)ê³¼ \\(w_0,w_1,...,w_m\\)ê³¼ì˜ linear combinationì´ì˜€ìŠµë‹ˆë‹¤.ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ íŠ¹ì •ìƒ˜í”Œì— ëŒ€í•œ ëª¨ìˆ˜ë¥¼ ì ì ˆí•˜ê²Œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ë©´ ê´€ì¸¡ì¹˜ê°€ ì–´ë–¤ í´ë˜ìŠ¤ì— ì†í• ì§€ í•©ë¦¬ì ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,ë¡œì§€ìŠ¤í‹±íšŒê·€ì—ì„œë„ ì„ í˜•íšŒê·€ì—ì„œì˜ ì•„ì´ë””ì–´ë¥¼ í•µì‹¬ì•„ì´ë””ì–´ë¥¼ ê°€ì§€ê³ ì™€ì„œ ì¶”ì •í•˜ê³ ì í•˜ëŠ” ëª¨ìˆ˜\\(p_i\\)ë¥¼ \\(x_0,x_1,...,x_m\\)ê³¼ \\(w_0,w_1,...,w_m\\)ì˜ linear combinationë¡œ í‘œí˜„í•˜ê³ ì í•©ë‹ˆë‹¤.\nì„ í˜•íšŒê·€ì˜ ì•„ì´ë””ì–´(linear combination) + ëª¨ìˆ˜ì— ëŒ€í•œ í‘œí˜„ì´ë¼ëŠ” ì¡°ê±´ì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ì„œ ìµœì¢…ì ì¸ ì‹ì€ ë‹¤ìŒê³¼ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ ê²ƒì…ë‹ˆë‹¤. - \\((x_0,x_1,...,x_m)\\,,(w_0,w_1,..,w_m)\\)ì˜ linear combination ì‹ì— ìˆì–´ì•¼ í•¨. - linearcombination = ëª¨ìˆ˜(ì¶”ì •í•˜ê³ ìí•˜ëŠ”ê°’)ì—¬ì•¼ í•¨.\nwhy linear combination?"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#ë³¸ê²©ì ì¸-ìœ ë„",
    "href": "posts/DL/Logistic Regression.html#ë³¸ê²©ì ì¸-ìœ ë„",
    "title": "Logistic Regression",
    "section": "ë³¸ê²©ì ì¸ ìœ ë„",
    "text": "ë³¸ê²©ì ì¸ ìœ ë„\n\nëª¨ìˆ˜ë¥¼ ë¡œì§€ìŠ¤í‹±í•¨ìˆ˜ë¡œ ë°”ê¾¸ê¸°\n\n\\((x_0,x_1,...,x_m)\\,,(w_0,w_1,..,w_m)\\)ì˜ linear combinationì´ ì‹ì— ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì„ í˜•ë°©ì •ì‹ì„ í•˜ë‚˜ ë§Œë“­ë‹ˆë‹¤. \\[\\begin{align}\nf(i) = x_{1,i}w_0 + x_{2,i}w_1 + x_2w_2 + ... + x_{m,i}w_m = X_iW \\nonumber \\\\\nwhere,X_i = \\,[x_{1,i},x_{2,i},\\dots,x_{m,i}]\\, ,W = \\,[w_0,w_1,\\dots,w_m]^\\text{T} \\nonumber \\\\\n\\end{align}\\]\nì¢Œë³€ì€ ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ê°’ì¸ ëª¨ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤. ì¢Œë³€ì„ ë°”ê¿”ë´…ë‹ˆë‹¤. \\[p_i = WX_i\\]\nì¢Œë³€ì˜ ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ ëª¨ìˆ˜ \\(p_i\\)ëŠ” í™•ë¥ ë³€ìˆ˜ \\(y = 1\\)ì¸ ì‚¬ê±´ì´ ì¼ì–´ë‚  í™•ë¥ ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ \\([0,1]\\)ì´ë¼ëŠ” ë²”ìœ„ë¥¼ ê°€ì§€ëŠ” ë°˜ë©´ ìš°ë³€ì˜ ê°’\\(WX_i\\)ì€ \\(\\,[-\\infty,\\infty]\\,\\)ì— ë²”ìœ„ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì—¬ê¸°ì„œ Odss Ratioë¥¼ ì¨ì„œ ëª¨ìˆ˜ \\(p_i\\)ë¥¼ í¬í•¨í•˜ë©° ë” ë„“ì€ rangeë¥¼ ê°–ë„ë¡ ì¢Œë³€ì„ ìˆ˜ì •í•©ë‹ˆë‹¤. \\[\\text{Odds Ratio} = \\frac{p_i}{1-p_i} = WX_i\\]\nì¢Œë³€ì„ Odds Ratioë¡œ ìˆ˜ì •í–ˆì§€ë§Œ ì—¬ì „íˆ ì¢Œë³€ì˜ ë²”ìœ„ëŠ”\\(\\,[0,\\infty]\\,\\)ìœ¼ë¡œ ìš°ë³€ì— ë¹„í•´ ì¢ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ Odds Ratioì— ë¡œì§“ë³€í™˜ì„ ì·¨í•˜ì—¬ ì¢Œë³€ì˜ ë²”ìœ„ë¥¼ \\(\\,[-\\infty,\\infty]\\)ë¡œ ë„“í˜€ì¤ë‹ˆë‹¤. \\[\\text{logit}(p) = \\text{ln}\\frac{p_i}{1-p_i} = WX_i\\]\n\nìœ„ ì‹ì„ í•´ì„í•˜ê¸° ìœ„í•´ \\(X\\)ì˜ ì²«ë²ˆì§¸ ìš”ì†Œì¸ \\(x_1\\)ì— ëŒ€ì‘í•˜ëŠ” íšŒê·€ê³„ìˆ˜ \\(w_1\\)ì´ í•™ìŠµê²°ê³¼ 3ìœ¼ë¡œ ì •í•´ì¡Œë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤.ë§Œì•½ \\(x_1\\)ì˜ ê°’ì´ 1ì¦ê°€í•œë‹¤ë©´ ë¡œê·¸ì˜¤ì¦ˆë¹„ê°€ 3ì¦ê°€í•©ë‹ˆë‹¤.\n\nì´ì œ ì–‘ë³€ì˜ ë²”ìœ„ëŠ” ë§ì¶°ì¡Œìœ¼ë¯€ë¡œ ì¶”ì •í•˜ê³ ì í•˜ëŠ” ë³€ìˆ˜ \\(p_i\\)ê°€ ì¢Œë³€ì— ì˜¤ë„ë¡ ì •ë¦¬í•´ë´…ì‹œë‹¤. \\[p_i = \\frac{1}{\\,(1 + e^{-WX_i})\\, }\\] (ì „ê°œ) \\(\\frac{p_i}{1-p_i} = e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\,(1-p_i)\\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\,e^{WX_i}-p_ie^{WX_i}\\) \\(\\Longleftrightarrow p_i + p_ie^{WX_i} = \\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i\\,(1 + e^{WX_i})\\, = \\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\frac{\\,e^{WX_i}}{\\,(1 + e^{WX_i})\\, }\\) \\(\\Longleftrightarrow p_i = \\frac{1}{\\,(1 + e^{-WX_i})\\, }\\)\n\nìµœì¢…ì ìœ¼ë¡œ, ì•ì„œ ëª©ì ì´ì—ˆë˜ Xì™€ Wì˜ ì„ í˜•ì¡°í•©ì´ ìˆ˜ì‹ë‚´ë¶€ì— ì¡´ì¬í•˜ë„ë¡ ìƒˆë¡­ê²Œ í‘œí˜„í•œ ëª¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\[p_i(y) = Pr\\,(y = 1|X_i;W)\\, = \\frac{1}{\\,1 + e^{-WX_i}\\,}\\]\n\n\në² ë¥´ëˆ„ì´ ë¶„í¬ì˜ pmf ì •ë¦¬\në² ë¥´ëˆ„ì´ë¶„í¬ì˜ ëª¨ìˆ˜ \\(p_i\\)ê°€ ìƒˆë¡­ê²Œ í‘œí˜„ë˜ì—ˆìœ¼ë¯€ë¡œ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë„ ìƒˆë¡­ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ìˆ˜ì‹ì€ ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ê°€ ëª¨ìˆ˜\\(W\\)ì— ê´€í•œ ì‹ìœ¼ë¡œ ë°”ë€Œì—ˆìŒì„ í‘œí˜„í•©ë‹ˆë‹¤.\n\\[\\begin{align}\nBern(y;p_i) = Pr\\,(Y_{i} = y|x_{1,i},x_{2,i},\\dots,x_{m,i};p_i) &=\n\\begin{cases}\np_i & \\text{if}\\,y=1 \\\\\n1-p_i & \\text{if}\\,y=0\n\\end{cases} \\\\\n&= p_i^{y}(1-p_i)^{1-y} \\\\\n&= \\frac{e^{yWX_i}}{1+e^{WX_i}}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#setting",
    "href": "posts/DL/Logistic Regression.html#setting",
    "title": "Logistic Regression",
    "section": "setting",
    "text": "setting\n\nimport torch\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n#sig = lambda z:torch.exp(z)/(1+torch.exp(z))\nsig = torch.nn.Sigmoid()"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#data",
    "href": "posts/DL/Logistic Regression.html#data",
    "title": "Logistic Regression",
    "section": "data",
    "text": "data\n\ntorch.manual_seed(2022)\nn=400\n\n#1 ëª¨ìˆ˜ Wê°€ì •\nW = torch.tensor(\n    [[-0.8467],\n    [0.041]]).float()\n\n#2 ê°ê°ì˜ ê´€ì¸¡ì¹˜(ë°ì´í„°ìš”ì†Œ)ì—ì„œì˜ ëª¨ìˆ˜ p_iì‹œê°í™”(ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì‹œê°í™”)\n_x = torch.linspace(-150,150,n).reshape(-1,1)\n_one = torch.ones((n,1))\nX = torch.concat((_one,_x),axis=1)\np_i = sig(X@W)\ny = torch.bernoulli(p_i)\nplt.xlim([-150,150])\nplt.plot(X[:,1],y,\"bo\",alpha=0.5)\nplt.plot(X[:,1],p_i,\"r--\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y,$p_i$\")\nplt.title(\"realizations $y_1,\\dots,y_{300}$ from $Bern(p_1),\\dots,Bern(p_{300})$\")\nplt.legend([\"y\",\"p\"])\n\n<matplotlib.legend.Legend at 0x1f2684bfe20>"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#gradient-descent-1",
    "href": "posts/DL/Logistic Regression.html#gradient-descent-1",
    "title": "Logistic Regression",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nimport torch\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n\ntorch.manual_seed(2022)\nn=400\n\n#2 ì„ì˜ì˜ Wì— ëŒ€í•œ estimated value(ì¶”ì •ì¹˜) What ì´ˆê¸°í™”\nWhat = torch.tensor(\n    [[0.],\n    [-0.03]],requires_grad=True)\n\n_x = torch.linspace(-150,150,n).reshape(-1,1)\n_one = torch.ones((n,1))\nX = torch.concat((_one,_x),axis=1)\nyhat = sig(X@What)\n\nplt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\nplt.plot(X[:,1].data,p_i,\"r\")\nplt.plot(X[:,1].data,yhat.data,\"g\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"realizations $y_1,\\dots,y_{60}$ from $Bern(p_1),\\dots,Bern(p_{60})$\")\nplt.legend([\"y\",\"$p_i$\",\"$\\hat{y}(\\hat{p_i})$\"])\n\n<matplotlib.legend.Legend at 0x1f268e11e50>\n\n\n\n\n\n\nloss_fn = torch.nn.BCELoss()\n\"\"\"\ndef BCE_Loss(yhat,y):\n    return torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat))\n\"\"\"\n\n'\\ndef BCE_Loss(yhat,y):\\n    return torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat))\\n'\n\n\n\n#custom sigmoid + torch.BCELoss ì“°ë©´ ì˜¤ë¥˜ ë°œìƒ. 0ê³¼ 1ì‚¬ì´ì˜ ë²”ìœ„ ì•„ë‹˜\n#torch.nn.Sigmoid + custom BCE Loss ì¨ë„ ì˜¤ë¥˜ë°œìƒ => nan\nplt.subplots(2,5,figsize=(20,8))\nplt.subplots_adjust(hspace=0.3)\ni=1\n\nfor epoch in range(200):\n    #1 yhat \n    yhat = sig(X@What)\n    #2 loss\n    loss = loss_fn(yhat,y)\n    if epoch % 20 == 0:\n        plt.subplot(2,5,i)\n        #plt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\n        plt.plot(X[:,1].data,p_i,\"r\")\n        plt.plot(X[:,1].data,yhat.data,\"g\")\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        #plt.title(\"realizations $y_1,\\dots,y_{60}$ from $Bern(p_1),\\dots,Bern(p_{60})$\")\n        plt.legend([\"p\",\"yhat\"])\n        title = \"loss : {}\".format(round(loss.tolist(),5))\n        plt.title(title)\n        i+=1\n    #3 derivative\n    loss.backward()\n    #4 update & clean\n    What.data = What.data - 0.00005 * What.grad\n    What.grad = None\n\n\n\n\n\nround(loss.tolist(),4)\n\n0.3109\n\n\n\nplt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\nplt.plot(X[:,1].data,p_i,\"r\")\nplt.plot(X[:,1].data,yhat.data,\"g\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Logistic Regression\")\nplt.legend([\"y\",\"$p_i$\",\"$\\hat{y}(\\hat{p_i})$\"])\nplt.gca().axes.xaxis.set_visible(False)\nplt.gca().axes.yaxis.set_visible(False)"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#ë² ë¥´ëˆ„ì´-ë¶„í¬-ì „ê°œ",
    "href": "posts/DL/Logistic Regression.html#ë² ë¥´ëˆ„ì´-ë¶„í¬-ì „ê°œ",
    "title": "Logistic Regression",
    "section": "1.ë² ë¥´ëˆ„ì´ ë¶„í¬ ì „ê°œ",
    "text": "1.ë² ë¥´ëˆ„ì´ ë¶„í¬ ì „ê°œ\n\\[\\begin{aligned}\np_i^y(1-p_i)^{1-y} &= \\ (\\frac{1}{\\,1 + e^{-WX_i}\\,})^y\\,\\,(1-\\frac{1}{\\,1 + e^{-WX_i}\\,})^{1-y} \\\\\n&= (\\frac{1}{1+e^{-WX_i}})^{y}(\\frac{e^{-WX_i}}{1+e^{-WXi}})^{1-y} \\\\\n&= (\\frac{1}{1+e^{-WX_i}})^{y}(\\frac{1}{1+e^{WXi}})^{1-y} \\\\\n&= (\\frac{1+e^{WX_i}}{1+e^{-WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= (\\frac{e^{WX_i}+e^{2WX_i}}{1+e^{WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= (\\frac{e^{WX_i}(1+e^{WX_i})}{1+e^{WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= e^{yWX_i}\\frac{1}{1+e^{WX_i}} \\\\\n&= \\frac{e^{yWX_i}}{1+e^{WX_i}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#nllì „ê°œwith-parameter-w",
    "href": "posts/DL/Logistic Regression.html#nllì „ê°œwith-parameter-w",
    "title": "Logistic Regression",
    "section": "2.NLLì „ê°œ(with parameter \\(W\\))",
    "text": "2.NLLì „ê°œ(with parameter \\(W\\))\n\\[\\begin{aligned}\nLL &= \\text{ln}(\\underset{i=1}{\\overset{n}{\\large{\\prod}}}\\,\\frac{e^{y_iWX_i}}{1+e^{WX_i}}) \\\\\n&=\\overset{n}{\\underset{i=1}{\\large{\\sum}}}(\\text{ln}\\frac{e^{y_iWX_i}}{1+e^{WX_i}}) \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}\\,[\\text{ln}e^{y_iWX_i} - \\text{ln}(1+e^{WX_i})]\\, \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}\\,[y_iWX_i - \\text{ln}(1+e^{WX_i})], \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}y_iWX_i - \\overset{n}{\\underset{i=1}{\\large{\\sum}}}ln(1+e^{WX_i}) \\\\\n\n\nNLL &= -\\overset{n}{\\underset{i=1}{\\large{\\sum}}}y_iWX_i + \\overset{n}{\\underset{i=1}{\\large{\\sum}}}ln(1+e^{WX_i}) \\\\\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#nllì „ê°œcross-entropy-ìœ ë„í•˜ê¸°",
    "href": "posts/DL/Logistic Regression.html#nllì „ê°œcross-entropy-ìœ ë„í•˜ê¸°",
    "title": "Logistic Regression",
    "section": "3.NLLì „ê°œ(Cross Entropy ìœ ë„í•˜ê¸°)",
    "text": "3.NLLì „ê°œ(Cross Entropy ìœ ë„í•˜ê¸°)\nì„ì˜ì˜ ië²ˆì§¸ í•­ì—ì„œì˜ í™•ë¥ ë³€ìˆ˜ \\(Y_i\\)ê°€ ë”°ë¥´ëŠ” ë² ë¥´ëˆ„ì´ ë¶„í¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \n\\[\\begin{aligned}\nBern(y|X_i;p_i) = (p_i)^y(1-p_i)^{y-1}\n\\end{aligned}\\]\nëª¨ìˆ˜ê°€ \\(p_i\\)ì¸ ê°ê°ì˜ ë² ë¥´ëˆ„ì´ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜ \\(Y_1,Y_2\\dots Y_n\\)ìœ¼ë¡œë¶€ nê°œì˜ realization(sample) \\(y_1,y_2\\dots y_n\\)ì— ëŒ€í•œ NLLëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \n\\[\\begin{aligned}\nNLL &= -\\text{ln}\\prod_{i=1}^{n}p_i^{y_i}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}\\text{ln}p_i^{y_i}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}\\text{ln}p_i^{y_i} + \\text{ln}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}y_i\\text{ln}p_i + (1-y_i)\\text{ln}(1-p_i)\n\\end{aligned}\\]\nì°¸ê³ ë§í¬ 1. ë¡œì§€ìŠ¤í‹± íšŒê·€ ì „ê°œ 2. ìœ„í‚¤í”¼ë””ì•„ - ë¡œì§€ìŠ¤í‹± íšŒê·€ 3. ratsgoâ€™s blog"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "",
    "text": "[Deep Learning Series - Part3]\nì•ˆë…•í•˜ì„¸ìš”!!ğŸ˜€ ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ë‹¤í•­ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ ì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€ì— ëŒ€í•´ì„œ ì •ë¦¬í•´ë³´ê³ ì í•©ë‹ˆë‹¤. ê³µë¶€í•˜ë©´ì„œ ìƒê°ë³´ë‹¤ ëª¨ë¥´ëŠ” ë‚´ìš©ì´ ë§ì•„ì„œ ë‹¤ì‹œ ì²˜ìŒë¶€í„° ê³µë¶€í•˜ê³  ë³µìŠµí•´ì•¼ í•˜ëŠ” ë‚´ìš©ì´ ë§ì•˜ë„¤ìš”. ì¡ë‹´ì€ ê·¸ë§Œí•˜ê³  ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤!! ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•´ìš”ğŸ˜"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#ê°€ì •-1",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#ê°€ì •-1",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "ê°€ì • (1)",
    "text": "ê°€ì • (1)\në¡œì§€ìŠ¤í‹±íšŒê·€ë¥¼ ë³µê¸°í•´ë³´ë©´â€¦ ì¢…ì†ë³€ìˆ˜ \\(y_i\\)ëŠ” ë² ë¥´ëˆ„ì´ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜\\(Y_i\\)ë¡œë¶€í„° ìƒ˜í”Œë§ëœ ê°’ìœ¼ë¡œ ê°€ì •í–ˆìŠµë‹ˆë‹¤. ë˜í•œ ë² ë¥´ëˆ„ì´ë¶„í¬ì˜ ëª¨ìˆ˜\\(W\\)ëŠ” ì£¼ì–´ì§„ ì¡°ê±´ì¸ \\(X_i\\)ì™€ íšŒê·€ê³„ìˆ˜(ê°€ì¤‘ì¹˜)\\(W\\)ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ ê°€ì •í–ˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ëª¨ìˆ˜ë¥¼ ê°€ì •í•˜ë©´ì„œ ë² ë¥´ëˆ„ì´ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë„ ìƒˆë¡œìš´ ëª¨ìˆ˜\\(W\\)ë¥¼ ê°€ì§€ê²Œë˜ì—ˆê³  Wë¥¼ ì ì ˆíˆ ì¶”ì •í•˜ë©´ ë°ì´í„°ê°€ 0ë˜ëŠ”1ì— ì†í•  í™•ë¥ ì„ ì•Œì•„ë‚´ê²Œ ë˜ì–´ í™•ë¥ ì´ ë” ë†’ì€ í´ë˜ìŠ¤ë¥¼ ì£¼ì–´ì§„ë°ì´í„°ì— ëŒ€í•œ í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡í–ˆì—ˆìŠµë‹ˆë‹¤.ë‹¤í•­ë¡œì§€ìŠ¤í‹±íšŒê·€ì™€ ì†Œí”„íŠ¸ë§¥ìŠ¤íšŒê·€ì—ì„œë„ ì´ëŸ¬í•œ ê³¼ì • ì¦‰,ë¶„í¬ë¥¼ ê°€ì •í•˜ê³  ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•˜ì—¬ í™•ë¥ ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ë§¤ì»¤ë‹ˆì¦˜ì€ ê±°ì˜ ê·¸ëŒ€ë¡œì…ë‹ˆë‹¤.\në¨¼ì € ë‹¤í•­ë¡œì§€ìŠ¤í‹±íšŒê·€ì™€ ì†Œí”„íŠ¸ë§¥ìŠ¤íšŒê·€ì—ì„œ ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê°€ì •ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤í•­ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ ì†Œí”„íŠ¸ë§¥ìŠ¤íšŒê·€ì—ì„œ ëª¨ë‘ ê°ê°ì˜ ê´€ì¸¡ì¹˜(each observation)ì—ì„œ ì¢…ì†ë³€ìˆ˜ì˜ realizationì¸ \\(y_i\\)ëŠ” í™•ë¥ ë³€ìˆ˜\\(Y_i\\)ë¡œë¶€í„° í‘œë³¸ì¶”ì¶œ(sampling)ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ì´ë•Œ ê°ê°ì˜ ê´€ì¸¡ì¹˜ì—ì„œì˜ í™•ë¥ ë³€ìˆ˜ \\(Y_i\\)ê°€ ë”°ë¥´ëŠ” ë¶„í¬ëŠ” ì„¤ëª…ë³€ìˆ˜ \\(x_{1,i},x_{2,i},\\dots,x_{M,i}\\)ê°€ ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§ˆ ë•Œ, ê°ê°ì˜ ë²”ì£¼(í´ë˜ìŠ¤)ì— ì†í•  í™•ë¥ ë“¤ì„ ëª¨ìˆ˜ë¡œ ê°€ì§€ëŠ” ì¹´í…Œê³ ë¦¬ë¶„í¬ë¥¼ ë”°ë¦…ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\nY_i|x_{1,i},x_{2,i},\\dots,x_{M,i} \\sim \\text{Cat}(y|x_{1,i},x_{2,i},\\dots,x_{M,i};\\mu_i)\n& =\n\\begin{cases}\n\\mu_{1,i} \\text{ if } y = (1,0,\\dots,0,0) \\\\\n\\mu_{2,i} \\text{ if } y = (0,1,\\dots,0,0) \\\\\n\\quad\\quad \\vdots \\\\\n\\mu_{K,i} \\text{ if } y = (0,0,\\dots,0,1) \\\\\n\\end{cases} \\\\\n&= \\mu_{1,i}^{y_1}\\mu_{2,i}^{y_2},\\dots,\\mu_{K,i}^{y_K} \\\\\n&= \\prod_{K=1}^{K}\\mu_{K,i}y_{K,i} \\\\\n\\end{aligned} \\\\\n&\n\\begin{aligned}\n&\\text{where, }\\\\\n&\\mu_i = {\\mu_{1,i},\\mu_{2,i},\\dots,\\mu_{K,i}} \\\\\n&\\mu_{1,i} = Pr(Y_i = (1,0,\\dots,0)|x_{1,i},\\dots,x_{M,i}) \\\\\n&\\mu_{2,i} = Pr(Y_i = (0,1,\\dots,0)|x_{1,i},\\dots,x_{M,i}) \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\vdots \\\\\n&\\mu_{K,i} = Pr(Y_i = (0,0,\\dots,0,1)|x_{1,i},\\dots,x_{M,i}) \\\\\n\\end{aligned}\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#ê°€ì •-2",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#ê°€ì •-2",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "ê°€ì • (2)",
    "text": "ê°€ì • (2)\nê°ê°ì˜ ê´€ì¸¡ì¹˜ì—ì„œ í™•ë¥ ë³€ìˆ˜\\(Y_i\\)ê°€ ë”°ë¥´ëŠ” ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜\\(\\mu_i\\)ëŠ” ë°ì´í„°í¬ì¸íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ ì„¤ëª…ë³€ìˆ˜(X_i)ì™€ ì‹œí–‰ë§ˆë‹¤ ë³€í•˜ì§€ ì•ŠëŠ” ê³ ì •ëœ íšŒê·€ê³„ìˆ˜(W)ì˜ ì¼ì°¨ê²°í•©ì„ í¬í•¨í•˜ëŠ” ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ì£¼ì–´ì§„ Xê°’ì„ Wì™€ ì¼ì°¨ê²°í•©í•˜ì—¬ ì¶”ì •í•˜ê³ ì í•˜ëŠ” ê°’ì„ í‘œí˜„í•˜ëŠ” ì„ í˜•íšŒê·€ì˜ í•µì‹¬ì•„ì´ë””ì–´ì´ì ëŒ€ë¶€ë¶„ì˜ íšŒê·€ë¬¸ì œì—ì„œ ì‚¬ìš©í•˜ëŠ” ì¤‘ìš”í•œ ì•„ì´ë””ì–´ ì…ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mu_{k,i}  = \\mu_{k,i}(X_i;W_{k,i}) = \\mu_{k,i}(X_iW_{k,i}) =  Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i)\\\\\n&\\text{where},\\\\\n&w_{m,k} : \\text{$k$ë²ˆì§¸ ëª¨ìˆ˜ë¥¼ í‘œí˜„í•˜ê¸°ìœ„í•´ $m$ë²ˆì§¸ ê°’ê³¼ ê³±í•´ì§€ëŠ” ê°€ì¤‘ì¹˜} \\\\\n&x_{m,i} : \\text{i-th ê´€ì¸¡ì¹˜ì˜ $m$ë²ˆì§¸ ë…ë¦½ë³€ìˆ˜ì˜ ê°’} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-thê´€ì¸¡ì¹˜ì˜ feature vector(ë‹¨,$x_{0,i}$ = 1)} \\\\\n&W_k = [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : ì¹´í…Œê³ ë¦¬ ë¶„í¬ì˜ ì„ì˜ì˜ k-th ëª¨ìˆ˜$\\mu_k$ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ì•„ë†“ì€ ë²¡í„°} \\\\\n&\\mu_{k,i} = \\text{i-th ê´€ì¸¡ì¹˜ì˜ $k$ë²ˆì§¸ ëª¨ìˆ˜}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xwì˜-ì„ í˜•ì¡°í•©ì„-í¬í•¨í•œ-ëª¨ìˆ˜ì˜-í‘œí˜„-ìœ ë„í•˜ê¸°",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xwì˜-ì„ í˜•ì¡°í•©ì„-í¬í•¨í•œ-ëª¨ìˆ˜ì˜-í‘œí˜„-ìœ ë„í•˜ê¸°",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "X,Wì˜ ì„ í˜•ì¡°í•©ì„ í¬í•¨í•œ ëª¨ìˆ˜ì˜ í‘œí˜„ ìœ ë„í•˜ê¸°",
    "text": "X,Wì˜ ì„ í˜•ì¡°í•©ì„ í¬í•¨í•œ ëª¨ìˆ˜ì˜ í‘œí˜„ ìœ ë„í•˜ê¸°\nìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ëŒ€ë¶€ë¶„ì˜ íšŒê·€ì—ì„œ ëª¨ë¸ë§ì˜ í•µì‹¬ì•„ì´ë””ì–´ëŠ” ì¶”ì •í•˜ê³ ì í•˜ëŠ” ëŒ€ìƒì„ ì„¤ëª…ë³€ìˆ˜ì™€ ê°€ì¤‘ì¹˜ì˜ ì¼ì°¨ê²°í•©(ì„ í˜•ì¡°í•©)ì´ í¬í•¨ë˜ë„ë¡ í‘œí˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤í•­ë¡œì§€ìŠ¤í‹±íšŒê·€ë„ ì¶”ì •í•˜ê³ ì í•˜ëŠ” ëª¨ìˆ˜\\(\\mu_i = (\\mu_{1,i},\\mu_{2,i},\\dots,\\mu_{K,i})\\)ë¥¼ ê°ê°ì„ ì„¤ëª…ë³€ìˆ˜ì™€ ê°€ì¤‘ì¹˜ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ í‘œí˜„í•´ì•¼ í•©ë‹ˆë‹¤.ì´ì§„ë¡œì§€ìŠ¤í‹±íšŒê·€ì™€ì—ì„œë„ ì´ë ‡ê²Œ ëª¨ìˆ˜ë¥¼ í‘œí˜„í–ˆì—ˆëŠ”ë° ë‹¤í•­ë¡œì§€ìŠ¤í‹±íšŒê·€ì—ì„œëŠ” ì¼ì°¨ê²°í•©ìœ¼ë¡œ í‘œí˜„í•´ì•¼í•  ëª¨ìˆ˜ê°€ ì¢€ ë” ë§ìŠµë‹ˆë‹¤. -_-;;\nì°¨ê·¼ì°¨ê·¼ í•œë²ˆ ìœ ë„í•´ë³´ê² ìŠµë‹ˆë‹¤. ì¼ë‹¨ Kê°œì˜ ëª¨ìˆ˜ë¥¼ í‘œí˜„í•˜ëŠ” ì¼ì°¨ê²°í•©ì„ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¼ì°¨ê²°í•©ì—ì„œ xëŠ” ê´€ì¸¡ì¹˜ë§ˆë‹¤ ì¡´ì¬í•˜ëŠ” ì„¤ëª…ë³€ìˆ˜ì˜ ê°’ì— ë”°ë¼ì„œ íšŒê·€ê³„ìˆ˜(ê°€ì¤‘ì¹˜)ì¸ WëŠ” ê´€ì¸¡ì¹˜ì— ë”°ë¼ì„œ ë³€í•˜ì§€ ì•ŠëŠ” ì¼ì •í•œ ê°’ì…ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i-\\text{ln}Z \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\\n&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i {\\text{ (ì„ì˜ì˜ kë²ˆì§¸ í•­)}}\\\\  \n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n&where,\\\\\n&w_{m,k} : \\text{$k$ë²ˆì§¸ ëª¨ìˆ˜ë¥¼ í‘œí˜„í•˜ê¸°ìœ„í•´ $m$ë²ˆì§¸ ê°’ê³¼ ê³±í•´ì§€ëŠ” ê°€ì¤‘ì¹˜} \\\\\n&x_{m,i} : \\text{i-th ê´€ì¸¡ì¹˜ì˜ $m$ë²ˆì§¸ ë…ë¦½ë³€ìˆ˜ì˜ ê°’} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-thê´€ì¸¡ì¹˜ì˜ feature vector(ë‹¨,$x_{0,i}$ = 1)} \\\\\n&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : ì¹´í…Œê³ ë¦¬ ë¶„í¬ì˜ ì„ì˜ì˜ k-th ëª¨ìˆ˜$\\mu_k$ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ì•„ë†“ì€ ë²¡í„°} \\\\\n\\end{aligned}\\]\n í•œ ê°€ì§€ ìœ ì˜í•´ì•¼ í•  ì ì€ ë§ˆì§€ë§‰ ëª¨ìˆ˜ëŠ” ì¼ì°¨ê²°í•©ìœ¼ë¡œ í‘œí˜„í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¹´í…Œê³ ë¦¬ë¶„í¬ì—ì„œ ëª¨ìˆ˜ì˜ ì´í•©ì€ 1ì´ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ \\(K\\)ë²ˆì§¸ ëª¨ìˆ˜ëŠ” 1ì—ì„œ ì „ë¶€ ë¹¼ë©´ ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\nê·¸ëŸ°ë° ì„£ë¶ˆë¦¬ ì¼ì°¨ê²°í•©ì„ ë§Œë“¤ë‹¤ë³´ë‹ˆ â€¦ ì¢Œë³€ì— ìˆëŠ” ëª¨ìˆ˜ëŠ” \\([0,1]\\)ì˜ ë²”ìœ„ì´ê³  ìš°ë³€ì€ \\([-\\infty,\\infty]\\)ì˜ ë²”ìœ„ì´ë¯€ë¡œ ê°€ì§€ë¯€ë¡œ ì–‘ë³€ì˜ ë²”ìœ„ê°€ ì „í˜€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì¢Œë³€ì„ Odds Ratio(ì—„ë°€íˆ Odds RatioëŠ” ì•„ë‹ˆì§€ë§Œ í†µì¼ì„±ì„ ìœ„í•´ Odds Ratioë¼ê³  í•˜ê² ìŠµë‹ˆë‹¤.) + Logit transformì„ ì·¨í•˜ì—¬ ì¢Œë³€ì´ ìš°ë³€ê³¼ ê°™ì€ ë²”ìœ„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ í™•ì¥í•˜ì—¬ ì¤ë‹ˆë‹¤. (ë¡œê·¸ì•ˆì— ìˆëŠ” ë¶„ëª¨ê°€ Kë²ˆì§¸ í´ë˜ìŠ¤ì— ëŒ€í•œ í•­ì„ì„ ìœ ì˜í•©ë‹ˆë‹¤.) \\[\\text{ln}\\frac{\\mu_{k,i}}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = \\text{ln}\\frac{Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i;W_k)}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = W_k^TX_i\\]\nì›ë˜ì˜ ëª©ì ì€ ëª¨ìˆ˜ì— ëŒ€í•œ ì¼ì°¨ê²°í•©ì´ í¬í•¨ëœ í•­ì„ ì–»ëŠ” ê²ƒì´ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì •ë¦¬í•˜ì—¬ ëª¨ìˆ˜ì— ëŒ€í•œ í‘œí˜„ì„ ì–»ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n\\mu_{k,i} = Pr(Y_i = (0,\\dots,0,1_{k-th},0,\\dots,0|X_i;W_k) = Pr(Y_i = K|X_i)e^{X_iW_k}\n\\end{aligned}\\]\nì—¬ê¸°ê¹Œì§€ í•´ì„œ ëª¨ìˆ˜ì— ëŒ€í•œ í‘œí˜„ì„ ì–»ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ \\(Y_i\\)ê°€ \\(K\\)ë²ˆì§¸ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì€ ì¹´í…Œê³ ë¦¬ë¶„í¬ì—ì„œì˜ ëª¨ìˆ˜ì— ëŒ€í•œ ì œì•½ì¡°ê±´ì„ í™œìš©í•˜ì—¬ ë” ê°„ë‹¨í•˜ê²Œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&Pr(Y_i = K|X_i) = 1- \\sum_{k=1}^{K-1}Pr(Y_i = K|X_i)e^{X_iW_k} = 1-Pr(Y_i = K|X_i)\\sum_{k=1}^{K-1}e^{X_iW_k} \\\\\n&\\Longleftrightarrow Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{k=1}^{K-1}e^{X_iW_k}}\n\\end{aligned}\\]\në” ê°„ë‹¨í•˜ê²Œ í‘œí˜„ëœ í•­ìœ¼ë¡œ ë‹¤ì‹œ ì •ë¦¬í•˜ì—¬ ì“°ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mu_{k,i}=Pr(Y_i = k|X_i) = Pr(Y_i = K|X_i)e^{X_iW_k} = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}}\\\\\n&\\text{ì¸ë±ìŠ¤ ê²¹ì¹˜ë¯€ë¡œ ì‹œê·¸ë§ˆì˜ $k \\rightarrow j$}\n\\end{aligned}\\]\n ìµœì¢…ì ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„í¬ì˜ ëª¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì „ê°œí•˜ëŠ” ê³¼ì •ì´ ë§ˆì§€ë§‰ \\(K\\)ë²ˆì§¸ í•­ì€ ì œì™¸í•œì±„ ì§„í–‰ë˜ì—ˆìœ¼ë¯€ë¡œ Kë²ˆì§¸ í•­ì—ëŒ€í•œ í™•ë¥ ì€ ë”°ë¡œ ì¨ì¤ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mu_{k,i}=Pr(Y_i = k|X_i) = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}} \\text{(ë‹¨, $k != K$)}\\\\\n&\\mu_{K,i}=Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{j=1}^{K-1}e^{X_iW_k}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#estimation",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#estimation",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "Estimation",
    "text": "Estimation\në” ê³µë¶€í•´ ì˜¤ê² ìŠµë‹ˆë‹¤ ^__^;;"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#ê°€ì •",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#ê°€ì •",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "ê°€ì •",
    "text": "ê°€ì •\nì†Œí”„íŠ¸ë§¥ìŠ¤íšŒê·€ì˜ ê°€ì •ì€ ë¡œì§€ìŠ¤í‹±íšŒê·€ì˜ ê°€ì •ê³¼ ìŠµë‹ˆë‹¤. ê° datapointì—ì„œì˜ ì¢…ì†ë³€ìˆ˜ì˜ ê°’ì€ ì¹´í…Œê³ ë¦¬ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜ì—ì„œ ìƒ˜í”Œë§ë˜ì—ˆìœ¼ë©° ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜ëŠ” ê° datapointë§ˆë‹¤ ë³€í•˜ëŠ” ì„¤ëª…ë³€ìˆ˜ì™€ íšŒê·€ê³„ìˆ˜(ê°€ì¤‘ì¹˜)ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xwì˜-ì„ í˜•ì¡°í•©ì„-í¬í•¨í•œ-ëª¨ìˆ˜ì˜-í‘œí˜„-ìœ ë„í•˜ê¸°-1",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xwì˜-ì„ í˜•ì¡°í•©ì„-í¬í•¨í•œ-ëª¨ìˆ˜ì˜-í‘œí˜„-ìœ ë„í•˜ê¸°-1",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "X,Wì˜ ì„ í˜•ì¡°í•©ì„ í¬í•¨í•œ ëª¨ìˆ˜ì˜ í‘œí˜„ ìœ ë„í•˜ê¸°",
    "text": "X,Wì˜ ì„ í˜•ì¡°í•©ì„ í¬í•¨í•œ ëª¨ìˆ˜ì˜ í‘œí˜„ ìœ ë„í•˜ê¸°\nì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€ë§ˆì°¬ê°€ì§€ë¡œ ì¶”ì •í•˜ê³ ì í•˜ëŠ” ëª¨ìˆ˜ë¥¼ ì„¤ëª…ë³€ìˆ˜ì™€ ê°€ì¤‘ì¹˜ì˜ ì¼ì°¨ê²°í•©ì´ í¬í•¨ëœ í•­ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.\në¨¼ì € ì„¤ëª…ë³€ìˆ˜ì™€ ê°€ì¤‘ì¹˜ì˜ ì¼ì°¨ê²°í•©í˜•íƒœë¡œ ëª¨ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì„ì˜ì˜ ië²ˆì§¸ ê´€ì¸¡ì¹˜ê°€ ê°ê°ì˜ ë²”ì£¼ì— \\((1,2,...,K)\\) ì†í•  í™•ë¥ ì„ ì˜ë¯¸í•˜ëŠ” ëª¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i-\\text{ln}Z \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\\n&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i \\\\  \n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n&where,\\\\\n&w_{m,k} : \\text{$k$ë²ˆì§¸ ëª¨ìˆ˜ë¥¼ í‘œí˜„í•˜ê¸°ìœ„í•´ $m$ë²ˆì§¸ ê°’ê³¼ ê³±í•´ì§€ëŠ” ê°€ì¤‘ì¹˜} \\\\\n&x_{m,i} : \\text{i-th ê´€ì¸¡ì¹˜ì˜ $m$ë²ˆì§¸ ë…ë¦½ë³€ìˆ˜ì˜ ê°’} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-thê´€ì¸¡ì¹˜ì˜ feature vector(ë‹¨,$x_{0,i}$ = 1)} \\\\\n&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : ì¹´í…Œê³ ë¦¬ ë¶„í¬ì˜ ì„ì˜ì˜ k-th ëª¨ìˆ˜$\\mu_k$ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ì•„ë†“ì€ ë²¡í„°} \\\\\n\\end{aligned}\\]\nì´ë ‡ê²Œ ë‚˜íƒ€ë‚´ê³  ë³´ë‹ˆ ì¢Œë³€ê³¼ 0~1ì‚¬ì´ì˜ ìˆ˜ë§Œ ê°–ì§€ë§Œ ìš°ë³€ì€ ì–´ë–¤ ìˆ˜ë˜ì§€ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë²”ìœ„ë¥¼ ë§ì¶° ì£¼ê¸° ìœ„í•´ì„œ ì¢Œë³€ì— ë¡œê·¸ë¥¼ ì”Œì›Œ ë¡œê·¸í™•ë¥ ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ ìš°ë³€ì— \\(-lnZ\\)ë¼ëŠ” normalizating factorë¥¼ ë”í•´ì¤ë‹ˆë‹¤. ë‹¤ìŒê³¼ì •ì—ì„œ ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜ì˜ í•©ì´ 1ì´ë˜ë„ë¡ í•˜ëŠ” í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ì˜ íŠ¹ì§•ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\text{ln}\\mu_{1,i} = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\text{ln}\\mu_{2,i} = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\text{ln}\\mu_{k,i} = w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{M,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\text{ln}\\mu_{K-1,i} = w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i-\\text{ln}Z \\\\\n\\\\\n\\end{aligned}\\]\në”°ë¼ì„œ,ì„ì˜ì˜ \\(k\\)ë²ˆì§¸ ëª¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\[\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th}|X_i;W_k) = \\frac{1}{Z}e^{W_k^TX_i}\\]\nì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ì œì•½ì¡°ê±´ ì¦‰,ëª¨ìˆ˜ëŠ” ê°ê°ì˜ ë²”ì£¼ì— ì†í•  í™•ë¥ ì„ ë‚˜íƒ€ë‚´ë¯€ë¡œ ì´í•©ì´ 1ì„ì„ í™œìš©í•©ë‹ˆë‹¤. ì´ë¥¼ í™œìš©í•˜ì—¬ Zë¥¼ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\sum_{k=1}^{K}{\\mu_{k,i}} =\\sum_{k=1}^{K}{Pr(Y_i=k)}= \\frac{1}{Z}\\sum_{k=1}^{K}e^{W_k^TX_i} = 1\\\\\n&\\Longleftrightarrow Z = \\sum_{k=1}^{K}e^{W_k^TX_i}\n\\end{aligned}\\]\nìµœì¢…ì ìœ¼ë¡œ, ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. - ì¶”ì •í•˜ê³ ìí•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜ëŠ” \\(\\mu_k\\)ëŠ” \\(W_k\\)ì™€ \\(X_i\\)ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ í‘œí˜„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì´ë¯€ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€ë¼ëŠ” ì´ë¦„ì´ ë¶™ì—ˆìŠµë‹ˆë‹¤. \\[\\mu_{c,i}(X_i;W) = Pr(Y_i = (0,0,\\dots,1_{c-th},0,\\dots,0)|X_i;W_k) = \\frac{e^{W_c^TX_i}}{\\sum_{k=1}^{K}e^{W_k^TX_i}} = softmax(c,W_1^TX_i,W_2^TX_i,\\dots,W_K^TX_i)\\] - ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ìœ„ì—ì„œ êµ¬í•œ ëª¨ìˆ˜ë¡œ ë‹¤ì‹œ ì •ë¦¬í•˜ë©´ í™•ë¥ ì§ˆëŸ‰ í•¨ìˆ˜ëŠ” ìƒˆë¡œìš´ ëª¨ìˆ˜ \\(W_1,W_2,\\dots,W_K\\)ë¥¼ ê°€ì§‘ë‹ˆë‹¤.(ì¸ë±ìŠ¤ \\(k->j,c->k\\)) \\[Y_i \\sim Cat(y|X_i;W_1,W_2,\\dots,W_K) = \\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#mle",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#mle",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "MLE",
    "text": "MLE\nì—¬ê¸°ê¹Œì§€ì˜ ê³¼ì •ìœ¼ë¡œë¶€í„° ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜ëŠ” ì„¤ëª…ë³€ìˆ˜ì™€ ê°€ì¤‘ì¹˜(íšŒê·€ê³„ìˆ˜)ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ í‘œí˜„ë˜ë©° ë˜í•œ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ê°€ ìƒˆë¡œìš´ ëª¨ìˆ˜ \\(W = (W_1,W_2,\\dots,W_K)\\)ë¡œ í‘œí˜„ë˜ì—ˆìŠµë‹ˆë‹¤.ë§Œì•½ ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜ë§Œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ë©´ ìš°ë¦¬ëŠ” ë°ì´í„°í¬ì¸íŠ¸ê°€ ì–´ë–¤ ë²”ì£¼ì— ì†í•  í™•ë¥ ì´ ê°€ì¥ ë†’ì€ì§€ ì•Œ ìˆ˜ ìˆìœ¼ë©° ë²”ì£¼ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜\\(W\\)ë¥¼ MLEë¡œ ì¶”ì •í•©ë‹ˆë‹¤.\ní™•ë¥ ë¶„í¬ì—ì„œ ì„ì˜ì˜ ëª¨ìˆ˜\\(W = (W_1,W_2,\\dots,W_K)\\)ë¥¼ ê°€ì •í•  ë•Œ, í™•ë¥ ë³€ìˆ˜ \\(Y_1,Y_2,\\dots,Y_N\\)ìœ¼ë¡œë¶€í„° realizationì¸ \\(y_1,y_2,\\dots,y_N\\)ì´ ë‚˜ì˜¬ ê°€ëŠ¥ë„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\nL({W};X_i|y_1,y_2,\\dots,y_n) &= Pr_{Y_1,Y_2,\\dots,Y_N}(y1,y2,\\dots,y_n|X_i;W)\\\\\n&= \\prod_{i=1}^{N}Pr_{Y_i}(Y_i=y_i|X_i;W) \\\\\n&= \\prod_{i=1}^{N}\\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\\\\\n\\end{aligned}\n\\\\\n&\\text{where } \\{W\\} = \\{W1,W2,\\dots,W_N\\}\n\\end{aligned}\\]\nìœ„ì™€ ê°™ì€ ê°€ëŠ¥ë„ë¥¼ ìµœì†Œí™” í•˜ëŠ” \\(W\\)ë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤.ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤\n\\[\\begin{aligned}\n\\overset{*}{\\{W\\}} = \\underset{\\{W\\}}{\\text{argmax}} \\prod_{i=1}^{N}\\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn êµ¬í˜„.html",
    "href": "posts/DL/Pytorch Rnn êµ¬í˜„.html",
    "title": "pytorchë¡œ Rnnêµ¬í˜„í•˜ê¸°",
    "section": "",
    "text": "hi?hi!ê°€ ë°˜ë³µë˜ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë‹¤ìŒ ë¬¸ìê°€ ë­ê°€ ë‚˜ì˜¬ì§€ ì˜ˆì¸¡í•˜ëŠ” RNNëª¨í˜• ë§Œë“¤ê¸°"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn êµ¬í˜„.html#vectorization",
    "href": "posts/DL/Pytorch Rnn êµ¬í˜„.html#vectorization",
    "title": "pytorchë¡œ Rnnêµ¬í˜„í•˜ê¸°",
    "section": "vectorization",
    "text": "vectorization\n\nì—¬ëŸ¬ê°€ì§€ ë°©ë²•ì´ ìˆìœ¼ë‚˜(tf-idf,dense vector,one-hot encoding ë“±ë“±â€¦) ì—¬ê¸°ì„œëŠ” ì›í•«ì¸ì½”ë”© ì‚¬ìš©\n\n\ndef mapping(txt,map_dict):\n    return [map_dict[chr]for chr in txt]\ntxt_mapped = mapping(txt,map_dict)\nprint(txt_mapped[:10])\n\ndef onehot_encoding(txt_mapped):\n    seq_encoded = torch.nn.functional.one_hot(torch.tensor(txt_mapped))\n    return seq_encoded.float()\nsequence_data_encoded = onehot_encoding(txt_mapped)\nprint(sequence_data_encoded[:10])\n\n[2, 3, 0, 2, 3, 1, 2, 3, 0, 2]\ntensor([[0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.]])\n\n\në°ì´í„° ì‚´ì§ ë³€í˜• í•˜ë‚˜ì˜ ê¸´ sequence dataë¥¼ RNNì˜ ì…ë ¥ìœ¼ë¡œ í•´ë„ ë˜ì§€ë§Œ ì²˜ë¦¬ì†ë„,ì„±ëŠ¥ì„ ê³ ë ¤í–ˆì„ ë•Œ ìê·¸ë§ˆí•œ sequencedataë¡œ ë¶„ë¦¬í•˜ì—¬ ì…ë ¥í•´ì£¼ëŠ”ê²Œ ë” ì¢‹ì€ ë°©ë²•ì„. ë¶„ë¦¬í•˜ëŠ” ë°©ë²•ë„ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì„ ìˆ˜ ìˆê² ëŠ”ë° ì—¬ê¸°ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë¶„ë¦¬í•¨ raw sequence data : hi?hi!hi?hi!hi?hi! â€¦â€¦â€¦.. sequence1 : (x,y) = (hi?,h) sequence2 : (x,y) = (i?h,i) sequence3 : (x,y) = (?hi,!) â€¦\n\ndef create_seqdataset(seq_data,seq_length):\n    #x = seq_data[:-1]\n    #y = seq_data[1:]\n    seqs_x = []\n    seqs_y = []\n    for idx in range(0,len(seq_data)-seq_length):\n        seqs_x.append(seq_data[idx:idx+seq_length])\n        seqs_y.append(seq_data[idx+seq_length])\n    return torch.stack(seqs_x),torch.stack(seqs_y)\n    #return seq_x,seq_y\n\nx_data,y_data = create_seqdataset(sequence_data_encoded,3)\nprint(x_data.shape,y_data.shape)\n\ntorch.Size([57, 3, 4]) torch.Size([57, 4])\n\n\n\nì™œ ì €ëŸ° shapeì„ ë§ì¶° ì£¼ëŠ”ê°€?\nì—¬ê¸°ì„œ ë‚˜ì˜¤ëŠ” x_data.shape = \\((57,3,4)\\)ê°€ ì‚´ì§ ë‚œí•´í•¨.  íŒŒì´í† ì¹˜ ê³µì‹ë¬¸ì„œì— ë”°ë¥´ë©´ batch_first = Trueë¡œ ì„¤ì •í•  ê²½ìš°,rnnê³„ì—´ì˜ ëª¨ë¸ì— ë„£ì–´ì¤˜ì•¼ í•˜ëŠ” í…ì„œì˜ shapeì€ \\((N,L,H_{in})\\) = (batch size,sequnce length,input_size)ì´ê³  dataloaderë¼ëŠ” ì¼ì¢…ì˜ ë°ì´í„° ì¤‘ê°„ê´€ë¦¬ì?ë¥¼ í•œ ë²ˆ ê±°ì³ì„œ ëª¨ë¸ì— ì…ë ¥ë¨. dataloaderì—ì„œ ë‚˜ì˜¤ëŠ” output.shape = \\((N,L,H_{in})\\)ì´ ë˜ê¸° ìœ„í•´ì„œëŠ” input.shape = \\((D,L,H_{in}\\)(DëŠ” ë¶„ë¦¬ëœ ì‹œí€€ìŠ¤ì˜ ê°¯ìˆ˜)ì´ì–´ì•¼ í•¨(ì¦‰ ì…ë ¥í…ì„œì˜ ì°¨ì›ì´ 3ê°œì—¬ì•¼ ì¶œë ¥í…ì„œì˜ ì°¨ì›ë„3ê°œì´ê³  ì°¨ì›ì´ ë‚˜ì˜¤ëŠ” ìˆœì„œë„ ì €ëŸ°ì‹ì´ ë˜ì–´ì•¼ í•¨). ë”°ë¼ì„œ ì €ë ‡ê²Œ ì„¤ì •í•¨.\n\n\níŒŒë¼ë¯¸í„° ì ê¹ ì„¤ëª…\nbatch sizeëŠ” ë°°ì¹˜ì˜ ì´ ê°¯ìˆ˜(ë°°ì¹˜ì•ˆì— ìˆëŠ” ì›ì†Œì˜ ê°¯ìˆ˜ ì•„ë‹˜!), sequnce lengthëŠ” ì‹œí€€ìŠ¤ë°ì´í„°ì˜ ê¸¸ì´ì´ì timestemp(ì‹œì )ì˜ ì´ ê°¯ìˆ˜(ê¸¸ì´), \\(H_{in}\\)ì€ each timestep(ê° ì‹œì )ë§ˆë‹¤ ì…ë ¥ë˜ëŠ” ë²¡í„°ì˜ ê¸¸ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŒ. ìœ„ì²˜ëŸ¼ ì›í•«ì¸ì½”ë”©ì„ í•œ ê²½ìš° \\(H_{in}\\)ì€ ì‹œí€€ìŠ¤ë°ì´í„°ì— ìˆëŠ” ë¬¸ìì˜ ê°¯ìˆ˜ë¡œ ê²°ì •ë˜ë¯€ë¡œ 4ì´ê³  Lì€ create_seqdatasetí•¨ìˆ˜ì—ì„œ ì¸ìˆ˜ë¡œ ë„£ì–´ì¤€ 3(sequnce_length)ì´ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ N(batch_size)ì€ torch.utils.data.DataLoaderì•ˆì— ì¸ìˆ˜ë¡œ ë„£ì–´ì£¼ëŠ” batch_sizeë¡œ ì¸í•´ì„œ ì¼ì •í•œ ê°¯ìˆ˜ë¡œ ë°°ì¹˜ë¥¼ ë‚˜ëˆ„ì—ˆì„ë•Œ ë‚˜ì˜¤ëŠ” ë°°ì¹˜ë“¤ì˜ ì´ ìˆ«ìì„.rnn ë¬¸ì„œì—ì„œ ì„¤ëª…í•˜ëŠ” batch_sizeëŠ” torch.utils.dada.DataLoaderì—ì„œ ì„¤ì •í•œ batch_sizeì˜ ê°¯ìˆ˜ë§Œí¼ ë°ì´í„°ë¥¼ ëª¨ì•„ì„œ ì—¬ëŸ¬ê°œì˜ ë°°ì¹˜ë¡œ ë§Œë“¤ì—ˆì„ë•Œ ë‚˜ì˜¤ëŠ” ë°°ì¹˜ì˜ ì´ ê°¯ìˆ˜ë¼ê³  ë³´ë©´ë¨.(í—·ê°ˆë¦¬ëŠ” ë¶€ë¶„â€¦.)"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn êµ¬í˜„.html#í•™ìŠµ-ì¤€ë¹„í•˜ê¸°",
    "href": "posts/DL/Pytorch Rnn êµ¬í˜„.html#í•™ìŠµ-ì¤€ë¹„í•˜ê¸°",
    "title": "pytorchë¡œ Rnnêµ¬í˜„í•˜ê¸°",
    "section": "í•™ìŠµ ì¤€ë¹„í•˜ê¸°",
    "text": "í•™ìŠµ ì¤€ë¹„í•˜ê¸°\n\ndefine architecture,loss,optimizer\ndata check\n\n\n#architecture,loss,optimizer \ntorch.manual_seed(2022)\nrnn = torch.nn.RNN(4,20,batch_first = True)\nlinr = torch.nn.Linear(20,4)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=1e-3)\n\n\nds = torch.utils.data.TensorDataset(x_data,y_data)\ndl = torch.utils.data.DataLoader(ds,batch_size=8,drop_last=True)\n\nfor idx,(x,y) in enumerate(dl):\n    if idx ==5:\n        break\n    print(x.shape,y.shape)\n\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\n\n\nìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ë°ì´í„°ë¡œë”ë¥¼ ê±°ì³ì„œ ë‚˜ì˜¤ëŠ” í…ì„œëŠ” RNNì— ë°”ë¡œ ì…ë ¥ë  ê²ƒì„. input.shape = \\((N,L,H_{in}) = (8,3,4)\\)"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn êµ¬í˜„.html#ëª¨í˜•í•™ìŠµ",
    "href": "posts/DL/Pytorch Rnn êµ¬í˜„.html#ëª¨í˜•í•™ìŠµ",
    "title": "pytorchë¡œ Rnnêµ¬í˜„í•˜ê¸°",
    "section": "ëª¨í˜•í•™ìŠµ",
    "text": "ëª¨í˜•í•™ìŠµ\n\nfor epoch in range(0,101):\n    for tr_x,tr_y in dl:\n        #1 output\n        hidden,hT = rnn(tr_x)\n        #print(hidden.shape)\n        output = linr(hT[-1])\n        #2 loss\n        loss = loss_fn(output,tr_y)\n        #3 derivative\n        loss.backward()\n        #4 update & clean\n        optimizer.step()\n        optimizer.zero_grad()\n    if epoch % 10 == 0:\n        print(f'epoch : {epoch},loss : {round(loss.tolist(),5)}')\n\nepoch : 0,loss : 1.31779\nepoch : 10,loss : 0.69453\nepoch : 20,loss : 0.19338\nepoch : 30,loss : 0.05891\nepoch : 40,loss : 0.02861\nepoch : 50,loss : 0.01791\nepoch : 60,loss : 0.0126\nepoch : 70,loss : 0.00947\nepoch : 80,loss : 0.00744\nepoch : 90,loss : 0.00602\nepoch : 100,loss : 0.00499\n\n\npytorchì˜ rnnì„ ê±°ì³ì„œ ë‚˜ì˜¤ëŠ” outputì€ ë‘ ê°€ì§€ì„. - hidden : ê°€ì¥ ê¹Šì´ ìœ„ì¹˜í•œ íˆë“ ë ˆì´ì–´ì˜ ê°ê°ì˜ ì‹œì ì—ì„œì˜ ì¶œë ¥ê°’ì„ ëª¨ì•„ë†“ì€ í…ì„œ - hT : ëª¨ë“  íˆë“ ë ˆì´ì–´ì—ì˜ ë§ˆì§€ë§‰ ì‹œì (ì‹œì T)ì—ì„œì˜ ì¶œë ¥ê°’ì„ ëª¨ì•„ë†“ì€ í…ì„œ - ì™¸ìš°ê¸°! ìœ„ì¹˜ : ê°€ì¥ê¹Šì€ <=> ëª¨ë“  , ì‹œì  : ê°ê°ì˜ <=> ë§ˆì§€ë§‰\nìœ„ì™€ê°™ì€ ì„¤ì •ì—ì„œëŠ” ê°€ì¥ ê¹Šì´ ìœ„ì¹˜í•œ íˆë“ ë ˆì´ì–´ì˜ ë§ˆì§€ë§‰ì‹œì ì—ì„œì˜ ì¶œë ¥ê°’ë§Œì´ ìš°ë¦¬ëŠ” ë‹¤ìŒì—ì˜¬ ë¬¸ìì—´ì„ ì˜ˆì¸¡í•  ë•Œ í•„ìš”í•˜ë¯€ë¡œ hT[-1]ì„ í•˜ì—¬ ê·¸ ê°’ì„ ê°€ì ¸ì˜´."
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory.html",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory.html",
    "title": "[PRML ì½ê¸°] 1 - í™•ë¥ ë¡  ê°œìš”",
    "section": "",
    "text": "Figure1.9\n\n\në¹¨ê°•ìƒ‰,íŒŒë‘ìƒ‰ ìƒì ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê³  ì„ íƒí•œ ìƒìì•ˆì—ì„œ ì‚¬ê³¼(ì´ˆë¡) ë˜ëŠ” ì˜¤ë Œì§€(ì£¼í™©)ë¥¼ êº¼ë‚¸ë‹¤ê³  í•©ì‹œë‹¤. ì—¬ëŸ¬ë²ˆ ë°˜ë³µí–ˆì„ ë•Œ, ë¹¨ê°•ìƒ‰ ìƒìì™€ íŒŒë‘ìƒ‰ ìƒìê°€ ì„ íƒëœ ë¹„ìœ¨ì´ ê°ê° 40%,60%ë¼ê³  ì•Œë ¤ì ¸ ìˆëŠ” ìƒíƒœì…ë‹ˆë‹¤. ë¯¸ë˜ì— ì„ íƒëœ ìƒìë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ë¥¼ Bë¼ê³  í•˜ë©´ ì‹¤ì œë¡œ ìƒìë¥¼ ì„ íƒí•˜ê¸° ì „ê¹Œì§€ëŠ” ê°ê°ì˜ ìƒìë¥¼ ë½‘ì„ í™•ë¥ (ê°€ëŠ¥ì„±)ë§Œì´ ì¡´ì¬í•˜ë¯€ë¡œ ë³€ìˆ˜BëŠ” í™•ë¥ ë³€ìˆ˜ ì…ë‹ˆë‹¤. ì´ í™•ë¥ ë³€ìˆ˜ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ì€ r ë˜ëŠ” bë¡œ ë‘ ê°€ì§€ ì…ë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ ë¯¸ë˜ì— ì„ íƒëœ ê³¼ì¼ì„ ë‚˜íƒ€ë‚´ëŠ” í™•ë¥ ë³€ìˆ˜ Fë¥¼ ë†“ì„ ìˆ˜ ìˆê³  Fê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ì„ a ë˜ëŠ” oë¡œ ë†“ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në¹ˆë„ì£¼ì˜ ê´€ì ì—ì„œ ì–´ë– í•œ ì‚¬ê±´ì´ ë°œìƒí•  í™•ë¥ ì€ ë§¤ìš° ì—¬ëŸ¬ë²ˆ ì‹œí–‰ì„ ë°˜ë³µí–ˆì„ë•Œ, ì–´ë–¤ ì‚¬ê±´ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ì˜ ë¹„ìœ¨(fraction,ratio)ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì£¼ì‚¬ìœ„ ëˆˆì´ 3ì´ë‚˜ì˜¬ í™•ë¥ ì´ 50%ë¼ê³  í•˜ë©´ 100ë²ˆ ë˜ì¡Œì„ë•Œ 50ë²ˆì •ë„ëŠ” 3ì´ ë‚˜ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ì˜ ë¬¸ì œì—ì„œ ì—¬ëŸ¬ë²ˆ ë°˜ë³µí–ˆì„ë•Œ ë¹¨ê°•ìƒ‰ ë˜ëŠ” íŒŒë‘ìƒ‰ìƒìì¸ ì‚¬ê±´ì´ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ê°ê° ì „ì²´ì—ì„œ 40%,60%ì˜€ë‹¤ê³  í–ˆìœ¼ë¯€ë¡œ ì´ëŠ” í™•ë¥ ì…ë‹ˆë‹¤. ë˜í•œ í™•ë¥ ë³€ìˆ˜ Bê°€ rì„ ì·¨í•˜ëŠ” ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ê³¼ í™•ë¥ ë³€ìˆ˜ Bê°€ bë¥¼ ì·¨í•˜ëŠ” ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ì´ë¼ê³  ë§í•˜ë©° ë‹¤ìŒê³¼ ê°™ì´ ì ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\np(B = r) = 0.4 \\\\\np(B = b) = 0.6\n\\end{aligned}\\]\nê°ê°ì˜ ìƒìë¥¼ ì„ íƒí•˜ëŠ” ì‚¬ê±´ì˜ í™•ë¥ ì€ í™•ë¥ ì˜ ì •ì˜ì— ì˜í•´ì„œ [0,1]ì‚¬ì´ì˜ êµ¬ê°„ì—ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤. ë˜í•œ ê°ê°ì˜ ìƒìë¥¼ ì„ íƒí•˜ëŠ” ì‚¬ê±´ì€ ìƒí˜¸ë² íƒ€ì ì´ë©´ì„œ ì‹œí–‰ìœ¼ë¡œë¶€í„° ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë“  ê²°ê³¼ë“¤ ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ í™•ë¥ ì˜ í•©ì€ 1ì…ë‹ˆë‹¤.\nëª‡ ê°€ì§€ ê¶ê¸ˆí•œ ì ì´ ìƒê²¼ìŠµë‹ˆë‹¤. â€œì‚¬ê³¼(ì´ˆë¡)ì´ë‚˜ ì˜¤ë Œì§€(ì£¼í™©)ê°€ ë‚˜ì˜¬ í™•ë¥ ì€?â€ ë˜ëŠ” â€œì‚¬ê³¼ë¥¼ ë½‘ì•˜ì„ ë•Œ ì–´ë–¤ ìƒìë¥¼ ì„ íƒí•  ê°€ëŠ¥ì„±ì´ ë†’ì€ì§€?â€ì— ëŒ€í•´ì„œ ê¶ê¸ˆí•©ë‹ˆë‹¤. ì´ëŠ” sum ruleê³¼ product ruleì„ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.\n\n\n\n\n\n\nFigure1.9\n\n\ní™•ë¥ ë³€ìˆ˜ \\(X,Y\\)ê°€ ì¡´ì¬í•˜ê³  \\(X\\)ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ì€ \\(x_i(i=1,2,\\dots,M)\\) \\(Y\\)ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ì€ \\(y_j(j=1,2,\\dots,L)\\)ë¼ê³  í•©ì‹œë‹¤. ì´ \\(N\\)ë²ˆì„ ì‹œí–‰í–ˆë‹¤ê³  í•  ë•Œ,ì‹œí–‰ì˜ ê²°ê³¼ ì¤‘ \\(X = x_i\\)ì´ë©´ì„œ ë™ì‹œì— \\(Y=y_i\\)ì¸ ê²½ìš°ëŠ” \\(n_{ij}\\)ë²ˆ ë‚˜ì™”ìœ¼ë©° \\(X = x_i\\)ì¸ ê²½ìš°ëŠ” \\(c_i\\)ë²ˆ ë‚˜ì™”ê³  í™•ë¥ ë³€ìˆ˜ \\(Y = y_j\\)ì¸ ê²½ìš°ëŠ” \\(r_j\\)ë²ˆì´ ë‚˜ì™”ìŠµë‹ˆë‹¤.\ní™•ë¥ ë³€ìˆ˜ \\(X = x_i\\)ì´ê³  \\(Y=y_j\\)ì¸ ì‚¬ê±´ì´ ë™ì‹œì— ë°œìƒí•  í™•ë¥ ì„ \\(X=x_i,Y=y_j\\)ì¼ ë•Œì˜ ê²°í•©í™•ë¥ (joint probability)ë¼ê³  í•©ë‹ˆë‹¤.\n\\[p(X = x_i,Y = y_j)\\]\nê²°í•©í™•ë¥ ì€ ë§¤ìš° ì—¬ëŸ¬ë²ˆ ì‹œí–‰í–ˆì„ ë•Œ, \\(X=x_i,Y=y_j\\)ì¸ ì‚¬ê±´ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ì˜ \\(n_{ij}\\) ë¹„ìœ¨ì…ë‹ˆë‹¤.\n\\[N \\rightarrow \\infty,\\,\\, p(X=x_i,Y=y_j) = \\frac{n_{ij}}{N}\\]\nì‹œí–‰ìœ¼ë¡œë¶€í„° í™•ë¥ ë³€ìˆ˜ \\(X = x_i\\)ì¸ ì‚¬ê±´ì´ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ì•Œê¸°ìœ„í•´ì„œëŠ” \\(c_i\\)ëŠ” \\(X = x_i,Y = y_j(\\text{for } j=1,2,\\dots,L)\\)ì¸ ì‚¬ê±´ì´ ë°œìƒí•˜ëŠ” ëª¨ë“  ê²½ìš°ë¥¼ ì „ë¶€ ë‹¤ ë”í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ ì‚¬ê³¼ë¥¼ ì„ íƒí•˜ëŠ” \\(F = a\\)ì´ ê²½ìš°ëŠ” ì‚¬ê³¼ë¥¼ ì„ íƒí•˜ê³  ìƒìê°€ íŒŒë‘ìƒ‰ ìƒìì¸ \\(F=a,B=r\\) ì‚¬ê±´ì´ ë°œìƒí•œ ê²½ìš°ì™€ ì‚¬ê³¼ë¥¼ ì„ íƒí•˜ê³  ìƒìê°€ ë¹¨ê°•ìƒ‰ ìƒìì¸\\(F=a,B=b\\) ì‚¬ê±´ì´ ë°œìƒí•œ ê²½ìš°ì´ë¯€ë¡œ ì‹œí–‰ìœ¼ë¡œë¶€í„° ë‘ ê°€ì§€ ì¼€ì´ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ê²½ìš°ë¥¼ ëª¨ë‘ ì„¸ì–´ì•¼ í•©ë‹ˆë‹¤.\n\\[c_i = \\sum_{j=1}^{L}n_{ij}\\]\nê²°ê³¼ì ìœ¼ë¡œ ,\\(X = x_i\\)ì¸ ì‚¬ê±´ì˜ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\np(X = x_i) &= \\frac{c_{i}}{N} \\\\\n&=\\frac{\\sum_{j=1}^{L}n_{ij}}{N}\\\\\n&= p(X=x_i,Y=y_1) + p(X=x_i,Y=y_2) + \\dots + p(X=x_i,Y=y_L) \\\\\n&= \\sum_{j=1}^{L}p(X=x_i,Y=y_j) \\\\\n\\end{aligned}\\]\nìœ„ì™€ ê°™ì´ í•˜ë‚˜ì˜ í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•œ í™•ë¥ ì„ êµ¬í•  ë•Œ, ë‹¤ë¥¸ í™•ë¥ ë³€ìˆ˜ì™€ì˜ ëª¨ë“  ê²°í•©í™•ë¥ ì„ ë”í•˜ì—¬ êµ¬í•˜ëŠ” ë²•ì¹™ì„ sum rule of probabilityë¼ê³  í•©ë‹ˆë‹¤. ì´ë•Œ ë‹¤ë¥¸ í™•ë¥ ë³€ìˆ˜ì™€ ê²°í•©í™•ë¥ ì„ marginalizing ë˜ëŠ” summing outí•˜ì—¬ êµ¬í•˜ë¯€ë¡œ marginal probabilityë¼ê³  í•©ë‹ˆë‹¤.\nì‹œí–‰ì˜ ê²°ê³¼ê°€ \\(X=x_i\\)ì¸ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°ì— ëŒ€í•´ì„œë§Œ ê³ ë ¤í•´ë³¸ë‹¤ê³  í•©ì‹œë‹¤. ì¡°ê±´ì— ë§ëŠ” ê²½ìš°ì•ˆì—ì„œ \\(Y = y_j\\)ì¸ ì‚¬ê±´ì´ ë‚˜ì˜¤ëŠ” íšŸìˆ˜ì˜ ë¹„ëŠ” \\(p(Y=y_j|X=x_i)\\)ë¼ê³  í‘œê¸°í•˜ë©° ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[p(Y=y_j|X=x_i) = \\frac{n_{ij}}{c_j}\\]\nì´ë¥¼ \\(X=x_i\\)ë¡œ ì£¼ì–´ì¡Œì„ ë•Œ, \\(Y=y_j\\)ì¸ ì‚¬ê±´ì— ëŒ€í•œ ì¡°ê±´ë¶€í™•ë¥ ì´ë¼ê³  í•©ë‹ˆë‹¤. ì¡°ê±´ë¶€ í™•ë¥ ì˜ ê²½ìš° ê¸°í˜¸\\(|\\) ë‹¤ìŒì— ì¡°ê±´ì´ ì˜¤ë©° ë¶„ëª¨ì—ëŠ” ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì‚¬ê±´ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ê°€ ëª‡ ë²ˆì¸ì§€ ê·¸ íšŸìˆ˜ì— ê°’ì´ ì˜¤ì§€ë§Œ ì¡°ê±´ë¶€í™•ë¥ ì´ ì•„ë‹Œ ê·¸ëƒ¥ í™•ë¥ ì˜ ê²½ìš° ë¶„ëª¨ëŠ” ëª‡ ë²ˆ ì‹œí–‰í–ˆëŠ”ì§€ ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ëŠ” ì¡°ê±´ë¶€í™•ë¥ ì€ ì¼ë°˜ì ì¸ í™•ë¥ ê³¼ ë‹¤ë¥´ê²Œ ì–´ë–¤ íŠ¹ì •í•œ ì¡°ê±´ì•ˆì—ì„œ ë‹¤ë¥¸ì‚¬ê±´ì´ ë‚˜ì˜¤ëŠ” ë¹„ìœ¨ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\nìœ„ì—ì„œ ì •ì˜í•œ ì¡°ê±´ë¶€í™•ë¥ ë¡œ ê²°í•©í™•ë¥ ì„ ë‹¤ì‹œ ì ì–´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[p(X=x_i,Y=y_j) = \\frac{n_{ij}}{N} = \\frac{n_{ij}}{c_i}\\frac{c_i}{N} = p(Y=y_j|X=x_i)p(X = x_i)\\]\nì¦‰ ê²°í•©í™•ë¥ ì€ \\(X=x_i\\)ì¸ ì‚¬ê±´ì´ ë°œìƒí•œ í™•ë¥ ê³¼ ë°œìƒí•œ ì‚¬ê±´ì„ ì¡°ê±´\\(X=x_i\\)ìœ¼ë¡œí•˜ê³  \\(Y=y_j\\)ê°€ ë°œìƒí•  í™•ë¥ ì˜ ê³±ê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ëŠ” ì–´ëŠì •ë„ ì§ê´€ê³¼ ì¼ì¹˜í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆëŠ”ë° ì˜ˆë¥¼ ë“¤ìë©´ ë¹¨ê°•ìƒ‰ìƒìì—ì„œ ì‚¬ê³¼ë¥¼ ë½‘ì„ ê°€ëŠ¥ì„±ì€ ë¨¼ì € ë¹¨ê°•ìƒ‰ìƒìë¥¼ ê³ ë¥´ê³  ê·¸ ë‹¤ìŒ ì‚¬ê³¼ë¥¼ ë½‘ì„ ê°€ëŠ¥ì„±ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\n\n\nìœ„ì—ì„œ ì ì€ ê²°í•©í™•ë¥ ë¡œë¶€í„° ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ì–´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)}\\]\nì´ë¥¼ ë² ì´ì¦ˆì •ë¦¬ ë¼ê³  í•©ë‹ˆë‹¤. ë² ì´ì¦ˆ ì •ë¦¬ì—ì„œ \\(p(Y|X)\\)ëŠ” posterior probability(ì‚¬í›„í™•ë¥ )ë¡œ ì–´ë–¤ ì¡°ê±´ ë˜ëŠ” ì¦ê±°ê°€ ë°œê²¬ë˜ì—ˆì„ë•Œì˜ í™•ë¥ ì…ë‹ˆë‹¤. \\(p(Y)\\)ëŠ” prior probabilityë¡œ ì–´ë–¤ ì¦ê±° ë˜ëŠ” ì¡°ê±´ì´ ë°œê²¬ë˜ê¸°ì „ì˜ í™•ë¥ ì…ë‹ˆë‹¤. ë² ì´ì¦ˆ ì •ë¦¬ë¡œë¶€í„° ìš°ë¦¬ëŠ” posteriorì™€ priorì˜ ê´€ê³„ ì¦‰,ì¡°ê±´ ë˜ëŠ” ì¦ê±°ê°€ ë°œê²¬ë˜ê¸° ì „,í›„ì˜ í™•ë¥ ì‚¬ì´ì˜ ìˆ˜ì‹ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì‚¬ì „í™•ë¥ ì„ ì•Œê³  ìˆë‹¤ë©´ ê·¸ í™•ë¥ ì„ í†µí•˜ì—¬ ì‚¬í›„í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)} = \\frac{p(X|Y)p(Y)}{\\sum_{Y}p(X,Y)} = \\frac{p(X|Y)p(Y)}{\\sum_{Y}p(X|Y)p(Y)}\\]\në¶„ëª¨ë¥¼ sum ruleê³¼ product ruleì— ì˜í•˜ì—¬ ë” ì „ê°œí•˜ë©´ ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤. ì‚¬í›„í™•ë¥  \\(p(Y|X)\\)ëŠ” \\(X\\)ë¼ëŠ” ì¦ê±°,ì¡°ê±´ì´ ì£¼ì–´ì§ˆ ë•Œ ê²°ê³¼ \\(Y\\)ì— ëŒ€í•œ í™•ë¥ ì´ì—ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ìˆ˜ì‹ì„ ê³°ê³°íˆ ë³´ë©´ â€¦ \\(X\\)ê°€ ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§ˆë•Œ ê²°ê³¼\\(Y\\)ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì„ êµ¬í•˜ê¸° ìœ„í•˜ì—¬ \\(Y\\)ê°€ ì£¼ì–´ì§ˆë•Œì˜ \\(X\\)ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë² ì´ì¦ˆ ì •ë¦¬ì—ì„œ í•µì‹¬ì€ ì–´ë–¤ ì¡°ê±´ì´ ì£¼ì–´ì§€ê³  ê²°ê³¼ì— ëŒ€í•œ í™•ë¥ ì„ êµ¬í•  ë•Œ, ê²°ê³¼ë¥¼ ì¡°ê±´ìœ¼ë¡œ ì¡°ê±´ì„ ê²°ê³¼ë¡œ ì—­ìœ¼ë¡œ ë°”ê¾¼ í™•ë¥ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì¦‰ \\(X\\)ê°€ ì¡°ê±´ì¼ ë•Œ, \\(Y\\)ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì´ ì˜ êµ¬í•´ì§€ì§€ ì•ŠëŠ”ë‹¤ë©´ ì´ë¥¼ ë’¤ì§‘ì–´ì„œ \\(Y\\)ê°€ ì¡°ê±´ì´ê³  \\(X\\)ê°€ ê²°ê³¼ì¼ë•Œì˜ í™•ë¥ ì„ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në² ì´ì¦ˆ ì •ë¦¬ì—ì„œ ë¶„ëª¨ëŠ” normalization ìƒìˆ˜ë¡œ ëª¨ë“  \\(Y\\)ì— ëŒ€í•˜ì—¬ í™•ë¥ ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ í•©ë‹ˆë‹¤.\nì˜ˆì‹œë¡œ ëŒì•„ê°€ì„œ ì˜¤ë Œì§€ ë˜ëŠ” ì‚¬ê³¼ê°€ ë‚˜ì˜¬ í™•ë¥ ê³¼ ì‚¬ê³¼ë¥¼ ê³¨ëì„ ë•Œ ì–´ë–¤ ìƒìë¥¼ ê³¨ëì„ í™•ë¥ ì´ ë†’ì€ì§€ë¥¼ ê³„ì‚°í•´ë´…ì‹œë‹¤. ê°ê°ì˜ ê²½ìš°ì— ëŒ€í•´ í™•ë¥ ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{align}\n&p(B = r) = 0.4 \\\\\n&p(B = b) = 0.6 \\\\\n&p(F = a | B = r) = \\frac{1}{4}\\\\\n&p(F = o | B = r) = \\frac{3}{4}\\\\\n&p(F = a | B = b) = \\frac{3}{4}\\\\\n&p(F = o | B = b) = \\frac{1}{4}\\\\\n\\end{align}\\]\n(3)(4),(5)(6) ê°ê°ì˜ í•©ì€ normalization constantë¡œ ì¸í•´ í•©ì´ 1ì´ ë˜ëŠ”ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ì–´ì„œ ì›ë˜ ê¶ê¸ˆí–ˆë˜ ì²«ë²ˆì§¸ ë¬¸ì œì¸ ì‚¬ê³¼ ë˜ëŠ” ì˜¤ë Œì§€ê°€ ë‚˜ì˜¬ í™•ë¥ ì„ Product ruleë¡œ ê³„ì‚°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\np(F = a) &= p(F=a,B=r) + p(F=a,B=b) \\\\\n&=p(F=a|B=r)p(B=r) + p(F=a|B=b)p(B=b) \\\\\n&=\\frac{1}{4}\\times\\frac{4}{10} + \\frac{3}{4}\\times\\frac{6}{10}\\\\\n&= \\frac{11}{20}\\\\\n\\end{aligned}\n\\\\\n&p(F=b) = 1-\\frac{11}{20}= \\frac{9}{20}\n\\end{aligned}\\]\në˜ë‹¤ë¥¸ ë¬¸ì œì¸ ì˜¤ë Œì§€ ë˜ëŠ” ì‚¬ê³¼ë¥¼ ë½‘ì•˜ì„ë•Œ ì–´ë–¤ ë°•ìŠ¤ë¥¼ ì„ íƒí–ˆëŠ”ì§€ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤. ì¦‰ ì•Œê³ ì‹¶ì€ í™•ë¥ ì€ \\(p(B|F=a)\\) ë˜ëŠ” \\(p(B|F=o)\\)ì…ë‹ˆë‹¤. ê·¸ëŸ°ë° ìš°ë¦¬ì—ê²Œ ì£¼ì–´ì§„ í™•ë¥ ë“¤ì€ ì‚¬ì „í™•ë¥ ì¸ \\(p(F)\\)ì™€ ì¡°ê±´ê³¼ ê²°ê³¼ê°€ ì—­ìœ¼ë¡œ ë’¤ì§‘íŒ í™•ë¥ ì¸ \\(p(F|B)\\)ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ,Bayes Ruleì„ ì‚¬ìš©í•˜ì—¬ ì›í•˜ëŠ” í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\\begin{align}\n&p(B|F) = \\frac{p(F|B)p(F)}{p(B)} \\\\\n&p(B=r|F=o) = \\frac{p(F=o|B=r)p(B=r)}{p(F=o)} = \\frac{2}{3}\\\\\n&\\leftrightarrow p(B=b|F=o) = 1-\\frac{2}{3} = \\frac{1}{3}\n\\end{align}\\]\nì˜¤ë Œì§€ë¥¼ í™•ì¸í•˜ê¸°ì „ê¹Œì§€ëŠ” ë¹¨ê°•ìƒ‰ ë°•ìŠ¤ì¼ í™•ë¥ ì´ ì ˆë°˜ì´ ì•ˆë˜ëŠ” 0.4 ì˜€ëŠ”ë° ì˜¤ë Œì§€ë¥¼ í™•ì¸í•˜ê³ ëŠ” \\(\\frac{2}{3}\\)ë¡œ í™•ë¥ ì´ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì£¼ì–´ì§€ëŠ” ì •ë³´,ì£¼ê±´ì´ í™•ë¥ ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ êµ¬í•œ í™•ë¥ ì´ ì§ê´€ì ìœ¼ë¡œ ê·¸ë¦¼ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ê³¼ë„ ì¼ì¹˜í•¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\në‘ í™•ë¥ ë³€ìˆ˜ê°€ ë…ë¦½ì´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\text{X and Y are independent random variables} \\iff p(X,Y) = p(X)p(Y)\\]\nì˜ˆì‹œì—ì„œ í™•ì¸í•´ë´…ì‹œë‹¤.\n\\[\\begin{aligned}\n&p(B=r|F=o) = \\frac{2}{3} ,p(B=r) = \\frac{4}{10}\\\\\n&p(B=r|F=o) \\not = p(B=r) \\Longleftrightarrow \\text{X and Y are dependent}\n\\end{aligned}\\]\në§Œì•½ ë‘ ë°•ìŠ¤ì•ˆì— ë“¤ì–´ìˆëŠ” ì˜¤ëœì§€ì™€ ì‚¬ê³¼ê°€ ê°™ì€ ë¹„ìœ¨ë¡œ ë“¤ì–´ìˆë‹¤ê³  í•œë‹¤ë©´â€¦\n\\[\\begin{aligned}\np(F=o | B=r) = p(F = o) \\\\\np(F=a | B=r) = p(F = a)  \\\\\np(F=o | B=b) = p(F = o)  \\\\\np(F=a | B=b) = p(F = a)\n\\end{aligned}\\]\në”°ë¼ì„œ ìƒìì•ˆì— ìˆëŠ” ê³¼ì¼ì˜ ê°¯ìˆ˜ê°€ ê°™ì„ ê²½ìš°, ë‘ í™•ë¥ ë³€ìˆ˜ëŠ” ë…ë¦½ì…ë‹ˆë‹¤.\n\n\n\n\në¹ˆë„ì£¼ì˜ ê´€ì ì—ì„œ í™•ë¥ ì€ ë§¤ìš°ì—¬ëŸ¬ë²ˆ ì‹œí–‰í–ˆì„ ë•Œ, ì–´ë–¤ ì‚¬ê±´ì´ ë°œìƒí•˜ëŠ”(ë‚˜ì˜¤ëŠ”) ë¹„ìœ¨(ratio)ì…ë‹ˆë‹¤.\nSum ruleì€ ì—¬ëŸ¬í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•œ ê²°í•©í™•ë¥ ì´ ì£¼ì–´ì§ˆ ë•Œ, íŠ¹ì •í•œ í•˜ë‚˜ì˜ ë³€ìˆ˜ì— ëŒ€í•œ í™•ë¥ (marginal probability)ì„ êµ¬í•¨\n\n\\[p(X) = \\sum_Yp(X,Y)\\]\n\nProduct ruleì€ ì¡°ê±´ë¶€í™•ë¥ ê³¼ ì£¼ë³€í™•ë¥ ì‚¬ì´ì˜ ê³±ìœ¼ë¡œ ê²°í•©í™•ë¥ ì„ êµ¬í•¨\n\n\\[p(X,Y) = p(Y|X)p(X) = p(X|Y)p(Y)\\]\n\në² ì´ì¦ˆì •ë¦¬ëŠ” posterior(ì‚¬í›„í™•ë¥ )ë¥¼ prior(ì‚¬ì „í™•ë¥ )ê³¼ ì¡°ê±´ê³¼ ê²°ê³¼ê°€ ë°”ë€Œì—ˆì„ë•Œì˜ í™•ë¥ ì„ í†µí•´ì„œ êµ¬í•©ë‹ˆë‹¤. ë˜í•œ ì‚¬ì „í™•ë¥ ê³¼ ì‚¬í›„í™•ë¥ ì‚¬ì´ì˜ ê´€ê³„(ìˆ˜ì‹)ì…ë‹ˆë‹¤.\n\n\\[p(X|Y) = \\frac{p(Y|X)p(Y)}{p(X)} = \\frac{p(Y|X)p(Y)}{\\sum_Yp(X,Y)p(Y)}\\]\n\në‘ í™•ë¥ ë³€ìˆ˜ê°€ ë…ë¦½ì¼ ê²½ìš° , ê²°í•©í™•ë¥ (ë¶„í¬)ëŠ” ê°ê°ì˜ ë³€ìˆ˜ì— ëŒ€í•œ (ì£¼ë³€)í™•ë¥ ì˜ ê³±ì…ë‹ˆë‹¤\n\n\\[\\text{X and Y are independent random variables} \\iff p(X,Y) = p(X)p(Y)\\]\n\n\n\n\n\ní™•ë¥ ë³€ìˆ˜ê°€ ë…ë¦½ì´ë¼ë©´ \\(Y\\)ì˜ ì¡°ê±´ë¶€í™•ë¥ ì€ ì¡°ê±´\\(X\\)ê°€ ì–´ë–¤ ê°’ì´ë˜ê°„ì— ì „í˜€ ì˜í–¥ì´ ì—†ìŠµë‹ˆë‹¤(ë…ë¦½ì ì…ë‹ˆë‹¤).\n\\[\\begin{aligned}\n&p(Y|X) = p(Y) \\\\\n&\\leftrightarrow p(Y|X) = \\frac{p(X,Y)}{p(X)} = p(Y)\\\\\n&\\leftrightarrow p(X,Y) = p(X)p(Y)\n\\end{aligned}\\]\në°˜ëŒ€ë¡œ í•´ë„ ì„±ë¦½í•©ë‹ˆë‹¤.\n\n\n\n\nê°€ì¥ ì‰½ê²Œ ì´í•´í•˜ëŠ” ë² ì´ì¦ˆ ì •ë¦¬(Bayesâ€™ Law)\nIndependent Random Variables\nPRML 1.2-probability theory"
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html",
    "title": "[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)",
    "section": "",
    "text": "PRMLì„ ì½ê³  ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#probability-density-function",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#probability-density-function",
    "title": "[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)",
    "section": "Probability density function",
    "text": "Probability density function\ní™•ë¥ ë°€ë„í•¨ìˆ˜ëŠ” ì—°ì†í™•ë¥ ë³€ìˆ˜ê°€ ë¯¸ì†Œêµ¬ê°„ì•ˆì— ì†í•˜ëŠ” ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ì„ ë¯¸ì†Œêµ¬ê°„ì˜ ê¸¸ì´ë¡œ ë‚˜ëˆˆ í™•ë¥ ë°€ë„ê°’ì„ í•¨ìˆ«ê°’ìœ¼ë¡œ ê°€ì§€ëŠ” í™•ë¥ í•¨ìˆ˜ë¡œ ì •ì˜í•©ë‹ˆë‹¤.\n\\[p(x) \\overset{\\Delta}{=} \\lim_{\\Delta x \\rightarrow 0}\\frac{p(x<X\\leq x+\\Delta x))}{\\Delta x}\\]\ní™•ë¥ ë°€ë„í•¨ìˆ˜ë¥¼ ì •ì ë¶„í•˜ë©´ í™•ë¥ ë³€ìˆ˜ê°€ ì„ì˜ì˜ êµ¬ê°„ì•ˆì— ì†í•˜ëŠ” ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ì¦ëª…)\n\\[\\begin{aligned}\nP(a < X \\leq b) = \\int_{a}^{b}p(u)d(u)\n\\end{aligned}\\]\ní™•ë¥ ë°€ë„í•¨ìˆ˜ëŠ” ë‹¤ìŒì˜ ë‘ ê°€ì§€ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤. ì²«ë²ˆì§¸ ì‹ì€ í™•ë¥ (ë°€ë„)ëŠ” ë°˜ë“œì‹œ 0ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‘ë²ˆì§¸ ì‹ì—ì„œ í™•ë¥ ë³€ìˆ˜ëŠ” ë°˜ë“œì‹œ \\((-\\infty,\\infty]\\)ì¸ êµ¬ê°„ì•ˆì— ì†í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n\\[\\begin{align}\np(x) \\geq 0 \\\\\n\\int_{-\\infty}^{\\infty}f(t)dt = 1\n\\end{align}\\]"
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#probabiltiy-variable-transform",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#probabiltiy-variable-transform",
    "title": "[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)",
    "section": "Probabiltiy variable transform",
    "text": "Probabiltiy variable transform\nì´ ë¶€ë¶„ì˜ ë‚´ìš©ì€ PRMLì— ìˆëŠ” ë‚´ìš©ì„ ê°ìƒ‰í•œ ë¶€ë¶„ì…ë‹ˆë‹¤. í‹€ë¦°ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”!!\nì—°ì†í™•ë¥ ë³€ìˆ˜ \\(X\\)ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ë¥¼ \\(p_X(x)\\)ë¼ í•  ë•Œ, ë³€ìˆ˜ë¥¼ ë³€í™˜í•˜ì—¬ \\(X\\)ë¥¼ \\(Y\\)ì— ê´€í•œ ì‹\\(X = g(Y)\\)ë¡œ í‘œí˜„í–ˆë‹¤ê³  í•´ë´…ì‹œë‹¤. ëª©ì ì€ í™•ë¥ ë³€ìˆ˜ \\(Y\\)ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ \\(p_Y(y)\\)ë¥¼ ì–»ëŠ” ê²ƒì…ë‹ˆë‹¤. \\(\\Delta x\\rightarrow 0 \\Delta y \\rightarrow 0\\)ì´ë¼ê³  í•œë‹¤ë©´ ë‹¤ìŒì´ ì„±ë¦½í•©ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\lim_{\\Delta x \\rightarrow 0}\\frac{p(x < X \\leq x + \\Delta x)}{\\Delta x} \\times \\Delta x \\overset{\\sim}{=} \\lim_{\\Delta y \\rightarrow 0}\\frac{p(y < Y \\leq y + \\Delta y)}{\\Delta y} \\times \\Delta y \\\\\n&\\Longleftrightarrow p_X(x)dx \\overset{\\sim}{=} p_Y(y)dy\n\\end{aligned}\\]\nìœ—ì‹ì€ Jacobian factorì— ì˜í•´ ë“±ì‹ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\np_Y(y) &= p_X(x) \\begin {vmatrix} \\frac{dx}{dy} \\end {vmatrix} \\\\\n&= p_X(g(y))|g^{'}(y)|\n\\end{aligned}\\]\ní™•ë¥ ë³€ìˆ˜ì˜ ë³€í™˜ì€ í™•ë¥ ë¶„í¬í•¨ìˆ˜ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ë¬¸ì œì—ì„œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. ë³€í™˜í•  ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ë©´ ìµœëŒ€í™”í•´ì•¼í•˜ëŠ” í™•ë¥ í•¨ìˆ˜ë¥¼ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#sum-rule-product-rule-of-continuous-variable",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#sum-rule-product-rule-of-continuous-variable",
    "title": "[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)",
    "section": "Sum rule & Product rule of continuous variable",
    "text": "Sum rule & Product rule of continuous variable\nì´ì‚°í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•´ì„œëŠ” Sum ruleê³¼ Product ruleì„ ì‚´í´ë´¤ì—ˆì§€ë§Œ ì—°ì†í™•ë¥ ë³€ìˆ˜ ëŒ€í•´ì„œëŠ” ë³´ì§€ ì•Šì•˜ì—ˆìŠµë‹ˆë‹¤. ì—°ì†í™•ë¥ ë³€ìˆ˜ì˜ ê²½ìš° ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì—„ë°€í•œ ì¦ëª…ì€ measure theroyë¡œ ì¦ëª…í•´ì•¼ í•˜ë¯€ë¡œ .. ìƒëµí•˜ê² ìŠµë‹ˆë‹¤.(ê°„ëµí•œ ì¦ëª…)\n\\[\\begin{aligned}\n&p(x) = \\int_y f(x,y)dy \\\\\n&p(x,y) = p(y|x)p(x)\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#expectations-and-variances",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#expectations-and-variances",
    "title": "[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)",
    "section": "Expectations and Variances",
    "text": "Expectations and Variances\ní•¨ìˆ˜ì˜ ê¸°ëŒ“ê°’(ë˜ëŠ” í‰ê· )ì€ í•¨ìˆ«ê°’ì´ ì–´ë–¤ ê°’ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„í¬í•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤. ê°€ëŠ¥í•œ ëª¨ë“ \\(x\\)ì— ëŒ€í•˜ì—¬ í•¨ìˆ«ê°’ê³¼ ê·¸ë•Œì˜ í™•ë¥ ë¶„í¬ì˜ ê°’ì„ ê³±í•˜ì—¬ ì–»ì€ ê°€ì¤‘í‰ê· ì…ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mathbb{E}[f] = \\sum_x p(x)f(x) \\quad \\text{If X is a discrete R.V} \\\\\n&\\mathbb{E}[f] = \\int_x p(x)f(x)dx \\quad \\text{If X is a continuous R.V}\n\\end{aligned}\\]\ní‘œë³¸ì˜ í¬ê¸°ê°€ ë¬´í•œí•  ê²½ìš°, í‘œë³¸ìœ¼ë¡œ ë¶€í„° êµ¬í•œ í•¨ìˆ«ê°’ì˜ í‰ê· ê³¼ ê¸°ëŒ“ê°’ì€ ê°’ì´ ê°™ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ì„œ í™•ë¥ ë¶„í¬ì˜ ê¸°ëŒ“ê°’ì„ ì•Œ ìˆ˜ ìˆë‹¤ë©´ í‘œë³¸ì´ ì ë‹¹íˆ í¬ê¸°ê°€ í´ ê²½ìš° í•¨ìˆ«ê°’ì´ ì–´ëŠì •ë„ ì¼ì§€ ëŒ€ëµì ìœ¼ë¡œ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\\mathbb{E}[f] = \\lim_{N \\rightarrow \\infty}\\frac{1}{N}\\sum_{n=1}^{N}f(x_n)\\]\në‹¤ë³€ìˆ˜í•¨ìˆ˜ëŠ” ì—¬ëŸ¬ê°œì˜ ë³€ìˆ˜ë¥¼ ê°€ì§€ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ê°ê°ì˜ ë³€ìˆ˜ê°€ ë”°ë¥´ëŠ” í™•ë¥ ë¶„í¬ì¤‘ì—ì„œ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ê·¸ë•Œì˜ í™•ë¥ ë¶„í¬ì™€ í•¨ìˆ«ê°’ì˜ ê¸°ëŒ“ê°’ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ ê¸°ëŒ“ê°’ì€ ë‚˜ë¨¸ì§€ í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•œ í•¨ìˆ˜ê°€ ë©ë‹ˆë‹¤.\n\\[\\mathbb{E}_x[f(x,y)] = f(y)\\]\ní•¨ìˆ˜ì˜ ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì€ ì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬ì™€ì˜ ê°€ì¤‘í‰ê· ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \\(y\\)ê°€ ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§ˆ ë•Œ, \\(x\\)ì˜ ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\mathbb{E}_x[f|y] = \\sum_x{p(x|y)}{f(x)}\\]\ní™•ë¥ ë³€ìˆ˜ \\(f(x)\\)ì˜ ë¶„ì‚°(variance)ëŠ” í•¨ìˆ˜ê°€ ê¸°ëŒ“ê°’ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì–¼ë§ˆë‚˜ í¼ì ¸ìˆëŠ”ì§€ ì•Œë ¤ì¤ë‹ˆë‹¤. í¸ì°¨ì œê³±ì˜ ê¸°ëŒ“ê°’(í‰ê· )ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mathbb{E}[x] = \\int_xxp(x)dx \\text{ or } \\sum_xxp(x)\\\\\n&\n\\begin{aligned}\n\\text{var}[x]\n&= \\mathbb{E}[(x - \\mathbb{E}[x])^2] \\\\\n&= \\mathbb{E}[x^2] - \\mathbb{E}[x]^2\\\\\n\\end{aligned}\n\\end{aligned}\\]\në‘ ê°œì˜ í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•´ì„œ ê³µë¶„ì‚°ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.(2ë²ˆì§¸ ì‹ì— ëŒ€í•œ ì „ê°œ)\n\\[\\begin{align}\n\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}] \\\\\n&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}[x]\\mathbb{E}[y]\n\\end{align}\\]"
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#appendix",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#appendix",
    "title": "[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)",
    "section": "Appendix",
    "text": "Appendix\n\ní™•ë¥ ë°€ë„í•¨ìˆ˜ì— ê´€í•œ ì—¬ëŸ¬ê°€ì§€ ì¦ëª…\nëˆ„ì ë¶„í¬í•¨ìˆ˜ëŠ” ì—°ì†í™•ë¥ ë³€ìˆ˜ê°€ \\((-\\infty,x]\\)ì¸ êµ¬ê°„ì•ˆì— ì†í•  í™•ë¥ ì…ë‹ˆë‹¤.\n\\[F(x) = P(-\\infty<X\\leq x)\\]\në”°ë¼ì„œ,ì—°ì†í™•ë¥ ë¶„í¬ì˜ ë¶„ìë¥¼ ëˆ„ì ë¶„í¬í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëˆ„ì ë¶„í¬í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ê°€ í™•ë¥ ë°€ë„í•¨ìˆ˜ì´ë©° ëˆ„ì ë¶„í¬í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°,ë³€í™”ìœ¨ì´ í™•ë¥ ë°€ë„í•¨ìˆ˜ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n\\[p(x) = \\lim_{\\Delta x \\rightarrow 0}\\frac{p(x<X\\leq x+\\Delta x))}{\\Delta x} = \\lim_{\\Delta x \\rightarrow 0}\\frac{F(x+\\Delta x) - F(x)}{\\Delta x} = \\frac{dF}{dx}\\]\nëˆ„ì ë¶„í¬í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ê°€ í™•ë¥ ë°€ë„í•¨ìˆ˜ì´ë¯€ë¡œ í™•ë¥ ë°€ë„í•¨ìˆ˜ì˜ ì ë¶„ì€ ëˆ„ì ë¶„í¬í•¨ìˆ˜ì…ë‹ˆë‹¤.\n\\[\\int_{-\\infty}^{x}f(t)dt = F(x) = P(-\\infty<X\\leq x)\\]\nì„ì˜ì˜ êµ¬ê°„ \\((a,b]\\)ì‚¬ì´ì— í™•ë¥ ë³€ìˆ˜ \\(X\\)ê°€ ì†í•˜ëŠ” ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\nP(a < X \\leq b) &= P(-\\infty < X \\leq b) - P(-\\infty < X \\leq a) \\\\\n&= F(b) - F(a) \\\\\n&= \\int_{-\\infty}^{b}f(t)dt - \\int_{-\\infty}^{a}f(t)dt \\\\\n&= \\int_{a}^{b}f(t)dt\n\\end{aligned}\\]\n\n\nê³µë¶„ì‚° ì „ê°œí•˜ê¸°\n\\[\\begin{aligned}\n\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}] \\\\\n&=\\mathbb{E}_{x,y}[xy - x\\mathbb{E}[y] - y\\mathbb{E}[x] + \\mathbb{E}[x]\\mathbb{E}[y]]\\\\\n&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}_{x,y}[x\\mathbb{E}[y]] - \\mathbb{E}_{x,y}[y\\mathbb{E}[x]] + \\mathbb{E}[x]\\mathbb{E}[y]]\\\\\n\\end{aligned}\\]\nì—¬ê¸°ì„œ \\(\\mathbb{E}_{x,y}[x\\mathbb{E}[y]]\\)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\int_{\\infty}^{\\infty}\\int_{\\infty}^{\\infty}x\\mathbb{E}[y]p(y)p(x)dydx &=  \\int_{\\infty}^{\\infty}x\\mathbb{E}[y]p(x)\\bigg(\\int_{\\infty}^{\\infty}p(y)dy\\bigg)dx \\\\\n&= \\int_{\\infty}^{\\infty}x\\mathbb{E}[y]p(x)dx \\\\\n&= \\mathbb{E}[y]\\int_{\\infty}^{\\infty}xp(x)dx \\\\\n&= \\mathbb{E}[y]\\mathbb{E}[x]\n\\end{aligned}\\]\në§ˆì°¬ê°€ì§€ë¡œ \\(\\mathbb{E}_{x,y}[y\\mathbb{E}[x]]\\)ë„ ê°™ì€ ê°’ì„ ê°€ì§„ë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[xy] - \\mathbb{E}_{x,y}[x\\mathbb{E}[y]] - \\mathbb{E}_{x,y}[y\\mathbb{E}[x]] + \\mathbb{E}[x]\\mathbb{E}[y]]\\\\\n&= \\mathbb{E}_{x,y}[xy] - \\mathbb{E}[y]\\mathbb{E}[x] - \\mathbb{E}[x]\\mathbb{E}[y] + \\mathbb{E}[x]\\mathbb{E}[y] \\\\\n&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}[x]\\mathbb{E}[y]\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Linear Algebra/Ax=bì˜ í•´ì˜ ê°¯ìˆ˜ ì•Œì•„ë‚´ê¸°/Ax = bì˜ í•´ì„.html",
    "href": "posts/Linear Algebra/Ax=bì˜ í•´ì˜ ê°¯ìˆ˜ ì•Œì•„ë‚´ê¸°/Ax = bì˜ í•´ì„.html",
    "title": "Ax=bì˜ í•´ì˜ ê°¯ìˆ˜ ì•Œì•„ë‚´ê¸°",
    "section": "",
    "text": "ìœ íˆ¬ë¸Œ - í˜íœí•˜ì„ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ê°•ì˜ë¥¼ ì •ë¦¬í•˜ê¸° ìœ„í•´ ì‘ì„±í•œ ê¸€ì…ë‹ˆë‹¤.Rankì™€ Null spaceë¡œ Ax=bì˜ í•´ì˜ ê°¯ìˆ˜ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.\nì—°ë¦½ì¼ì°¨ë°©ì •ì‹ì€ í–‰ë ¬ Ax=bë¡œ ë‚˜íƒ€ë‚´ê³  í–‰ë ¬ì˜ ë­í¬ì™€ ì—´ê³µê°„ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì˜ ê°¯ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì—´ê³µê°„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ Ax=bì˜ í•´ì„\ní–‰ë ¬\\(A = \\begin{bmatrix}a_1 & a_2 & \\dots & a_n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\)ì™€ ë²¡í„°\\(x = \\begin{bmatrix}x_1,x_2,\\dots,x_n\\end{bmatrix}^T\\in \\mathbb{R}^{n \\times 1}\\)ì˜ ê³±ì„ ìƒê°í•´ë³´ì. í–‰ë ¬ê³¼ ë²¡í„°ì˜ ê³±ì€ ë‹¤ìŒê³¼ ê°™ì´ ì—´ë²¡í„°ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ ë°”ë¼ë³¼ ìˆ˜ ìˆë‹¤.\n\\[\\begin{aligned}\nAx = \\begin{bmatrix}a_1 & a_2 & \\dots & a_n\\end{bmatrix}\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots\\\\\nx_n\n\\end{bmatrix}\n= a_1x_1 + a_2x_2 + \\dots a_nx_n\n\\end{aligned}\\]\ní–‰ë ¬ Aì˜ ì—´ê³µê°„ì€ ì—´ë²¡í„°ë“¤ì˜ ì„ í˜•ìƒì„±ì´ë‹¤.\n\\[\\begin{aligned}\n&\\text{C}(A) = \\text{span}(a_1,a_2,\\dots,a_n) = \\{c_1a_1 + c_2a_2 + \\dots c_na_n|c_1,c_2,\\dots,c_n \\in K\\}\n\\end{aligned}\\]\nê·¸ëŸ¬ë¯€ë¡œ, í–‰ë ¬ \\(A\\)ì˜ ì—´ê³µê°„ì€ ì„ì˜ì˜ \\(x\\)ì— ëŒ€í•˜ì—¬ ê°€ëŠ¥í•œ \\(Ax\\)ì˜ ëª¨ë“  ì§‘í•©ê³¼ ê°™ë‹¤. ê°€ëŠ¥í•œ ëª¨ë“  ìŠ¤ì¹¼ë¼\\(x_1,x_2,\\dots,x_n\\)ë¡œ ë²¡í„°ì˜ ì¼ì°¨ê²°í•©ì´ ì´ë£¨ëŠ” ì§‘í•©ì€ ìƒì„±(span)ê³¼ ê°™ê¸° ë•Œë¬¸ì´ë‹¤ \\[\\text{C}(A) = \\text{span}(a_1,a_2,\\dots,a_n) = \\{Ax |x\\in K^n\\}\\]\në§Œì•½ \\(Ax = b\\)ì¸ ë°©ì •ì‹ì˜ í•´ \\(x\\)ë¥¼ êµ¬í•˜ë ¤ í•œë‹¤ í•˜ì. \\(\\text{C}(A)\\)ëŠ” ê°€ëŠ¥í•œ \\(Ax\\)ì˜ ëª¨ë“  ì§‘í•©ì´ì˜€ìœ¼ë¯€ë¡œ ë§Œì•½ \\(\\text{C}(A)\\)ê°€ \\(b\\)ë¥¼ í¬í•¨í•œë‹¤ë©´(ì¦‰,\\(b\\)ê°€ ì—´ê³µê°„ì˜ ì›ì†Œë¼ë©´) \\(Ax=b\\)ì¸ \\(x\\)ê°€ ì¡´ì¬í•˜ì—¬ ë°©ì •ì‹ì˜ í•´ê°€ ì¡´ì¬í•˜ê³  \\(\\text{C}(A)\\)ê°€ \\(b\\)ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ \\(Ax = b\\)ì¸ \\(x\\)ê°’ì´ ì¡´ì¬í•˜ì§€ ì•Šê³  ë”°ë¼ì„œ ë°©ì •ì‹ì˜ í•´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n\nCase1 : Aê°€ full column rankì¼ ë•Œ\në¬¸ì œ : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) ì¼ë•Œ, \\(Ax = b\\)ë¥¼ ë§Œì¡±í•˜ëŠ” xì˜ ê°¯ìˆ˜ëŠ”?\n ìœ„ì˜ ë¬¸ì œì—ì„œ AëŠ” full column rankì´ë‹¤. full column rankì¼ ê²½ìš° \\(Ax\\)ì˜ ëª¨ì–‘ì€ ìœ„ì™€ ê°™ê³  ì—´ê³µê°„ì€ mì°¨ì› ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì´ì nì°¨ì› ë²¡í„°ê³µê°„ì´ë‹¤.ì•ì—ì„œ â€œmì°¨ì› ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì¸ nì°¨ì› ë²¡í„°ê³µê°„â€ ì´ë¼ í–ˆëŠ”ë° ê·¸ ì´ìœ ëŠ” (ì—´)ë²¡í„°ê°€ mì°¨ì› ë²¡í„°ê³µê°„ì˜ ì›ì†Œ(ë²¡í„°)ì´ê¸° ë•Œë¬¸ì´ë‹¤. mì°¨ì› ë²¡í„°ê³µê°„ì— ìˆëŠ” (ì—´)ë²¡í„° nê°œë¥¼ ê¸°ì €ë¡œ ìƒì„±ëœ ê³µê°„ì´ê¸° ë•Œë¬¸ì— mì°¨ì› ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì´ë©´ì„œ ë™ì‹œì— nì°¨ì› ë²¡í„°ê³µê°„ì´ ëœë‹¤. \nìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ì—´ê³µê°„ì€ ì„ì˜ì˜ xì— ëŒ€í•˜ì—¬ ê°€ëŠ¥í•œ \\(Ax\\)ì˜ ì§‘í•©ì´ê¸°ì— ë°©ì •ì‹ì˜ í•´ê°€ ì¡´ì¬í•˜ë ¤ë©´ bê°€ ì—´ê³µê°„ì•ˆì— ìˆìœ¼ë©´ ë˜ê³  ì•„ë‹ˆë©´ ë°”ê¹¥ì— ìˆìœ¼ë©´ ëœë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ ê²½ìš° bì˜ ìœ„ì¹˜ëŠ” ì–´ë–»ê²Œ ë ê¹Œ?\n\nê°€ëŠ¥í•œ bì˜ ìœ„ì¹˜ëŠ” 2ê°€ì§€ì´ë‹¤. 1ì˜ ê²½ìš°ëŠ” ì—´ê³µê°„ì•ˆì— bê°€ ì—†ëŠ” ê²½ìš°ë‹¤. ì´ ê²½ìš° bëŠ” mì°¨ì› ë²¡í„°ê³µê°„ì—ëŠ” ìˆìœ¼ë©´ì„œ(\\(b \\in R^{m \\times 1}\\)ì´ê¸°ì— ë‹¹ì—°í•˜ë‹¤) ë¶€ë¶„ê³µê°„ì¸ nì°¨ì› ì—´ê³µê°„ì•ˆì—ëŠ” ì—†ê²Œëœë‹¤. ë”°ë¼ì„œ ì´ ê²½ìš° í•´ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. 2ì˜ ê²½ìš°ëŠ” ì—´ê³µê°„ì•ˆì— bê°€ ìˆëŠ” ê²½ìš°ë‹¤. ì´ ê²½ìš° bëŠ” mì°¨ì› ë²¡í„°ê³µê°„ì— ì†í•˜ë©´ì„œ ë™ì‹œì— ë¶€ë¶„ê³µê°„ì¸ nì°¨ì› ë²¡í„°ê³µê°„ì—ë„ ì†í•œë‹¤.\nì´ë ‡ê²Œ ìƒê°í•˜ë©´ ëë‚œ ê²ƒ ê°™ì€ë° í•œê°€ì§€ ë” ìƒê°í•´ì•¼ í•  ê²ƒì´ ìˆë‹¤. ë°”ë¡œ null spaceì´ë‹¤. ë§Œì•½ Ax = bë¥¼ ë§Œì¡±í•˜ëŠ” í•˜ë‚˜ì˜ í•´ì¸ \\(x_{particular}\\)ê°€ ìˆë‹¤ê³  í•´ë³´ì. ì´ë•Œ \\(Ax=0\\)ì„ ë§Œì¡±í•˜ëŠ” ë„ê³µê°„ì˜ ì„ì˜ì˜ ì›ì†Œì¸ \\(x\\)ë¥¼ \\(x_{null}\\in N(A)\\)ì´ë¼ê³  í•˜ë©´ ë‹¤ìŒì´ ì„±ë¦½í•œë‹¤.\n\\[A(x_{particular} + x_{null}) = Ax_{particular} + Ax_{null} = b\\]\nìœ—ì‹ì— ì˜í•´ì„œ null spaceì˜ ì›ì†Œì¸ \\(x_{null}\\)ê³¼ \\(x_{particular}\\)ì˜ í•©ë„ ë°©ì •ì‹ì˜ í•´ì´ë¯€ë¡œ í•´ë¥¼ êµ¬í• ë•Œ null spaceë„ í™•ì¸í•´ì•¼ í•œë‹¤. null spaceì— ëŒ€í•œ xë¥¼ ì¶”ê°€í•œ ì™„ì „í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[x_{complete} = x_{particular} + x_{null}\\]\nê·¸ë ‡ë‹¤ë©´ x_{null}ì„ ì–´ë–»ê²Œ í™•ì¸í•  ìˆ˜ ìˆì„ê¹Œ? ë­í¬-ë„ë¦¬í‹° ì •ë¦¬ë¡œ í™•ì¸í•  ìˆ˜ ìˆëŠ”ë° ìœ„ì™€ê°™ì´ full column rankì¸ ê²½ìš°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&\\text{rank}(A) + \\text{nulity(A)} = n\\\\\n&\\Longleftrightarrow \\text{nulity(A)} = n - rank(A) = n - n = 0\n\\end{aligned}\\]\në­í¬-ë„ë¦¬í‹° ì •ë¦¬ì— ì˜í•˜ì—¬ null spaceì˜ ì°¨ì›ì´ 0ì„ì„ í™•ì¸í–ˆë‹¤. ì°¨ì›ì´ 0ì¸ ë„ê³µê°„(ë²¡í„°ê³µê°„)ì€ \\(\\{\\bf{0}\\}\\)ì´ë¯€ë¡œ \\(x_{null} \\in N(A)\\)ì¸ \\(x_{null} ={\\bf 0}\\)ì´ë‹¤. ë”°ë¼ì„œ 2ë²ˆì˜ ê²½ìš°, \\(x_{particular}\\)ê°€ ì¡´ì¬í•˜ì—¬ \\(Ax = 0\\)ì¼ ê²½ìš°ì˜ ì™„ì „í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\\therefore  x_{total} = x_{particular} + x_{null} = x_{particular} + {\\bf 0} = x_{particular}\\]\nê²°ë¡ ì ìœ¼ë¡œ, full column rankì¼ ê²½ìš° í•´ê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ë‹¨ í•˜ë‚˜ ì¡´ì¬í•œë‹¤.\n\n\nCase2 : Aê°€ full row rankì¼ ë•Œ\në¬¸ì œ : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = m<n\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) ì¼ë•Œ, \\(Ax = b\\)ë¥¼ ë§Œì¡±í•˜ëŠ” xì˜ ê°¯ìˆ˜ëŠ”?\n ìœ„ì˜ ë¬¸ì œì—ì„œ AëŠ” full row rankì´ë‹¤. ìœ„í•´ì„œ í•œ ê²ƒì²˜ëŸ¼ ë¨¼ì € \\(C(A)\\)ì™€ \\(b\\)ê°€ ì–´ë–»ê²Œ ìœ„ì¹˜í•˜ê³  ìˆì„ì§€ íŒŒì•…í•˜ê³  null spaceë¥¼ ë”°ì ¸ ì™„ì „í•´ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ë¨¼ì € \\(C(A)\\)ë¥¼ ìƒê°í•´ë³´ì. \\(C(A)\\)ëŠ” í–‰ë ¬ì˜ ë­í¬ê°€ mì´ê¸°ì— nê°œì˜ (ì—´)ë²¡í„° ì¤‘ ì„ í˜•ë…ë¦½ì¸ column vectorëŠ” mê°œ ë¿ì´ë‹¤. ë”°ë¼ì„œ ë²¡í„°ì˜ spanì¸ ì—´ê³µê°„ì€ mì°¨ì› ë²¡í„°ê³µê°„ì´ë‹¤. (nê°œì˜ ì—´ë²¡í„°ê°€ ì¡´ì¬í•˜ì§€ë§Œ,ì„ í˜•ì¢…ì†ì´ê¸°ë–„ë¬¸ì´ë‹¤.)\nì´ì „ë¬¸ì œì™€ ë‹¤ë¥¸ì ë„ ì¡´ì¬í•˜ëŠ”ë° ë°”ë¡œ ì—´ê³µê°„ì´ (ì—´)ë²¡í„°ê°€ ì¡´ì¬í•˜ëŠ” ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì´ ì•„ë‹ˆë¼ëŠ” ì ì´ë‹¤. ì´ ë¬¸ì œì—ì„œ ê°ê°ì˜ ì—´ë²¡í„°ëŠ” mì°¨ì› ê³µê°„ì˜ ë²¡í„°ì´ê³  ì—´ë²¡í„°ì˜ ìƒì„±ë„ mì°¨ì› ë²¡í„°ê³µê°„ì´ê¸°ë•Œë¬¸ì— ì—´ë²¡í„°ê°€ ì¡´ì¬í•˜ëŠ” ë°”ë¡œ ê·¸ ê³µê°„ì´ ì—´ê³µê°„ì´ë‹¤.\nìœ„ì™€ ê°™ì´ ì—´ê³µê°„ì— ëŒ€í•´ì„œ ìƒê°í–ˆìœ¼ë©´ ì´ì œ bì— ëŒ€í•´ì„œ ìƒê°í•´ë³¼ ì°¨ë¡€ë‹¤. bì˜ ìœ„ì¹˜ëŠ” ì–´ë–»ê²Œ ë ê¹Œ? bëŠ” Aì˜ ì—´ê³µê°„ì— ì†í•˜ëŠ” ë²¡í„°ì¼ê¹Œ ì•„ë‹ê¹Œ?\n\nbì˜ ê²½ìš° \\(b \\in \\mathbb{R}^{m \\times 1}\\)ì´ê¸°ì— ì—´ê³µê°„ì— ì†í•˜ëŠ” ë²¡í„°ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ full row rankì¸ ê²½ìš°ëŠ” ë°˜ë“œì‹œ í•´ê°€ ì¡´ì¬í•œë‹¤.\nì—¬ê¸°ì„œ ì™„ì „í•´ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œ null spaceë„ ìƒê°í•´ì•¼í•œë‹¤. ë­í¬-ë„ë¦¬í‹° ì •ë¦¬ì— ì˜í•´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&rank(A) + nulity(A) = n \\\\\n&\\Longleftrightarrow nulity(A) = n - rank(A) =  n - m\n\\end{aligned}\\]\në­í¬ì •ë¦¬ì— ì˜í•´ì„œ null spaceëŠ” n-mì°¨ì›ì˜ ë²¡í„°ê³µê°„ì´ë‹¤. ì´ ê²½ìš° ê°€ëŠ¥í•œ \\(x_{null}\\)ì€ ë¬´í•œíˆ ë§ì´ ì¡´ì¬í•˜ë¯€ë¡œ í•´ê°€ ë¬´ìˆ˜íˆ ë§ë‹¤. ì™„ì „í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\therefore\\,\\,  &x_{complete} = x_{particular} + x_{null}\\\\\n&\\text{where, } x_{null} \\in N(A) = \\mathbb{R}^{n-m}\n\\end{aligned}\\]\nê²°ë¡ ì ìœ¼ë¡œ, full row rankì˜ ê²½ìš° í•´ëŠ” ë¬´ìˆ˜íˆ ë§ë‹¤.\n\n\nCase3 : Aê°€ full rankì¼ ë•Œ\në¬¸ì œ : \\(A \\in \\mathbb{R}^{m \\times m},\\text{rank}(A) = m\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) ì¼ë•Œ, \\(Ax = b\\)ë¥¼ ë§Œì¡±í•˜ëŠ” xì˜ ê°¯ìˆ˜ëŠ”?\n\nì‚¬ì‹¤ ì´ ë¬¸ì œì˜ í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[x = A^{-1}b\\] ê·¸ëŸ¬ë¯€ë¡œ,full rankì¸ ê²½ìš° í•´ì˜ ê°¯ìˆ˜ê°€ 1ê°œì´ë‹¤.\nìœ„ì²˜ëŸ¼ ê°„ë‹¨í•˜ê²Œ í•´ë¥¼ êµ¬í•  ìˆ˜ ìˆì§€ë§Œ â€¦ ê·¸ë˜ë„ ê¸°í•˜í•™ì ìœ¼ë¡œ ìƒê°í•´ë³´ê¸° ìœ„ì—ì„œ í–ˆë˜ ê²ƒì²˜ëŸ¼ ë”°ì ¸ë³´ì. column spaceëŠ” mì°¨ì› ë²¡í„°ê³µê°„ì´ë‹¤. column vectorëŠ” column space ê·¸ ìì²´ì¸ mì°¨ì› ë²¡í„°ê³µê°„ì˜ ì›ì†Œì´ë‹¤. \\(b\\in \\mathbb{R}^{m \\times 1}\\)ì˜ ê²½ìš° mì°¨ì› ë²¡í„°ê³µê°„ì˜ ì›ì†Œì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, column spaceëŠ” ë°˜ë“œì‹œ bë¥¼ í¬í•¨í•˜ë©° ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\në„ê³µê°„ì„ ë”°ì§€ê¸° ìœ„í•´ ë­í¬-ë„ë¦¬í‹° ì •ë¦¬ë¥¼ ì‚¬ìš©í•´ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&\\text{rank}(A)+\\text{nulity}(A) = m\\\\\n&\\Longleftrightarrow \\text{nulity}(A) = m - \\text{rank}(A) = m - m = 0\n\\end{aligned}\\]\nnull spaceëŠ” ì°¨ì›ì´ 0ì´ë¯€ë¡œ \\(N(A) = \\{{\\bf 0}\\}\\)ì´ê³  ë‹¤ìŒê³¼ ê°™ë‹¤. \\[x_{complete} = x_{particular} + x_{null} = x_{particular} + {\\bf 0} = x_{particular}\\]\nê²°ê³¼ì ìœ¼ë¡œ, full rankì¸ ê²½ìš° í•´ëŠ” ë°˜ë“œì‹œ ì¡´ì¬í•˜ë©° ê°¯ìˆ˜ëŠ” 1ê°œì´ë‹¤.\n\n\nCase4 : Aê°€ rank deficient ì¼ ë•Œ\në¬¸ì œ : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = \\alpha < \\text{min}(m,n)\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) ì¼ë•Œ, \\(Ax = b\\)ë¥¼ ë§Œì¡±í•˜ëŠ” xì˜ ê°¯ìˆ˜ëŠ”?\n\nìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” Aê°€ rank deficientì¸ ê²½ìš° ì¤‘, \\(m<n\\) ì¸ ê²½ìš°ë¥¼ ìƒê°í•´ë³´ì. í–‰ë ¬ Aì˜ column spaceëŠ” mì°¨ì› ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì´ì \\(\\alpha\\)ì°¨ì›ì˜ ë²¡í„°ê³µê°„ì´ë‹¤.\\(b \\in \\mathbb{R}^{m \\times 1}\\)ì´ë¯€ë¡œ ê°€ëŠ¥í•œ ê²½ìš°ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n\n1ë²ˆì˜ ê²½ìš°ë¼ë©´ bëŠ” column spaceì˜ ì›ì†Œê°€ ì•„ë‹ˆë¯€ë¡œ ë°©ì •ì‹ì˜ í•´ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ë§Œì•½ 2ë²ˆì˜ ê²½ìš°ë¼ë©´ í•´ê°€ ì¡´ì¬í•œë‹¤. ì´ë•Œì—ëŠ” null spaceë¥¼ ê³ ë ¤í•œ ì™„ì „í•´ë¥¼ ë”°ì ¸ì•¼í•˜ë¯€ë¡œ ë­í¬-ë„ë¦¬í‹° ì •ë¦¬ë¥¼ í™•ì¸í•œë‹¤.\n\\[\\begin{aligned}\n&\\text{rank}(A) + \\text{nulity}(A) = \\alpha + \\text{nulity}(A) = n \\\\\n&\\Longleftrightarrow \\text{nulity}(A) = n - \\alpha > 0\n\\end{aligned}\\]\në­í¬ì •ë¦¬ì— ì˜í•´ì„œ null spaceëŠ” \\(n - \\alpha\\)ì°¨ì›ì˜ ë²¡í„°ê³µê°„ì´ë‹¤. ì´ ê²½ìš° null spaceì˜ ì„ì˜ì˜ ì›ì†Œì¸ \\(x_{null}\\)ëŠ” ë¬´í•œíˆ ë§ì´ ì¡´ì¬í•˜ë¯€ë¡œ í•´ê°€ ë¬´ìˆ˜íˆ ë§ë‹¤. ì™„ì „í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\therefore\\,\\,  &x_{complete} = x_{particular} + x_{null}\\\\\n&\\text{where, } x_{null} \\in N(A) = \\mathbb{R}^{n-\\alpha}\n\\end{aligned}\\]\nê²°ë¡ ì ìœ¼ë¡œ, rank deficientì˜ ê²½ìš° í•´ëŠ” ë¬´ìˆ˜íˆ ë§ê±°ë‚˜ í•´ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n\nì •ë¦¬\n\n\n\n\n\n\n\n\nrank type\nexpression\ní•´ì˜ ê°¯ìˆ˜\n\n\n\n\nfull column rank\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) = n<m\\)\ní•´ê°€ ì—†ê±°ë‚˜ í•´ê°€ í•œê°œë§Œ ì¡´ì¬í•œë‹¤.\n\n\nfull row rank\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) = m<n\\)\ní•´ê°€ ë¬´ìˆ˜íˆ ë§ë‹¤.\n\n\nfull rank\n\\(A \\in \\mathbb{R}^{m \\times m}\\),\\(\\text{rank}(A) = m\\)\ní•´ê°€ í•œê°œë§Œ ì¡´ì¬í•œë‹¤.\n\n\nrank deficient\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) < \\text{min}(m,n)\\)\ní•´ê°€ ì—†ê±°ë‚˜ í•´ê°€ ë¬´ìˆ˜íˆ ë§ë‹¤.\n\n\n\n\n\nì°¸ê³ ìë£Œ\ní˜íœí•˜ì„ - [ì„ ëŒ€] 2-11ê°•. Ax=b ì˜ í•´ì˜ ìˆ˜ ì•Œì•„ë‚´ê¸° í”„ë¦°í‚¤í”¼ì•„"
  },
  {
    "objectID": "posts/Linear Algebra/Least Squares/Least Squares.html",
    "href": "posts/Linear Algebra/Least Squares/Least Squares.html",
    "title": "Least Squares",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m,x \\in \\mathbb{R}^{n \\times 1},b \\in \\mathbb{R}^{m \\times 1}\\) ê°€ ì£¼ì–´ì§€ê³  ë°©ì •ì‹ \\(Ax = b\\)ë¥¼ ë§Œì¡±í•˜ëŠ” í•´ì¸ \\(x\\)ë¥¼ êµ¬í•  ìˆ˜ ì—†ì„ ë•Œ, \\(Ax\\)ê°€ \\(b\\)ì™€ ê°€ì¥ ë¹„ìŠ·í•˜ê²Œ í•˜ëŠ” \\(x\\)ë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/Least Squares/Least Squares.html#projection-matrix",
    "href": "posts/Linear Algebra/Least Squares/Least Squares.html#projection-matrix",
    "title": "Least Squares",
    "section": "projection matrix",
    "text": "projection matrix\nìœ„í•´ì„œ êµ¬í•œ \\(\\hat{x}\\)ë¥¼ \\(A\\hat{x}\\)ì— ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\[A\\hat{x} = (A^TA)^{-1}A^Tb\\]\nìœ„ ì‹ì€ ìš°ë³€ì˜ \\(b\\)ì— \\(A(A^TA)^{-1}A^T\\)ë¥¼ ê³±í•˜ì—¬ \\(C(A)\\)ì—ì„œ \\(b\\)ì™€ ê°€ì¥ ë¹„ìŠ·í•˜ë©´ì„œ(ê±°ë¦¬ê°€ ê°€ì¥ ê°€ê¹Œìš°ë©´ì„œ) \\(b\\)ë¥¼ \\(C(A)\\)ì— ì •ì‚¬ì˜(projection) í•œ ë²¡í„° \\(A\\hat{x}\\)ì„ ì–»ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ \\(A(A^TA)^{-1}A^T\\)ë¥¼ projection matrixë¼ ë¶€ë¥´ê³  \\(p_A\\)ë¡œ í‘œê¸°í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html",
    "title": "rank & null space(kernel)",
    "section": "",
    "text": "ìœ íˆ¬ë¸Œ - í˜íœí•˜ì„ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ê°•ì˜ë¥¼ ì •ë¦¬í•˜ê¸° ìœ„í•´ ì‘ì„±í•œ ê¸€ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#ì˜ˆì œ",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#ì˜ˆì œ",
    "title": "rank & null space(kernel)",
    "section": "ì˜ˆì œ",
    "text": "ì˜ˆì œ\n\\[\\begin{pmatrix}\n1&2&3\\\\\n0&0&0\n\\end{pmatrix}\\]\nìœ„ì— ìˆëŠ” í–‰ë ¬ì—ì„œ ì„ í˜•ë…ë¦½ì¸ ì—´ë²¡í„°ì˜ ê°¯ìˆ˜ = 1ì´ë‹¤. ë˜í•œ ì„ í˜•ë…ë¦½ì¸ ì—´ë²¡í„°ì˜ ê°¯ìˆ˜ = ì„ í˜•ë…ë¦½ì¸ í–‰ë²¡í„°ì˜ ê°¯ìˆ˜ = ì—´ê³µê°„ì˜ ì°¨ì› = í–‰ê³µê°„ì˜ ì°¨ì›ì´ë¯€ë¡œ ë­í¬ì •ë¦¬ë„ ì„±ë¦½í•œë‹¤.\nìœ„ì™€ ê°™ì€ \\(2 \\times 3\\) í–‰ë ¬ì€ ë­í¬ë³´ë‹¤ í–‰,ì—´ì˜ ê°¯ìˆ˜ê°€ ë§ì´ ë¶€ì¡±í•˜ë¯€ë¡œ rank-deficient ë¼ê³  í•œë‹¤.\n\\[\\begin{pmatrix}\n1&0&1\\\\\n0&1&1\n\\end{pmatrix}\\]\nìœ„ì— ìˆëŠ” í–‰ë ¬ì—ì„œ ì„ í˜•ë…ë¦½ì¸ ì—´ë²¡í„°ì˜ ê°¯ìˆ˜ = 2ì´ë‹¤. ë˜í•œ ì„ í˜•ë…ë¦½ì¸ ì—´ë²¡í„°ì˜ ê°¯ìˆ˜ = ì„ í˜•ë…ë¦½ì¸ í–‰ë²¡í„°ì˜ ê°¯ìˆ˜ = ì—´ê³µê°„ì˜ ì°¨ì› = í–‰ê³µê°„ì˜ ì°¨ì›ì´ë¯€ë¡œ ë­í¬ì •ë¦¬ë„ ì„±ë¦½í•œë‹¤.\nìœ„ì™€ ê°™ì€ í–‰ë ¬ì€ í–‰ì˜ ê°¯ìˆ˜ë§Œí¼ ë­í¬ê°€ ë‹¤ ì°¨ìˆìœ¼ë¯€ë¡œ full row rankë¼ í•œë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#ìš©ì–´ì •ë¦¬",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#ìš©ì–´ì •ë¦¬",
    "title": "rank & null space(kernel)",
    "section": "ìš©ì–´ì •ë¦¬",
    "text": "ìš©ì–´ì •ë¦¬\n\\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m \\Rightarrow\\) ì—´ë²¡í„°ê°€ ëª¨ë‘ ì„ í˜•ë…ë¦½,full column rank,ìœ„ì•„ë˜ë¡œ ê¸¸ì­‰í•˜ê³  ì–‘ì˜†ì€ ì¢ì€ ì§ì‚¬ê°í–‰ë ¬ \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n>m \\Rightarrow\\) í–‰ë²¡í„°ê°€ ëª¨ë‘ ì„ í˜•ë…ë¦½,full row rank,ìœ„ì•„ë˜ê°€ ì¢ê³  ì¢Œìš° ì–‘ì˜†ìœ¼ë¡œ ê¸¸ì­‰í•œ ì§ì‚¬ê°í–‰ë ¬ \\(A \\in \\mathbb{R}^{n \\times n},\\text{rank}(A) = n \\Rightarrow\\) í–‰ë°±í„°,ì—´ë²¡í„°ê°€ ëª¨ë‘ ì„ í˜•ë…ë¦½,full rank,ìœ„,ì•„ë˜,ì–‘,ì˜†ì˜ ê¸¸ì´ê°€ ëª¨ë‘ ê°™ì€ ì •ì‚¬ê°í–‰ë ¬ \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) < \\text{min}(n,m)\\) = ì„ í˜•ì¢…ì†ì¸ í–‰ë²¡í„°,ì—´ë²¡í„° ë°˜ë“œì‹œ ì¡´ì¬,rank deficient"
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#ì˜ˆì œ1",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#ì˜ˆì œ1",
    "title": "rank & null space(kernel)",
    "section": "ì˜ˆì œ1)",
    "text": "ì˜ˆì œ1)\ní–‰ë ¬ A = \\(\\begin{pmatrix}1&0&1 \\\\0&1&1\\end{pmatrix}\\)ì¼ ë•Œ, í–‰ë ¬Aì˜ ì˜ê³µê°„ì€?\në¨¼ì € ë§Œì¡±í•˜ëŠ” \\(x\\)ë¥¼ ì°¾ì•„ë³´ì. ì–´ë–¤ ë°©ë²•ì´ë˜ ì‚¬ìš©ê°€ëŠ¥í•˜ì§€ë§Œ ë¬¸ì œê°€ ê°„ë‹¨í•˜ë¯€ë¡œ ì§ê´€ì ìœ¼ë¡œ í’€ì´í•œë‹¤.\n\\[\\begin{aligned}\n&Ax = x_1\\begin{pmatrix}1\\\\0\\end{pmatrix} + x_2\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + x_3\\begin{pmatrix}1\\\\1\\end{pmatrix} = {\\bf 0} \\\\\n&\\Longleftrightarrow\n\\begin{cases}\nx_1 + x_3 = 0 \\\\\nx_2 + x_3 = 0\n\\end{cases}\n\\\\\n&\\Longleftrightarrow x_3 = t,x_2 = -t,x_1 = t \\\\\n&\\Longleftrightarrow x = t\\begin{pmatrix}1\\\\-1\\\\1\\end{pmatrix} \\\\\n&\\therefore \\text{N}(A) = \\Bigg\\{t\\begin{pmatrix}1\\\\-1\\\\1 \\end{pmatrix}|t \\in \\mathbb{R}\\Bigg\\}\n\\end{aligned}\\]\në°©ì •ì‹ì˜ í•´ë¥¼ êµ¬í•´ë³´ë‹ˆ 1)\\([1,-1,1]^T\\)ì˜ spanì´ ì˜ê³µê°„ì´ë©° 2)ì˜ê³µê°„ì€ í–‰ë ¬ê³±ì— ì˜í•´ì„œ(ì¤‘ê°„ì—ì„œ ì°¨ì›ì¼ì¹˜ \\(2 \\times 3 ê³¼ 3 \\times 1\\)) í–‰ë²¡í„°ê°€ ì¡´ì¬í•˜ëŠ” ì°¨ì›ì¸ 3ì°¨ì› ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì¸ 1ì°¨ì› ë²¡í„°ê³µê°„ì„ ìƒì„±í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.\nì˜ê³µê°„ì˜ ì›ì†Œ(ë°©ì •ì‹ì˜ í•´)ëŠ” ë¬´ìˆ˜íˆ ë§ìŒì´ ìëª…í•œë° ì™œëƒí•˜ë©´ \\(Ax = 0\\)ì„ ë§Œì¡±í•˜ëŠ” ì„ì˜ì˜ xë²¡í„°ì— ëŒ€í•œ ìŠ¤ì¹¼ë¼ë°°ì— ëŒ€í•´ ë‹¤ìŒì´ ì„±ë¦½í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\n\\[\\begin{aligned}\n&Ax = {\\bf 0} \\\\\n&\\Longleftrightarrow cAx = c{\\bf 0} = {\\bf 0} \\\\\n&\\Longleftrightarrow A(cx) ={\\bf 0} \\\\\n\\end{aligned}\\]\në”°ë¼ì„œ \\(x\\)ì˜ ìŠ¤ì¹¼ë¼ë°°ì¸ \\(cx\\)ë„ \\(A\\)ì™€ ê³±í•´ì ¸ì„œ \\({\\bf 0}\\) ë§Œë“¤ê¸° \\(x\\)ì˜ ìŠ¤ì¹¼ë¼ë°°ë„ ì˜ê³µê°„ì˜ ì›ì†Œì´ë‹¤. \nrank-nulity theoreomë„ ì„±ë¦½í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì˜ê³µê°„ì˜ ì°¨ì›ì€ ì˜ê³µê°„ì˜ ê¸°ì €ì˜ ê°¯ìˆ˜ì¸ë° \\([1,-1,1]^T\\)ì´ ê¸°ì €ì˜ ì¡°ê±´ì¸ 1)ì„ í˜•ìƒì„± = ë²¡í„°ê³µê°„ 2)ì„ í˜•ë…ë¦½ ì´ë¼ëŠ” ë‘ ì¡°ê±´ì„ ë§Œì¡±í•˜ë¯€ë¡œ ì°¨ì›ì€ 1ì´ë‹¤.ë­í¬ëŠ” ë‹¤ë¥¸ë°©ì‹ìœ¼ë¡œë„ êµ¬í•  ìˆ˜ ìˆì§€ë§Œ í–‰ë ¬ì´ ê°„ë‹¨í•´ì„œ ë°”ë¡œ2ì„ì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ë‹¤ \\[\\text{nulity}(A) + \\text{rank}(A)= n   \\Longleftrightarrow  1 + 2 = 3\\]"
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#ì˜ˆì œ2",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#ì˜ˆì œ2",
    "title": "rank & null space(kernel)",
    "section": "ì˜ˆì œ2)",
    "text": "ì˜ˆì œ2)\ní–‰ë ¬ A = \\(\\begin{pmatrix}1&2&3 \\\\0&0&0\\end{pmatrix}\\)ì¼ ë•Œ, í–‰ë ¬Aì˜ ì˜ê³µê°„ì€?\në§ˆì°¬ê°€ì§€ë¡œ ë§Œì¡±í•˜ëŠ” \\(x\\)ë¥¼ ë¨¼ì € ì°¾ì•„ë³´ì.\n\\[\\begin{aligned}\n&Ax = x_1\\begin{pmatrix}1\\\\0\\end{pmatrix} + x_2\\begin{pmatrix}2 \\\\ 0\\end{pmatrix} + x_3\\begin{pmatrix}3\\\\0\\end{pmatrix} = {\\bf 0} \\\\\n&\\Longleftrightarrow\nx_1 + 2x_2 + 3x_3 = 0 \\\\\n&\\Longleftrightarrow x_3 = t,x_2 = q,x_1 = -2q -3t \\\\\n&\\Longleftrightarrow x = \\begin{pmatrix}-2q-3t\\\\q\\\\t\\end{pmatrix} = q\\begin{pmatrix}-2\\\\1\\\\0\\end{pmatrix} + t\\begin{pmatrix}-3\\\\0\\\\1\\end{pmatrix} \\\\\n&\\therefore \\text{N}(A) = \\Bigg\\{q\\begin{pmatrix}-2\\\\1\\\\0 \\end{pmatrix} + t\\begin{pmatrix}-3\\\\0\\\\1\\end{pmatrix}|t,q \\in \\mathbb{R}\\Bigg\\}\n\\end{aligned}\\]\në°©ì •ì‹ì˜ í•´ë¥¼ êµ¬í•´ë³´ë‹ˆ 1)ë‘ ë²¡í„°ì˜ spanì´ ì˜ê³µê°„ì´ë©° 2)ìƒì„±ëœ ì˜ê³µê°„ì€ í–‰ë ¬ê³±ì— ì˜í•´ì„œ(ì¤‘ê°„ì—ì„œ ì°¨ì›ì¼ì¹˜ \\(2 \\times 3\\) ê³¼ \\(3 \\times 1\\)) í–‰ë²¡í„°ê°€ ì¡´ì¬í•˜ëŠ” ì°¨ì›ì¸ 3ì°¨ì› ë²¡í„°ê³µê°„ì•ˆì—ì„œ ë¶€ë¶„ê³µê°„ì¸ 1ì°¨ì› ë²¡í„°ê³µê°„ì„ ìƒì„±í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\\(Ax = 0\\)ì„ ë§Œì¡±í•˜ëŠ” 2ê°œì˜ xì˜ ì„ í˜•ì¡°í•©ì´ ëª¨ë‘ ì˜ê³µê°„ì˜ ì›ì†Œì„ì„ í™•ì¸í•´ë³´ì.\n\\[\\begin{aligned}\n&Ax_1 = {\\bf 0},Ax_2 = {\\bf 0} \\\\\n&\\Longleftrightarrow cAx_1 = c{\\bf 0}={\\bf 0},cAx_2 = c{\\bf 0}={\\bf 0} \\\\\n&\\Longleftrightarrow A(cx_1) + A(cx_2) = A(cx_1 + cx_2) = {\\bf 0} \\\\\n\\end{aligned}\\]\në”°ë¼ì„œ \\(x\\) ë°©ì •ì‹ì„ ë§Œì¡±í•˜ëŠ” ë‘ ë²¡í„°ì˜ ì„ í˜•ì¡°í•©ì¸ \\(c(x_1 + cx_2)\\)ë„ \\(A\\)ì™€ ê³±í•´ì ¸ì„œ ì˜ê³µê°„ì˜ ì›ì†Œì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. \në§ˆì°¬ê°€ì§€ë¡œ rank-nulity theoreomë„ ì„±ë¦½í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì˜ê³µê°„ì˜ ì°¨ì›ì€ ì˜ê³µê°„ì˜ ê¸°ì €ì˜ ê°¯ìˆ˜ì´ê³  \\([-2,1,0]^T,[-3,0,1]^T\\)ê°€ ê¸°ì €ì˜ ì¡°ê±´ì¸ 1)ì„ í˜•ìƒì„± = ë²¡í„°ê³µê°„(ì—¬ê¸°ì„œ ì˜ê³µê°„) 2)ì„ í˜•ë…ë¦½ ì´ë¼ëŠ” ë‘ ì¡°ê±´ì„ ë§Œì¡±í•˜ë¯€ë¡œ ì˜ê³µê°„ì˜ ê¸°ì €ëŠ” \\([-2,1,0]^T,[-3,0,1]^T\\)ì´ê³  ì°¨ì›ì€ 2ì´ë‹¤.ë­í¬ëŠ” ë‹¤ë¥¸ë°©ì‹ìœ¼ë¡œë„ êµ¬í•  ìˆ˜ ìˆì§€ë§Œ í–‰ë ¬ì´ ê°„ë‹¨í•´ì„œ ë°”ë¡œ1ì„ì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ë‹¤ \\[\\text{nulity}(A) + \\text{rank}(A)= n   \\Longleftrightarrow  2 + 1 = 3\\]"
  },
  {
    "objectID": "posts/Linear Algebra/spanê³¼ columns space.html",
    "href": "posts/Linear Algebra/spanê³¼ columns space.html",
    "title": "Linear Combination & Span",
    "section": "",
    "text": "ìœ íˆ¬ë¸Œ - í˜íœí•˜ì„ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ê°•ì˜ë¥¼ ì •ë¦¬í•˜ê¸° ìœ„í•´ ì‘ì„±í•œ ê¸€ì…ë‹ˆë‹¤.\n\nlinear Combination\në²¡í„° \\(\\bf{v_1,v_2,\\dots,v_n}\\)ì˜ ì„ í˜•ê²°í•©(ë˜ëŠ” ì¼ì°¨ê²°í•©)ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[w_1{\\bf v_1} + w_2{\\bf v_2} + \\dots + w_n{\\bf v_n}\\] ì„ í˜•ê²°í•©ì˜ ê²°ê³¼ëŠ” ì—¬ëŸ¬ê°€ì§€ ë²¡í„°ë“¤ì˜ ê²°í•©ì´ë‹¤. ì—¬ê¸°ì„œ ê°ê°ì˜ \\(w_1,w_2,\\dots,w_n\\)ì€ ìŠ¤ì¹¼ë¼ì´ë©° ëŒ€ì‘í•˜ëŠ” \\(x_1,x_2,\\dots,x_n\\)ë¥¼ ê²°í•©ì— ì‚¬ìš©í•˜ëŠ” ì •ë„ë¥¼ ì˜ë¯¸í•œë‹¤. ë§Œì•½ \\(w_1 = 0.0001\\)ì´ë©´ ì—¬ëŸ¬ ë²¡í„°ë“¤ì„ ê²°í•©í•˜ì§€ë§Œ ê·¸ ê²°í•© ì¤‘ \\(\\bf v_1\\)ì´ ì•„ì£¼ ì‚¬ìš©í•˜ì—¬ ê²°í•©í•˜ëŠ” ê²ƒì´ê³  \\(w_2 = 120\\)ì´ë¼ë©´ ê²°í•©ì—ì„œ \\(\\bf v_2\\)ë¥¼ ì•„ì£¼ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.\n\n\nspan\në²¡í„° \\(v_1,v_2,\\dots,v_n\\)ì˜ ì„ í˜•ê²°í•©ì—ì„œ \\(w_1,w_2,\\dots,w_n\\)(ìŠ¤ì¹¼ë¼,ê²°í•©ì— ì‚¬ìš©í•˜ëŠ” ì •ë„)ë¥¼ ë°”ê¿¨ì„ë•Œ ê°€ëŠ¥í•œ ëª¨ë“  ë²¡í„°ë“¤ì˜ ì§‘í•©ì´ë©° ì¦‰,ë²¡í„°ì˜ ì„ í˜•ê²°í•©ìœ¼ë¡œ ê°€ëŠ¥í•œ ëª¨ë“  ì§‘í•©ë“¤ì´ë©° ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\\text{span}({\\bf{v_1,v_2,\\dots,v_n}}) := \\{w_1{\\bf{v_1}} + w_2{\\bf{v_2}} + \\dots + w_n{\\bf{v_n}}:w_1,w_2,\\dots,w_n \\in K\\}\\]  spanì€ ì„ í˜•ê²°í•©ìœ¼ë¡œ ë§Œë“¤ì–´ì§€ëŠ” ë˜ë‹¤ë¥¸ ë²¡í„°ê³µê°„ì´ë‹¤. ì—¬ê¸°ì„œ KëŠ” fieldë¥¼ ì˜ë¯¸í•˜ëŠ”ë° ìŠ¤ì¹¼ë¼ëŠ” fieldë¼ëŠ” ë˜ë‹¤ë¥¸ ì§‘í•©ìœ¼ë¡œë¶€í„° ê°€ì ¸ì˜¨ ì›ì†Œì´ê¸°ë•Œë¬¸ì— ê·¸ë ‡ë‹¤.ì„ì˜ì˜ ë²¡í„°ë“¤ì˜ spanì€ ì–´ë–¨ê¹Œ? ì•„ë˜ì˜ ê·¸ë¦¼ì„ í™•ì¸í•´ë³´ì.\n<ì°¸ê³ > spanì€ ë™ì‚¬ë¡œë„ ì‚¬ìš©í•œë‹¤. ex : ë²¡í„°ê³µê°„ì„ ìƒì„±í•œë‹¤. ì—´ê³µê°„ì€ ì—´ë²¡í„°ë“¤ì´ ìƒì„±í•˜ëŠ” ê³µê°„ì´ë‹¤.(=ì—´ê³µê°„ì€ ì—´ë²¡í„°ì˜ ìƒì„±ì´ë‹¤.).ê¸°ì €ë“¤ì´ ë²¡í„°ê³µê°„ì„ ìƒì„±í•œë‹¤.\n:ëŠ” ì¡°ê±´ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nplt.style.use(\"ggplot\")\n\ndef linrcmb(v1,v2):\n    linrcomb_x= []\n    linrcomb_y= []\n\n    _w = np.linspace(-50,50,300).tolist()\n    for i in range(2000):\n    #w1,w2ë¥¼ -100~100ì‚¬ì´ì˜ ì„ì˜ì˜ ìˆ«ìë¡œ\n        w1 = random.sample(_w,1)\n        w2 = random.sample(_w,1)\n        #print(w1,w2)\n    #ì„ í˜•ê²°í•© ê³„ì‚°\n        linrcomb = w1 * v1 + w2 * v2\n    #ì‹œê°í™”ë¥¼ ìœ„í•´ ì„ í˜•ê²°í•©ì˜ xê°’ yê°’ ë”°ë¡œ ëª¨ì•„ë†“ê¸°\n        linrcomb_x.append(linrcomb[0][0])\n        linrcomb_y.append(linrcomb[1][0])\n    return linrcomb_x,linrcomb_y\n\nv1 = np.array([[0],[0]])\nv2 = np.array([[0],[0]])\nx,y = linrcmb(v1,v2)\nfig,ax = plt.subplots(figsize=(30,5))\nplt.subplot(1,5,1)\nplt.title(\"$v_1 = [0,0]^{T},v_2 = [0,0]^T$\")\nplt.scatter(x,y,color=\"black\")\n\nv1 = np.array([[1],[0]])\nv2 = np.array([[-3],[0]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,2)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [-3,0]^T$\")\nplt.scatter(x,y)\n\nv1 = np.array([[1],[1]])\nv2 = np.array([[0],[0]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,3)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [0,0]^T$\")\nplt.scatter(x,y,color=\"green\",alpha=1)\n\n\nv1 = np.array([[1],[0]])\nv2 = np.array([[0],[1]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,4)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [0,1]^T$\")\nplt.scatter(x,y,color=\"purple\",alpha=1)\n\ndef linrcmb2(v1,v2):\n    linrcomb_x= []\n    linrcomb_y= []\n\n    _w = (np.linspace(-250,250,300)).tolist()\n    for i in range(2000):\n        w1 = random.sample(_w,1)[0] * 100 #ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ ê°’ ì¡°ì ˆ\n        w2 = random.sample(_w,1)[0] \n        #ì„ í˜•ê²°í•© ê³„ì‚°\n        linrcomb = w1 * v1 + w2 * v2\n        #ì‹œê°í™”ë¥¼ ìœ„í•´ ì„ í˜•ê²°í•©ì˜ xê°’ yê°’ ë”°ë¡œ ëª¨ì•„ë†“ê¸°\n        linrcomb_x.append(linrcomb[0][0])\n        linrcomb_y.append(linrcomb[1][0])\n    \n    return linrcomb_x,linrcomb_y\nlinrcmb2(v1,v2)\n\nv1 = np.array([[-1],[0]])\nv2 = np.array([[2],[2]])\nx,y = linrcmb2(v1,v2)\nplt.subplot(1,5,5)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [2,2]^T$\")\nplt.scatter(x,y,color=\"blue\",alpha=1)\nplt.subplots_adjust(wspace=0.4,hspace=0.5)\nplt.suptitle(\"span$(v_1,v_2)$\",y=1.02,fontsize=20)\n\n\nText(0.5, 1.02, 'span$(v_1,v_2)$')\n\n\n\n\n\nì  í•˜ë‚˜ëŠ” ë²¡í„° í•˜ë‚˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. 1ë²ˆì§¸ ê·¸ë¦¼ì—ì„œ ë²¡í„°ë“¤ì˜ ìƒì„±(span)ì€ 2ì°¨ì› ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì´ì 0ì°¨ì› ë²¡í„°ê³µê°„(ì )ì´ë©° 2,3ë²ˆì§¸ ê·¸ë¦¼ì—ì„œ ë²¡í„°ë“¤ì˜ ìƒì„±(span)ì€ 2ì°¨ì› ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì´ì 1ì°¨ì› ë²¡í„°ê³µê°„(ì§ì„ )ì´ë‹¤. ì´ì™€ëŠ” ë‹¤ë¥´ê²Œ 3ë²ˆì§¸ ê·¸ë¦¼ì—ì„œì˜ ë²¡í„°ë“¤ì˜ ìƒì„±(span)ì€ 3ì°¨ì› ë²¡í„°ê³µê°„ ê·¸ ìì²´ì¸ë° ê·¸ ì´ìœ ëŠ” 3ì°¨ì› ë²¡í„°ê³µê°„ì˜ ëª¨ë“  ì ì„ í‘œí˜„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.\n\n\ncolumn Space\nì—´ê³µê°„(columns space)ì€ í–‰ë ¬ì˜ ì—´ë²¡í„°ë“¤ì˜ span ì¦‰, í–‰ë ¬ì˜ ì—´ë²¡í„°ë“¤ë¡œ ê°€ëŠ¥í•œ ëª¨ë“  ì„ í˜•ì¡°í•©(ë²¡í„°)ì˜ ì§‘í•©ì…ë‹ˆë‹¤. \\(v_1,v_2,\\dots,v_n\\)ì´ í–‰ë ¬Aì˜ ì—´ë²¡í„°ë“¤ì´ë¼ê³  í•  ë•Œ, ì—´ê³µê°„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\[\\text{C}(A) = \\text{span}(v_1,v_2,\\dots,v_n) = \\{w_1{\\bf v_1} + w_2{\\bf v_2} + \\dots + w_n{\\bf v_n}:w_1,w_2,\\dots,w_n \\in K\\}\\] ì—´ê³µê°„ì€ ë°©ì •ì‹ \\(Ax = b\\)ì˜ í•´ì˜ ê°¯ìˆ˜ë¥¼ íŒŒì•…í•˜ëŠ”ë° ì“°ì…ë‹ˆë‹¤.\n\n\nì°¸ê³ ìë£Œ\n[ì„ í˜•ëŒ€ìˆ˜] ë²¡í„°ê³µê°„(vector space), ë²¡í„° ë¶€ë¶„ê³µê°„(vector subspace), ìƒì„±ê³µê°„(span), ì°¨ì›(dimension) ìœ„í‚¤í”¼ë””ì•„-Linear span ìœ„í‚¤í”¼ë””ì•„-Row and columns spaces í˜íœí•˜ì„ - [ì„ ëŒ€] 2-6ê°•. span ê³¼ column space (ì—´ê³µê°„) ì§ê´€ì  ì„¤ëª…"
  },
  {
    "objectID": "posts/Linear Algebra/í–‰ë ¬ê³±ì— ëŒ€í•œ ì—¬ëŸ¬ê°€ì§€ ì‹œê°.html",
    "href": "posts/Linear Algebra/í–‰ë ¬ê³±ì— ëŒ€í•œ ì—¬ëŸ¬ê°€ì§€ ì‹œê°.html",
    "title": "í–‰ë ¬ê³±ì— ëŒ€í•œ ì—¬ëŸ¬ê°€ì§€ ê´€ì ",
    "section": "",
    "text": "ìœ íŠœë¸Œ - í˜íœí•˜ì„ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ ê°•ì˜ ì •ë¦¬ìš© ì…ë‹ˆë‹¤.\n\ní–‰ë ¬ê³±ì€ ë‚´ì ì´ë‹¤.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},B \\in \\mathbb{R}^{n \\times p} \\\\\n&AB =\n\\begin{bmatrix}\na_1^T\\\\\na_2^T\\\\\n\\vdots\\\\\na_m^T\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 & b_2 & \\dots & b_p\n\\end{bmatrix}=\n\\begin{bmatrix}\na_1^Tb_1 & a_1^Tb_2 & \\dots & a_1^Tb_p \\\\\na_2^Tb_1 & a_2^Tb_2 & \\dots & a_2^Tb_p \\\\\n\\vdots & \\vdots  & \\vdots & \\vdots \\\\\na_{m}^Tb_1 & a_m^Tb_2 & \\dots & a_m^Tb_p \\\\\n\\end{bmatrix}\n\\end{aligned}\\]\n\n\ní–‰ë ¬ê³±ì€ rank-1 matrixì˜ í•©ì´ë‹¤.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},B \\in \\mathbb{R}^{n \\times p} \\\\\n&AB =\n\\begin{bmatrix}\na_1 & a_2 & \\dots & a_n\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1^T \\\\\nb_2^T \\\\\n\\vdots \\\\\nb_n^T\n\\end{bmatrix}\n= a_1b_1^T + a_2b_2^T + \\dots + a_nb_n^T\n\\end{aligned}\\]\n\n\ní–‰ë ¬ê³¼ ë²¡í„°ì˜ ê³±ì€ ì—´ê³µê°„ì— ì†í•œ ì„ì˜ì˜ ë²¡í„°ì´ë‹¤.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},x \\in \\mathbb{R}^{n \\times 1} \\\\\n&Ax =\n\\begin{bmatrix}\na_1 & a_2 & \\dots & a_n\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n= a_1x_1 + a_2x_2 + \\dots + a_nx_n\n\\end{aligned}\\]\n\\(a_1,a_2,\\dots,a_n\\)ì€ ë²¡í„° \\(x_1,x_2,\\dots,x_n\\)ì€ ìŠ¤ì¹¼ë¼ì´ë‹¤. í–‰ë ¬ê³±ì€ ìœ„ì™€ê°™ì´ ì—´ê³µê°„ì˜ ê¸°ì €(í–‰ë ¬\\(A\\)ì˜ ì—´ë²¡í„°)ì™€ ìŠ¤ì¹¼ë¼(ë¯¸ì§€ìˆ˜ë²¡í„°\\(x\\)ì˜ ì›ì†Œ)ì™€ì˜ ì¼ì°¨ê²°í•©ì´ë¯€ë¡œ ê¸°ì €ì¸ ì—´ë²¡í„°ê°€ ìƒì„±(span)í•˜ëŠ” ì—´ê³µê°„ì˜ ì›ì†Œì´ë‹¤. ì´ëŠ” ë°©ì •ì‹ \\(Ax=b\\)ì˜ í•´ì˜ ê°¯ìˆ˜ë¥¼ ì•Œì•„ë‚´ëŠ”ë°ì— ì‚¬ìš©í•˜ëŠ” ì¤‘ìš”í•œ ê°œë…ì´ë‹¤.(ì°¸ê³  : ë°©ì •ì‹ Ax = bì˜ í•´ì˜ ê°¯ìˆ˜ ì•Œì•„ë‚´ê¸°)\nì—´ê³µê°„(column space) : í–‰ë ¬ì—ì„œ (ì—´)ë²¡í„°ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ë²¡í„°ê³µê°„. ì—´ë²¡í„°ì˜ span\n\n\ní–‰ë²¡í„°ì™€ í–‰ë ¬ì˜ ê³±ì€ row space(í–‰ê³µê°„)ì— ì†í•œ ì„ì˜ì˜ ë²¡í„°ì´ë‹¤.\n\\[\\begin{aligned}\n&\\text{Let }x \\in \\mathbb{R}^{1 \\times n},X \\in \\mathbb{R}^{n \\times p} \\\\\n&xA =\n\\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\n\\vdots \\\\\na_n\n\\end{bmatrix}\n= x_1a_1^T + x_2a_2^T + \\dots + x_na_n^T\n\\end{aligned}\\]\n\\(a_1^T,a_2^T,\\dots,a_n^T\\)ì€ ë²¡í„° \\(x_1,x_2,\\dots,x_n\\)ì€ ìŠ¤ì¹¼ë¼ì´ë‹¤. í–‰ë ¬ê³±ì€ ìœ„ì™€ê°™ì´ í–‰ê³µê°„ì˜ ê¸°ì €(í–‰ë ¬\\(A\\)ì˜ í–‰ë²¡í„°)ì™€ ìŠ¤ì¹¼ë¼(\\(x\\)ì˜ ì›ì†Œ)ì™€ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ ê¸°ì €ì¸ í–‰ë²¡í„°ê°€ ìƒì„±í•˜ëŠ” í–‰ê³µê°„ì˜ ì›ì†Œì´ë‹¤.\ní–‰ê³µê°„ : í–‰ë ¬ì—ì„œ í–‰ë²¡í„°ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ë²¡í„°ê³µê°„. í–‰ë²¡í„°ì˜ span\n\n\nì°¸ê³ ë¬¸í—Œ\ní˜íœí•˜ì„ - ì„ ëŒ€ 2-5ê°•. í–‰ë ¬ì˜ ê³±ì…ˆê³¼ ë„¤ ê°€ì§€ ê´€ì  (ì—´ê³µê°„ (column space) ë“±)"
  },
  {
    "objectID": "posts/numpy/np.meshgrid.html",
    "href": "posts/numpy/np.meshgrid.html",
    "title": "np.meshgrid",
    "section": "",
    "text": "np.meshgrid"
  },
  {
    "objectID": "posts/numpy/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html",
    "href": "posts/numpy/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html",
    "title": "Finite Difference Method with np.gradient",
    "section": "",
    "text": "\\({\\bf x} =\\begin{bmatrix}x_1&x_2&\\dots&x_m\\end{bmatrix}^T\\)ì¼ ë•Œ, \\(x\\)ì— ëŒ€í•œ ë‹¤ë³€ìˆ˜í•¨ìˆ˜ \\(f({\\bf x})\\)ì˜ gradientëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&\\text{gradient of }f({\\bf{x}}) = \\frac{\\partial f}{\\partial {\\bf x}} = \\nabla f(\\bf{x}) =\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_m}\n\\end{pmatrix}\n\\end{aligned}\\]\ní•¨ìˆ˜ê°€ ê°€ì§€ëŠ” ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•´ì„œ í¸ë¯¸ë¶„ í•œ ë’¤ ëª¨ì•„ë†“ì€ ë²¡í„°ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. í•¨ìˆ˜ì˜ ìˆ˜ì‹ì„ ì•Œê³  ë¯¸ë¶„ì´ ê°€ëŠ¥í•˜ë©´ ìš°ë¦¬ëŠ” í•´ì„ì ìœ¼ë¡œ ë¯¸ë¶„í•´ì„œ(ë¯¸ë¶„ê³µì‹ì¨ì„œ) ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ êµ¬í•˜ê³  ê°ê°ì˜ ì–´ë–¤ pointì—ì„œì˜ í¸ë¯¸ë¶„ê³„ìˆ˜ë“¤ë„ êµ¬í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ê°€ ì£¼ì–´ì§„ ë°ì´í„°ëŠ” í•¨ìˆ˜fì˜ í•¨ìˆ«ê°’ë“¤ì´ ì£¼ì–´ì§„ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nimport numpy as np\nf = np.array([1,2,4,7,11,17],dtype = float)\nprint(\"f(x)\")\nprint(f)\n\nf(x)\n[ 1.  2.  4.  7. 11. 17.]\n\n\nìœ„ì™€ ê°™ì€ í•¨ìˆ«ê°’ë“¤ë§Œ ì£¼ì–´ì§ˆë•Œì—ëŠ” ì›ë˜ì˜ í•¨ìˆ˜ë¥¼ ì•Œê¸°ëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ ë„í•¨ìˆ˜ë¥¼ í†µí•œ ì •í™•í•œ ë¯¸ë¶„ê³„ìˆ˜ë¥¼ êµ¬í•˜ê¸°ê°€ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ì£¼ì–´ì§„ ë°ì´í„°ë¡œ \\(\\bf x\\)ì—ì„œì˜ ë¯¸ë¶„ê³„ìˆ˜ì˜ ê°’ì„ ê·¼ì‚¬ì ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆëŠ”ë° ì´ë¥¼ ìˆ˜ì¹˜ë¯¸ë¶„ì´ë¼ í•œë‹¤."
  },
  {
    "objectID": "posts/numpy/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì°¨ì›-ë°°ì—´ì˜-ê²½ìš°",
    "href": "posts/numpy/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì°¨ì›-ë°°ì—´ì˜-ê²½ìš°",
    "title": "Finite Difference Method with np.gradient",
    "section": "1ì°¨ì› ë°°ì—´ì˜ ê²½ìš°",
    "text": "1ì°¨ì› ë°°ì—´ì˜ ê²½ìš°\n\nEx) \\(dx\\) = 1\në„˜íŒŒì´ 1ì°¨ì› ë°°ì—´ì´ ë‹¤ìŒê³¼ ê°™ì´ ì£¼ì–´ì ¸ ìˆë‹¤ê³  í•˜ì.\n\nfrom IPython.display import display, Markdown\nf = np.array([1,2,4,7,11,16],dtype=float)\nprint(f)\n\n[ 1.  2.  4.  7. 11. 16.]\n\n\nnp.gradientí•¨ìˆ˜ëŠ” 1ì°¨ì› ë°°ì—´ì˜ ë‚´ë¶€ì— ìˆëŠ” ê°ê°ì˜ ê°’ë“¤ì€ \\(x\\)ê°’ì´ ê±°ë¦¬ê°€ \\(dx\\)=1ì”© ë³€í™”í• ë•Œë§ˆë‹¤ì˜ í•¨ìˆ«ê°’\\(f(x)\\)ë“¤ë¡œ ì´í•´í•œë‹¤. ì¦‰,ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nfor i in range(len(f)):\n    display(Markdown(rf'$x_{i+1}$ì—ì„œì˜ í•¨ìˆ«ê°’ $f(x_{i+1})$ = {f[i]}'))\n\n\\(x_1\\)ì—ì„œì˜ í•¨ìˆ«ê°’ \\(f(x_1)\\) = 1.0\n\n\n\\(x_2\\)ì—ì„œì˜ í•¨ìˆ«ê°’ \\(f(x_2)\\) = 2.0\n\n\n\\(x_3\\)ì—ì„œì˜ í•¨ìˆ«ê°’ \\(f(x_3)\\) = 4.0\n\n\n\\(x_4\\)ì—ì„œì˜ í•¨ìˆ«ê°’ \\(f(x_4)\\) = 7.0\n\n\n\\(x_5\\)ì—ì„œì˜ í•¨ìˆ«ê°’ \\(f(x_5)\\) = 11.0\n\n\n\\(x_6\\)ì—ì„œì˜ í•¨ìˆ«ê°’ \\(f(x_6)\\) = 16.0\n\n\nìœ„ì—ì„œ ë„˜íŒŒì´ì˜ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ëê°’ì„ ì œì™¸í•œ ë‚´ë¶€ì˜ ìš”ì†Œì—ëŠ” ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬ ì–‘ëê°’ì— ëŒ€í•´ì„œëŠ” í›„í–¥ì°¨ë¶„ ë˜ëŠ” ì „í–¥ì°¨ë¶„ì„ ì‚¬ìš©í•œë‹¤ê³  ì–¸ê¸‰í–ˆì—ˆë‹¤. ê³„ì‚°í•œ,\\(x_2,x_5\\)ì—ì„œ ë¯¸ë¶„ê³„ìˆ˜ì˜ 2ì°¨ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&f^{'}(x_2) \\overset{\\sim}{=} \\frac{f(x_3)-f(x_1)}{2h} = \\frac{4-1}{2} = 1.5 \\\\\n&f^{'}(x_5) \\overset{\\sim}{=} \\frac{f(x_6)-f(x_4)}{2h} = \\frac{16-7}{2} = 4.5 \\\\\n&\\text{where, } h = x_3-x_2 = x_2-x_1 = 1\n\\end{aligned}\\]\n1ì°¨ì› ë°°ì—´ì˜ ê°€ì¥ ì²˜ìŒì— ì˜¤ëŠ” ê°’ì— ì „í–¥ì°¨ë¶„ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•˜ê³  ê°€ì¥ ë§ˆì§€ë§‰ì— ì˜¤ëŠ” ê°’ì—ì„œëŠ” í›„í–¥ì°¨ë¶„ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•œë‹¤.\n\\[\\begin{aligned}\n&f^{'}(x_1) = \\frac{f(x_2) - f(x_1)}{h} = \\frac{2-1}{1} = 1 \\\\\n&f^{'}(x_6) = \\frac{f(x_6) - f(x_5)}{h} = \\frac{16-11}{1} = 5\n\\end{aligned}\\]\nê³„ì‚°í•œ ê°’ê³¼ ì‹¤ì œë¡œ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸.\n\nnum_diff = np.gradient(f)\nprint(\"np.gradientì˜ ì¶œë ¥ê°’\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradientì˜ ì¶œë ¥ê°’\n[1.  1.5 2.5 3.5 4.5 5. ]\n\n\n\\(x_1\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 1.0\n\n\n\\(x_2\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 1.5\n\n\n\\(x_3\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 2.5\n\n\n\\(x_4\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 3.5\n\n\n\\(x_5\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 4.5\n\n\n\\(x_6\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 5.0\n\n\n\n\nEx) \\(dx \\not = 1\\) (defaultê°€ ì•„ë‹ ê²½ìš°)\nê±°ë¦¬\\(dx=2\\)ì¼ë•Œ ê³„ì‚°í•œ,\\(x_2,x_5\\)ì—ì„œ ë¯¸ë¶„ê³„ìˆ˜ì˜ 2ì°¨ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&f^{'}(x_2) \\overset{\\sim}{=} \\frac{f(x_3)-f(x_1)}{2h} = \\frac{4-1}{4} = 0.75 \\\\\n&f^{'}(x_5) \\overset{\\sim}{=} \\frac{f(x_6)-f(x_4)}{2h} = \\frac{16-7}{4} = 2.25 \\\\\n&\\text{where, } h = x_3-x_1 = x_6-x_4 = 2\n\\end{aligned}\\]\nê±°ë¦¬ê°€ \\(dx=2\\)ì¼ë•Œ ë°°ì—´ì˜ ì–‘ ëê°’ì—ì„œ ì „í–¥,í›„í–¥ì°¨ë¶„ê·¼ì‚¬ë¥¼ í†µí•œ ë¯¸ë¶„ê³„ìˆ˜ì˜ ê°’ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&f^{'}(x_1) = \\frac{f(x_2) - f(x_1)}{h} = \\frac{2-1}{2} = 0.5 \\\\\n&f^{'}(x_6) = \\frac{f(x_6) - f(x_5)}{h} = \\frac{16-11}{2} = 2.5\n\\end{aligned}\\]\nxê°’ ì‚¬ì´ì˜ ê±°ë¦¬\\(dx\\)ë¥¼ ë°”ê¾¸ê³  ì‹¶ë‹¤ë©´? => ë‘ë²ˆì§¸ ì¸ìˆ˜ì— ìŠ¤ì¹¼ë¼ ëŒ€ì…í•˜ë©´ ëœë‹¤.\n\ndx = 2\nnum_diff = np.gradient(f,dx)\nprint(\"np.gradientì˜ ì¶œë ¥ê°’\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradientì˜ ì¶œë ¥ê°’\n[0.5  0.75 1.25 1.75 2.25 2.5 ]\n\n\n\\(x_1\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 0.5\n\n\n\\(x_2\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 0.75\n\n\n\\(x_3\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 1.25\n\n\n\\(x_4\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 1.75\n\n\n\\(x_5\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 2.25\n\n\n\\(x_6\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 2.5\n\n\n\n\nEx) xê°’ì˜ ì¢Œí‘œë¥¼ ì§ì ‘ ì •í•´ì£¼ëŠ” ê²½ìš°\nì´ì „ì—ëŠ” ê°ê°ì˜ ì¸ë±ìŠ¤ê°„ì˜ ê±°ë¦¬ëŠ” ëª¨ë‘ ë™ì¼í•˜ê²Œ ê¸°ë³¸ê°’ 1ì´ê±°ë‚˜ ë‹¤ë¥¸ê°’ì„ ì‚¬ìš©í–ˆë‹¤. ê·¸ëŸ¬ì§€ ì•Šê³  \\(x_1,x_2,\\dots,x_6\\)ì˜ ì¢Œí‘œë¥¼ ì§ì ‘ ì§€ì •í•´ì£¼ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤. í•¨ìˆ˜ì˜ 2ë²ˆì¬ ì¸ìˆ˜ì— ì¢Œí‘œë¥¼ ì§ì ‘ ë„£ì–´ì£¼ë©´ ëœë‹¤.\në¨¼ì € xê°’ì˜ ì¢Œí‘œë¥¼ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  í•´ë³´ì.\n\nx = np.array([0., 1., 1.5, 3.5, 4., 6.], dtype=float)\nf = np.array([1,2,4,7,11,16],dtype=float)\nprint(\"ê°ê°ì˜ ì¢Œí‘œì™€ í•¨ìˆ«ê°’\")\nfor i in range(len(x)):\n    display(Markdown(rf'$x_{i+1}$ = {x[i]}, $f(x_{i+1})$ = {f[i]}'))\n\nê°ê°ì˜ ì¢Œí‘œì™€ í•¨ìˆ«ê°’\n\n\n\\(x_1\\) = 0.0, \\(f(x_1)\\) = 1.0\n\n\n\\(x_2\\) = 1.0, \\(f(x_2)\\) = 2.0\n\n\n\\(x_3\\) = 1.5, \\(f(x_3)\\) = 4.0\n\n\n\\(x_4\\) = 3.5, \\(f(x_4)\\) = 7.0\n\n\n\\(x_5\\) = 4.0, \\(f(x_5)\\) = 11.0\n\n\n\\(x_6\\) = 6.0, \\(f(x_6)\\) = 16.0\n\n\nê°ê°ì˜ ì¢Œí‘œì—ì„œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ì„ êµ¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.(ìˆ˜ì‹ ê³„ì‚°ì€ ì˜ ëª¨ë¥´ê² ë„¤ìš” â€¦ ì¶”í›„ì— ë” ê³µë¶€í•˜ê² ìŠµë‹ˆë‹¤!)\n\nnum_diff = np.gradient(f,x)\nprint(\"np.gradientì˜ ì¶œë ¥ê°’\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradientì˜ ì¶œë ¥ê°’\n[1.  3.  3.5 6.7 6.9 2.5]\n\n\n\\(x_1\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 1.0\n\n\n\\(x_2\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 2.9999999999999996\n\n\n\\(x_3\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 3.5\n\n\n\\(x_4\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 6.700000000000001\n\n\n\\(x_5\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 6.899999999999999\n\n\n\\(x_6\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 2.5"
  },
  {
    "objectID": "posts/numpy/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì°¨ì›-ë°°ì—´ì˜-ê²½ìš°-1",
    "href": "posts/numpy/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì°¨ì›-ë°°ì—´ì˜-ê²½ìš°-1",
    "title": "Finite Difference Method with np.gradient",
    "section": "2ì°¨ì› ë°°ì—´ì˜ ê²½ìš°",
    "text": "2ì°¨ì› ë°°ì—´ì˜ ê²½ìš°\n2ì°¨ì› ë°°ì—´ì˜ ê²½ìš° axis=0(ì„¸ë¡œì¶•)ê³¼ axis=1(ê°€ë¡œì¶•) ë‘ ì¶•ë°©í–¥ìœ¼ë¡œ ê³„ì‚°í•œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ì„ ë°˜í™˜í•œë‹¤.axis=0ì¼ ê²½ìš° ê°ê°ì˜ ì—´ë§ˆë‹¤ ë”°ë¡œë”°ë¡œ ë…ë¦½ì ìœ¼ë¡œ \\(x_1,x_2...\\)ì— ëŒ€í•œ í•¨ìˆ«ê°’\\(f(x_1),f(x_2),\\dots\\)ì´ ìˆë‹¤ê³  ìƒê°í•˜ë©´ ë˜ê³  axis=1ì¼ ê²½ìš° ê°ê°ì˜ í–‰ë§ˆë‹¤ ë”°ë¡œë”°ë¡œ ë…ë¦½ì ìœ¼ë¡œ \\(x_1,x_2...\\)ì— ëŒ€í•œ í•¨ìˆ«ê°’\\(f(x_1),f(x_2),\\dots\\)ì´ ìˆë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤.ë˜í•œ 1ì°¨ì› ë°°ì—´ê³¼ ìœ ì‚¬í•˜ê²Œ ê°ê°ì˜ í–‰,ì—´ì˜ ëê°’ì—ëŠ” ì „í–¥orí›„í–¥ì°¨ë¶„ê·¼ì‚¬ë¥¼ í–‰,ì—´ì˜ ë‚´ë¶€ì— ìˆëŠ” ê°’ì€ ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•œë‹¤.\n\nEx) \\(dx=1,dy=1\\)\n2ì°¨ì› ë°°ì—´ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nnp.array([[1, 2, 6], [3, 4, 5]], dtype=float)\n\narray([[1., 2., 6.],\n       [3., 4., 5.]])\n\n\n\nax0_difcoef,ax1_difcoef= np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float))\nprint(f'axis = 0 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \\n{ax0_difcoef}')\nprint(f'axis = 1 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \\n{ax1_difcoef}')\n\naxis = 0 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \n[[ 2.  2. -1.]\n [ 2.  2. -1.]]\naxis = 1 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \n[[1.  2.5 4. ]\n [1.  1.  1. ]]\n\n\n\n\nEx) \\(dx \\not = 1,dy \\not = 1\\) (defaultê°€ ì•„ë‹Œ ê²½ìš°)\nê°ê°ì˜ í–‰,ì—´ë§ˆë‹¤ ê±°ë¦¬ë¥¼ ë”°ë¡œ ì„¤ì •í•´ì£¼ê³  ì‹¶ì€ ê²½ìš°? => ìŠ¤ì¹¼ë¼ 2ê°œ ì¸ìˆ˜ë¡œ ì „ë‹¬.\n\ndx = 2;dy = 2\nax0_difcoef,ax1_difcoef= np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float),dx,dy)\nprint(f'axis = 0 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \\n{ax0_difcoef}')\nprint(f'axis = 1 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \\n{ax1_difcoef}')\n\naxis = 0 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \n[[ 1.   1.  -0.5]\n [ 1.   1.  -0.5]]\naxis = 1 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \n[[0.5  1.25 2.  ]\n [0.5  0.5  0.5 ]]"
  },
  {
    "objectID": "posts/numpy/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì „í–¥ì°¨ë¶„ê·¼ì‚¬-ìœ ë„",
    "href": "posts/numpy/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì „í–¥ì°¨ë¶„ê·¼ì‚¬-ìœ ë„",
    "title": "Finite Difference Method with np.gradient",
    "section": "ì „í–¥ì°¨ë¶„ê·¼ì‚¬ ìœ ë„",
    "text": "ì „í–¥ì°¨ë¶„ê·¼ì‚¬ ìœ ë„\n\\(x_1,x_2,\\dots,x_{i-1},x_i,x_{i+1},\\dots,x_n\\)ê³¼ ê°ê°ì— ëŒ€ì‘í•˜ëŠ” í•¨ìˆ«ê°’ \\(f(x_1),f(x_2),\\dots,f(x_{i-1}),f(x_i),f(x_{i+1}),\\dots,f(x_n)\\) ì£¼ì–´ì§„ ë°ì´í„°ë¼ê³  ê°€ì •í•˜ì. ëª©ì ì€ x_iì—ì„œì˜ ë¯¸ë¶„ê³„ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. \\(a = x_i\\)ì—ì„œ í•¨ìˆ˜\\(f(x)\\)ì˜ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ ê·¼ì‚¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f(x) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x-x_i) + \\frac{f^{''}(x_i)}{2!}(x-x_i)^2 + \\dots \\]\n\\(x=x_{i+1}\\)ì—ì„œì˜ í•¨ìˆ«ê°’ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f(x_{i+1}) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x_{i+1}-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x_{i+1}-x_i) + \\frac{f^{''}(x_i)}{2!}(x_{i+1}-x_i)^2 + \\dots \\]\n\\(f'(x_i)\\)ê°€ í¬í•¨ëœí•­ë§Œ ë‚¨ê²¨ë‘ê³  ë‚˜ë¨¸ì§€ëŠ” ì´í•­í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f^{'}(x_i)(x_{i+1}-x_i) = f(x_{i+1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}(x_{i+1}-x_i)^2 + \\dots\\]\n\\(h = x_{i+1}-x_i\\)ë¡œ ë‘ê³  ì–‘ë³€ì„ hë¡œ ë‚˜ëˆ„ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f'(x_i) = \\frac{f(x_{i+1})}{h} - \\frac{f(x_i)}{h} - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!}\\]\nì—¬ê¸°ì„œ ìš°ë³€ì˜ ë‘ê°œì˜ í•­ë§Œ ë‚¨ê²¨ë‘ê³  \\(O(h) = - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\)ë¼ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i+1})-f(x_i)}{h}\\\\\n&O(h) = - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\\\\n&\\text{where, } h = x_{i+1} - x_i \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/numpy/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#í›„í–¥ì°¨ë¶„ê·¼ì‚¬-ìœ ë„",
    "href": "posts/numpy/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#í›„í–¥ì°¨ë¶„ê·¼ì‚¬-ìœ ë„",
    "title": "Finite Difference Method with np.gradient",
    "section": "í›„í–¥ì°¨ë¶„ê·¼ì‚¬ ìœ ë„",
    "text": "í›„í–¥ì°¨ë¶„ê·¼ì‚¬ ìœ ë„\n\\(x_1,x_2,\\dots,x_{i-1},x_i,x_{i+1},\\dots,x_n\\)ê³¼ ê°ê°ì— ëŒ€ì‘í•˜ëŠ” í•¨ìˆ«ê°’ \\(f(x_1),f(x_2),\\dots,f(x_{i-1}),f(x_i),f(x_{i+1}),\\dots,f(x_n)\\) ì£¼ì–´ì§„ ë°ì´í„°ë¼ê³  ê°€ì •í•˜ì. ëª©ì ì€ x_iì—ì„œì˜ ë¯¸ë¶„ê³„ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. \\(a = x_i\\)ì—ì„œ í•¨ìˆ˜\\(f(x)\\)ì˜ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ ê·¼ì‚¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f(x) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x-x_i) + \\frac{f^{''}(x_i)}{2!}(x-x_i)^2 + \\dots \\]\n\\(f(x_i)\\)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f(x_{i-1}) = \\sum_{n=0}^{\\infty}\\frac{f^n(x_i)}{n!}(x_{i-1}-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x_{i-1}-x_i) + \\frac{f^{''}(x_i)}{2!}(x_{i-1}-x_i)^2+\\dots \\]\n1ì°¨ë¯¸ë¶„ì´ í¬í•¨ëœ í•­ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì´í•­í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f^{'}(x_i)(x_{i-1}-x_i) = f(x_{i-1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}(x_{i-1}-x_i)^2-\\frac{f^{'''}(x_i)}{3!}(x_{i-1}-x_i)^3-\\dots \\]\n\\(h = x_{i} - x_{i-1}\\)ë¡œ ë†“ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f^{'}(x_i)(-h) = f(x_{i-1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}h^2+\\frac{f^{'''}(x_i)}{3!}h^3-\\dots \\]\nì–‘ë³€ì„ \\(-h\\)ë¡œ ë‚˜ëˆ„ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\nf^{'}(x_i) &= \\frac{f(x_{i-1})}{-h} + \\frac{f(x_i)}{h} + \\frac{f^{''}(x_i)}{2!}h-\\frac{f^{'''}(x_i)}{3!}h^2+\\dots \\\\\n&=\\frac{f(x_i)-f(x_{i-1}) }{h} +  \\frac{f^{''}(x_i)}{2!}h-\\frac{f^{'''}(x_i)}{3!}h^2+\\dots\n\\end{aligned}\\]\në§ˆì°¬ê°€ì§€ë¡œ ìš°ë³€ì˜ ë‘ê°œ í•­ë§Œ ë‚¨ê²¨ë‘ê³  \\(O(h) = \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} + \\dots\\)ë¼ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i})-f(x_{i-1})}{h}\\\\\n&O(h) = \\frac{h}{2!}f^{''}(x_i) - \\frac{h^2}{3!}f{'''}(x_i)+\\dots \\\\\n&\\text{where, } h = x_{i} - x_{i-1}, \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/numpy/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬-ìœ ë„",
    "href": "posts/numpy/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬-ìœ ë„",
    "title": "Finite Difference Method with np.gradient",
    "section": "ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬ ìœ ë„",
    "text": "ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬ ìœ ë„\nì „í–¥ì°¨ë¶„ê·¼ì‚¬ì™€ í›„í–¥ì°¨ë¶„ê·¼ì‚¬ì˜ ìœ ë„ê³¼ì •ì—ì„œì˜ í…Œì¼ëŸ¬ ì „ê°œì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f'(x_i) = \\frac{f(x_{i+1})}{h} - \\frac{f(x_i)}{h} - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - ...\\] \\[f'(x_i) = \\frac{f(x_{i})}{h} - \\frac{f(x_{i-1})}{h} + \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} + ...\\]\në‘ ì‹ì„ ë”í•´ì£¼ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&2f^{'}(x_i) = \\frac{f(x_{i+1})-f(x_{i-1})}{h} - \\frac{2h^2f^{'''}(x_i)}{3!} \\\\\n&\\Leftrightarrow f^{'}(x_i) = \\frac{f(x_{i+1})-f(x_{i-1})}{2h} - \\frac{h^2f^{'''}(x_i)}{3!} - ...\n\\end{aligned}\\]\në§ˆì°¬ê°€ì§€ë¡œ ìš°ë³€ì˜ ë‘ê°œ í•­ë§Œ ë‚¨ê²¨ë‘ê³  ì ˆë‹¨ì˜¤ì°¨\\(O(h^2) = -\\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\)ë¼ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i+1})-f(x_{i-1})}{2h}\\\\\n&O(h) = - \\frac{h^2}{3!}f{'''}(x_i)+\\dots \\\\\n&\\text{where, } h = x_{i} - x_{i-1}\\,\\,\\text{or}\\,\\, h = x_{i+1} - x_i\\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html",
    "href": "posts/open/Dakon competetion chisquare test.html",
    "title": "Untitled",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom scipy.stats import chisquare\nimport warnings\nwarnings.filterwarnings('ignore')\ntest_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/test.csv\"\ntrain_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/train.csv\"\n\n\nclass CFG:\n    SEED = 42\n\n\ntrain = pd.read_csv(train_path)\ntrain_len = len(train)\ntest = pd.read_csv(test_path)\nid_test = test[\"id\"]\ntest = pd.read_csv(test_path)\n\n\ndataset = pd.concat([train,test],axis=0).reset_index(drop=True)\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      id\n      father\n      mother\n      gender\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      ...\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      0\n      0\n      0\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      ...\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      0\n      0\n      0\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      ...\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      0\n      0\n      0\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      ...\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      TRAIN_003\n      0\n      0\n      0\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      ...\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      TRAIN_004\n      0\n      0\n      0\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      ...\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      0\n      0\n      0\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      ...\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      0\n      0\n      0\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      ...\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      0\n      0\n      0\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      ...\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      0\n      0\n      0\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      ...\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      0\n      0\n      0\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      ...\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows Ã— 21 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#fathermothergender",
    "href": "posts/open/Dakon competetion chisquare test.html#fathermothergender",
    "title": "Untitled",
    "section": "father,mother,gender",
    "text": "father,mother,gender\n\nfather,mother,gender = 0 => drop\n\n\ndataset = dataset.drop(columns = [\"father\",\"mother\",\"gender\"])\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      id\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      TRAIN_003\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      TRAIN_004\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows Ã— 18 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_01",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_01",
    "title": "Untitled",
    "section": "SNP_01",
    "text": "SNP_01\n\nclass = B ì¸ ê²½ìš°, AAëŠ” ì•„ì˜ˆ ì—†ìŒ = > feature extraction hasGG ì¶”ê°€\n\nì¶”í›„ ê³ ë ¤ì‚¬í•­ - class = A ì¸ ê²½ìš°, AAê°€ ì¢€ ë†’ìŒ - class = B ì¸ ê²½ìš°, GGê°€ ì••ë„ì ìœ¼ë¡œ ë†’ìŒ - class = C ì¸ ê²½ìš°, GGê°€ ì¢€ ë†’ìŒ\n\ncol = \"SNP_01\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_02",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_02",
    "title": "Untitled",
    "section": "SNP_02",
    "text": "SNP_02\n\nclass = Aì¸ ê²½ìš°, AAëŠ” ì—†ìŒ => f.e has02AA\n\nì¶”í›„ ê³ ë ¤ - B,ì˜ ê²½ìš° AG->GG-AA ìˆœ - C,ì˜ ê²½ìš° AG->GG->AA ìˆœ\n\ncol = \"SNP_02\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_03",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_03",
    "title": "Untitled",
    "section": "SNP_03",
    "text": "SNP_03\n\nAì¸ ê²½ìš° , ë¬´ì¡°ê±´ AAë§Œ ì¡´ì¬ => feature extraction\n\nì¶”í›„ ê³ ë ¤ ë‚˜ë¨¸ì§€ëŠ” ë­ ëŒ€ì¶© ê³ ë¥´ê²Œ\n\ncol = \"SNP_03\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    plt.ylim(20,60)\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_04",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_04",
    "title": "Untitled",
    "section": "SNP_04",
    "text": "SNP_04\n\nCì¸ ê²½ìš° , GGëŠ” ì—†ìŒ => feature extraction has04GG\n\nì¶”í›„ ê³ ë ¤ ë‚˜ë¨¸ì§€ëŠ” ë­ ëŒ€ì¶© ê³ ë¥´ê²Œ - class Aì¸ ê²½ìš°,AAê°€ ì••ë„ì ìœ¼ë¡œ ë‚®ìŒ => feature extraction\n\ncol = \"SNP_04\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_05",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_05",
    "title": "Untitled",
    "section": "SNP_05",
    "text": "SNP_05\n\nclass = Aì¸ ê²½ìš°, CCëŠ” ì—†ìŒ\n\n\ncol = \"SNP_05\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_06",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_06",
    "title": "Untitled",
    "section": "SNP_06",
    "text": "SNP_06\n\nclass = Aì¸ ê²½ìš°, AAëŠ” ì—†ìŒ => feature extraction has06AA\n\n\ncol = \"SNP_06\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_07",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_07",
    "title": "Untitled",
    "section": "SNP_07",
    "text": "SNP_07\n\nAì˜ ê²½ìš° => AAëŠ” ì—†ìŒ =>f.e has07AA\nB,Cì˜ ê²½ìš° => GGëŠ” ì—†ìŒ=>f.e has07GG\n\n\ncol = \"SNP_07\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_08",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_08",
    "title": "Untitled",
    "section": "SNP_08",
    "text": "SNP_08\n\nAì˜ ê²½ìš° => GGëŠ” ì—†ìŒ =>f.e has08GG\n\n\ncol = \"SNP_08\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_09",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_09",
    "title": "Untitled",
    "section": "SNP_09",
    "text": "SNP_09\n\nCì˜ ê²½ìš° => ëŒ€ë¶€ë¶„ì´ AAì´ê³  GAì™€ GGëŠ” ì‚¬ì‹¤ ì—†ë‹¤ê³  ë´ë„ ë¬´ë°©=> f.e has09AA\nBì˜ ê²½ìš° => GGëŠ” ê±°ì˜ ì—†ìŒ => f.e has09GG\n\nì•ì„  ê²½ìš°ë“¤ì€ ë¹ˆë„ìˆ˜ê°€ ë‚®ì€ ê²½ìš°ë„¤ëŠ” ë”°ë¡œ feature extractionì„ ì•ˆí•´ì¤¬ëŠ”ë° ì™œ ì—¬ê¸°ì„œëŠ” í•´? => ì—¬ê¸°ì„œëŠ” ë‚®ì€ ê²ƒë“¤ì˜ ë¹ˆë„ìˆ˜ê°€ 1ë¡œ ë„ˆë¬´ ë‚®ìŒ,ê·¸ë˜ì„œ ì—¬ê¸°ëŠ” í•¨.\n\ncol = \"SNP_09\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset.loc[dataset[\"class\"] == \"B\",col].value_counts()\n\nA A    91\nG A    22\nG G     1\nName: SNP_09, dtype: int64\n\n\n\ndataset.loc[dataset[\"class\"] == \"C\",col].value_counts()\n\nA A    78\nG A     1\nName: SNP_09, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_10",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_10",
    "title": "Untitled",
    "section": "SNP_10",
    "text": "SNP_10\n\nAì™€ Bì—ì„œ ë‚®ì€ê°’ë“¤ì´ ìˆê¸´ í•œë° â€¦ ê·¸ë˜ë„ íŠ¹ì´ì  1ì¸ ì •ë„ëŠ” ì•„ë‹˜\n\n\ncol = \"SNP_10\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"B\",col]\nx.value_counts()\n\nG G    110\nA G      4\nName: SNP_10, dtype: int64\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"A\",col]\nx.value_counts()\n\nA G    34\nA A    32\nG G     3\nName: SNP_10, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_11",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_11",
    "title": "Untitled",
    "section": "SNP_11",
    "text": "SNP_11\n\nAì˜ ê²½ìš° => AAëŠ” ì—†ìŒ =>f.e has11AA\n\n\ncol = \"SNP_11\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_12",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_12",
    "title": "Untitled",
    "section": "SNP_12",
    "text": "SNP_12\n\nAì˜ ê²½ìš° => AAê°€ ìˆëŠ” ê´€ì¸¡ì¹˜ê°€ ê±°ì˜ ì—†ìŒ,GGê°€ ìˆëŠ” ê´€ì¸¡ì¹˜ëŠ” ë§ìŒ\nB,Cì˜ ê²½ìš° => GGê°€ ìˆëŠ” ê´€ì¸¡ì¹˜ê°€ ê±°ì˜ ì—†ìŒ,AAê°€ ìˆëŠ” ê´€ì¸¡ì¹˜ëŠ” ë§ìŒ\n\n=>f.e has12AA =>f.e has12GG\n\ncol = \"SNP_12\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset.loc[dataset[\"class\"] == \"A\",col].value_counts()\n\nG G    48\nG A    20\nA A     1\nName: SNP_12, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_13",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_13",
    "title": "Untitled",
    "section": "SNP_13",
    "text": "SNP_13\n\nAì˜ ê²½ìš° => AAëŠ” ì—†ìŒ => f.e has13AA\n\n\ncol = \"SNP_13\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"A\",col]\nx.value_counts()\n\nG G    66\nA G     3\nName: SNP_13, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_14",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_14",
    "title": "Untitled",
    "section": "SNP_14",
    "text": "SNP_14\n\nBì˜ ê²½ìš° => AAë§Œ ì¡´ì¬ =>f.e has14AA ë¥¼ ì¶”ê°€í•´ì„œ AAì¸ê²ƒë“¤ì˜ ê³„ìˆ˜ë¥¼ ê²°ì •í•˜ë„ë¡\nCì˜ ê²½ìš° CCê°€ ìˆê¸´ í•œë° .. ì—„ì²­ë‚®ê¸´í•¨(ê·¸ë˜ë„ 1ì€ ì•„ë‹ˆë‹ˆê¹Œ f.eëŠ” ì•ˆí•¨)\n\n\ncol = \"SNP_14\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"C\",col]\nx.value_counts()\n\nA A    60\nC A    17\nC C     2\nName: SNP_14, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#snp_15",
    "href": "posts/open/Dakon competetion chisquare test.html#snp_15",
    "title": "Untitled",
    "section": "SNP_15",
    "text": "SNP_15\n\në³„ë‹¤ë¥¸ íŠ¹ì§• ì—†ìŒ\n\n\ncol = \"SNP_15\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      id\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      TRAIN_003\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      TRAIN_004\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows Ã— 18 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#classë³„-ê´€ì¸¡ì¹˜ì˜-ìˆ«ì",
    "href": "posts/open/Dakon competetion chisquare test.html#classë³„-ê´€ì¸¡ì¹˜ì˜-ìˆ«ì",
    "title": "Untitled",
    "section": "classë³„ ê´€ì¸¡ì¹˜ì˜ ìˆ«ì",
    "text": "classë³„ ê´€ì¸¡ì¹˜ì˜ ìˆ«ì\n\nclass imbalance? => No\n\n\nx = dataset[\"class\"].value_counts().index\ny = dataset[\"class\"].value_counts().values\nplt.bar(x,y)\n\n<BarContainer object of 3 artists>"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#chi2-test-between-abc",
    "href": "posts/open/Dakon competetion chisquare test.html#chi2-test-between-abc",
    "title": "Untitled",
    "section": "chi^2 test, between [â€œAâ€,â€œBâ€,â€œCâ€]",
    "text": "chi^2 test, between [â€œAâ€,â€œBâ€,â€œCâ€]\n\nëª©ì  : ê°ê°ì˜ í´ë˜ìŠ¤ì—ì„œ ê°ê°ì˜ SNPë³€ìˆ˜ì—ì„œ ë‚˜ì˜¤ëŠ” G G or A g ë“±ë“±.. ì˜ ê°’ì˜ ë¹ˆë„ìˆ˜ê°€ ê°™ì€ì§€ ì•„ë‹ˆë©´ ë‹¤ë¥¸ì§€ ì¹´ì´ì œê³±ê²€ì •ì„ í†µí•´ì„œ íŒŒì•…í•œ í›„, í´ë˜ìŠ¤ë³„ë¡œ ë¹ˆë„ìˆ˜ ì°¨ì´ìˆìœ¼ë©´ ê·¸ í–‰ë§Œ ì¶”ê°€\n\n\n\"\"\"\n#1 ê°ê°ì˜ ë…ë¦½ë³€ìˆ˜(ê²€ì •ì—ì„œëŠ” class)ë³„ë¡œ SNP_01ì˜ íŠ¹ì •ê°’(ì˜ˆë¥¼ ë“¤ë©´ GG)ê°€ ë‚˜ì˜¤ëŠ” ê´€ì¸¡ì¹˜ ì„¸ì–´ë³´ê¸°\n\n#ì»¬ëŸ¼,í´ë˜ìŠ¤ ì§€ì •\nsnp_name = [col_name for col_name in dataset.columns if \"SNP\" in col_name]\nclass_name = [\"A\",\"B\",\"C\"]\n\nsignif_col = []\nunsignif_col = []\nfor snp in snp_name:\n    _snp_unique = dataset.loc[:,snp].unique().tolist()\n    for unq_vl in _snp_unique:\n        _condition = (dataset.loc[:,snp] == unq_vl)\n        _data = dataset.loc[_condition,[snp,\"class\"]].dropna(axis=0).value_counts().droplevel(axis=0,level=0).copy()\n        _data_class = _data.index\n        for cl_name in class_name:\n            if cl_name not in _data_class:\n                #print(\"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” class : \",cl_name)\n                #print(\"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í´ë˜ìŠ¤ ì¶”ê°€í›„ df\")\n                _data = _data.append(pd.Series({cl_name:0}))\n\n        _data = _data[class_name]\n        #íŠ¹ì •ì»¬ëŸ¼ì˜ íŠ¹ì •ê°’ì— ëŒ€í•´ì„œ ì¹´ì´ì œê³±ê²€ì • ìˆ˜í–‰\n        #ì˜ˆë¥¼ ë“¤ì–´ì„œ ê°ê°ì˜ í´ë˜ìŠ¤ A,B,Cì—ì„œ SNP_01ì˜ A Aê°’ì„ ê°€ì§€ëŠ” ë¹ˆë„ê°€ ê°™ì€ì§€ ë‹¤ë¥¸ì§€ ìˆ˜í–‰\n        f_obs = _data.values.tolist()\n        p_value = chisquare(f_obs)[1].round(5)\n        if p_value < 0.01: #ìœ ì˜í™•ë¥  0.01\n            #print(f\"{snp}={unq_vl}\")\n            #print(\"p_value :\",p_value)\n            name = snp+\"=\"+unq_vl\n            signif_col.append(name)\n        else:\n            name = snp+\"=\"+unq_vl\n            unsignif_col.append(name)\nlen(signif_col),len(unsignif_col)\n\"\"\"\n\n'\\n#1 ê°ê°ì˜ ë…ë¦½ë³€ìˆ˜(ê²€ì •ì—ì„œëŠ” class)ë³„ë¡œ SNP_01ì˜ íŠ¹ì •ê°’(ì˜ˆë¥¼ ë“¤ë©´ GG)ê°€ ë‚˜ì˜¤ëŠ” ê´€ì¸¡ì¹˜ ì„¸ì–´ë³´ê¸°\\n\\n#ì»¬ëŸ¼,í´ë˜ìŠ¤ ì§€ì •\\nsnp_name = [col_name for col_name in dataset.columns if \"SNP\" in col_name]\\nclass_name = [\"A\",\"B\",\"C\"]\\n\\nsignif_col = []\\nunsignif_col = []\\nfor snp in snp_name:\\n    _snp_unique = dataset.loc[:,snp].unique().tolist()\\n    for unq_vl in _snp_unique:\\n        _condition = (dataset.loc[:,snp] == unq_vl)\\n        _data = dataset.loc[_condition,[snp,\"class\"]].dropna(axis=0).value_counts().droplevel(axis=0,level=0).copy()\\n        _data_class = _data.index\\n        for cl_name in class_name:\\n            if cl_name not in _data_class:\\n                #print(\"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” class : \",cl_name)\\n                #print(\"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í´ë˜ìŠ¤ ì¶”ê°€í›„ df\")\\n                _data = _data.append(pd.Series({cl_name:0}))\\n\\n        _data = _data[class_name]\\n        #íŠ¹ì •ì»¬ëŸ¼ì˜ íŠ¹ì •ê°’ì— ëŒ€í•´ì„œ ì¹´ì´ì œê³±ê²€ì • ìˆ˜í–‰\\n        #ì˜ˆë¥¼ ë“¤ì–´ì„œ ê°ê°ì˜ í´ë˜ìŠ¤ A,B,Cì—ì„œ SNP_01ì˜ A Aê°’ì„ ê°€ì§€ëŠ” ë¹ˆë„ê°€ ê°™ì€ì§€ ë‹¤ë¥¸ì§€ ìˆ˜í–‰\\n        f_obs = _data.values.tolist()\\n        p_value = chisquare(f_obs)[1].round(5)\\n        if p_value < 0.01: #ìœ ì˜í™•ë¥  0.01\\n            #print(f\"{snp}={unq_vl}\")\\n            #print(\"p_value :\",p_value)\\n            name = snp+\"=\"+unq_vl\\n            signif_col.append(name)\\n        else:\\n            name = snp+\"=\"+unq_vl\\n            unsignif_col.append(name)\\nlen(signif_col),len(unsignif_col)\\n'"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#chi2-test-between-bc",
    "href": "posts/open/Dakon competetion chisquare test.html#chi2-test-between-bc",
    "title": "Untitled",
    "section": "chi^2 test, between â€œBâ€,â€œCâ€",
    "text": "chi^2 test, between â€œBâ€,â€œCâ€\n\nëª©ì  : ê°ê°ì˜ í´ë˜ìŠ¤ì—ì„œ ê°ê°ì˜ SNPë³€ìˆ˜ì—ì„œ ë‚˜ì˜¤ëŠ” G G or A g ë“±ë“±.. ì˜ ê°’ì˜ ë¹ˆë„ìˆ˜ê°€ ê°™ì€ì§€ ì•„ë‹ˆë©´ ë‹¤ë¥¸ì§€ ì¹´ì´ì œê³±ê²€ì •ì„ í†µí•´ì„œ íŒŒì•…í•œ í›„, í´ë˜ìŠ¤ë³„ë¡œ ë¹ˆë„ìˆ˜ ì°¨ì´ìˆìœ¼ë©´ ê·¸ í–‰ë§Œ ì¶”ê°€\n\n\n# EDAê³¼ì •ì—ì„œ trait == 1 ì´ë©´ ë°˜ë“œì‹œ Aì˜€ìŒ,ë”°ë¼ì„œ trait ë³€ìˆ˜ëŠ” ì œê±°í•˜ê³  ë‚˜ì¤‘ì— trait == 1ì´ë©´ ë°˜ë“œì‹œ 1ë¡œ ì œì¶œ\nidx = dataset[dataset.trait == 1].index\n_dt = dataset.drop(index=idx)\n\n\n_dt\n\n\n\n\n\n  \n    \n      \n      id\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      4\n      TRAIN_004\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      5\n      TRAIN_005\n      2\n      G G\n      G G\n      C A\n      A A\n      C C\n      A A\n      A A\n      G A\n      A A\n      G G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n317 rows Ã— 18 columns\n\n\n\n\n#1 ê°ê°ì˜ ë…ë¦½ë³€ìˆ˜(ê²€ì •ì—ì„œëŠ” class)ë³„ë¡œ SNP_01ì˜ íŠ¹ì •ê°’(ì˜ˆë¥¼ ë“¤ë©´ GG)ê°€ ë‚˜ì˜¤ëŠ” ê´€ì¸¡ì¹˜ ì„¸ì–´ë³´ê¸°\n\n#ì»¬ëŸ¼,í´ë˜ìŠ¤ ì§€ì •\nsnp_name = [col_name for col_name in _dt.columns if \"SNP\" in col_name]\nclass_name = [\"B\",\"C\"]\n\nsignif_col = []\nunsignif_col = []\nfor snp in snp_name:\n    _snp_unique = _dt.loc[:,snp].unique().tolist()\n    for unq_vl in _snp_unique:\n        _condition = (_dt.loc[:,snp] == unq_vl)\n        _data = _dt.loc[_condition,[snp,\"class\"]].dropna(axis=0).value_counts().droplevel(axis=0,level=0).copy()\n        _data_class = _data.index\n        for cl_name in class_name:\n            if cl_name not in _data_class:\n                #print(\"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” class : \",cl_name)\n                #print(\"ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í´ë˜ìŠ¤ ì¶”ê°€í›„ df\")\n                _data = _data.append(pd.Series({cl_name:0}))\n\n        _data = _data[class_name]\n        #íŠ¹ì •ì»¬ëŸ¼ì˜ íŠ¹ì •ê°’ì— ëŒ€í•´ì„œ ì¹´ì´ì œê³±ê²€ì • ìˆ˜í–‰\n        #ì˜ˆë¥¼ ë“¤ì–´ì„œ ê°ê°ì˜ í´ë˜ìŠ¤ A,B,Cì—ì„œ SNP_01ì˜ A Aê°’ì„ ê°€ì§€ëŠ” ë¹ˆë„ê°€ ê°™ì€ì§€ ë‹¤ë¥¸ì§€ ìˆ˜í–‰\n        f_obs = _data.values.tolist()\n        p_value = chisquare(f_obs)[1].round(5)\n        if p_value < 0.05: #ìœ ì˜í™•ë¥  0.01\n            #print(f\"{snp}={unq_vl}\")\n            #print(\"p_value :\",p_value)\n            name = snp+\"_\"+unq_vl\n            signif_col.append(name)\n        else:\n            name = snp+\"_\"+unq_vl\n            unsignif_col.append(name)\nlen(signif_col),len(unsignif_col)\n\n(27, 17)\n\n\n\nsignif_col\n\n['SNP_01_G G',\n 'SNP_01_A A',\n 'SNP_02_A G',\n 'SNP_02_G G',\n 'SNP_02_A A',\n 'SNP_03_C A',\n 'SNP_03_C C',\n 'SNP_04_G A',\n 'SNP_04_A A',\n 'SNP_04_G G',\n 'SNP_05_A A',\n 'SNP_05_C C',\n 'SNP_06_A G',\n 'SNP_07_A A',\n 'SNP_07_G A',\n 'SNP_08_G G',\n 'SNP_08_A A',\n 'SNP_09_G A',\n 'SNP_10_G G',\n 'SNP_10_A G',\n 'SNP_10_A A',\n 'SNP_11_A G',\n 'SNP_11_G G',\n 'SNP_13_A A',\n 'SNP_14_A A',\n 'SNP_14_C A',\n 'SNP_15_A A']\n\n\n\nunsignif_col\n\n['SNP_01_A G',\n 'SNP_03_A A',\n 'SNP_05_C A',\n 'SNP_06_A A',\n 'SNP_06_G G',\n 'SNP_08_G A',\n 'SNP_09_A A',\n 'SNP_09_G G',\n 'SNP_11_A A',\n 'SNP_12_A A',\n 'SNP_12_G A',\n 'SNP_12_G G',\n 'SNP_13_G G',\n 'SNP_13_A G',\n 'SNP_14_C C',\n 'SNP_15_G A',\n 'SNP_15_G G']\n\n\n\n\"\"\"\nimport scipy.stats as stats\nimport numpy as np\n  \n# Make a 3 x 3 table\ndataset = np.array([[13, 17, 11], [4, 6, 9],\n                    [20, 31, 42]])\n  \n# Finding Chi-squared test statistic,\n# sample size, and minimum of rows\n# and columns\nX2 = stats.chi2_contingency(dataset, correction=False)\nN = np.sum(dataset)\nminimum_dimension = min(dataset.shape)-1\nX2\n\"\"\"\n\n'\\nimport scipy.stats as stats\\nimport numpy as np\\n  \\n# Make a 3 x 3 table\\ndataset = np.array([[13, 17, 11], [4, 6, 9],\\n                    [20, 31, 42]])\\n  \\n# Finding Chi-squared test statistic,\\n# sample size, and minimum of rows\\n# and columns\\nX2 = stats.chi2_contingency(dataset, correction=False)\\nN = np.sum(dataset)\\nminimum_dimension = min(dataset.shape)-1\\nX2\\n'"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#add-column-delete-column",
    "href": "posts/open/Dakon competetion chisquare test.html#add-column-delete-column",
    "title": "Untitled",
    "section": "add column & delete column",
    "text": "add column & delete column\n\ndef create_col(dataset,col,value):\n    _t = []\n    for val in dataset[col] == value:\n        if val == True:\n            _t.append(1)\n        else:\n            _t.append(0)\n    \n    col_name_base = \"has\"+col[-2:]\n    value_name = \"\"\n    for chr in value:\n        if chr != \" \":\n            value_name+=chr\n    col_name = col_name_base+value_name\n    print(col_name)\n    dataset[col_name] = _t\n\n    return dataset\n\n\"\"\"\ndataset = create_col(dataset,\"SNP_01\",\"G G\")\ndataset = create_col(dataset,\"SNP_02\",\"A A\")\ndataset = create_col(dataset,\"SNP_03\",\"A A\")\ndataset = create_col(dataset,\"SNP_04\",\"G G\")\ndataset = create_col(dataset,\"SNP_05\",\"C C\")\ndataset = create_col(dataset,\"SNP_06\",\"A A\")\ndataset = create_col(dataset,\"SNP_07\",\"A A\")\ndataset = create_col(dataset,\"SNP_07\",\"G G\")\ndataset = create_col(dataset,\"SNP_08\",\"G G\")\ndataset = create_col(dataset,\"SNP_09\",\"A A\")\ndataset = create_col(dataset,\"SNP_09\",\"G G\")\ndataset = create_col(dataset,\"SNP_11\",\"A A\")\ndataset = create_col(dataset,\"SNP_12\",\"A A\")\ndataset = create_col(dataset,\"SNP_12\",\"G G\")\ndataset = create_col(dataset,\"SNP_13\",\"A A\")\ndataset = create_col(dataset,\"SNP_14\",\"A A\")\n\"\"\"\n#dataset = dataset.drop(columns = [\"SNP_03\",\"SNP_04\",\"SNP_05\",\"SNP_06\",\"SNP_07\",\"SNP_08\",\"SNP_09\",\"SNP_11\",\"SNP_12\",\"SNP_13\",\"SNP_14\"])\n\n'\\ndataset = create_col(dataset,\"SNP_01\",\"G G\")\\ndataset = create_col(dataset,\"SNP_02\",\"A A\")\\ndataset = create_col(dataset,\"SNP_03\",\"A A\")\\ndataset = create_col(dataset,\"SNP_04\",\"G G\")\\ndataset = create_col(dataset,\"SNP_05\",\"C C\")\\ndataset = create_col(dataset,\"SNP_06\",\"A A\")\\ndataset = create_col(dataset,\"SNP_07\",\"A A\")\\ndataset = create_col(dataset,\"SNP_07\",\"G G\")\\ndataset = create_col(dataset,\"SNP_08\",\"G G\")\\ndataset = create_col(dataset,\"SNP_09\",\"A A\")\\ndataset = create_col(dataset,\"SNP_09\",\"G G\")\\ndataset = create_col(dataset,\"SNP_11\",\"A A\")\\ndataset = create_col(dataset,\"SNP_12\",\"A A\")\\ndataset = create_col(dataset,\"SNP_12\",\"G G\")\\ndataset = create_col(dataset,\"SNP_13\",\"A A\")\\ndataset = create_col(dataset,\"SNP_14\",\"A A\")\\n'"
  },
  {
    "objectID": "posts/open/Dakon competetion chisquare test.html#encoding",
    "href": "posts/open/Dakon competetion chisquare test.html#encoding",
    "title": "Untitled",
    "section": "encoding",
    "text": "encoding\n\n_dataset = dataset\n_dataset\n\n\n\n\n\n  \n    \n      \n      id\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      TRAIN_000\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      TRAIN_001\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      TRAIN_002\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      TRAIN_003\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      TRAIN_004\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      432\n      TEST_170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      433\n      TEST_171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      434\n      TEST_172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      435\n      TEST_173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      436\n      TEST_174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows Ã— 18 columns\n\n\n\n\nonehot-encoding trait != 1 (Aí´ë˜ìŠ¤ ë¹¼ê³ )\n\n_dt = _dataset.loc[_dataset.trait != 1,:].copy()\ncl = _dt[\"class\"]\n#print(cl)\none_hot_label = [col_name for col_name in _dt.columns if \"SNP\" in col_name]\nget_class = [\"id\"]+signif_col + [\"trait\",\"class\"]\ntrait_map = {1:0,2:1}\n\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\n_dt = pd.get_dummies(_dt,columns = one_hot_label)\n_dt[\"class\"] = cl.map(class_map)\ncond = ~pd.isna(_dt[\"class\"])\ntrain_ohe = _dt.loc[cond,:]\ntrain_ohe = train_ohe.loc[:,get_class]\ntrain_ohe[\"trait\"] = train_ohe[\"trait\"].map(trait_map)\ntrain_ohe[\"class\"] = train_ohe[\"class\"].astype(int)\n\ntest_ohe = _dt.loc[~cond,:][get_class]\ntest_ohe = test_ohe.drop(columns = \"class\")\ntest_ohe[\"trait\"] = test_ohe[\"trait\"].map(trait_map)\n\n\ntrain_id = train_ohe[\"id\"]\nX_train_ohe = train_ohe.drop(columns = [\"class\",\"id\"])\nY_train_ohe = train_ohe[\"class\"]\n\n\nX_train_ohe\n\n\n\n\n\n  \n    \n      \n      SNP_01_G G\n      SNP_01_A A\n      SNP_02_A G\n      SNP_02_G G\n      SNP_02_A A\n      SNP_03_C A\n      SNP_03_C C\n      SNP_04_G A\n      SNP_04_A A\n      SNP_04_G G\n      ...\n      SNP_10_G G\n      SNP_10_A G\n      SNP_10_A A\n      SNP_11_A G\n      SNP_11_G G\n      SNP_13_A A\n      SNP_14_A A\n      SNP_14_C A\n      SNP_15_A A\n      trait\n    \n  \n  \n    \n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      2\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      4\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      5\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      255\n      0\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      0\n      1\n      1\n      1\n      0\n      0\n      1\n    \n    \n      256\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      257\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      258\n      1\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      ...\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      261\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      1\n      ...\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n    \n  \n\n193 rows Ã— 28 columns\n\n\n\n\n\nlabel-encoding trait != 0 (Aí´ë˜ìŠ¤ ë¹¼ê³ )\n\n_dt = _dataset.loc[_dataset.trait != 1,:].copy()\ncl = _dt[\"class\"]\n#print(cl)\none_hot_label = [col_name for col_name in _dt.columns if \"SNP\" in col_name]\nget_class = [\"id\"]+signif_col + [\"trait\",\"class\"]\ntrait_map = {1:0,2:1}\n\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\n_dt = pd.get_dummies(_dt,columns = one_hot_label)\n_dt[\"class\"] = cl.map(class_map)\ncond = ~pd.isna(_dt[\"class\"])\ntrain_ohe = _dt.loc[cond,:]\ntrain_ohe = train_ohe.loc[:,get_class]\ntrain_ohe[\"trait\"] = train_ohe[\"trait\"].map(trait_map)\ntrain_ohe[\"class\"] = train_ohe[\"class\"].astype(int)\n\ntest_ohe = _dt.loc[~cond,:][get_class]\ntest_ohe = test_ohe.drop(columns = \"class\")\ntest_ohe[\"trait\"] = test_ohe[\"trait\"].map(trait_map)\n\n\n\nOne-hot encoding\n\n\"\"\"\n#one-hot encoding for distance base algorithm\ndataset_ohe = pd.get_dummies(_dataset,columns = _dataset.columns.drop(\"class\"),drop_first=True) #multicollinearityë¥¼ ë§‰ê¸°ìœ„í•œ drop_first ì˜µì…˜\ntrain_ohe = dataset_ohe[:train_len].copy()\ntest_ohe = dataset_ohe[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain_ohe[\"class\"]=train_ohe[\"class\"].map(class_map).astype(int)\nX_train_ohe = train_ohe.drop(columns = \"class\")\nY_train_ohe = train_ohe[\"class\"]\n\"\"\"\n\n'\\n#one-hot encoding for distance base algorithm\\ndataset_ohe = pd.get_dummies(_dataset,columns = _dataset.columns.drop(\"class\"),drop_first=True) #multicollinearityë¥¼ ë§‰ê¸°ìœ„í•œ drop_first ì˜µì…˜\\ntrain_ohe = dataset_ohe[:train_len].copy()\\ntest_ohe = dataset_ohe[train_len:].copy().drop(columns=\"class\")\\n\\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\\ntrain_ohe[\"class\"]=train_ohe[\"class\"].map(class_map).astype(int)\\nX_train_ohe = train_ohe.drop(columns = \"class\")\\nY_train_ohe = train_ohe[\"class\"]\\n'\n\n\n\n\nLabel encoding\n\n\"\"\"\nle_col = []\nfor col_name in dataset.columns.tolist():\n    if \"SNP\" in col_name:\n        le_col.append(col_name)\nle_col.append(\"trait\")\nfrom sklearn import preprocessing\nfor col in le_col:\n    le = preprocessing.LabelEncoder()\n    _col = dataset[col].tolist()\n    le.fit(_col)\n    dataset[col] = le.transform(_col)\ndataset\n\"\"\"\n\n'\\nle_col = []\\nfor col_name in dataset.columns.tolist():\\n    if \"SNP\" in col_name:\\n        le_col.append(col_name)\\nle_col.append(\"trait\")\\nfrom sklearn import preprocessing\\nfor col in le_col:\\n    le = preprocessing.LabelEncoder()\\n    _col = dataset[col].tolist()\\n    le.fit(_col)\\n    dataset[col] = le.transform(_col)\\ndataset\\n'\n\n\n\n\"\"\"\ntrain = dataset[:train_len].copy()\ntest = dataset[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain[\"class\"]=train[\"class\"].map(class_map).astype(int)\nX_train = train.drop(columns = \"class\")\nY_train = train[\"class\"]\n\"\"\"\n\n'\\ntrain = dataset[:train_len].copy()\\ntest = dataset[train_len:].copy().drop(columns=\"class\")\\n\\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\\ntrain[\"class\"]=train[\"class\"].map(class_map).astype(int)\\nX_train = train.drop(columns = \"class\")\\nY_train = train[\"class\"]\\n'"
  },
  {
    "objectID": "posts/open/Dakon competetion.html",
    "href": "posts/open/Dakon competetion.html",
    "title": "Untitled",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\ntest_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/test.csv\"\ntrain_path = \"C:/Users/22668/Desktop/github/sin-hoyeon/open/train.csv\"\n\n\nclass CFG:\n    SEED = 42\n\n\ntrain = pd.read_csv(train_path).drop(columns = [\"id\"])\ntrain_len = len(train)\ntest = pd.read_csv(test_path)\nid_test = test[\"id\"]\ntest = pd.read_csv(test_path).drop(columns = [\"id\"])\n\n\ndataset = pd.concat([train,test],axis=0)\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      father\n      mother\n      gender\n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      0\n      0\n      0\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      0\n      0\n      0\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      0\n      0\n      0\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      0\n      0\n      0\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      0\n      0\n      0\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      170\n      0\n      0\n      0\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      171\n      0\n      0\n      0\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      172\n      0\n      0\n      0\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      173\n      0\n      0\n      0\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      174\n      0\n      0\n      0\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows Ã— 20 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#fathermothergender",
    "href": "posts/open/Dakon competetion.html#fathermothergender",
    "title": "Untitled",
    "section": "father,mother,gender",
    "text": "father,mother,gender\n\nfather,mother,gender = 0 => drop\n\n\ndataset = dataset.drop(columns = [\"father\",\"mother\",\"gender\"])\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows Ã— 17 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_01",
    "href": "posts/open/Dakon competetion.html#snp_01",
    "title": "Untitled",
    "section": "SNP_01",
    "text": "SNP_01\n\nclass = B ì¸ ê²½ìš°, AAëŠ” ì•„ì˜ˆ ì—†ìŒ = > feature extraction hasGG ì¶”ê°€\n\nì¶”í›„ ê³ ë ¤ì‚¬í•­ - class = A ì¸ ê²½ìš°, AAê°€ ì¢€ ë†’ìŒ - class = B ì¸ ê²½ìš°, GGê°€ ì••ë„ì ìœ¼ë¡œ ë†’ìŒ - class = C ì¸ ê²½ìš°, GGê°€ ì¢€ ë†’ìŒ\n\ncol = \"SNP_01\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_02",
    "href": "posts/open/Dakon competetion.html#snp_02",
    "title": "Untitled",
    "section": "SNP_02",
    "text": "SNP_02\n\nclass = Aì¸ ê²½ìš°, AAëŠ” ì—†ìŒ => f.e has02AA\n\nì¶”í›„ ê³ ë ¤ - B,ì˜ ê²½ìš° AG->GG-AA ìˆœ - C,ì˜ ê²½ìš° AG->GG->AA ìˆœ\n\ncol = \"SNP_02\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_03",
    "href": "posts/open/Dakon competetion.html#snp_03",
    "title": "Untitled",
    "section": "SNP_03",
    "text": "SNP_03\n\nAì¸ ê²½ìš° , ë¬´ì¡°ê±´ AAë§Œ ì¡´ì¬ => feature extraction\n\nì¶”í›„ ê³ ë ¤ ë‚˜ë¨¸ì§€ëŠ” ë­ ëŒ€ì¶© ê³ ë¥´ê²Œ\n\ncol = \"SNP_03\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_04",
    "href": "posts/open/Dakon competetion.html#snp_04",
    "title": "Untitled",
    "section": "SNP_04",
    "text": "SNP_04\n\nCì¸ ê²½ìš° , GGëŠ” ì—†ìŒ => feature extraction has04GG\n\nì¶”í›„ ê³ ë ¤ ë‚˜ë¨¸ì§€ëŠ” ë­ ëŒ€ì¶© ê³ ë¥´ê²Œ - class Aì¸ ê²½ìš°,AAê°€ ì••ë„ì ìœ¼ë¡œ ë‚®ìŒ => feature extraction\n\ncol = \"SNP_04\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_05",
    "href": "posts/open/Dakon competetion.html#snp_05",
    "title": "Untitled",
    "section": "SNP_05",
    "text": "SNP_05\n\nclass = Aì¸ ê²½ìš°, CCëŠ” ì—†ìŒ\n\n\ncol = \"SNP_05\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_06",
    "href": "posts/open/Dakon competetion.html#snp_06",
    "title": "Untitled",
    "section": "SNP_06",
    "text": "SNP_06\n\nclass = Aì¸ ê²½ìš°, AAëŠ” ì—†ìŒ => feature extraction has06AA\n\n\ncol = \"SNP_06\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_07",
    "href": "posts/open/Dakon competetion.html#snp_07",
    "title": "Untitled",
    "section": "SNP_07",
    "text": "SNP_07\n\nAì˜ ê²½ìš° => AAëŠ” ì—†ìŒ =>f.e has07AA\nB,Cì˜ ê²½ìš° => GGëŠ” ì—†ìŒ=>f.e has07GG\n\n\ncol = \"SNP_07\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_08",
    "href": "posts/open/Dakon competetion.html#snp_08",
    "title": "Untitled",
    "section": "SNP_08",
    "text": "SNP_08\n\nAì˜ ê²½ìš° => GGëŠ” ì—†ìŒ =>f.e has08GG\n\n\ncol = \"SNP_08\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_09",
    "href": "posts/open/Dakon competetion.html#snp_09",
    "title": "Untitled",
    "section": "SNP_09",
    "text": "SNP_09\n\nCì˜ ê²½ìš° => ëŒ€ë¶€ë¶„ì´ AAì´ê³  GAì™€ GGëŠ” ì‚¬ì‹¤ ì—†ë‹¤ê³  ë´ë„ ë¬´ë°©=> f.e has09AA\nBì˜ ê²½ìš° => GGëŠ” ê±°ì˜ ì—†ìŒ => f.e has09GG\n\nì•ì„  ê²½ìš°ë“¤ì€ ë¹ˆë„ìˆ˜ê°€ ë‚®ì€ ê²½ìš°ë„¤ëŠ” ë”°ë¡œ feature extractionì„ ì•ˆí•´ì¤¬ëŠ”ë° ì™œ ì—¬ê¸°ì„œëŠ” í•´? => ì—¬ê¸°ì„œëŠ” ë‚®ì€ ê²ƒë“¤ì˜ ë¹ˆë„ìˆ˜ê°€ 1ë¡œ ë„ˆë¬´ ë‚®ìŒ,ê·¸ë˜ì„œ ì—¬ê¸°ëŠ” í•¨.\n\ncol = \"SNP_09\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset.loc[dataset[\"class\"] == \"B\",col].value_counts()\n\nA A    91\nG A    22\nG G     1\nName: SNP_09, dtype: int64\n\n\n\ndataset.loc[dataset[\"class\"] == \"C\",col].value_counts()\n\nA A    78\nG A     1\nName: SNP_09, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_10",
    "href": "posts/open/Dakon competetion.html#snp_10",
    "title": "Untitled",
    "section": "SNP_10",
    "text": "SNP_10\n\nAì™€ Bì—ì„œ ë‚®ì€ê°’ë“¤ì´ ìˆê¸´ í•œë° â€¦ ê·¸ë˜ë„ íŠ¹ì´ì  1ì¸ ì •ë„ëŠ” ì•„ë‹˜\n\n\ncol = \"SNP_10\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"B\",col]\nx.value_counts()\n\nG G    110\nA G      4\nName: SNP_10, dtype: int64\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"A\",col]\nx.value_counts()\n\nA G    34\nA A    32\nG G     3\nName: SNP_10, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_11",
    "href": "posts/open/Dakon competetion.html#snp_11",
    "title": "Untitled",
    "section": "SNP_11",
    "text": "SNP_11\n\nAì˜ ê²½ìš° => AAëŠ” ì—†ìŒ =>f.e has11AA\n\n\ncol = \"SNP_11\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_12",
    "href": "posts/open/Dakon competetion.html#snp_12",
    "title": "Untitled",
    "section": "SNP_12",
    "text": "SNP_12\n\nAì˜ ê²½ìš° => AAê°€ ìˆëŠ” ê´€ì¸¡ì¹˜ê°€ ê±°ì˜ ì—†ìŒ,GGê°€ ìˆëŠ” ê´€ì¸¡ì¹˜ëŠ” ë§ìŒ\nB,Cì˜ ê²½ìš° => GGê°€ ìˆëŠ” ê´€ì¸¡ì¹˜ê°€ ê±°ì˜ ì—†ìŒ,AAê°€ ìˆëŠ” ê´€ì¸¡ì¹˜ëŠ” ë§ìŒ\n\n=>f.e has12AA =>f.e has12GG\n\ncol = \"SNP_12\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\ndataset.loc[dataset[\"class\"] == \"A\",col].value_counts()\n\nG G    48\nG A    20\nA A     1\nName: SNP_12, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_13",
    "href": "posts/open/Dakon competetion.html#snp_13",
    "title": "Untitled",
    "section": "SNP_13",
    "text": "SNP_13\n\nAì˜ ê²½ìš° => AAëŠ” ì—†ìŒ => f.e has13AA\n\n\ncol = \"SNP_13\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"A\",col]\nx.value_counts()\n\nG G    66\nA G     3\nName: SNP_13, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_14",
    "href": "posts/open/Dakon competetion.html#snp_14",
    "title": "Untitled",
    "section": "SNP_14",
    "text": "SNP_14\n\nBì˜ ê²½ìš° => AAë§Œ ì¡´ì¬ =>f.e has14AA ë¥¼ ì¶”ê°€í•´ì„œ AAì¸ê²ƒë“¤ì˜ ê³„ìˆ˜ë¥¼ ê²°ì •í•˜ë„ë¡\nCì˜ ê²½ìš° CCê°€ ìˆê¸´ í•œë° .. ì—„ì²­ë‚®ê¸´í•¨(ê·¸ë˜ë„ 1ì€ ì•„ë‹ˆë‹ˆê¹Œ f.eëŠ” ì•ˆí•¨)\n\n\ncol = \"SNP_14\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")\n\n\n\n\n\nx=dataset.loc[dataset[\"class\"] == \"C\",col]\nx.value_counts()\n\nA A    60\nC A    17\nC C     2\nName: SNP_14, dtype: int64"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#snp_15",
    "href": "posts/open/Dakon competetion.html#snp_15",
    "title": "Untitled",
    "section": "SNP_15",
    "text": "SNP_15\n\në³„ë‹¤ë¥¸ íŠ¹ì§• ì—†ìŒ\n\n\ncol = \"SNP_15\"\n\nplt.subplots(1,3,figsize=(20,5))\ni=1\nfor cl in [\"A\",\"B\",\"C\"]:\n    plt.subplot(1,3,i)\n    plt.title(\"class = {}\".format(cl))\n    sns.countplot(x=dataset.loc[dataset[\"class\"] == cl,col])\n    i+=1\n#sns.countplot(dataset[dataset[\"class\"] == \"A\"],x=\"SNP_01\")"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#classë³„-ê´€ì¸¡ì¹˜ì˜-ìˆ«ì",
    "href": "posts/open/Dakon competetion.html#classë³„-ê´€ì¸¡ì¹˜ì˜-ìˆ«ì",
    "title": "Untitled",
    "section": "classë³„ ê´€ì¸¡ì¹˜ì˜ ìˆ«ì",
    "text": "classë³„ ê´€ì¸¡ì¹˜ì˜ ìˆ«ì\n\nclass imbalance? => No\n\n\nx = dataset[\"class\"].value_counts().index\ny = dataset[\"class\"].value_counts().values\nplt.bar(x,y)\n\n<BarContainer object of 3 artists>"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#one-hot-encoding",
    "href": "posts/open/Dakon competetion.html#one-hot-encoding",
    "title": "Untitled",
    "section": "One-hot encoding",
    "text": "One-hot encoding\n\n#one-hot encoding for distance base algorithm\ndataset_ohe = pd.get_dummies(dataset,columns = dataset.columns.drop(\"class\"),drop_first=True) #multicollinearityë¥¼ ë§‰ê¸°ìœ„í•œ drop_first ì˜µì…˜\ntrain_ohe = dataset_ohe[:train_len].copy()\ntest_ohe = dataset_ohe[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain_ohe[\"class\"]=train_ohe[\"class\"].map(class_map).astype(int)\nX_train_ohe = train_ohe.drop(columns = \"class\")\nY_train_ohe = train_ohe[\"class\"]\n\n\nX_train_ohe\n\n\n\n\n\n  \n    \n      \n      trait_1\n      SNP_01_1\n      SNP_01_2\n      SNP_02_1\n      SNP_02_2\n      SNP_03_1\n      SNP_03_2\n      SNP_04_1\n      SNP_04_2\n      SNP_05_1\n      ...\n      has07AA_1\n      has07GG_1\n      has08GG_1\n      has09AA_1\n      has09GG_1\n      has11AA_1\n      has12AA_1\n      has12GG_1\n      has13AA_1\n      has14AA_1\n    \n  \n  \n    \n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      0\n      1\n      0\n      1\n      ...\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      1\n      1\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      3\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n    \n    \n      4\n      1\n      0\n      1\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      257\n      1\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n    \n    \n      258\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      0\n      0\n      ...\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      259\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n    \n    \n      260\n      0\n      0\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      ...\n      0\n      1\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n      0\n    \n    \n      261\n      1\n      0\n      1\n      1\n      0\n      1\n      0\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n  \n\n262 rows Ã— 47 columns"
  },
  {
    "objectID": "posts/open/Dakon competetion.html#label-encoding",
    "href": "posts/open/Dakon competetion.html#label-encoding",
    "title": "Untitled",
    "section": "Label encoding",
    "text": "Label encoding\n\nle_col = []\nfor col_name in dataset.columns.tolist():\n    if \"SNP\" in col_name:\n        le_col.append(col_name)\nle_col.append(\"trait\")\nle_col\n\n['SNP_01',\n 'SNP_02',\n 'SNP_03',\n 'SNP_04',\n 'SNP_05',\n 'SNP_06',\n 'SNP_07',\n 'SNP_08',\n 'SNP_09',\n 'SNP_10',\n 'SNP_11',\n 'SNP_12',\n 'SNP_13',\n 'SNP_14',\n 'SNP_15',\n 'trait']\n\n\n\nfrom sklearn import preprocessing\nfor col in le_col:\n    le = preprocessing.LabelEncoder()\n    _col = dataset[col].tolist()\n    le.fit(_col)\n    dataset[col] = le.transform(_col)\ndataset\n\n\n\n\n\n  \n    \n      \n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      ...\n      has07AA\n      has07GG\n      has08GG\n      has09AA\n      has09GG\n      has11AA\n      has12AA\n      has12GG\n      has13AA\n      has14AA\n    \n  \n  \n    \n      0\n      1\n      2\n      1\n      0\n      1\n      1\n      0\n      0\n      2\n      0\n      ...\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      1\n      1\n      1\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      0\n      0\n      0\n      1\n    \n    \n      2\n      1\n      2\n      2\n      0\n      1\n      2\n      2\n      0\n      1\n      1\n      ...\n      1\n      0\n      0\n      0\n      0\n      1\n      1\n      0\n      1\n      1\n    \n    \n      3\n      0\n      0\n      2\n      0\n      1\n      0\n      2\n      2\n      0\n      2\n      ...\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n    \n    \n      4\n      1\n      2\n      2\n      2\n      0\n      2\n      0\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      170\n      1\n      1\n      2\n      2\n      0\n      1\n      1\n      0\n      2\n      0\n      ...\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      171\n      1\n      2\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n      0\n      0\n      1\n    \n    \n      172\n      1\n      2\n      0\n      0\n      0\n      1\n      1\n      0\n      0\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n    \n    \n      173\n      1\n      1\n      2\n      1\n      1\n      2\n      2\n      0\n      1\n      0\n      ...\n      1\n      0\n      0\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n    \n      174\n      1\n      2\n      2\n      2\n      1\n      1\n      0\n      1\n      2\n      0\n      ...\n      0\n      0\n      1\n      1\n      0\n      0\n      1\n      0\n      1\n      1\n    \n  \n\n437 rows Ã— 33 columns\n\n\n\n\ntrain = dataset[:train_len].copy()\ntest = dataset[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain[\"class\"]=train[\"class\"].map(class_map).astype(int)\nX_train = train.drop(columns = \"class\")\nY_train = train[\"class\"]\n\n\n#ì›í•«ì¸ì½”ë”© vs ë ˆì´ë¸”ì¸ì½”ë”©\nlen(X_train_ohe.columns),len(X_train.columns)\n\n(47, 32)\n\n\n\nlen(Y_train),len(Y_train_ohe)\n\n(262, 262)"
  },
  {
    "objectID": "posts/open/Dakon competition modeling.html",
    "href": "posts/open/Dakon competition modeling.html",
    "title": "HIHO",
    "section": "",
    "text": "import torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nimport torch\ntest_path = \"./test.csv\"\ntrain_path = \"./train.csv\"\n\n\n# %pip install plotly (jupyter notebook)\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n#pio.renderers.default = 'iframe_connected'\n#pio.renderers.default = \"vscode\"\npio.renderers.default = \"plotly_mimetype+notebook\""
  },
  {
    "objectID": "posts/open/Dakon competition modeling.html#íƒì§€í•œ-ì´ìƒì¹˜-ì œì™¸",
    "href": "posts/open/Dakon competition modeling.html#íƒì§€í•œ-ì´ìƒì¹˜-ì œì™¸",
    "title": "HIHO",
    "section": "íƒì§€í•œ ì´ìƒì¹˜ ì œì™¸",
    "text": "íƒì§€í•œ ì´ìƒì¹˜ ì œì™¸\n\noutliers = pd.read_csv(\"./outlierdetect.csv\").drop(columns = \"Unnamed: 0\")\nnormal_index = ~outliers.isoutlier\n\n\nX_train_ohe = X_train_ohe[normal_index];Y_train_ohe = Y_train_ohe[normal_index]\n\n\nX_train_ohe.shape\n\ntorch.Size([243, 47])"
  },
  {
    "objectID": "posts/open/Dakon competition torch.html",
    "href": "posts/open/Dakon competition torch.html",
    "title": "Untitled",
    "section": "",
    "text": "import seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nimport torch\ntest_path = \"./test.csv\"\ntrain_path = \"./train.csv\"\n\n\ntrain = pd.read_csv(train_path).drop(columns = [\"id\"])\ntrain_len = len(train)\ntest = pd.read_csv(test_path)\nid_test = test[\"id\"]\ntest = pd.read_csv(test_path).drop(columns = [\"id\"])\n\n\ndataset = pd.concat([train,test],axis=0)\ndataset = dataset.drop(columns = [\"father\",\"mother\",\"gender\"])\n\n\ndataset\n\n\n\n\n\n  \n    \n      \n      trait\n      SNP_01\n      SNP_02\n      SNP_03\n      SNP_04\n      SNP_05\n      SNP_06\n      SNP_07\n      SNP_08\n      SNP_09\n      SNP_10\n      SNP_11\n      SNP_12\n      SNP_13\n      SNP_14\n      SNP_15\n      class\n    \n  \n  \n    \n      0\n      2\n      G G\n      A G\n      A A\n      G A\n      C A\n      A A\n      A A\n      G G\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      1\n      2\n      A G\n      A G\n      C A\n      A A\n      A A\n      A G\n      A A\n      G A\n      A A\n      A G\n      A A\n      G A\n      G G\n      A A\n      A A\n      C\n    \n    \n      2\n      2\n      G G\n      G G\n      A A\n      G A\n      C C\n      G G\n      A A\n      G A\n      G A\n      A G\n      A A\n      A A\n      A A\n      A A\n      A A\n      B\n    \n    \n      3\n      1\n      A A\n      G G\n      A A\n      G A\n      A A\n      G G\n      G G\n      A A\n      G G\n      A G\n      G G\n      G G\n      G G\n      A A\n      G G\n      A\n    \n    \n      4\n      2\n      G G\n      G G\n      C C\n      A A\n      C C\n      A A\n      A A\n      A A\n      A A\n      G G\n      A A\n      A A\n      A G\n      A A\n      G A\n      C\n    \n    \n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n      ...\n    \n    \n      170\n      2\n      A G\n      G G\n      C C\n      A A\n      C A\n      A G\n      A A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      G A\n      NaN\n    \n    \n      171\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      A G\n      A A\n      A A\n      A G\n      A A\n      G A\n      NaN\n    \n    \n      172\n      2\n      G G\n      A A\n      A A\n      A A\n      C A\n      A G\n      A A\n      A A\n      A A\n      G G\n      A G\n      A A\n      A G\n      A A\n      G G\n      NaN\n    \n    \n      173\n      2\n      A G\n      G G\n      C A\n      G A\n      C C\n      G G\n      A A\n      G A\n      A A\n      G G\n      A G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n    \n      174\n      2\n      G G\n      G G\n      C C\n      G A\n      C A\n      A A\n      G A\n      G G\n      A A\n      G G\n      G G\n      A A\n      A A\n      A A\n      A A\n      NaN\n    \n  \n\n437 rows Ã— 17 columns\n\n\n\n\n_t = []\nfor val in dataset.SNP_01 == \"G G\":\n    if val == True:\n        _t.append(1)\n    else:\n        _t.append(0)\ndataset[\"has01GG\"] = _t\n\n_t = []\nfor val in dataset.SNP_02 == \"A A\":\n    if val == True:\n        _t.append(1)\n    else:\n        _t.append(0)\ndataset[\"has02AA\"] = _t\n\n\ndef create_col(dataset,col,value):\n    _t = []\n    for val in dataset[col] == value:\n        if val == True:\n            _t.append(1)\n        else:\n            _t.append(0)\n    \n    col_name_base = \"has\"+col[-2:]\n    value_name = \"\"\n    for chr in value:\n        if chr != \" \":\n            value_name+=chr\n    col_name = col_name_base+value_name\n    #print(col_name)\n    dataset[col_name] = _t\n\n    return dataset\n\ndataset = create_col(dataset,\"SNP_03\",\"A A\")\ndataset = create_col(dataset,\"SNP_04\",\"G G\")\ndataset = create_col(dataset,\"SNP_05\",\"C C\")\ndataset = create_col(dataset,\"SNP_06\",\"A A\")\ndataset = create_col(dataset,\"SNP_07\",\"A A\")\ndataset = create_col(dataset,\"SNP_07\",\"G G\")\ndataset = create_col(dataset,\"SNP_08\",\"G G\")\n\ndataset = create_col(dataset,\"SNP_09\",\"A A\")\ndataset = create_col(dataset,\"SNP_09\",\"G G\")\ndataset = create_col(dataset,\"SNP_11\",\"A A\")\n\ndataset = create_col(dataset,\"SNP_12\",\"A A\")\ndataset = create_col(dataset,\"SNP_12\",\"G G\")\n\ndataset = create_col(dataset,\"SNP_13\",\"A A\")\ndataset = create_col(dataset,\"SNP_14\",\"A A\")\n\n\n#one-hot encoding for distance base algorithm\ndataset_ohe = pd.get_dummies(dataset,columns = dataset.columns.drop(\"class\"),drop_first=True) #multicollinearityë¥¼ ë§‰ê¸°ìœ„í•œ drop_first ì˜µì…˜\ntrain_ohe = dataset_ohe[:train_len].copy()\ntest_ohe = dataset_ohe[train_len:].copy().drop(columns=\"class\")\n\nclass_map = {\"A\":0,\"B\":1,\"C\":2}\ntrain_ohe[\"class\"]=train_ohe[\"class\"].map(class_map).astype(int)\nX_train_ohe = train_ohe.drop(columns = \"class\")\nY_train_ohe = train_ohe[\"class\"]\n\n\nimport torch\nimport torch.nn as nn\n\n\nX_train_ohe = torch.from_numpy(X_train_ohe.values).float()\n#y_train_ohe = torch.from_numpy(pd.get_dummies(Y_train_ohe).values).float()\nY_train_ohe = torch.from_numpy(Y_train_ohe.values).long()\n\n\nclass mynet(nn.Module):\n    def __init__(self,in_features,dropout_p,l1_out=64):\n        super().__init__()\n        self.linr1 = torch.nn.Linear(in_features,l1_out)\n        self.relu1 = torch.nn.ReLU()\n        self.d1 = torch.nn.Dropout(p=dropout_p)\n        self.b1 = torch.nn.BatchNorm1d(l1_out)\n        # 256\n        \n        self.linr2 = torch.nn.Linear(l1_out,l1_out//2)\n        self.relu2 = torch.nn.ReLU()\n        self.d2 = torch.nn.Dropout(p=dropout_p)\n        self.b2 = torch.nn.BatchNorm1d(l1_out//2)\n        # 128\n\n        self.linr3 = torch.nn.Linear(l1_out//2,l1_out//4)\n        self.relu3 = torch.nn.ReLU()\n        self.d3 = torch.nn.Dropout(p=dropout_p)\n        self.b3 = torch.nn.BatchNorm1d(l1_out//4)\n        # 64\n        self.linr4 = torch.nn.Linear(l1_out//4,l1_out//8)\n        self.relu4 = torch.nn.ReLU()\n        self.d4 = torch.nn.Dropout(p=dropout_p)\n        self.b4 = torch.nn.BatchNorm1d(l1_out//8)\n        # 32\n        self.linr5 = torch.nn.Linear(l1_out//8,l1_out//16)\n        self.relu5 = torch.nn.ReLU()\n        self.d5 = torch.nn.Dropout(p=dropout_p)\n        self.b5 = torch.nn.BatchNorm1d(l1_out//16)\n        #16\n        self.linr6 = torch.nn.Linear(l1_out//16,3)\n\n    def forward(self,x):\n        out = self.b1(self.d1(self.relu1(self.linr1(x))))\n        out = self.b2(self.d2(self.relu2(self.linr2(out))))\n        out = self.b3(self.d3(self.relu3(self.linr3(out))))\n        out = self.b4(self.d4(self.relu4(self.linr4(out))))\n        out = self.b5(self.d5(self.relu5(self.linr5(out))))\n        out = self.linr6(out)\n        #out = self.b3(self.d3(self.relu3(self.linr3(x))))\n        return out\n\n\nfrom sklearn.model_selection import StratifiedKFold\nimport random\nepochs_list = [i for i in range(1400,2000,20)]\nweight_decay = np.linspace(0.001,0.0001,500).tolist()\nlr = np.linspace(1e-2,1e-5,500).tolist()\nhidden_nodes = [i for i in range(1300,1800)]\ndropout_p = np.linspace(0.6,0.8,1000).tolist()\nrs = [i for i in range(0,500)]\n\n\ndef dl_cv(epochs,weight_decay,learning_rate,hidden1_nodes,dropout_p,rs):\n\n    try_number=0\n    skf = StratifiedKFold(n_splits=5,shuffle=True)\n    skf.get_n_splits(X_train_ohe,Y_train_ohe)\n    \n    while True:\n        try_number+=1\n        print(f'try:{try_number}...')\n        train_accs = []\n        val_accs = []\n        hdly1 = random.sample(hidden1_nodes,1)[0]\n        lr = random.sample(learning_rate,1)[0]\n        wght_decay = random.sample(weight_decay,1)[0]\n        epoch = random.sample(epochs,1)[0]\n        drop_p = random.sample(dropout_p,1)[0]\n        r_seed = random.sample(rs,1)[0]\n        print(f'lr:{lr} wght_decay:{wght_decay} epochs:{epoch} hidden_l1nodes:{hdly1} dropout_prob:{drop_p}')\n        for train_index,valid_index in skf.split(X_train_ohe,Y_train_ohe): \n            torch.manual_seed(r_seed)\n            net = mynet(47,drop_p,l1_out=hdly1)\n            optimizer = torch.optim.Adam(net.parameters(),lr=lr,weight_decay = wght_decay)        \n            loss_fn = torch.nn.CrossEntropyLoss()\n            #KFold\n            train_index = train_index.tolist();valid_index = valid_index.tolist()\n            X_tr = X_train_ohe[train_index,:];y_tr = Y_train_ohe[train_index]\n            X_tst = X_train_ohe[valid_index,:];y_valid = Y_train_ohe[valid_index]\n\n            #fitting\n            for ep in range(epoch):\n                net.train()\n                #1 yhat\n                yhat = net(X_tr)\n                #2 loss\n                loss = loss_fn(yhat,y_tr)\n                if ep % 50 == 0:\n                    pass\n                    #print(loss)\n                #3 derivative\n                loss.backward()\n                #4 update\n                optimizer.step()\n                optimizer.zero_grad()\n            train_yhat = torch.argmax(net(X_tr),dim=1)\n            train_acc = torch.mean((train_yhat==y_tr).float())\n            print(\"trainacc : \",train_acc)\n            train_accs.append(train_acc)\n            net.eval()\n            with torch.no_grad():\n                val_yhat = torch.argmax(net(X_tst),dim=1)\n                val_acc = torch.mean((val_yhat == y_valid).float()).tolist()\n                print(\"validacc : \",val_acc)\n                val_accs.append(val_acc)\n            if val_acc < 0.97:\n                break\n\n        valid_accuracy = torch.mean(torch.tensor(val_accs))\n        print(f\"K-Fold train accuracy {torch.mean(torch.tensor(train_accs))}\")\n        print(f\"K-Fold valid accuracy {torch.mean(torch.tensor(val_accs))}\")\n        print(\"==========================================================\")\n        if valid_accuracy > 0.99:\n            path = \"./model{}.pth\".format(try_number)\n            torch.save(net,path)\n\n\nt = dl_cv(epochs_list,weight_decay,lr,hidden_nodes,dropout_p,rs)\n\ntry:1...\nlr:0.0039939879759519036 wght_decay:0.00039218436873747497 epochs:1400 hidden_l1nodes:1433 dropout_prob:0.6748748748748749\n\n\nKeyboardInterrupt: \n\n\n\nAssemble\n\ntest_ohe = torch.from_numpy(test_ohe.values).float()\n\n\nimport os\npath = \"C:/Users/22668/Desktop/ìƒˆ í´ë”\"\nmodels = []\nfor model in os.listdir(path):\n    md_path = os.path.join(path,model)\n    models.append(torch.load(md_path))\n\n\nassemble = torch.zeros(test_ohe.shape[0],3)\nassemble.shape\n\ntorch.Size([175, 3])\n\n\n\nsoft = torch.nn.Softmax(dim=1)\n\n\nfor model in models:\n    net = model\n    yhat = soft(model(test_ohe))\n    assemble +=yhat\n\n\ntest_predict = torch.argmax(assemble,dim=1)\nresult = pd.concat([pd.Series(id_test),pd.Series(test_predict)],axis=1)\nresult.columns = [\"id\",\"class\"]\nclass_map_inv = {0:\"A\",1:\"B\",2:\"C\"}\nresult[\"class\"] = result[\"class\"].map(class_map_inv)\n\n\nresult\n\n\n\n\n\n  \n    \n      \n      id\n      class\n    \n  \n  \n    \n      0\n      TEST_000\n      A\n    \n    \n      1\n      TEST_001\n      B\n    \n    \n      2\n      TEST_002\n      C\n    \n    \n      3\n      TEST_003\n      B\n    \n    \n      4\n      TEST_004\n      A\n    \n    \n      ...\n      ...\n      ...\n    \n    \n      170\n      TEST_170\n      B\n    \n    \n      171\n      TEST_171\n      C\n    \n    \n      172\n      TEST_172\n      C\n    \n    \n      173\n      TEST_173\n      B\n    \n    \n      174\n      TEST_174\n      B\n    \n  \n\n175 rows Ã— 2 columns\n\n\n\n\nresult.to_csv(\"./submission_dl.csv\",index=False)"
  },
  {
    "objectID": "posts/paper study/DQN review - ë³µì‚¬ë³¸.html",
    "href": "posts/paper study/DQN review - ë³µì‚¬ë³¸.html",
    "title": "DQN review",
    "section": "",
    "text": "ê°œìš”\n\nê¸°ì¡´ì˜ ë”¥ëŸ¬ë‹ê³¼ ê°•í™”í•™ìŠµì€ ê·¸ë¦¼ì˜ ì™¼ìª½ê³¼ ê°™ì´ ê³µí†µë¶„ëª¨ê°€ ê±°ì˜ ì—†ëŠ” ë¶„ì•¼ì˜€ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ 2013ë…„ êµ¬ê¸€ë”¥ë§ˆì¸ë“œì—ì„œ ë°œí‘œí•œ DQNë…¼ë¬¸ì—ì„œëŠ” ê°•í™”í•™ìŠµì˜ í•œ ë¶„ì•¼ì¸ Q-learningì— ë”¥ëŸ¬ë‹ì„ ì ‘ëª©ì‹œì¼°ê³  ê·¸ í›„ ê³„ì†í•´ì„œ ë°œì „í•˜ì—¬ ê±°ì˜ ëª¨ë“  ê°•í™”í•™ìŠµë…¼ë¬¸ì—ëŠ” ë”¥ëŸ¬ë‹ì´ ì‚¬ìš©ëœë‹¤ê³  í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ì œëŠ” ì˜¤ë¥¸ìª½ê³¼ ê°™ì´ ê°•í™”í•™ìŠµë„ ê±°ì˜ ë”¥ëŸ¬ë‹ì˜ í•˜ë‚˜ì˜ ë¶„ì•¼ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤.\n\n\nQ-learning\nQ-learningì€ Qê°’ì„ í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê·¸ë¦¬ë“œí˜•ì‹ì„ ê°€ì§„ ë¬¸ì œì—ì„œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°•í™”í•™ìŠµì—ì„œ ì£¼ì²´(agent)ëŠ” í˜„ì¬ì˜ ìƒíƒœ(state)ë¥¼ ê´€ì°°í•˜ì—¬ ì–´ë– í•œ í–‰ë™(action)ì´ ê°€ì¥ í° ë³´ìƒ(reward)ë¥¼ ê°€ì ¸ë‹¤ì£¼ëŠ”ì§€ í•™ìŠµí•˜ë©° Q-learningì—ì„œ ì´ëŸ¬í•œ í•™ìŠµì˜ ëŒ€ìƒì€ Qì…ë‹ˆë‹¤.\n\nìœ„ì˜ ê·¸ë¦¼ì€ í•™ìŠµì´ ëë‚œ Qê°’ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤.(ì‹¤ì œë¡œ ë§ëŠ” ìˆ˜ì¹˜ëŠ” ì•„ë‹˜)Q-learning ì•Œê³ ë¦¬ì¦˜ì—ì„œ agentëŠ” greedy actionì„ ì·¨í•©ë‹ˆë‹¤. ë”°ë¼ì„œ agentê°€ ê²©ìì˜ ì‹œì‘ì§€ì ì— ë“¤ì–´ê°€ê²Œ ëœë‹¤ë©´ greedy actionì„ í†µí•´ ê°€ì¥ í° Qê°’ì´ ìˆëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì‹œì‘ë¶€í„° ì¢…ë£Œì§€ì ê¹Œì§€ ì¼ì§ì„ ìœ¼ë¡œ ê°€ì¥ë¹ ë¥´ê²Œ ì´ë™í•©ë‹ˆë‹¤. ìœ„ì™€ ê°™ì€ ê·¸ë¦¬ë“œì—ì„œëŠ” ë” ë¹ ë¥´ê²Œ ê°€ê±°ë‚˜ ë¦¬ì›Œë“œë„ ë” ì¢‹ì€ ê³³ì€ ì—†ìœ¼ë¯€ë¡œ ì ì ˆí•˜ê²Œ í•™ìŠµì´ ëë‚¬ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nQ-update\n\\[Q(s_t,a_t) = (1-\\alpha)Q(s_t,a_t) + \\alpha(R_t + \\gamma \\underset{a_{t+1}}{\\text{argmax}}Q(s_{t+1},a_{t+1}))\\]"
  },
  {
    "objectID": "posts/paper study/DQN review.html",
    "href": "posts/paper study/DQN review.html",
    "title": "DQN review",
    "section": "",
    "text": "ê°œìš”\n\nê¸°ì¡´ì˜ ë”¥ëŸ¬ë‹ê³¼ ê°•í™”í•™ìŠµì€ ê·¸ë¦¼ì˜ ì™¼ìª½ê³¼ ê°™ì´ ê³µí†µë¶„ëª¨ê°€ ê±°ì˜ ì—†ëŠ” ë¶„ì•¼ì˜€ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ 2013ë…„ êµ¬ê¸€ë”¥ë§ˆì¸ë“œì—ì„œ ë°œí‘œí•œ D\\(Q\\)Në…¼ë¬¸ì—ì„œëŠ” ê°•í™”í•™ìŠµì˜ í•œ ë¶„ì•¼ì¸ \\(Q\\)-learningì— ë”¥ëŸ¬ë‹ì„ ì ‘ëª©ì‹œì¼°ê³  ê·¸ í›„ ê³„ì†í•´ì„œ ë°œì „í•˜ì—¬ ê±°ì˜ ëª¨ë“  ê°•í™”í•™ìŠµë…¼ë¬¸ì—ëŠ” ë”¥ëŸ¬ë‹ì´ ì‚¬ìš©ëœë‹¤ê³  í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ì œëŠ” ì˜¤ë¥¸ìª½ê³¼ ê°™ì´ ê°•í™”í•™ìŠµë„ ê±°ì˜ ë”¥ëŸ¬ë‹ì˜ í•˜ë‚˜ì˜ ë¶„ì•¼ë¡œ ìë¦¬ì¡ì•˜ìŠµë‹ˆë‹¤.\n\n\n\\(Q\\)-learning\n\\(Q\\)-learningì€ \\(Q\\)ê°’ì„ í•™ìŠµí•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ê·¸ë¦¬ë“œí˜•ì‹ì„ ê°€ì§„ ë¬¸ì œì—ì„œ í™œìš©ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê°•í™”í•™ìŠµì—ì„œ ì£¼ì²´(agent)ëŠ” í˜„ì¬ì˜ ìƒíƒœ(state)ë¥¼ ê´€ì°°í•˜ì—¬ ì–´ë– í•œ í–‰ë™(action)ì´ ê°€ì¥ í° ë³´ìƒ(reward)ë¥¼ ê°€ì ¸ë‹¤ì£¼ëŠ”ì§€ í•™ìŠµí•˜ë©° \\(Q\\)-learningì—ì„œ ì´ëŸ¬í•œ í•™ìŠµì˜ ëŒ€ìƒì€ \\(Q\\)ì…ë‹ˆë‹¤.\n\nìœ„ì˜ ê·¸ë¦¼ì€ í•™ìŠµì´ ëë‚œ \\(Q\\)ê°’ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤.(ì‹¤ì œë¡œ ë§ëŠ” ìˆ˜ì¹˜ëŠ” ì•„ë‹˜)\\(Q\\)-learning ì•Œê³ ë¦¬ì¦˜ì—ì„œ agentëŠ” greedy actionì„ ì·¨í•©ë‹ˆë‹¤. ë”°ë¼ì„œ agentê°€ ê²©ìì˜ ì‹œì‘ì§€ì ì— ë“¤ì–´ê°€ê²Œ ëœë‹¤ë©´ greedy actionì„ í†µí•´ ê°€ì¥ í° \\(Q\\)ê°’ì´ ìˆëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ì—¬ ì‹œì‘ë¶€í„° ì¢…ë£Œì§€ì ê¹Œì§€ ì¼ì§ì„ ìœ¼ë¡œ ê°€ì¥ë¹ ë¥´ê²Œ ì´ë™í•©ë‹ˆë‹¤. ìœ„ì™€ ê°™ì€ ê·¸ë¦¬ë“œì—ì„œëŠ” ë” ë¹ ë¥´ê²Œ ê°€ê±°ë‚˜ ë¦¬ì›Œë“œë„ ë” ì¢‹ì€ ê³³ì€ ì—†ìœ¼ë¯€ë¡œ ì ì ˆí•˜ê²Œ í•™ìŠµì´ ëë‚¬ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\\(Q\\)-update\n\\[ Q(s_t,a_t) = (1-\\alpha)Q(s_t,a_t) + \\alpha(R_t + \\gamma \\underset{a_{t+1}}{\\text{argmax}}Q(s_{t+1},a_{t+1}))\\]\n\\(Q\\)-learningì—ì„œëŠ” ìœ„ì™€ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ ê°ê°ì˜ \\(Q\\)ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ì¤‘ìš”í•œì ì€ \\(Q\\)ê°€ stateì™€ actionì˜ í•¨ìˆ˜ë¼ëŠ” ì ì…ë‹ˆë‹¤. ìœ„ì™€ ê°™ì€ ê¸¸ì°¾ê¸° ë¬¸ì œì˜ ê²½ìš° ê·¸ë ‡ê²Œ state(25ê°œ)ê°€ ë§ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤.ê·¸ëŸ¬ë‚˜ Atari ë²½ëŒê¹¨ê¸° ê²Œì„ê³¼ ê°™ì€ ê²½ìš°, ì›€ì§ì´ëŠ” ì£¼ì²´ì¸ ê°€ë¡œë§‰ëŒ€ë°” ìœ„ì¹˜,ë²½ëŒì˜ ê°¯ìˆ˜,ê¹¨ì§„ìœ„ì¹˜,ê³µì´ ë‚ ì•„ì˜¤ëŠ” ê°ë„ ë“±ë“± â€¦ ë§¤ìš° ë§ì€ stateê°€ ê°€ëŠ¥í•˜ê³  ì–´ë–¤ë°©í–¥ìœ¼ë¡œ ê³µì„ ë‚ ë¦´ì§€ì— ëŒ€í•œ actionë„ ìˆ˜ì—†ì´ ë§ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ê¸°ì¡´ì˜ ë°©ì‹ìœ¼ë¡œ Që¥¼ ì—…ë°ì´íŠ¸ í•˜ê¸°ìœ„í•´ì„œëŠ” ì´ëŸ¬í•œ ìˆ˜ë§ì€ ì¡°í•©ì— ëŒ€í•˜ì—¬ stateì™€ actionì„ ê¸°ì–µí•´ë†“ê³  ì—…ë°ì´íŠ¸ í•´ì•¼í•©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ì€ ì»´í“¨í„°ì˜ ë©”ëª¨ë¦¬ì— ë¶€ë‹´ì„ ì£¼ê³  exploration(íƒí—˜)í•˜ëŠ”ë° ê±¸ë¦¬ëŠ” ì‹œê°„ì„ ë” ì˜¤ë˜ ë§Œë“­ë‹ˆë‹¤.\nDeep-\\(Q\\)-learningì—ì„œëŠ” DNNì„ í†µí•´ í•¨ìˆ˜ë¡œì„œ \\(Q\\)ê°’ì„ ì €ì¥í•˜ì—¬ ìœ„ì™€ ê°™ì€ ë‹¨ì ì„ ì¤„ì…ë‹ˆë‹¤. ë˜í•œ loss functionì„ ì •ì˜í•˜ê³  gradient desentë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒˆë¡­ê²Œ Qê°’ì„ ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤.\n\n\n\nê·¸ë¦¼ì¶œì²˜ : ì´ê²ƒì €ê²ƒ í…Œí¬ ë¸”ë¡œê·¸"
  },
  {
    "objectID": "posts/Probability&Statistics/categori distribution.html",
    "href": "posts/Probability&Statistics/categori distribution.html",
    "title": "ì¹´í…Œê³ ë¦¬ ë¶„í¬",
    "section": "",
    "text": "ì¹´í…Œê³ ë¦¬ ë¶„í¬ì— ëŒ€í•œ ì •ë¦¬\n\nì¹´í…Œê³ ë¦¬ ë¶„í¬\nì¹´í…Œê³ ë¦¬ë¶„í¬ëŠ” ì‹œí–‰ì˜ í•œë²ˆì˜ ì‹œí–‰(ë˜ëŠ” ì‹¤í—˜)ìœ¼ë¡œë¶€í„° ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ì‚¬ê±´ì´ Kê°œì¸ í™•ë¥ ë¶„í¬ë¥¼ ëª¨ë¸ë§í• ë•Œ ì“°ì´ë©° ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&Cat({\\bf{x};\\bf{\\mu}}) =\n\\begin{cases}\n\\mu_1\\, (\\text{if } x = (1,0,0,0,\\dots,1)) \\\\\n\\mu_2\\, (\\text{if } x = (0,1,0,0,\\dots,1)) \\\\\n\\mu_3\\, (\\text{if } x = (0,0,1,0,\\dots,1)) \\\\\n\\vdots \\\\\n\\mu_k\\, (\\text{if } x = (0,0,0,0,\\dots,1)) \\\\\n\\end{cases}\n\\\\\n&\\text{where, }x = (x_1,x_2,\\dots,x_K),\\mu = (\\mu_1,\\mu_2,\\dots,\\mu_k)\n\\end{aligned}\\]\nì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ë³€ìˆ˜\\(\\bf{X}\\)ëŠ” Kê°œì˜ ì›ì†Œë¥¼ ê°€ì§€ëŠ” ì›í•«ì¸ì½”ë”©(one-hot encoded)ëœ ë²¡í„°ì´ë©° ê°ì›ì†ŒëŠ” indicate number(ì–´ë–¤ í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ ë‚˜íƒ€ë‚´ëŠ”)ì¸ 1ë˜ëŠ”0ì…ë‹ˆë‹¤. ëª¨ìˆ˜(ë²¡í„°)\\(\\mu\\)ë„ Kê°œì˜ ì›ì†Œë¥¼ ê°€ì§€ë©° ê°ê°ì˜ ì›ì†ŒëŠ” ì¹´í…Œê³ ë¦¬ í™•ë¥ ë¶„í¬ë¡œë¶€í„° ëŒ€ì‘í•˜ëŠ” ê²°ê³¼ê°’(ì›í•«ë²¡í„°)ì— ëŒ€í•œ í™•ë¥ ì…ë‹ˆë‹¤. ì¦‰,ê°ê°ì˜ ì›í•«ë²¡í„°ê°€ í‘œë³¸ì¶”ì¶œë  ê°€ëŠ¥ì„±(í™•ë¥ )ì„ ì•Œë ¤ì¤ë‹ˆë‹¤.\nìœ„ì™€ ê°™ì€ ì‚¬ì‹¤ë¡œë¶€í„° ë‹¤ìŒê³¼ ê°™ì€ 4ê°€ì§€ì˜ ì œì•½ì¡°ê±´ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n\n\\(\\mu_i\\)ëŠ” ì›í•«ë²¡í„°ê°€ ë‚˜ì˜¬ í™•ë¥ ì…ë‹ˆë‹¤.\n\n\\[0\\leq\\mu_i\\leq1\\]\n\ní™•ë¥ ì˜ í•©ì€ 1ì…ë‹ˆë‹¤.\n\n\\[\\sum_{i=1}^{K}\\mu_i = 1\\]\n\nì›í•«ë²¡í„°ì˜ ê° ì›ì†ŒëŠ” indicate numberì¸ 1ë˜ëŠ” 0ì…ë‹ˆë‹¤.\n\n\\[\\begin{aligned}\nx_i =\n\\begin{cases}\n0\\\\\n1\n\\end{cases}\n\\end{aligned}\\]\n\nì›í•«ë²¡í„°ì˜ ëª¨ë“  ì›ì†Œì˜ í•©ì€ 1ì…ë‹ˆë‹¤. \\[\\sum_{i=1}^{K}x_i = 1\\]"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html",
    "title": "Maximum likelyhood estimation",
    "section": "",
    "text": "ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •ë²•(MLE)ì— ëŒ€í•œ ì •ë¦¬"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#problem-setting",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#problem-setting",
    "title": "Maximum likelyhood estimation",
    "section": "Problem Setting",
    "text": "Problem Setting\n\\[\\begin{aligned}\n&\\text{Given}\\,x_1,x_2,...,x_N\\ \\text{which are realizations of a random sample }X_1,X_2,\\dots,X_N \\\\\n&\\text{where,}\\, X_1 \\sim f_1(x_1;\\theta),X_2 \\sim f_2(x_1;\\theta),\\dots,X_N \\sim f_N(x_N;\\theta) \\\\ \\\\\n\n&\\text{Goal : í™•ë¥ ë¶„í¬ì˜ ëª¨ìˆ˜ $\\theta$ë¥¼ ì ì¶”ì •í•˜ê¸°}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#what-is-estimation",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#what-is-estimation",
    "title": "Maximum likelyhood estimation",
    "section": " What is estimation? ",
    "text": "What is estimation? \n\nimport scipy as sp\nimport numpy as np\nimport matplotlib.pyplot as plt\nplt.style.use(\"ggplot\")\n\nplt.figure(figsize=(10,5))\nrv = sp.stats.norm(175,10)\nx = np.linspace(150,200,500)\nplt.plot(x,rv.pdf(x),\"b\")\nplt.title(\"$N(175,10)$\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\n\nText(0, 0.5, 'y')\n\n\n\n\n\nëŒ€í•œë¯¼êµ­ 20ëŒ€ ë‚¨ì„±ë“¤ì˜ í‚¤ê°€ ë‹¤ìŒê³¼ í™•ë¥ ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•´ë³´ê² ìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” â€œí‚¤ê°€ 175ê·¼ì²˜ì¸ ì‚¬ëŒì´ ëŒ€ë¶€ë¶„ì´ë„¤??â€ ë˜ëŠ” â€œí‚¤ê°€ 160ì´í•˜ì´ê±°ë‚˜ 190ì´ìƒì¸ ì‚¬ëŒì€ ê±°ì˜ ì—†ê² ë„¤?â€ ë“±ì˜ í•´ì„ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ í•´ì„ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ëŠ”ê²ƒì€ ìš°ë¦¬ê°€ í™•ë¥ ë¶„í¬ì˜ ëª¨ì–‘(ì •ê·œë¶„í¬)ê³¼ ëª¨ìˆ˜(í‰ê· ,ë¶„ì‚°)ì„ ê°€ì •í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ê·¹ë‹¨ì ìœ¼ë¡œ,ê°™ì€ í‰ê· ì´ë¼ëŠ” ëª¨ìˆ˜ë¥¼ ê°€ì§€ì§€ë§Œ ì „í˜€ë‹¤ë¥¸ ë¶„í¬ì¸ ë² ë¥´ëˆ„ì´ë¶„í¬ë¼ê³  ê°€ì •ì„ í•˜ë©´ í™•ë¥ ë³€ìˆ˜ì˜ ê°’ì´ ë”± ë–¨ì–´ì§€ëŠ” 2ê°œì˜ ì´ì‚°ì ì¸ ê°’ë§Œì„ ê°€ì§€ë¯€ë¡œ â€œì´í•˜â€ë¼ëŠ” í‘œí˜„ì´ë‚˜ â€œì´ìƒâ€ì´ë¼ëŠ” í‘œí˜„ì€ ì“¸ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.(ë¬¼ë¡  ë² ë¥´ëˆ„ì´ë¶„í¬ëŠ” ë¶„ì‚°ì€ ëª¨ìˆ˜ê°€ ì•„ë‹ˆê¸´í•©ë‹ˆë‹¤.) ëª¨ì–‘ì´ ê°™ì§€ë§Œ ëª¨ìˆ˜ê°€ ë‹¤ë¥¸ ê²½ìš°ë„ ë§ˆì°¬ê°€ì§€ì…ë‹ˆë‹¤. ê°™ì€ ì •ê·œë¶„í¬ë¼ë„ í‰ê· ì´ë‚˜ ë¶„ì‚°ì´ ë‹¤ë¥´ë©´ ì „í˜€ ë‹¤ë¥¸ í•´ì„ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤.\në§Œì•½ ìœ„ì™€ ê°™ì€ í•´ì„ì„ í•˜ê³ ì‹¶ìœ¼ë‚˜ 20ëŒ€ ë‚¨ì„±ë“¤ì˜ í‚¤ê°€ ë”°ë¥´ëŠ” ë¶„í¬ìì²´ë¥¼ ëª¨ë¥¸ë‹¤ë©´ í•´ì•¼í• ê¹Œìš”? ìœ„ì™€ ê°™ì€ í•´ì„ì„ í•˜ê¸°ìœ„í•´ì„œëŠ” í™•ë¥ ë¶„í¬ ë” ìƒì„¸íˆë§í•˜ìë©´ í™•ë¥ ë¶„í¬ì˜ ëª¨ì–‘ê³¼ í™•ë¥ ë¶„í¬ì˜ ëª¨ìˆ˜ë¥¼ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ í™•ë¥ ë¶„í¬ì˜ ì¶”ì •ì´ë¼ê³  í•©ë‹ˆë‹¤.\ní™•ë¥ ë¶„í¬ì˜ ì¶”ì •ì—ì„œ ë³´í†µ ë¶„í¬ì˜ ëª¨ì–‘ì€ ì–´ë– í•œ ë¶„í¬ë¡œ ê°€ì •í•©ë‹ˆë‹¤(ex ì •ê·œë¶„í¬,ë² íƒ€ë¶„í¬,ë² ë¥´ëˆ„ì´ë¶„í¬,ë‹¤í•­ë¶„í¬ ë“±ë“±â€¦). ê·¸ ë‹¤ìŒì€ í™•ë¥ ë¶„í¬ì˜ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë•Œ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²• ì¤‘ í•˜ë‚˜ê°€ ë°”ë¡œ ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •ë²•(Maximum likelihood estimation)ì…ë‹ˆë‹¤.\nì•„ê¹Œì „ ìƒí™©ì„ ë‹¤ì‹œ ìƒê°í•´ë´…ì‹œë‹¤. ìš°ë¦¬ëŠ” í™•ë¥ ë¶„í¬ë¡œë¶€í„° ëª‡ ê°€ì§€ì˜ í•´ì„ì„ í•  ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” í™•ë¥ ë¶„í¬ì˜ ëª¨ì–‘ê³¼ ëª¨ìˆ˜(í‰ê· ,ë¶„ì‚°)ë¥¼ ë¨¼ì € ê°€ì •ì„ í–ˆê¸° ë•Œë¬¸ì´ì—ˆê³ ,ì´ë¡œë¶€í„° íŠ¹ì •í•œ í‘œë³¸ì´ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ê°€ëŠ¥ì„±ì„ ì•Œ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.ì´ì™€ëŠ” ë°˜ëŒ€ë¡œ,ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •ë²•ì—ì„œëŠ” ì—­ìœ¼ë¡œ ìƒ˜í”Œì´ ì£¼ì–´ì ¸ìˆë‹¤ê³  ê°€ì •í•˜ê³  ê°€ì •í•œ í™•ë¥ ë¶„í¬ì˜ ëª¨ì–‘ì— ëŒ€í•´ì„œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ëª¨ìˆ˜ë¥¼ ì´ ëª¨ìˆ˜ë¥¼ ëª¨ìˆ˜ì— ëŒ€í•œ ì¶”ì •ëŸ‰(estimated value)ë¡œ í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#mleìµœëŒ€ê°€ëŠ¥ë„-ì¶”ì •ë²•",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#mleìµœëŒ€ê°€ëŠ¥ë„-ì¶”ì •ë²•",
    "title": "Maximum likelyhood estimation",
    "section": "MLE(ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •ë²•)",
    "text": "MLE(ìµœëŒ€ê°€ëŠ¥ë„ ì¶”ì •ë²•)"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#likelyhood-function",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#likelyhood-function",
    "title": "Maximum likelyhood estimation",
    "section": "Likelyhood function ",
    "text": "Likelyhood function \nìœ„ì—ì„œ ìƒ˜í”Œì´ ì£¼ì–´ì ¸ìˆë‹¤ê³  ê°€ì •í•˜ê³  ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ ëª¨ìˆ˜ê°€ ëª¨ìˆ˜ì— ëŒ€í•œ ì¶”ì •ëŸ‰ì´ë¼ê³  í–ˆìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ê°€ëŠ¥ì„±ì´ ê°€ì¥ ë†’ì€ ëª¨ìˆ˜ë¥¼ ì •í•˜ê¸° ìœ„í•´ì„œ ëª¨ìˆ˜ì˜ ê°€ëŠ¥ì„±ì´ë¼ëŠ” ì§€í‘œë¥¼ ì •ì˜í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. ëª¨ìˆ˜ì˜ ê°€ëŠ¥ì„±ì€ ì–´ë–»ê²Œ ì •ì˜í•´ì•¼ í• ê¹Œìš”?\n\nrv1 = sp.stats.norm(175,10)\nrv2 = sp.stats.norm(185,10)\nrv3 = sp.stats.norm(160,10)\nx = np.linspace(150,200,500)\nplt.figure(figsize=(10,5))\nplt.plot(x,rv1.pdf(x),\"b\")\nplt.plot(x,rv2.pdf(x),\"g\")\nplt.plot(x,rv3.pdf(x),\"purple\")\nplt.plot([177],[0],\"o\",color=\"black\",ms=\"10\")\nplt.plot \nplt.axvline(177,color = \"black\",linewidth = 1,linestyle = \"--\",ymin=0.07)\nplt.title(\"Normal distributions\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend([\"$N(175,10)$\",\"$N(170,10)$\",\"$N(165,10)$\"],loc=\"upper right\")\n\n<matplotlib.legend.Legend at 0x21d4e82fb20>\n\n\n\n\n\nê·¸ë¦¼ê³¼ ê°™ì´ í¬ê¸°ê°€1ì¸ í‘œë³¸ì„ 177 ì–»ì—ˆê³  í™•ë¥ ë¶„í¬ì˜ ëª¨ì–‘ì€ ì •ê·œë¶„í¬ë¡œ ê°€ì •í–ˆë‹¤ê³  í•´ë´…ì‹œë‹¤. ìƒ˜í”Œì´ ì£¼ì–´ì ¸ ìˆì„ë•Œ ê°€ëŠ¥ì„±ì´ ê°€ì¥ ë†’ì€ ëª¨ìˆ˜ëŠ” ë¬´ì—‡ì¼ê¹Œìš”? ê·¸ë˜í”„ë¥¼ ë“¤ì—¬ë‹¤ë³´ë‹ˆ ë³´ë¼ìƒ‰ í™•ë¥ ë¶„í¬ì—ì„œëŠ” ë­”ê°€ í‘œë³¸ì„ ì–»ê¸°ëŠ” í˜ë“¤ ê²ƒ ê°™ìŠµë‹ˆë‹¤. í‘œë³¸ì„ ë½‘ì„ í™•ë¥ (ì—„ë°€íˆëŠ” í™•ë¥ ë°€ë„ì´ì§€ë§Œ í™•ë¥ ë¡œ ì ê² ìŠµë‹ˆë‹¤.)ì´ ë„ˆë¬´ ë‚®ì•„ì„œ ê°€ëŠ¥ì„±ì´ ë„ˆë¬´ ë‚®ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì´ˆë¡ìƒ‰ì˜ ê²½ìš°ëŠ” ë³´ë¼ìƒ‰ë³´ë‹¤ëŠ” í™•ë¥ ì´ ë” í¬ê¸°ë•Œë¬¸ì— ê°€ëŠ¥ì„±ì´ ë” ì»¤ë³´ì…ë‹ˆë‹¤. íŒŒë‘ìƒ‰ì˜ ê²½ìš°ëŠ” í™•ë¥ ì´ ì´ˆë¡ìƒ‰ë³´ë‹¤ë„ ë” í½ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ 3ê°œì˜ ë¶„í¬ì¤‘ì—ì„œëŠ” íŒŒë‘ìƒ‰ í™•ë¥ ë¶„í¬ì˜ ëª¨ìˆ˜ì¸ 175ê°€ ê°€ëŠ¥ì„±ì´ ê°€ì¥ ì»¤ë³´ì´ê³  ì´ë¥¼ ì‹¤ì œ ëª¨ìˆ˜ì— ëŒ€í•œ ì¶”ì •ê°’ìœ¼ë¡œ í•˜ëŠ” ê²ƒì´ í•©ë¦¬ì ì…ë‹ˆë‹¤.\nìœ„ì˜ ì˜ˆì‹œì—ì„œ ê°€ëŠ¥ì„±ì„ í™•ì¸í•˜ê¸° ìœ„í•´ì„œ í™•ë¥ ë¶„í¬ì˜ ê°’(yì¶•,í™•ë¥ ,í™•ë¥ ë°€ë„)ì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤.ê²°êµ­ ìš°ë¦¬ê°€ ì •ì˜í•˜ê¸°ë¡œ ì§€í‘œì¸ ëª¨ìˆ˜ì˜ ê°€ëŠ¥ì„±ì€ ê²°êµ­ì€ í™•ë¥ (ë˜ëŠ” í™•ë¥ ë°€ë„) ê·¸ ìì²´ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ í™•ë¥ ê³¼ëŠ” ì•½ê°„ ë‹¤ë¥´ê²Œ í™•ë¥ (ë˜ëŠ” í™•ë¥ ë°€ë„)ì€ ëª¨ìˆ˜ë¥¼ ê³ ì •í•˜ê³  í™•ë¥ ë¶„í¬ì˜ ì½ëŠ” í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•œ í•¨ìˆ˜ì´ê³  ë°˜ë©´ì— ëª¨ìˆ˜ì˜ ê°€ëŠ¥ì„±ì€ ìƒ˜í”Œì€ ê³ ì •ë˜ìˆê³  ëª¨ìˆ˜ë¥¼ ë°”ê¿”ê°€ë©° ì„œë¡œë‹¤ë¥¸ í™•ë¥ ë¶„í¬ì—ì„œ ê°’ì„ ì½ëŠ” ëª¨ìˆ˜ì— ëŒ€í•œ í•¨ìˆ˜ë¼ëŠ” ì ì— ì°¨ì´ê°€ ìˆìŠµë‹ˆë‹¤. ëª¨ìˆ˜ì˜ ê°€ëŠ¥ì„±ì—ëŠ” ê°€ëŠ¥ë„(likelyhood)ë¼ëŠ” ì´ë¦„ì„ ë¶™ì—¬ì¤ë‹ˆë‹¤.\n\nrv1 = sp.stats.norm(175,10)\nrv2 = sp.stats.norm(185,10)\nrv3 = sp.stats.norm(160,10)\nx = np.linspace(150,200,500)\nplt.figure(figsize=(10,5))\nplt.plot(x,rv1.pdf(x),\"b\")\nplt.plot(x,rv2.pdf(x),\"g\")\nplt.plot(x,rv3.pdf(x),\"purple\")\ns = [173,177,175,180]\nplt.plot(s,[0,0,0,0],\"o\",color=\"black\",ms=\"10\")\nfor i in s:\n    temp = [i]\n    plt.axvline(temp,linestyle = \"--\",color=\"black\",linewidth=1,ymin=0.07,ymax = 0.96)\nplt.title(\"Normal distributions\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.legend([\"$N(175,10)$\",\"$N(170,10)$\",\"$N(165,10)$\"],loc=\"upper right\")\n\n<matplotlib.legend.Legend at 0x21d4e89d160>\n\n\n\n\n\nì´ë²ˆì—ëŠ” í¬ê¸°ê°€ 4ì¸ í‘œë³¸ì„ ì–»ì—ˆë‹¤ê³  í•´ë´…ì‹œë‹¤. ë¶„í¬ì˜ ëª¨ì–‘ì€ ë§ˆì°¬ê°€ì§€ë¡œ ì •ê·œë¶„í¬ë¡œ ê°€ì •í–ˆìŠµë‹ˆë‹¤. ì´ë²ˆì—ëŠ” ìƒ˜í”Œì˜ í¬ê¸°ê°€4ì´ë¯€ë¡œ ì´ì „ê³¼ëŠ” ë‹¤ë¥´ê²Œ í•˜ë‚˜ì˜ ë°ì´í„°í¬ì¸íŠ¸ë§Œ ë°˜ì˜í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ 4ê°œì˜ ë°ì´í„°í¬ì¸íŠ¸ë¥¼ ëª¨ë‘ ë°˜ì˜í•˜ì—¬ ê°€ëŠ¥ì„±ì´ ê°€ì¥ ë†’ì€ ëª¨ìˆ˜ë¥¼ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. \nìœ„ì—ì„œ 1ê°œì˜ ë°ì´í„°í¬ì¸íŠ¸(í‘œë³¸)ë¥¼ ê³ ë ¤í• ë•Œì—ëŠ” ì—¬ëŸ¬ê°€ì§€ ëª¨ìˆ˜ë¥¼ ë”°ë¥´ëŠ” ë¶„í¬ì—ì„œ 1ê°œì˜ í™•ë¥ ê°’ë§Œ ì½ê³  ì–´ë–¤ ëª¨ìˆ˜ê°€ ê°€ëŠ¥ì„±ì´ ê°€ì¥ ë†’ì€ì§€ í™•ì¸í•˜ê³  ê°€ëŠ¥ë„ë¼ëŠ” ê²ƒì„ ì •ì˜í–ˆìŠµë‹ˆë‹¤. 4ê°œì¼ë•Œë„ ë‹¤ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. 4ê°œì¼ë•Œì—ë„ í™•ë¥ ê°’ì„ ì½ëŠ”ë° ë‹¤ë§Œ ë‹¬ë¼ì§„ ê²ƒì€ 4ê°œì˜ í™•ë¥ ê°’ì„ ë™ì‹œì— í•¨ê»˜ ë°˜ì˜í•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. 4ê°œì¼ ê²½ìš°ì—ëŠ” 4ê°œì˜ í‘œë³¸ ëª¨ë‘ì— ëŒ€í•˜ì—¬ ë™ì‹œì— í•¨ê»˜ ê·¸ ê°€ëŠ¥ì„±ì„ ê³ ë ¤í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì—¬ê¸°ì„œëŠ” ê°€ëŠ¥ë„ì¸ í™•ë¥ ì„ ê°ê°ì˜ ë°ì´í„°í¬ì¸íŠ¸ë¥¼ ë½‘ì„ ë™ì‹œì— í•¨ê»˜ ë½‘ì„ í™•ë¥ ì´ì í™•ë¥ ì˜ ê³±ì¸ í™•ë¥ ì¸ ê²°í•©í™•ë¥ (joint distribution)ë¡œ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤.\nì •ë¦¬ ëª¨ìˆ˜ì˜ ê°€ëŠ¥ë„ëŠ” ìƒ˜í”Œê³¼ í™•ë¥ ë¶„í¬ì˜ ëª¨ì–‘ì´ ì£¼ì–´ì ¸ìˆì„ ë•Œ,ìƒ˜í”Œì´ ëª¨ìˆ˜ë¡œë¶€í„° ë‚˜ì˜¤ê¸°ê°€ ì–¼ë§ˆë‚˜ ê°€ëŠ¥í•œì§€ë¥¼ ì•Œë ¤ì£¼ë©° ê²°ê³¼ì ìœ¼ë¡œëŠ” (ê²°í•©)í™•ë¥  ì…ë‹ˆë‹¤. ë‹¤ë§Œ í™•ë¥ (ë˜ëŠ” í™•ë¥ ë¶„í¬)ì˜ ê²½ìš°ì—ëŠ” ëª©ì ì´ ëª¨ìˆ˜ê°€ ì •í•´ì§„ í™•ë¥ ë¶„í¬ë¡œë¶€í„° ì½ëŠ” ê·¸ ê°’ì„ ì½ì–´ ì–´ë–¤ ìƒ˜í”Œ xê°’ì„ ì·¨í•  ê°€ëŠ¥ì„±ì„ íŒŒì•…í•˜ëŠ” ê²ƒì´ê³  ë°˜ë©´ ê°€ëŠ¥ë„ëŠ” ìƒ˜í”Œì´ ì´ë¯¸ ë½‘í˜€ì„œ ì •í•´ì ¸ìˆì„ë•Œ, ì„œë¡œë‹¤ë¥¸ ëª¨ìˆ˜ë¥¼ ê°€ì§€ëŠ” í™•ë¥ ë¶„í¬ì˜ ê°’ì„ ì½ì–´ì„œ ì–´ë–¤ ëª¨ìˆ˜ê°€ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ì§€ íŒŒì•…í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ìˆ˜í•™ì ìœ¼ë¡œ í‘œê¸°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{align}\nL(\\theta|D) = L(\\theta|x_1,x_2,\\dots,x_N) &= f_{X_1,X_2,\\dots,X_N}(x_1,x_2,\\dots,x_N;\\theta) \\\\\n&= \\prod_{i=1}^{N}f_{X_i}(x_i;\\theta)\n\\end{align}\\]\nì‹(1)ì€ ê°€ëŠ¥ë„ = ê²°í•©í™•ë¥ ì„ì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì‹(2)ëŠ” í™•ë¥ ë³€ìˆ˜ê°€ ë…ë¦½ì¼ ê²½ìš°ì˜ ê²°í•©í™•ë¥  = í™•ë¥ ì˜ ê³±ì…ë‹ˆë‹¤. ê°ê°ì˜ ìƒ˜í”Œì— ëŒ€í•œ í™•ë¥ ë³€ìˆ˜ëŠ” ëª¨ë‘ ë…ë¦½ì´ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#likelyhoodì˜-ìµœëŒ“ê°’-êµ¬í•˜ê¸°",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#likelyhoodì˜-ìµœëŒ“ê°’-êµ¬í•˜ê¸°",
    "title": "Maximum likelyhood estimation",
    "section": "Likelyhoodì˜ ìµœëŒ“ê°’ êµ¬í•˜ê¸° ",
    "text": "Likelyhoodì˜ ìµœëŒ“ê°’ êµ¬í•˜ê¸° \nì—¬ê¸°ê¹Œì§€ ê°€ëŠ¥ë„í•¨ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì„ ì•Œì•„ë´¤ìŠµë‹ˆë‹¤. ê°€ëŠ¥ë„í•¨ìˆ˜ë¥¼ ìµœëŒ€í™” í• ë•Œì˜ ê°’ì´ ì£¼ì–´ì§„ ìƒ˜í”Œì— ëŒ€í•´ì„œ ê°€ì¥ ê°€ëŠ¥ì„±ì´ ë†’ì€ í™•ë¥ ë¶„í¬ì˜ ëª¨ìˆ˜ì´ë¯€ë¡œ ê·¸ë•Œì˜ ê°’ìœ¼ë¡œ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•©ë‹ˆë‹¤. MLEì¶”ì •ëŸ‰ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MLE} &= \\underset{\\theta}{\\text{argmax}}\\,L(\\theta|x_1,x_2 \\dots x_n) \\\\\n&=  \\underset{\\theta}{\\text{argmax}},f_{X_1,X_2,\\dots,X_n}(x_1,x_2,\\dots,x_n|\\theta) \\\\\n&= \\underset{\\theta}{\\text{argmax}}\\,\\prod_{i=1}^{N}f_{X_i}(x_i;\\theta)\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#ë² ì´ì¦ˆ-ì •ë¦¬ì—-ì˜í•œ-mle",
    "href": "posts/Probability&Statistics/Maximum likelyhood estimation/MLE.html#ë² ì´ì¦ˆ-ì •ë¦¬ì—-ì˜í•œ-mle",
    "title": "Maximum likelyhood estimation",
    "section": "ë² ì´ì¦ˆ ì •ë¦¬ì— ì˜í•œ MLE",
    "text": "ë² ì´ì¦ˆ ì •ë¦¬ì— ì˜í•œ MLE\n\\(D=(t_1,t_2,\\dots,t_n)\\)ëŠ” ìƒ˜í”Œ(ë°ì´í„°ì…‹), \\(\\theta\\)ëŠ” ìš°ë¦¬ê°€ ì•Œê³ ì‹¶ì€ í™•ë¥ ë¶„í¬ì˜ ëª¨ìˆ˜ë¼ê³  í•©ì‹œë‹¤. ë² ì´ì¦ˆì •ë¦¬ëŠ” ì¦ê±° ë˜ëŠ” ì¡°ê±´ì´ ì£¼ì–´ì§€ê¸° ì „ì˜ ì‚¬ì „í™•ë¥  \\(p(D)\\)ì™€ ì£¼ì–´ì§„ í›„ì˜ ì‚¬í›„í™•ë¥  \\(p(\\theta|D)\\)ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤.\n\\[\\begin{aligned}\np(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{p(D)} \\propto p(\\theta|D) \\times p(\\theta)\n\\end{aligned}\\]\nì—¬ê¸°ì„œ ì‚¬í›„í™•ë¥ ì€ \\(\\theta\\)ì— ëŒ€í•œ í™•ë¥ ë¶„í¬ë¡œ ë°ì´í„°(ìƒ˜í”Œ)ì´ ì£¼ì–´ì§ˆë•Œ í™•ë¥ ë¶„í¬ì˜ ì„ì˜ì˜ ëª¨ìˆ˜\\(\\theta\\)ê°€ ì–¼ë§ˆë‚˜ ê°€ëŠ¥í•œì§€ ë˜ëŠ” ë¶ˆí™•ì‹¤í•œì§€ ê·¸ ì •ë„ë¥¼ ì•Œë ¤ì£¼ëŠ” í™•ë¥ ì„ í•¨ìˆ«ê°’ìœ¼ë¡œ ê°€ì§€ëŠ” í™•ë¥ í•¨ìˆ˜ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì‚¬í›„í™•ë¥ ì„ ìµœëŒ€ë¡œ í•˜ëŠ” ëª¨ìˆ˜\\(\\theta\\)ê°€ ë°ì´í„°ì…‹ì´ ì£¼ì–´ì ¸ìˆì„ ë•Œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€,ê°€ëŠ¥ì„±ì´ ë†’ì€ ëª¨ìˆ˜ì´ë¯€ë¡œ ê·¸ë•Œì˜ ê°’ì„ í™•ë¥ ë¶„í¬ì˜ ëª¨ìˆ˜ë¡œ ì¶”ì •í•˜ë©´ ë©ë‹ˆë‹¤.\në¬¸ì œëŠ” ì™¼ìª½ì˜ í™•ë¥ ë¶„í¬ëŠ” ë°”ë¡œ ì•Œê¸°ê°€ ì‰½ì§€ ì•Šë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë² ì´ì¦ˆì •ë¦¬ë¥¼ í†µí•˜ì—¬ ìš°ë³€ì˜ ì‹ì„ ìµœëŒ€í™” í•˜ëŠ” ê°’ì„ êµ¬í•©ë‹ˆë‹¤. ìš°ë³€ì˜ ì‹ì—ì„œ ë¶„ëª¨ëŠ” ì£¼ì–´ì§„ ë°ì´í„°ì— ì˜í•˜ì—¬ ê³ ì •ëœ ìƒìˆ˜(normalization constantë¼ê³  í•©ë‹ˆë‹¤)ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ìµœëŒ“ê°’ì„ êµ¬í•˜ëŠ”ë° ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¶„ìì— ìˆëŠ” \\(p(D|\\theta)p(\\theta)\\)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” \\(\\theta\\)ë¥¼ ì°¾ìœ¼ë©´ ë©ë‹ˆë‹¤.\nì—¬ê¸°ì„œ \\(p(D|\\theta)\\)ë¥¼ ê°€ëŠ¥ë„(likelyhood)ë¼ í•©ë‹ˆë‹¤. MLEì—ì„œëŠ” ë¶„ìì—ì„œ ê°€ëŠ¥ë„ë§Œ ìµœëŒ€ë¡œ í•˜ëŠ” \\(\\theta\\)ë¥¼ êµ¬í•©ë‹ˆë‹¤. MAPë¼ëŠ” ë‹¤ë¥¸ ë°©ë²•ì€ ë¶„ìì— ìˆëŠ” \\(p(D|\\theta)p(\\theta)\\)ë¥¼ ìµœëŒ€í™” í•˜ëŠ” \\(\\theta\\)ë¥¼ êµ¬í•œë‹¤ê³  í•©ë‹ˆë‹¤.\n\nì˜ˆì‹œ - ì •ê·œë¶„í¬ ë°ì´í„°ê°€ ì£¼ì–´ì§„ ê²½ìš°\nëŒ€í•œë¯¼êµ­ 20ëŒ€ ë‚¨ì„±ë“¤ì˜ í‚¤ì˜ ë¶„í¬ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•´ë³´ê² ìŠµë‹ˆë‹¤. ìƒ˜í”Œë§í•˜ì—¬ í¬ê¸°ê°€ 4ì¸í‘œë³¸[173,177,175,180]ì„ ì–»ì€ ìƒíƒœì…ë‹ˆë‹¤. ëª©ì ì€ ì •ê·œë¶„í¬ì˜ ëª¨ìˆ˜ì¸ \\(\\mu\\)ë¥¼ MLEë¡œ ì¶”ì •í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\në‹¤ìŒì„ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤\n\\[\\begin{aligned}\n\\underset{\\theta}{\\text{argmax }}L(\\theta;x_1,x_2,x_3) &= \\prod_{i=1}^{4}f_{X_i}(x_i;\\theta)\\\\\n&= \\underset{\\theta}{\\text{argmax }}f_{X_1}(x_1;\\theta)f_{X_2}(x_2;\\theta)f_{X_2}(x_2;\\theta)f_{X_2}(x_2;\\theta)\\\\\n&= \\underset{\\theta}{\\text{argmax }}\\mathcal{N}(x_1;\\mu,\\sigma^2)\\mathcal{N}(x_2;\\mu,\\sigma^2)\\mathcal{N}(x_3;\\mu,\\sigma^2)\\mathcal{N}(x_4;\\mu,\\sigma^2) \\\\\n&= \\underset{\\theta}{\\text{argmax }}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left({-\\frac{(173-\\mu)^2}{2\\sigma^2}}\\right) \\cdot \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left({-\\frac{(177-\\mu)^2}{2\\sigma^2}}\\right) \\\\ \\cdot &\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left({-\\frac{(175-\\mu)^2}{2\\sigma^2}}\\right) \\cdot\\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left({-\\frac{(178-\\mu)^2}{2\\sigma^2}}\\right)\\\\\n&= \\underset{\\theta}{\\text{argmax }}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(-\\frac{(173-\\mu)^2 + (177-\\mu)^2 + (175-\\mu)^2 + (178-\\mu)^2}{\\sigma^2})\n\\end{aligned}\\]\nìœ„ì˜ ê°€ëŠ¥ë„ í•¨ìˆ˜ê°€ ìµœëŒ€ê°€ ë˜ë ¤ë©´ expì˜ ì§€ìˆ˜ì˜ ë¶„ìì¸ \\(-[(173-\\mu)^2 + (177-\\mu)^2 + (175-\\mu)^2 + (178-\\mu)^2]\\)ê°€ ìµœëŒ€ê°€ ë˜ë©´ ë©ë‹ˆë‹¤.(ë¶„ì‚°ì€ ê³ ë ¤í•˜ì§€ ì•Šê² ìŠµë‹ˆë‹¤.)ë¶„ìëŠ” 2ì°¨í•¨ìˆ˜ì´ë¯€ë¡œ ìµœëŒ€ê°€ ë˜ëŠ” ì§€ì ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ë„˜íŒŒì´ë¥¼ í™œìš©í•˜ì—¬ ê³„ì‚°í•˜ê² ìŠµë‹ˆë‹¤.\n\nx = np.linspace(120,232,50000)\ny = -((173-x)**2 +(177-x)**2+(175-x)**2+(178-x)**2)\nmax_idx = np.where(y == np.max(y))\nprint(x[max_idx])\nplt.axvline(x[max_idx],color=\"black\",linestyle=\"--\")\nplt.plot(x,y,\"b\")\nplt.scatter(x[max_idx],y[max_idx],s=100,c=\"black\",marker=\"s\")\nplt.title(f\"max(y) = {round(np.max(y),3)} if $\\mu$ = {round(x[max_idx][0],3)}, \")\n\n[175.750235]\n\n\nText(0.5, 1.0, 'max(y) = -14.75 if $\\\\mu$ = 175.75, ')\n\n\n\n\n\nê°€ëŠ¥ë„í•¨ìˆ˜ë¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” ëª¨ìˆ«ê°’ì€ 175.75ì…ë‹ˆë‹¤. ë”°ë¼ì„œ MLEì— ì˜í•œ ëª¨ìˆ˜ì— ëŒ€í•œ ì¶”ì •ê°’ì€ 175.75ì…ë‹ˆë‹¤. \\[\\hat{\\theta}_{MLE} = 175.75\\]\n\n\nLL\nLLì€ log likelyhoodì˜ ì•½ìë¡œ likelyhoodì— logë¥¼ ì·¨í•´ì¤€ ê°’ì…ë‹ˆë‹¤. ë¡œê·¸í•¨ìˆ˜ë¥¼ ì·¨í•´ë„ í•¨ìˆ˜ê°€ MLEì˜ ê³„ì‚°ê²°ê³¼(ê°€ëŠ¥ë„ê°€ ìµœëŒ€ì¸ ëª¨ìˆ˜ì— ëŒ€í•œ ì¶”ì •ê°’)ì˜ ìœ„ì¹˜ê°€ ë³€í•˜ì§€ ì•Šê³  ê³„ì‚°ì„ ê³±ì…ˆì„ ë”í•˜ê¸°ë¡œ ë°”ê¿”ì„œ ê³„ì‚°í•˜ê¸°ì— ë” í¸ë¦¬í•˜ê¸° ë•Œë¬¸ì— LLì„ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. \\[LL = \\text{ln }L(\\theta|x_1,x_2 \\dots,x_N)\\]\n\n\nNLL\nNLLì€ LLì— -(negative)ë¥¼ ê³±í•´ì¤€ ê°’ì…ë‹ˆë‹¤. í•¨ìˆ˜ê°€ ìµœëŒ€ì¸ ì§€ì ì„ ì°¾ëŠ” ë¬¸ì œë¥¼ ìµœì†Œì¸ ì§€ì ì„ ì°¾ëŠ” ë¬¸ì œë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë¨¸ì‹ ëŸ¬ë‹ì´ë‚˜ ìµœì í™”ì—ì„œ ì‚¬ìš©í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤. \\[NLL = -\\text{ln }L(\\theta|x_1,X_2 = x_2 \\dots x_N)\\]"
  },
  {
    "objectID": "posts/Probability&Statistics/notation.html",
    "href": "posts/Probability&Statistics/notation.html",
    "title": "í™•ë¥ ë¶„í¬ì—ì„œ ;ì™€|ì˜ ì‚¬ìš©",
    "section": "",
    "text": "í™•ë¥ ë¶„í¬ì—ì„œ ;ì™€|ì— ê´€í•œ notation ì •ë¦¬\n\n;ì˜ ì‚¬ìš©\n; í•¨ìˆ˜ì—ì„œ íŠ¹ì • ë³€ìˆ˜ë¥¼ ê³ ì •ì‹œì¼œë†“ì„ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ì´ë³€ìˆ˜í•¨ìˆ˜ë¥¼ ìƒê°í•´ë³´ì \\[f(x,y)\\] ë…ë¦½ë³€ìˆ˜ xì™€ yë¥¼ ê°–ëŠ” í•¨ìˆ˜ì´ë©° ë‘ ë³€ìˆ˜ ëª¨ë‘ì— ëŒ€í•´ì„œ ë¯¸ë¶„ê°€ëŠ¥í•˜ë‹¤. ì—¬ê¸°ì„œ ë³€ìˆ˜ yê°€ ì–´ë–¤ ì–´ë–¤ ê°’ìœ¼ë¡œ ê³ ì •ëœê²ƒì´ ì•Œë ¤ì§€ê±°ë‚˜ ë˜ëŠ” ê³ ì •ëœìƒí™©ì„ ê°€ì •í•œ í›„ xì— ëŒ€í•´ì„œë§Œ ê´€ì‹¬ì´ ìˆë‹¤ê³  í•´ë³´ì(ë¯¸ë¶„,ê·¹í•œë“±ì„ ì•„ë§ˆë„?ì·¨í•˜ê³  ì‹¶ë‹¤..ì•„ë§ˆë„â€¦) ì´ë•ŒëŠ” ì´ë³€ìˆ˜í•¨ìˆ˜ë¡œ í‘œê¸°í•˜ëŠ”ê²ƒì´ ì•„ë‹Œ ì¼ë³€ìˆ˜í•¨ìˆ˜ë¡œ í‘œê¸°í•´ì•¼í•œë‹¤. ê·¸ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œê¸°í•˜ë©´ ëœë‹¤.\n\\[\nf(x;y)\n\\]\nì´ë ‡ê²Œ í‘œê¸°í•˜ë©´ yê°’ì€ ì´ì œ ì–´ë–¤ ê°’ìœ¼ë¡œ ì´ë¯¸ ì„¤ì •(setting)ë˜ê±°ë‚˜ ê³ ì •(fixed)ë˜ì–´ìˆìŒì„ ì˜ë¯¸í•œë‹¤. ë¹„ìŠ·í•˜ê²Œ ê·¸ëƒ¥ ê³ ì •ëœ íŒŒë¼ë¯¸í„°ê°€ ì´ ë‹¤ìŒì—ë„ ì˜¬ ìˆ˜ ìˆë‹¤. ë² ë¥´ëˆ„ì´ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[f_X(x;p) = p^x(1-p)^{1-x}\\]\nsetting,fixedëœ íŒŒë¼ë¯¸í„°ì¸ pë¥¼ ;ë‹¤ìŒì— í‘œì‹œí–ˆë‹¤.\n\n\n|ì˜ ì‚¬ìš©\n|ëŠ” given that,ifì´ë¼ëŠ” ì˜ë¯¸ì´ë©° |ì˜ ë‹¤ìŒì—ëŠ” ë³´í†µ ë¨¼ì € ê¹”ê³ ê°€ê±°ë‚˜ ì£¼ì–´ì§€ëŠ” ì „ì œ,ìƒí™©,ì¡°ê±´,ì¦ê±° ë“±ì´ ë‚˜ì˜¨ë‹¤. í™•ë¥ ,í†µê³„ ë¶„ì•¼ì—ì„œë§Œ ì“°ì´ë©° ì¡°ê±´ë¶€ í™•ë¥ ì´ë‚˜,ë² ì´ì§€ì•ˆì—ì„œ ë§ì´ ì“°ì´ëŠ” ê¸°í˜¸ë¼ê³  í•œë‹¤. ì¤‘ìš”í•œ ì ì€ íŠ¹íˆ |ë‹¤ìŒì— ì˜¤ëŠ” ìƒí™©,ì¡°ê±´,ì¦ê±°ì™€ ê°™ì€ ê²ƒë“¤ì€ ì–´ë–¤ ê´€ì ì´ëƒì— ë”°ë¼ì„œ ìƒìˆ˜ë‚˜ í™•ë¥ ë³€ìˆ˜ì˜ ê´€ì ìœ¼ë¡œ ìƒê°ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. íŠ¹íˆ ë² ì´ì§€ì•ˆì—ì„œëŠ” í™•ë¥ ë³€ìˆ˜ë¡œ ìƒê°í•œë‹¤.\nì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬(ì¡°ê±´ë¶€í™•ë¥ ë°€ë„í•¨ìˆ˜,ì¡°ê±´ë¶€ í™•ë¥ í•¨ìˆ˜)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\np_{X|Y}(x|y) = \\frac{p_{XY}(x,y)}{p_Y(y)}\\\\\np_{X|Y}(y|x) = \\frac{p_{XY}(x,y)}{p_X(x)}\n\\]  ê°€ì¥ ìœ—ì‹ì„ í•´ì„í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. - ì¢Œë³€ : í™•ë¥ ë³€ìˆ˜ Yê°€ ì£¼ì–´ì§„(given)ìƒí™©ì—ì„œ í™•ë¥ ë³€ìˆ˜Xì˜ í™•ë¥ (ë˜ëŠ” í™•ë¥ ë°€ë„)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ì´ë©° ì‹œí–‰ìœ¼ë¡œë¶€í„° í™•ë¥ ë³€ìˆ˜ Xê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ xë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. ì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬ì—ì„œ Yì˜ ê°’ì€ yë¡œ ì£¼ì–´ì ¸ ìˆë‹¤ê³  ê°€ì •í•˜ë¯€ë¡œ ì…ë ¥í•˜ëŠ” ë³€ìˆ˜ê°€ ì•„ë‹ˆë‹¤. ë”°ë¼ì„œ í™•ë¥ ë³€ìˆ˜ YëŠ” ë³€ìˆ˜ê°€ ì•„ë‹ˆë¼ ëª¨ìˆ˜(parameter)ì´ë‹¤. - ìš°ë³€ : ì¡°ê±´ë¶€í™•ë¥ ë¶„í¬ ê³µì‹.\n##  ê²°ë¡  ; vs |\n\\[p_{\\theta}(x) = p(x;\\theta) = p(x|\\theta)\\]  ;ëŠ” ìˆ˜í•™ì˜ ëª¨ë“  ë¶„ì•¼ì—ì„œ ì“°ì´ì§€ë§Œ |ëŠ” í™•ë¥ í†µê³„ë¶„ì•¼ì—ì„œë§Œ ì“°ì¸ë‹¤. ;ë‹¤ìŒì—ëŠ” ì´ë¯¸ ì„¤ì •ë˜ê±°ë‚˜ ê³ ì •ëœ ê°’ì´ ë‚˜ì˜¤ë©° |ë‹¤ìŒì—ëŠ” ë¨¼ì € ì£¼ì–´ì§€ëŠ” ì¡°ê±´ì´ ë‚˜ì˜¨ë‹¤. ê·¼ë° ;ë¥¼ ë§ì´ ì“°ì§€ ì•Šê³  |ë’¤ì— ê·¸ëƒ¥ ê³ ì •ëœ ê°’ì„ ì¨ë²„ë¦¬ëŠ” ê²½ìš°ê°€ ë§ë‹¤. ì´ëŠ” ì§€ì–‘í•´ì•¼ í•  í‘œí˜„ì´ë‹¤.\nì°¸ê³ ë§í¬ ë§í¬1(;ì˜ ì‚¬ìš©ë²•) ë§í¬2(ì¡°ê±´ë¶€í™•ë¥ ë¶„í¬) ë§í¬3(í‘œê¸°ë²•ì˜ í˜¼ìš©)"
  },
  {
    "objectID": "posts/Probability&Statistics/sample space and random variable.html",
    "href": "posts/Probability&Statistics/sample space and random variable.html",
    "title": "í™•ë¥ ë¡  ìš©ì–´ì •ë¦¬",
    "section": "",
    "text": "Experiment and trial\nê°€ëŠ¥í•œ ê²°ê³¼ë“¤ì´ ë¯¸ë¦¬ ì •í•´ì ¸ìˆê³  ë¬´í•œíˆ ë°˜ë³µê°€ëŠ¥í•œ ê³¼ì •ì„ í™•ë¥ ì‹¤í—˜(experiment) ë˜ëŠ” ì‹œí–‰(trial)ì´ë¼ê³  í•©ë‹ˆë‹¤.\n\n\nsample space & event\nì–´ë–¤ ì„ì˜ì˜ ì‹œí–‰ì„ ê°€ì •í•´ë³¸ë‹¤ê³  í•©ì‹œë‹¤. ì‹œí–‰ìœ¼ë¡œë¶€í„° ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë“  ê²°ê³¼ë“¤ì˜ ì§‘í•©ì„ ìš°ë¦¬ëŠ” í‘œë³¸ê³µê°„(sample space)ë¼ê³  í•©ë‹ˆë‹¤. ì‚¬ê±´ì€ í‘œë³¸ê³µê°„ì´ë¼ëŠ” ì§‘í•©ì˜ ë¶€ë¶„ì§‘í•©ì…ë‹ˆë‹¤. ì—¬ëŸ¬ê°œì˜ ì›ì†Œ(ê²°ê³¼)ë“¤ì´ ëª¨ì—¬ì„œ í•˜ë‚˜ì˜ ì‚¬ê±´ì´ ë  ìˆ˜ ìˆìœ¼ë©° ë‹¨ í•˜ë‚˜ì˜ ì›ì†Œë¡œë„ ì‚¬ê±´ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹œí–‰ì˜ ê²°ê³¼ê°€ ì–´ë– í•œ ì‚¬ê±´(ë¶€ë¶„ì§‘í•©)ì— ì†í•˜ëŠ” ê²½ìš° ìš°ë¦¬ëŠ” â€œ~ì¸ ì‚¬ê±´ì´ ë°œìƒí–ˆë‹¤!â€ë¼ê³  í‘œí˜„í•©ë‹ˆë‹¤.\n\n\nrandom variable\ní™•ë¥ ë³€ìˆ˜ëŠ” í™•ë¥ ì‹¤í—˜ ë˜ëŠ” ì‹œí–‰ìœ¼ë¡œë¶€í„° ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ê²°ê³¼ë¥¼ ëŒ€ì‹  ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ì…ë‹ˆë‹¤. ì‹œí–‰ì„ í•˜ê¸°ì „ê¹Œì§€ëŠ” ê·¸ ê°’ì´ ì •í•´ì§€ì§€ ì•Šê³  í™•ë¥ ë¶„í¬ë§Œ ì¡´ì¬í•˜ë©° ì‹œí–‰ì„ í•˜ë©´ í™•ë¥ ë¶„í¬ì— ì˜í•´ì„œ ê²°ê³¼ê°€ ì •í•´ì§€ê³  ì‹¤ìˆ˜ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤. ìˆ˜í•™ì ìœ¼ë¡œëŠ” í‘œë³¸ê³µê°„ì˜ ì›ì†Œë¥¼ ì •ì˜ì—­ìœ¼ë¡œ í•˜ì—¬ ì‹¤ìˆ˜ë¥¼ ëŒ€ì‘ì‹œí‚¤ëŠ” â€œí•¨ìˆ˜â€ì…ë‹ˆë‹¤.\n\n\nprobability distribution(function)\ní™•ë¥ ë¶„í¬(í™•ë¥ í•¨ìˆ˜)ë€ í™•ë¥ ë³€ìˆ˜ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ì‹¤ìˆ˜ê°’ì— ê°ê°ì˜ ì‹¤ìˆ˜ë¥¼ ì·¨í•  ê°€ëŠ¥ì„±ì¸ í™•ë¥  ë˜ëŠ” í™•ë¥ ë°€ë„ë¥¼ ëŒ€ì‘ì‹œí‚¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n\n\nì˜ˆì‹œ\në™ì „ì„ ë‘ë²ˆ ë˜ì§€ëŠ” ì‹œí–‰ì„ 3ë²ˆ ë°˜ë³µí•˜ì—¬ í¬ê¸°ê°€ 3ì¸ í‘œë³¸ì„ ì–»ì—ˆë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤. ë™ì „ì˜ ì•ë©´ì„ H(head)ë¼ í•˜ê³  ë’·ë©´ì„ T(tail)ì´ë¼ê³  í•  ë•Œ í‘œë³¸ê³µê°„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\Omega = \\{HH,HT,TH,TT\\}\\]\ní‘œë³¸ê³µê°„ì— ìˆëŠ” ê²°ê³¼ë“¤ ì¤‘ì—ì„œ ë™ì „ì˜ ì•ë©´ì´ 1ê°œë¼ë„ ìˆëŠ” ê²½ìš° ì‚¬ê±´A ë™ì „ì˜ ì•ë©´ì´ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°ë¥¼ ì‚¬ê±´Bë¼ í•©ì‹œë‹¤. ì‚¬ê±´Aì™€ BëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[A = \\{HH,HT,TH\\},B=\\{TT\\}\\]\n\\(X_1,X_2,X_3\\)ëŠ” ê°ê° ì²«ë²ˆì§¸ ë‘ë²ˆì§¸ ì„¸ë²ˆì§¸ ì‹œí–‰ì˜ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ì´ë©° í™•ë¥ ë¶„í¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ê³  í•©ì‹œë‹¤.\n\n\n\n                                                \n\n\nìœ„ì˜ í™•ë¥ ë¶„í¬ë¥¼ ë°˜ì˜í•˜ì—¬ ì‹œí–‰ì˜ ê²°ê³¼ê°€ ê²°ì •ë©ë‹ˆë‹¤. ì‹œí–‰ìœ¼ë¡œë¶€í„° ì–»ì€ í‘œë³¸ì€ \\((1,1,0)\\)ì´ë©° \\(X_1 = 1 ,X_2 =1 ,X_3 =0\\) ì…ë‹ˆë‹¤. ì‹œí–‰ì˜ ê²°ê³¼ ì–»ì€ ì‹¤ì œë¡œ ê´€ì°°ëœ í‘œë³¸ì€ \\((x_1,x_2,x_3)\\) ì´ëŸ°ì‹ìœ¼ë¡œ ê°ê°ì˜ ì›ì†Œë¥¼ ì†Œë¬¸ìì¸ ë¯¸ì§€ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\n\ni.i.d & randomsample & realization\nìœ„ì˜ ë™ì „ë˜ì§€ê¸° ì‹¤í—˜ì—ì„œ ê°ê°ì˜ í™•ë¥ ë³€ìˆ˜ëŠ” ì´ì „ì— ë˜ì§„ ê²°ê³¼ê°€ ì´í›„ì— ë˜ì§€ëŠ” ê²°ê³¼ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê³ (ì¦‰,í™•ë¥ ë¶„í¬ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê³ ) ë™ì¼í•œ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë¶„í¬ì˜€ìŠµë‹ˆë‹¤. ì´ì™€ ì—¬ëŸ¬ê°œì˜ í™•ë¥ ë³€ìˆ˜ê°€ ì„œë¡œê°„ì— ë…ë¦½ì´ë©° ë™ì¼í•œ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜ë“¤ì„ Independent and identically distributed random variablesë¼ê³  í•©ë‹ˆë‹¤. ìœ„ì˜ ë¶„í¬ëŠ” ë² ë¥´ëˆ„ì´ ë¶„í¬ë¥¼ ë”°ë¥´ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[X_1,X_2,X_3 \\overset{i.i.d}{\\sim} \\text{Bernoulli(p = 0.7)}\\]\nrandomsampleì€ i.i.dì¸ ì—¬ëŸ¬ê°œì˜ í™•ë¥ ë³€ìˆ˜ì˜ ëª¨ìŒì…ë‹ˆë‹¤. \\(X1,X2,X3\\)ë¥¼ ë§í•©ë‹ˆë‹¤.\nrealizationì€ ê´€ì°°ëœ ê²°ê³¼ ê°ê°ì„ ë§í•©ë‹ˆë‹¤. \\(x1\\)ë„ realization \\(x2\\)ë„ realization \\(x3\\)ë„? ëª¨ë‘ realizationì…ë‹ˆë‹¤. \\(x1,x2,x3\\)ë¥¼ ëª¨ì•„ì„œ í™•ë¥ ë³€ìˆ˜ \\(X_1,X_2,X_3\\)ì˜ realizationsì´ë¼ê³  í•©ë‹ˆë‹¤.\n\n\nì°¸ê³ ìë£Œ\nwikipedia - Experiment (probability theory) StackExchange - What is the difference between random variable and random sample? wikipedia - i.i.d ì •ë³´í†µì‹ ê¸°ìˆ ìš©ì–´í•´ì„¤"
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html",
    "title": "íŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹",
    "section": "",
    "text": "íŒŒì´ì¬ ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹\nì „ë¶ëŒ€í•™êµ ìµœê·œë¹ˆ êµìˆ˜ë‹˜ì˜ ë”¥ëŸ¬ë‹ deepcopy-shallowcopy íŠ¹ê°• ì¤‘,ë³€ìˆ˜ì™€ í• ë‹¹ë¬¸ ë¶€ë¶„ì„ ì¬êµ¬ì„±í•œ ê¸€ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#íŒŒì´ì¬ì—ì„œì˜-ë³€ìˆ˜",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#íŒŒì´ì¬ì—ì„œì˜-ë³€ìˆ˜",
    "title": "íŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹",
    "section": "íŒŒì´ì¬ì—ì„œì˜ ë³€ìˆ˜",
    "text": "íŒŒì´ì¬ì—ì„œì˜ ë³€ìˆ˜\n\níŒŒì´ì¬ì˜ ë³€ìˆ˜ëŠ” ë©”ëª¨ë¦¬ìƒì— ì €ì¥ëœ ê°ì²´ë¥¼ ì°¸ì¡°(reference)í•©ë‹ˆë‹¤.\në”°ë¼ì„œ ë³€ìˆ˜ëŠ” ìë°”ì—ì„œ ì°¸ì¡°ë³€ìˆ˜ì™€ ê°™ìŠµë‹ˆë‹¤.\në³€ìˆ˜ëŠ” ê°ì²´(object)ë¥¼ ë¶€ë¥´ëŠ” ë³„ì¹­(ë‹¤ë¥¸ì´ë¦„),ê°ì²´ì— ë¶™ì—¬ì§„ í¬ìŠ¤íŠ¸ì‡,ë˜ë‹¤ë¥¸ ë ˆì´ë¸” ì´ë¼ê³  ìƒê°í•˜ëŠ” ê²ƒì´ ë¹„ìœ ì ìœ¼ë¡œ ë§ìŠµë‹ˆë‹¤.\nì•ìœ¼ë¡œ ê°ì²´ì— ì ‘ê·¼í•˜ê¸° ìœ„í•´ì„œëŠ” aë¼ëŠ” ë³€ìˆ˜(ë³„ì¹­,ê°ì²´)ë¥¼ ì°¾ìœ¼ë©´ ë©ë‹ˆë‹¤.\në§ˆì¹˜ Cì–¸ì–´ì˜ í¬ì¸í„°ì™€ ìœ ì‚¬í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#íŒŒì´ì¬ì—ì„œì˜-í• ë‹¹ë¬¸",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#íŒŒì´ì¬ì—ì„œì˜-í• ë‹¹ë¬¸",
    "title": "íŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹",
    "section": "íŒŒì´ì¬ì—ì„œì˜ í• ë‹¹ë¬¸(=)",
    "text": "íŒŒì´ì¬ì—ì„œì˜ í• ë‹¹ë¬¸(=)\n\n=ì€ íŒŒì´ì¬(ë¿ë§Œì•„ë‹ˆë¼ ëŒ€ë¶€ë¶„ì˜ í”„ë¡œê·¸ë˜ë°ì–¸ì–´)ì—ì„œ í• ë‹¹ë¬¸ì…ë‹ˆë‹¤.\ní• ë‹¹ë¬¸ì„ ì‹¤í–‰í•˜ë©´ = ì˜¤ë¥¸ìª½ì— ìˆëŠ” ê°ì²´ë¥¼ ë¨¼ì € ë©”ëª¨ë¦¬ ìƒì— ìƒì„±í•˜ê±°ë‚˜ ê°€ì ¸ì˜µë‹ˆë‹¤.\n=ì˜ ì™¼ìª½ì—ëŠ” ë³€ìˆ˜ê°€ ì¡´ì¬í•˜ë©° íŒŒì´ì¬ì€ ë³€ìˆ˜ì— ìƒì„±ëœ ê°ì²´ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.\në³€ìˆ˜ëŠ” ê°ì²´ì— ë¶™ì—¬ì§€ëŠ” ë³„ì¹­,ë ˆì´ë¸”ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ê°ì²´ì— ë³€ìˆ˜ê°€ í• ë‹¹ë˜ê³ ë‚˜ë©´ í• ë‹¹ëœ ë³€ìˆ˜ì™€ ê°ì²´ì— ëŒ€í•´ì„œ â€œë³€ìˆ˜ê°€ ê°ì²´ì— ë°”ì¸ë”©(ë¬¶ì´ë‹¤)ë˜ì–´ìˆë‹¤â€ê³  í‘œí˜„í•©ë‹ˆë‹¤.\n\n\na = [1,2,3]\nprint(id(a))\n\n1819154486912\n\n\nìœ„ ì½”ë“œì—ì„œ ê°ì²´[1,2,3]ì— ë³€ìˆ˜ aê°€ ë°”ì¸ë”© ë˜ì—ˆìŠµë‹ˆë‹¤.ë‚´ë¶€ë™ì‘ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 1. ë©”ëª¨ë¦¬ ì£¼ì†Œ(1819161042880)ì— ë¦¬ìŠ¤íŠ¸ ê°ì²´[1,2,3]ì„ ìƒì„±í•©ë‹ˆë‹¤ 2. ìƒì„±ëœ ë¦¬ìŠ¤íŠ¸ê°ì²´ë¥¼ ë³€ìˆ˜ aì— í• ë‹¹í•©ë‹ˆë‹¤. aëŠ” ê°ì²´[1,2,3]ì— ë¶™ì—¬ì§€ëŠ” ë³„ì¹­,ë ˆì´ë¸”ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì—ì¼ë¦¬ì–´ì‹±\nì—ì¼ë¦¬ì–´ì‹±ì€ í•˜ë‚˜ì˜ ê°ì²´ë¥¼ ì—¬ëŸ¬ê°œì˜ ë³€ìˆ˜ê°€ ì°¸ì¡°í•˜ê²Œ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í•˜ë‚˜ì˜ ê°ì²´ì— ì—¬ëŸ¬ê°œì˜ ë³„ì¹­,ë³„ëª…,ë ˆì´ë¸”ì„ ë¶™ì´ëŠ” ê²ƒì´ë¼ê³ ë„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nb = a\nprint(id(a));print(id(b))\nprint(a);print(b)\n\n1819154486912\n1819154486912\n[1, 2, 3]\n[1, 2, 3]"
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#idvalue",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#idvalue",
    "title": "íŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹",
    "section": "id,value",
    "text": "id,value\nidëŠ” ê°ì²´ê°€ ê°€ì§€ëŠ” ë©”ëª¨ë¦¬ìƒì˜ ê³ ìœ í•œ ì£¼ì†Œì…ë‹ˆë‹¤. ì„œë¡œë‹¤ë¥¸ ê°ì²´ëŠ” ë‹¤ë¥¸ ê°’ì„ ê°€ì§ˆìˆ˜ë„ ê°™ì€ê°’ì„ ê°€ì§ˆìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\na=[1,2,3]\nb=a\na.append(4)\nc=[1,2,3,4]\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\n\na:[1, 2, 3, 4] b:[1, 2, 3, 4] c:[1, 2, 3, 4]\nid(a):1819154849280,id(b):1819154849280,id(c):1819155097408\n\n\në³€ìˆ˜a,b,cëŠ” ëª¨ë‘ ê°™ì€ value(ê°’)ì„ ê°€ì§‘ë‹ˆë‹¤.aì™€bëŠ” ê°™ì€ ê°ì²´ì— ë°”ì¸ë”©ë˜ì–´ìˆì§€ë§Œ cëŠ” ë˜ë‹¤ë¥¸ ê°ì²´ì— ë°”ì¸ë”©ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#is-vs",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#is-vs",
    "title": "íŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹",
    "section": "is vs ==",
    "text": "is vs ==\n- isëŠ” ê°ì²´ë¹„êµì—°ì‚°ìë¡œ ë‘ ë³€ìˆ˜ê°€ ë™ì¼í•œ ê°ì²´ë¥¼ ì°¸ì¡°í•˜ëŠ”ì§€ ì•„ë‹ˆë©´ ë‹¤ë¥¸ê°ì²´ë¥¼ ì°¸ì¡°í•˜ëŠ”ì§€ í™•ì¸í•œ í›„ True or Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. íŒŒì´ì¬ì€ ë‚´ë¶€ì ìœ¼ë¡œ ë™ì¼í•œ ê°ì²´ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë‹¨í• ë•Œì—ëŠ” ë©”ëª¨ë¦¬ì£¼ì†Œë¥¼ í™•ì¸í•œë‹¤ê³  í•©ë‹ˆë‹¤. - ==ëŠ” ê°’ë¹„êµì—°ì‚°ìë¡œ ë‘ ë³€ìˆ˜ê°€ ì°¸ì¡°í•˜ëŠ” ê°ì²´ì˜ ê°’ì´ ê°™ì€ì§€ ì•„ë‹ˆë©´ ê°’ì´ ë‹¤ë¥¸ì§€ë¥¼ í™•ì¸í•œ í›„ True or Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. - ì°¸ì¡°í•˜ë‹¤ëŠ” ë­”ê°€ ì˜ ì™€ë‹¿ëŠ”ë° ì°¸ì¡°`ë¼ëŠ” ìš©ì–´ëŠ” ì˜ ì™€ë‹¿ì§€ê°€ ì•ŠìŠµë‹ˆë‹¤â€¦ë³€ìˆ˜ê°€ ì°¸ì¡°í•˜ëŠ”(ë˜ëŠ” ê°€ë¦¬í‚¤ëŠ”,ì§€ì¹­í•˜ëŠ”)ê°ì²´(object) ê·¸ ìì²´ì…ë‹ˆë‹¤.\n\ncode1\n\na=[1,2,3] #1\nprint(\"appendí•˜ê¸° ì „ id(a):\",id(a))\nb=a #2 ì—ì¼ë¦¬ì–´ì‹±,ë™ì¼í•œ ê°ì²´ë¥¼ ê°€ë¦¬í‚¤ë„ë¡ í•¨.\na.append(4) #3\nc=[1,2,3,4] #4\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\nprint(\"aì˜ ì°¸ì¡°(reference)ì™€ bì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”??\",a is b)\nprint(\"aì˜ ì°¸ì¡°ì™€ bì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”?\",a == b)\nprint(\"aì˜ ì°¸ì¡°(reference)ì™€ cì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”??\",a is c)\nprint(\"aì˜ ì°¸ì¡°ì™€ cì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”?\",a == c)\n\nappendí•˜ê¸° ì „ id(a): 1819155097408\na:[1, 2, 3, 4] b:[1, 2, 3, 4] c:[1, 2, 3, 4]\nid(a):1819155097408,id(b):1819155097408,id(c):1819155103296\naì˜ ì°¸ì¡°(reference)ì™€ bì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”?? True\naì˜ ì°¸ì¡°ì™€ bì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”? True\naì˜ ì°¸ì¡°(reference)ì™€ cì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”?? False\naì˜ ì°¸ì¡°ì™€ cì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”? True\n\n\nì½”ë“œì„¤ëª… 1. ë³€ìˆ˜aì— [1,2,3]ì„ í• ë‹¹í•©ë‹ˆë‹¤.aëŠ” [1,2,3]ì„ ì°¸ì¡°í•©ë‹ˆë‹¤. 2. ì—ì¼ë¦¬ì–´ì‹±ìœ¼ë¡œ ë³€ìˆ˜bë„ [1,2,3]ì„ ì°¸ì¡°í•©ë‹ˆë‹¤. 3. ë³€ìˆ˜aê°€ ì°¸ì¡°í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ê°ì²´[1,2,3]ì— 4ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. 4. ë³€ìˆ˜cì— [1,2,3,4]ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.\n\n\ncode2 - ì‚´ì§ ì‹¬í™”\n\na=[1,2,3] #1\nprint(\"ì¬í• ë‹¹ í•˜ê¸° ì „ id(a):\",id(a))\nb=a #2ì—ì¼ë¦¬ì–´ì‹±,ë™ì¼í•œ ê°ì²´ë¥¼ ê°€ë¦¬í‚¤ë„ë¡ í•¨\na=[1,2,3]+[4] #3ì¬í• ë‹¹\nc=[1,2,3,4] #4\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\nprint(\"aì˜ ì°¸ì¡°(reference)ì™€ bì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”??\",a is b)\nprint(\"aì˜ ì°¸ì¡°ì™€ bì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”?\",a == b)\nprint(\"aì˜ ì°¸ì¡°(reference)ì™€ cì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”??\",a is c)\nprint(\"aì˜ ì°¸ì¡°ì™€ cì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”?\",a == c)\n\nì¬í• ë‹¹ í•˜ê¸° ì „ id(a): 1819155102592\na:[1, 2, 3, 4] b:[1, 2, 3] c:[1, 2, 3, 4]\nid(a):1819184303168,id(b):1819155102592,id(c):1819184334272\naì˜ ì°¸ì¡°(reference)ì™€ bì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”?? False\naì˜ ì°¸ì¡°ì™€ bì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”? False\naì˜ ì°¸ì¡°(reference)ì™€ cì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”?? False\naì˜ ì°¸ì¡°ì™€ cì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”? True\n\n\nì½”ë“œì„¤ëª… 1. ë³€ìˆ˜aì— [1,2,3]ì„ í• ë‹¹í•©ë‹ˆë‹¤.aëŠ” [1,2,3]ì„ ì°¸ì¡°í•©ë‹ˆë‹¤. 2. ì—ì¼ë¦¬ì–´ì‹±ìœ¼ë¡œ ë³€ìˆ˜bë„ [1,2,3]ì„ ì°¸ì¡°í•©ë‹ˆë‹¤. 3. ë³€ìˆ˜aì— ë¦¬ìŠ¤íŠ¸ê°ì²´[1,2,3,4]ë¥¼ ì¬í• ë‹¹í•©ë‹ˆë‹¤. 4. ë³€ìˆ˜cì— [1,2,3,4]ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.\na,b,c ê°ê°ì˜ ê°’ì€ code1ê³¼ code2ì—ì„œ ëª¨ë‘ ê°™ìŠµë‹ˆë‹¤. ì°¨ì´ì ì€ code1ì—ì„œëŠ” aê°€ ì°¸ì¡°í•˜ëŠ” ë¦¬ìŠ¤íŠ¸[1,2,3]ì— 4ë¥¼ ì¶”ê°€í•˜ê³  code2ì—ì„œëŠ” aì— ë¦¬ìŠ¤íŠ¸[1,2,3,4]ë¥¼ ì¬í• ë‹¹í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.ì¤‘ìš”í•œ ì°¨ì´ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n- code1ì— appendì „ í›„ì˜ aê°€ ì°¸ì¡°í•˜ëŠ” ê°ì²´ëŠ” ì£¼ì†ŒëŠ” ë³€í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë³´ì•„ ë™ì¼í•œ ê°ì²´ì— ì›ì†Œë§Œ ì¶”ê°€ë˜ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. - ë°˜ë©´ code2ì—ì„œ í• ë‹¹ë¬¸ ì „ í›„ì˜ aê°€ ì°¸ì¡°í•˜ëŠ” ê°ì²´ì˜ ì£¼ì†Œê°€ ë³€í•©ë‹ˆë‹¤. - ì´ì „ì— ì—†ì—ˆë˜ 1)ê°ì²´ê°€ ë©”ëª¨ë¦¬ì— ìƒì„±ë˜ê³  2)ë³€ìˆ˜aëŠ” ì´ì „ì˜ [1,2,3]ì„ ë” ì´ìƒ ì°¸ì¡°í•˜ì§€ ì•Šê³  ìƒì„±ëœ ê°ì²´[1,2,3,4]ë¥¼ ì°¸ì¡°**í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#ì¸í„°ë‹",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#ì¸í„°ë‹",
    "title": "íŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹",
    "section": "ì¸í„°ë‹",
    "text": "ì¸í„°ë‹\nì¸í„°ë‹ì´ë€ ì´ë¯¸ ìƒì„±ëœ ê°ì²´ë¥¼ ì¬ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ê°ì²´ì˜ ë¹ ë¥¸ ì¬ì‚¬ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©° ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•œë‹¤ê³  í•©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„ë©ë‹ˆë‹¤. 1. ì„ì˜ì˜ í• ë‹¹ë¬¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. 2. = ì˜¤ë¥¸ìª½ì— ìˆëŠ” ê°ì²´ê°€ Intern ì»¬ë ‰ì…˜ì— ë“±ë¡ë˜ì–´ ìˆëŠ”ì§€ ì•„ë‹Œì§€ í™•ì¸í•©ë‹ˆë‹¤. 3. ë“±ë¡ë˜ì–´ ìˆëŠ” ê°ì²´ì˜ ê²½ìš° ê·¸ ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì°¸ì¡°í•©ë‹ˆë‹¤. ë“±ë¡ë˜ì§€ ì•Šì€ ê²½ìš° ë©”ëª¨ë¦¬ì— ê°ì²´ë¥¼ ìƒì„±í•˜ë©° ë³€ìˆ˜ëŠ” ìƒì„±ëœ ê°ì²´ì˜ ì£¼ì†Œë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤\nìì£¼ ì‚¬ìš©í•˜ëŠ” ê°ì²´ì˜ ê²½ìš° ì§ì ‘ Intern ì»¬ë ‰ì…˜ì— ë“±ë¡í•  ìˆ˜ ìˆê³  ë¹ ë¥´ê²Œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. -5~256ì‚¬ì´ì˜ ì •ìˆ˜ì´ê±°ë‚˜ 20ì ë¯¸ë§Œì˜ ë¬¸ìì—´ì€ í• ë‹¹ë¬¸ì„ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ Inter ì»¬ë ‰ì…˜ì— ë“±ë¡ë©ë‹ˆë‹¤. ë”°ë¼ì„œ í•´ë‹¹í•˜ëŠ” ì •ìˆ˜ë‚˜ ë¬¸ìì—´ì„ ë˜ë‹¤ë¥¸ í• ë‹¹ë¬¸ì— ì‹¤í–‰í•˜ë©´ ë³€ìˆ˜ëŠ” ê°™ì€ ê°ì²´ë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤.\nì˜ˆì œ1-ì¸í„°ë‹ X\n\na=1+2021\nb=2023-1\nc=2022\nprint(id(a),id(b),id(c))\nprint(a,b,c)\n\n1819155040400 1819155039600 1819155040688\n2022 2022 2022\n\n\na,b,cëŠ” ì„œë¡œë‹¤ë¥¸ ê°ì²´ë¥¼ ì°¸ì¡°í•˜ë©° ê°ì²´ë“¤ì€ ëª¨ë‘ ê°™ì€ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.\nì˜ˆì œ2-ì¸í„°ë‹ O\n\na=1+2 \nb=4-1\nc=3\nprint(id(a),id(b),id(c))\nprint(a,b,c)\n\n1819075897712 1819075897712 1819075897712\n3 3 3\n\n\na,b,cëŠ” ëª¨ë‘ê°™ì€ ê°ì²´ë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤. ë‚´ë¶€ì ì¸ ë™ì‘ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 1. a=1+2 í• ë‹¹ë¬¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. 2. í• ë‹¹ë˜ëŠ” ê°ì²´ê°€ -5~256ì‚¬ì´ì˜ ì •ìˆ˜ì´ë¯€ë¡œ ìë™ìœ¼ë¡œ Intern ì»¬ë ‰ì…˜ì— ë“±ë¡ë©ë‹ˆë‹¤. 3. bì™€cì—ë„ ì •ìˆ˜ 3ì„ í• ë‹¹í•©ë‹ˆë‹¤. 3ì€ Intern ì»¬ë ‰ì…˜ ë“±ë¡ë˜ì–´ìˆëŠ” ê°ì²´ì´ë©° ë©”ëª¨ë¦¬ìƒì— ìƒì„±ë˜ì–´ìˆëŠ” ê°ì²´ì´ë¯€ë¡œ ìƒˆë¡œìš´ 3ê°ì²´ê°€ ìƒì„±ë˜ì§€ b,cëŠ” ì´ë¯¸ ìƒì„±ëœ 3ê°ì²´ë¥¼ ê°€ë¦¬í‚µë‹ˆë‹¤.\nì°¸ê³ ë§í¬1 : https://guebin.github.io/DL2022/posts/Appendix/2022-12-14-A1.html ì°¸ê³ ë§í¬2 : http://pythonstudy.xyz/python/article/512-%ED%8C%8C%EC%9D%B4%EC%8D%AC-Object-Interning"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html",
    "href": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html",
    "title": "plotly ì‹œê°í™” ëª¨ìŒ",
    "section": "",
    "text": "Code\n# %pip install plotly (jupyter notebook)\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\npio.renderers.default = \"plotly_mimetype+notebook\"\n\n\n\n\nCode\nimport pandas as pd\ntimesData = pd.read_csv(\"./timesData.csv\")\ntimesData.head(5)\n\n\n\n\n\n\n  \n    \n      \n      world_rank\n      university_name\n      country\n      teaching\n      international\n      research\n      citations\n      income\n      total_score\n      num_students\n      student_staff_ratio\n      international_students\n      female_male_ratio\n      year\n    \n  \n  \n    \n      0\n      1\n      Harvard University\n      United States of America\n      99.7\n      72.4\n      98.7\n      98.8\n      34.5\n      96.1\n      20,152\n      8.9\n      25%\n      NaN\n      2011\n    \n    \n      1\n      2\n      California Institute of Technology\n      United States of America\n      97.7\n      54.6\n      98.0\n      99.9\n      83.7\n      96.0\n      2,243\n      6.9\n      27%\n      33 : 67\n      2011\n    \n    \n      2\n      3\n      Massachusetts Institute of Technology\n      United States of America\n      97.8\n      82.3\n      91.4\n      99.9\n      87.5\n      95.6\n      11,074\n      9.0\n      33%\n      37 : 63\n      2011\n    \n    \n      3\n      4\n      Stanford University\n      United States of America\n      98.3\n      29.5\n      98.1\n      99.2\n      64.3\n      94.3\n      15,596\n      7.8\n      22%\n      42 : 58\n      2011\n    \n    \n      4\n      5\n      Princeton University\n      United States of America\n      90.9\n      70.3\n      95.4\n      99.9\n      -\n      94.2\n      7,929\n      8.4\n      27%\n      45 : 55\n      2011\n    \n  \n\n\n\n\n\n\nCode\ntimesData.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2603 entries, 0 to 2602\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   world_rank              2603 non-null   object \n 1   university_name         2603 non-null   object \n 2   country                 2603 non-null   object \n 3   teaching                2603 non-null   float64\n 4   international           2603 non-null   object \n 5   research                2603 non-null   float64\n 6   citations               2603 non-null   float64\n 7   income                  2603 non-null   object \n 8   total_score             2603 non-null   object \n 9   num_students            2544 non-null   object \n 10  student_staff_ratio     2544 non-null   float64\n 11  international_students  2536 non-null   object \n 12  female_male_ratio       2370 non-null   object \n 13  year                    2603 non-null   int64  \ndtypes: float64(4), int64(1), object(9)\nmemory usage: 284.8+ KB"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#add-markers-and-text",
    "href": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#add-markers-and-text",
    "title": "plotly ì‹œê°í™” ëª¨ìŒ",
    "section": "add markers and text",
    "text": "add markers and text\n\n\nCode\n#1 data frame\ndf = timesData.iloc[:100]\n\n#2 trace and data\ntrace = go.Scatter(\n    x = df.world_rank,\n    y = df.citations,\n    mode = \"lines+markers\", #add marker,\n    marker = dict(color = \"rgba(16,112,2,0.8)\"),\n    text = df.university_name #add text\n)\ndata = [trace]\n#3 layout and data\nlayout = go.Layout(\n    title = \"citation\",\n    xaxis = dict(title = \"World Rank\",ticklen = 5)\n)\n\n#4 create figure\nfig = go.Figure(data = data,layout = layout)\n\n#5 plot figure\nfig.show()\n\n\n\n                                                \n\n\nversion2ê°€ ë­”ê°€ ë” ì¢‹ì„ë“¯?"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#add-markers-and-text-1",
    "href": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#add-markers-and-text-1",
    "title": "plotly ì‹œê°í™” ëª¨ìŒ",
    "section": "add markers and text",
    "text": "add markers and text\n\n\nCode\n#1 data frame\ndf2014 = timesData[timesData.year == 2014].iloc[:100,:]\n\n#2 trace,data\ntrace = go.Scatter(\n    x = df2014.world_rank,\n    y = df2014.citations,\n    mode = \"markers\",\n    #marker = dict(color = \"green\",opacity=0.8), #alpha(ë¶ˆíˆ¬ëª…ë„) ì¡°ì ˆ vs1\n    marker = dict(color = \"rgba(255,128,2,0.8)\"), #alpha(ë¶ˆíˆ¬ëª…ë„) ì¡°ì ˆ vs2\n    text = df2014.university_name,\n\n)\ndata = [trace]\n\n#3 layout\nlayout = go.Layout(xaxis = dict(title = \"World Rank\"),yaxis = dict(title = \"Citation\"))\n\n#4 create figure\nfig = go.Figure(data=data,layout=layout)\n\n#5 plot\nfig.show()"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#ì—¬ëŸ¬ê°œì˜-ì°¨íŠ¸-ê²¹ì²˜-ê·¸ë¦¬ê¸°",
    "href": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#ì—¬ëŸ¬ê°œì˜-ì°¨íŠ¸-ê²¹ì²˜-ê·¸ë¦¬ê¸°",
    "title": "plotly ì‹œê°í™” ëª¨ìŒ",
    "section": "ì—¬ëŸ¬ê°œì˜ ì°¨íŠ¸ ê²¹ì²˜ ê·¸ë¦¬ê¸°",
    "text": "ì—¬ëŸ¬ê°œì˜ ì°¨íŠ¸ ê²¹ì²˜ ê·¸ë¦¬ê¸°\n\nì—¬ê¸°ì„œëŠ” histogramìœ¼ë¡œ í–ˆìœ¼ë‚˜ ë‹¤ë¥¸ì°¨íŠ¸ë“¤ë„ ê°€ëŠ¥\n\n\n\nCode\n#1.dataframe\nx2011 = timesData.student_staff_ratio[timesData.year == 2011]\nx2012 = timesData.student_staff_ratio[timesData.year == 2012]\n#2.trace&data\ntrace1 = go.Histogram(\n    x=x2011,\n    #opacity=0.7, #ë¶ˆíˆ¬ëª…ë„ ì¡°ì ˆ\n    name=\"2011\", #ë²”ë¡€(legend)ë¥¼ ì„¤ì •í•˜ê¸° ìœ„í•œ ì´ë¦„ ì„¤ì •\n    marker=dict(color=\"rgb(171,50,96)\",opacity=0.7)\n)\n\ntrace2 = go.Histogram(\n    x=x2012,\n    name=\"2012\",\n    marker=dict(color=\"blue\",opacity=0.7)\n)\ndata=[trace1,trace2]\n#3.layout\nlayout = go.Layout(\n    barmode = \"overlay\", #trace ê²¹ì³ ê·¸ë¦¬ê¸°\n    xaxis=dict(title=\"students-staff ratio\"),\n    yaxis=dict(title=\"count\"),\n    title = dict(text = \"histogram\",x = 0.5)\n)\n#4 figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()\n\n\n\n                                                \n\n\nì°¸ê³ ìë£Œ - Opacityì™€ alpha? : OpacityëŠ” markerì•ˆíŒì—ì„œ ëª¨ë‘ ì“°ì¼ ìˆ˜ ìˆìœ¼ë©° alphaëŠ” rgbaì™€ ì“¸ë•Œë§Œ ì…ë ¥,ê°™ì€ ì—­í• ì„ í•¨. ë‹¨,Opacityë¥¼ markerì˜ ë°–ì—ì„œ ì…ë ¥í•˜ë©´ traceì•ˆì—ì„œ ë°€ë„ë¥¼ í‘œí˜„ í•˜ì§€ ëª»í•¨. ë‹¤ë¥¸ traceë¼ë¦¬ ê²¹ì¹ ë•Œì—ëŠ” ë°€ë„í‘œí˜„ë¨.(ê°™ì€ traceì—ì„œë§Œ ì•ˆë¨.)\n\n\nCode\n# 1.data frame\ndataframe = timesData[timesData.year == 2015]\n\n#2.trace and data\ndata = []\nfor col in [\"world_rank\",\"citations\",\"income\",\"total_score\"]:\n    _trace = go.Scatter(\n        x = dataframe[\"world_rank\"],\n        y = dataframe[col],\n        mode = \"lines\"\n    )\n    data.append(_trace)\n\n#3. layout\nlayout = go.Layout(\n    xaxis=dict(\n        domain=[0, 0.45]\n    ),\n    yaxis=dict(\n        domain=[0, 0.45]\n    ),\n    xaxis2=dict(\n        domain=[0.55, 1]\n    ),\n    xaxis3=dict(\n        domain=[0, 0.45],\n        anchor='y3'\n    ),\n    xaxis4=dict(\n        domain=[0.55, 1],\n        anchor='y4'\n    ),\n    yaxis2=dict(\n        domain=[0, 0.45],\n        anchor='x2'\n    ),\n    yaxis3=dict(\n        domain=[0.55, 1]\n    ),\n    yaxis4=dict(\n        domain=[0.55, 1],\n        anchor='x4'\n    ),\n    title = 'Research, citation, income and total score VS World Rank of Universities'\n)\n\n#4. fig\nfig = make_subplots(rows=2,cols=2)\n#5. plot\nrow = 1\ncol = 1\nfor trace in data:\n    fig.append_trace(trace,row=row,col=col)\n    col+=1\n    if col > 2:\n        col = 1\n        row+=1\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{\"type\": \"xy\"}, {\"type\": \"polar\"}],\n           [{\"type\": \"domain\"}, {\"type\": \"scene\"}]],\n)\n\nfig.add_trace(go.Bar(y=[2, 3, 1]),\n              row=1, col=1)\n\nfig.add_trace(go.Barpolar(theta=[0, 45, 90], r=[2, 3, 1]),\n              row=1, col=2)\n\nfig.add_trace(go.Pie(values=[2, 3, 1]),\n              row=2, col=1)\n\nfig.add_trace(go.Scatter3d(x=[2, 3, 1], y=[0, 0, 0],\n                           z=[0.5, 1, 2], mode=\"lines\"),\n              row=2, col=2)\n\nfig.update_layout(height=700, showlegend=False)\n\nfig.show()"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#vector-fieldquiver-plot",
    "href": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#vector-fieldquiver-plot",
    "title": "plotly ì‹œê°í™” ëª¨ìŒ",
    "section": "Vector field(quiver plot)",
    "text": "Vector field(quiver plot)\n\nì‚¬ì „ì¤€ë¹„\n\nnp.meshgrid : xì¢Œí‘œ,yì¢Œí‘œë¥¼ ê°€ì§€ëŠ” ë²¡í„°ë¥¼ ì…ë ¥í–ˆì„ë•Œ, ë‘ ë²¡í„°ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê²©ìì˜ ì¢Œí‘œ(x,y)ë¥¼ ì¶œë ¥\n\n\n\nCode\nimport numpy as np\nx_coord = np.arange(0,2,.2)\ny_coord = np.arange(0,2,.2)\nx,y = np.meshgrid(np.arange(0,2,.2),np.arange(0,2,.2))\nprint(x_coord.shape,y_coord.shape)\nprint(x.shape,y.shape)\n\n\n(10,) (10,)\n(10, 10) (10, 10)\n\n\n\nê²©ì(grid,matrix)ì— í•¨ìˆ˜ ì ìš©í•˜ë©´? => matrix(x,y ê°ê°ì˜ ì¢Œí‘œ)ì˜ ëª¨ë“  ìš”ì†Œì— í•¨ìˆ˜ê°€ ì ìš©ë¨\n\n\n\nCode\nprint(np.cos(x).shape,np.sin(x).shape)\n\n\n(10, 10) (10, 10)\n\n\n\në°°ì—´ì˜ ìš”ì†Œ ê°’ ì°¨ë¡€ëŒ€ë¡œ ì½ì–´ë³´ê¸° â€¦\n\n(0,0),(0.2,0),(0.4,0) â€¦ (1.8,0) => (0,0.2),(0.2,0.2),(0.4,0.2)â€¦ xì¢Œí‘œ ë‹¤ ì½ê³  yì¢Œí‘œì¦ê°€ ê·¸ ë‹¤ìŒ xì¢Œí‘œ ë‹¤ ì½ê³  yì¢Œí‘œ ì¦ê°€ â€¦\n\n\nCode\nx,y\n\n\n(array([[0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8]]),\n array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n        [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n        [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n        [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6],\n        [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],\n        [1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],\n        [1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2],\n        [1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4],\n        [1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6],\n        [1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8]]))\n\n\n\n\nGradient Vector Field\n\n\\(\\nabla f = xe^{-x^2-y^2}\\)\n\n\nCode\n#1.prepare data\nx,y = np.meshgrid(np.arange(-2,2,0.2),np.arange(-2,2,.25)) #ì¢Œí‘œ\nz = x*np.exp(-x**2-y**2) #í•¨ìˆ˜\n\ndx=0.2;dy=0.25 #dx,dy\nv,u = np.gradient(z,dx,dy) #í•¨ìˆ˜ì˜ ê·¸ë ˆë””ì–¸íŠ¸(ê°ì¢Œí‘œì—ì„œì˜ ë¯¸ë¶„ê³„ìˆ˜)\n\n\n\n\nCode\n#2.trace and data => ìƒëµ\n#3.fig\nfig = ff.create_quiver(x,y,u,v,scale=.25,arrow_scale=.4,name=\"quiver\",line_width=1)\nfig.add_trace(go.Scatter(x=[-.7,.75],y=[0,0],\n                         mode=\"markers\",\n                         marker_size=12,\n                         name=\"points\"))\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nx = np.linspace(-1,1,100)\ny = np.linspace(-1,1,100)\nxx,yy = np.meshgrid(x,y)\nfor i in range()\n\n\n(array([[-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        ...,\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ]]),\n array([[-1.        , -1.        , -1.        , ..., -1.        ,\n         -1.        , -1.        ],\n        [-0.97979798, -0.97979798, -0.97979798, ..., -0.97979798,\n         -0.97979798, -0.97979798],\n        [-0.95959596, -0.95959596, -0.95959596, ..., -0.95959596,\n         -0.95959596, -0.95959596],\n        ...,\n        [ 0.95959596,  0.95959596,  0.95959596, ...,  0.95959596,\n          0.95959596,  0.95959596],\n        [ 0.97979798,  0.97979798,  0.97979798, ...,  0.97979798,\n          0.97979798,  0.97979798],\n        [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n          1.        ,  1.        ]]))\n\n\n\n\n\n1. ì‹œì \n\nì¢…ì ì€ í™”ì‚´í‘œë¡œ í‘œì‹œí•´ì•¼ í•˜ë¯€ë¡œ ì‹œì ë§Œ ë§Œë“¤ê¸°\n\n\n\nCode\nimport plotly.graph_objs as go\n\n\n\n\nCode\n#1. prepare data\n\n#ì²«ë²ˆì§¸ ë²¡í„°ì˜ ì‹œì  x[0],y[0],z[0] ì¢…ì  x[1],y[1],z[1]\n#ë‘ë²ˆì§¸ ë²¡í„°ì˜ ì‹œì  x[2],y[2],z[2] ì¢…ì  x[2],y[2],z[2]\n#ë‘ ê°œì”© ë¬¶ì„\nx = [10.1219, 10.42579, 15.21396, 15.42468, 20.29639,20.46268, 25.36298, 25.49156]\ny = [5.0545,  5.180104, 5.0545,   5.20337,  5.0545,  5.194271, 5.0545,   5.231627]\nz = [5.2713,  5.231409, 5.2713,   5.231409, 5.2713 ,  5.235852,  5.2713, 5.231627]\n#pairs = [(0,1),(2,3),(4,5),(6,7)]\n[coord for coord in range(0,len(x),2)]\n\n\n[0, 2, 4, 6]\n\n\n\n\nCode\n#2. trace,data(trace set)\ntrace1 = go.Scatter3d(\n    x=[x[coord] for coord in range(0,len(x),2)],\n    y=[y[coord] for coord in range(0,len(y),2)],\n    z=[z[coord] for coord in range(0,len(z),2)],\n    mode = \"markers\",\n    line=dict(color=\"red\")\n)\ndata = [trace1]\n\n#3. Layout\nlayout = go.Layout(title=dict(text = \"vectors\"))\n\n#4. figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()\n\n\n\n                                                \n\n\n\n\n2. ì„  ë§Œë“¤ê¸°\n\n\nCode\n#1.prepare data\nx_lines = list()\ny_lines = list()\nz_lines = list()\n\nfor i in range(len(x)):\n    x_lines.append(x[i])\n    y_lines.append(y[i])\n    z_lines.append(z[i])\n    #plotlyì—ì„œ Scatterì˜ line modeëŠ” ì ê³¼ ì  ì‚¬ì´ì— ì„ ì„ ë§Œë“¦\n    #0,1ë²ˆì§¸ ìë¦¬ì˜ ì¢Œí‘œì—ëŠ” ì‹œì ,ì¢…ì ì„ ë„£ê³  3ë²ˆì§¸ ìë¦¬ì— Noneì„ ì¶”ê°€í•˜ì—¬ ì ì„ ë§Œë“¤ì§€ ì•ŠìŒ \n    #ë”°ë¼ì„œ, ì„ ì´ ìƒê¸°ì§€ ì•ŠìŒ\n    if i % 2 == 1:    \n        x_lines.append(None)\n        y_lines.append(None)\n        z_lines.append(None)\n\n#2.trace and tr_set(=data)\ntrace2 = go.Scatter3d(\n    x=x_lines,\n    y=y_lines,\n    z=z_lines,\n    mode = \"lines\",\n    line = dict(width = 2, color = 'rgb(255, 0,0)')\n)\ndata = [trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\n#ì¤‘ê°„ì²´í¬\ndata = [trace1,trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\n3.ì¢…ì  ë§Œë“¤ê¸°\n\n\nCode\ndata = [trace1,trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nimport plotly.graph_objs as go\n# plotly.offline.init_notebook_mode()\n\nx = [10.1219, 10.42579, 15.21396, 15.42468, 20.29639,20.46268, 25.36298, 25.49156]\ny = [5.0545,  5.180104, 5.0545,   5.20337,  5.0545,  5.194271, 5.0545,   5.231627]\nz = [5.2713,  5.231409, 5.2713,   5.231409, 5.2713 ,  5.235852,  5.2713, 5.231627]\n\npairs = [(0,1), (2,3),(4,5), (6,7)]\n\n## plot ONLY the first ball in each pair of balls\ntrace1 = go.Scatter3d(\n    x=[x[p[0]] for p in pairs],\n    y=[y[p[0]] for p in pairs],\n    z=[z[p[0]] for p in pairs],\n    mode='markers',\n    name='markers',\n    line=dict(color='red')\n)\n\ngo.Figure(data=trace1)"
  }
]