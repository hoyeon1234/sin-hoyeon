<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.2.280">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>HIHO - Batch Normalization: Accelerating Deep Network Training b y Reducing Internal Covariate Shift</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-JE4129QJZV"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-JE4129QJZV', { 'anonymize_ip': true});
</script>

  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">HIHO</span>
    </a>
  </div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../about.html">
 <span class="menu-text">About me</span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/hoyeon1234/sin-hoyeon/tree/main/posts"><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
              <div id="quarto-search" class="" title="Search"></div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Batch Normalization: Accelerating Deep Network Training b y Reducing Internal Covariate Shift</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#problem-setting" id="toc-problem-setting" class="nav-link active" data-scroll-target="#problem-setting"><span style="color:black"><strong>Problem setting</strong></span></a></li>
  <li><a href="#normalization-via-mini-batch-statistics" id="toc-normalization-via-mini-batch-statistics" class="nav-link" data-scroll-target="#normalization-via-mini-batch-statistics"><span style="color:black"><strong>Normalization via Mini-Batch Statistics</strong></span></a></li>
  <li><a href="#training-and-inference-with-batch-normalized-networks" id="toc-training-and-inference-with-batch-normalized-networks" class="nav-link" data-scroll-target="#training-and-inference-with-batch-normalized-networks"><span style="color:black"><strong>Training and Inference with Batch-Normalized Networks</strong></span></a></li>
  <li><a href="#introduction" id="toc-introduction" class="nav-link" data-scroll-target="#introduction"><span style="color:black"><strong>Introduction</strong></span></a></li>
  </ul>
</nav>
    </div>
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="problem-setting" class="level1">
<h1><span style="color:black"><strong>Problem setting</strong></span></h1>
<p>Deep Neural Network는 training중에 파라미터가 계속해서 학습되고 minibatch 각각의 분포가 다르기때문에 <strong>hidden layer에 입력되는 input data의 분포가 계속해서 변화(internal covariate shift)</strong>한다. 또한 hidden layer의 수가 많은 Deep nueral network에서는 여러번의 파라미터 연산이 반복되기 때문에 <strong>깊이 위치하는 hidden layer일수록 이전에 학습했던 input의 분포와 많이 다른 분포를 가진 데이터가 입력</strong>된다.이는 크게 두 가지의 문제를 가진다.</p>
<ul>
<li>레이어의 파라미터 학습이 어려움</li>
<li>Gradient vanishing or exploding</li>
</ul>
<p>첫 번째는 학습하는데 어려움을 가진다는 것이다. 만약 input data의 분포가 고정되어있다면 그에 맞는 파라미터를 계속해서 학습하며 결과적으로 layer의 파라미터는 어떠한 값으로 수렴할 것이다.그러나 분포가 internal covariate shift가 (학습되는 파라미터로 인해)일어난다면 계속해서 새로운 분포에 맞춰서 파라미터를 수정해야 하기 때문에 학습하는데 어려움이 있다. 논문에서는 이를 좀 다른방식으로 설명하는데 training 셋의 분포와 testset의 분포가 같으면 학습이 잘되고 다르다면 학습이 안되는 것과 유사하다고 한다.</p>
<p>두 번째는 Gradient vanishing 또는 exploding이다.</p>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> torch</span>
<span id="cb1-3"><a href="#cb1-3"></a>plt.figure(figsize<span class="op">=</span>(<span class="dv">10</span>,<span class="dv">5</span>))</span>
<span id="cb1-4"><a href="#cb1-4"></a>sig <span class="op">=</span> torch.nn.Sigmoid()</span>
<span id="cb1-5"><a href="#cb1-5"></a>x <span class="op">=</span> torch.linspace(<span class="op">-</span><span class="dv">20</span>,<span class="dv">20</span>,<span class="dv">50</span>)</span>
<span id="cb1-6"><a href="#cb1-6"></a>z <span class="op">=</span> sig(x)</span>
<span id="cb1-7"><a href="#cb1-7"></a>point_x <span class="op">=</span> torch.tensor(<span class="dv">10</span>)</span>
<span id="cb1-8"><a href="#cb1-8"></a>point_z <span class="op">=</span> sig(point_x)</span>
<span id="cb1-9"><a href="#cb1-9"></a>plt.plot(x,z)</span>
<span id="cb1-10"><a href="#cb1-10"></a>plt.scatter(point_x,point_z,s<span class="op">=</span><span class="dv">80</span>,color <span class="op">=</span> <span class="st">"red"</span>)</span>
<span id="cb1-11"><a href="#cb1-11"></a>plt.axvline(point_x,color<span class="op">=</span><span class="st">"black"</span>,linestyle<span class="op">=</span><span class="st">"--"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>&lt;matplotlib.lines.Line2D at 0x1f70973fac0&gt;</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="batchnormalization_files/figure-html/cell-2-output-2.png" class="img-fluid"></p>
</div>
</div>
<p>input의 분포의 변화로 인해 임의의 노드에서 시그모이드(<span class="math inline">\(g\)</span>)의 input <span class="math inline">\(x = Wu + b\)</span>라고 가정해보자. 만약 <span class="math inline">\(|x|\)</span>가 너무 커서 saturation regime에 존재한다면 <span class="math inline">\(\frac{\partial{g}}{\partial{x}} \approx 0\)</span>이며 기울기가 vanishing되고 (이는 backpropagation되므로)파라미터의 업데이트가 일어나지 않게 된다.</p>
<p>위와 같은 문제점을 해결하기 위해 크게 다음과 같은 2가지의 방법이 시도되어왔다.</p>
<ul>
<li>lower learning rate =&gt; training time의 상승</li>
<li>careful parameter initialization</li>
</ul>
<p>위의 방법은 internal covariate shift를 어느정도 해결하긴 하지만 단점도 존재한다.(학습시간의 상승 등등 …)</p>
<p>논문에서는 <strong>internal covariate shift를 해결</strong>하기 위해 <strong>normalization for each training mini-batch(Batch Normalization)</strong>을 수행한다.</p>
</section>
<section id="normalization-via-mini-batch-statistics" class="level1">
<h1><span style="color:black"><strong>Normalization via Mini-Batch Statistics</strong></span></h1>
<p><img src="./BN algorithm.PNG" class="img-fluid"></p>
<p>먼저 notation과 내가 헷갈렸던 점을 잠깐 짚고 넘어간다.</p>
<ul>
<li>논문을 기준으로 BN-layer는 parameter가 존재하는 FC-layer나 conv-layer와 activation function(layer)사이에 존재한다.(그러나 이는 비교적 자유로우며 후에 다시 나온다.)</li>
<li><span class="math inline">\(\mathcal{B} = \{x_{1...m}\}\)</span>는 data set에서 m개의 datapoint를 네트워크에 입력하면 어떤 hiddenlayer에 존재하는 임의의 노드 하나에서 activation function(layer)을 통과하기 전 m개의 값이 존재하는데 그 값들을 지칭한다. 그 사이의 값을 <span class="math inline">\(x\)</span>라고 하면 <span class="math inline">\(x_{1...m}\)</span>이 된다.</li>
<li><span class="math inline">\(\mu_{\mathcal{B}}\)</span>는 minibatch를 구성하는 datapoint 각각을 네트워크에 입력하여 얻은 모든 <span class="math inline">\(x\)</span>에 대한 평균을 의미한다.크기가 <span class="math inline">\(m\)</span>인 minibatch에 대하여 <span class="math inline">\(x_1,\dots,x_m\)</span>의 평균이다.</li>
<li><span class="math inline">\(\sigma^2_{\mathcal{B}}\)</span>는 마찬가지로 minibatch여 얻은 <span class="math inline">\(x\)</span>값들의 분산이다.</li>
<li><span class="math inline">\(\hat{x_i}\)</span>는 minibatch에서 i-th datapoint에 위의 연산을 통해 얻은 값이다. <span class="math inline">\(\mu_{\mathcal{B}}\)</span>와 <span class="math inline">\(\sigma^2_{\mathcal{B}}\)</span>를 사용하므로 minibatch에서 계산한 모든<span class="math inline">\(x\)</span>가 사용된다.</li>
<li><span class="math inline">\(\hat{y_i}\)</span>는 learnable parameter인 <span class="math inline">\(\gamma,\beta\)</span>를 추가한 값이다.</li>
</ul>
<p>평균~normalizae까지 살펴보면 minibatch를 하나의 단위로하여 activation function의 input인 <span class="math inline">\(x\)</span>를 normalization하는 것을 의미한다. Problem setting에서 internal covariate shift가 일어나면서 나타나는 두 가지의 단점을 설명했다. normalization까지의 과정은 결국 normal gaussian distribution으로 바꿔서 <strong>activation function으로 입력되는 input의 distribution을 가능한 비슷하게</strong> 하고자 하는 것이다.</p>
<p>여기서 한 가지 중요한 사실은 normalization만 수행하면 network의 표현력을 감소시킨다는 것이다. sigmoid함수를 한 가지 예시로 들어보자. normalization으로 input data의 분포가 normal gaussian distribution가 되었다고 생각해보면 대부분 sigmoid의 linear한 영역에 존재할 것이다. 따라서 nonlinearity를 잃어버리게 된며 이는 네트워크의 표현력을 감소시키므로 좋지 않다.(linearity + nonlinearity는 DNN은 높은 표현력을 가짐을 기억하자) 그러므로, 이러한 점을 막기위해 여기에 추가적으로 학습이 가능한 파라미터 <span class="math inline">\(\gamma\)</span>를 곱해주고 <span class="math inline">\(\beta\)</span>를 더해줌으로서 optimal에 다가갈 수 있도록 분포를 학습을 통하여 shifting,scaling하여 network의 표현력을 유지한다.</p>
<p>예를 들어 학습된 <span class="math inline">\(\gamma\)</span>,<span class="math inline">\(\beta\)</span>는 다음과 같을 것이다.<br> <strong>(nonlinearity를 유지하는 것이 optimal한 경우)</strong> <br> <span class="math display">\[\gamma \approx \sqrt{var[x]},\beta \approx \mathbb{E}[x] \rightarrow \hat{x_i}\approx x_i  \]</span> <strong>(linearity를 얻는 것이 optimal한 경우)</strong><br> <span class="math display">\[\gamma \approx 1,\beta \approx 0 \rightarrow \hat{x_i} \approx \frac{x_i-\mu_\mathcal{B}}{\sqrt{\sigma_\mathcal{B}^2-\epsilon}}\]</span></p>
<p>정리하자면 BatchNormalization은 <strong>Batch단위의 normalization,learnable parameter</strong>를 추가하여<strong>internal covariate shift를 막고 fixed된 distribution을 만듬</strong>과 동시에 <strong>nonlinearity를 유지</strong>함으로서 <strong>gradient vanishing(exploding),학습의 어려움,표현력의 감소</strong>와 같은 문제를 해결했다고 할 수 있다.</p>
</section>
<section id="training-and-inference-with-batch-normalized-networks" class="level1">
<h1><span style="color:black"><strong>Training and Inference with Batch-Normalized Networks</strong></span></h1>
<p>training에서는 minibatch단위로 평균,분산을 구하여 normalization할 수 있지만 test에서는 이와는 다르게 minibatch단위로 data가 입력되지 않으며 또한 입력되는 데이터만 사용하여 값을 예측하길 원한다. 따라서 <strong>training에서 각각의 배치들로부터 얻은 평균들과 분산들을 저장</strong>해놓고 <strong>test에서는 이 값들로 다시 평균을 취하여 normalization</strong>을 한다. 이때 단순한 평균을 취하는 것이 아니라 parameter가 어느정도 <strong>학습된 상태에서 얻어진 minibatch들의 데이터를 더 많이 고려</strong>하기 위해서 <strong>movingaverage나 exponentialaverage를 사용</strong>한다. movingaverage는 학습단계에서 얻어진 값(평균,분산)의 일부를 직접 지정하여 평균을 구하고 exponentialaverage는 어느정도 안정된 상태의 값들에 높은 가중치를 부여하여 평균,분산을 구한다.</p>
<span class="math display">\[\begin{aligned}
&amp;\hat{x} = \frac{x - \mathbb{E}[x]}{\text{Var}[x] + \epsilon}\\
&amp;y = \frac{\gamma}{\sqrt{\text{var}[x] + \epsilon}}\cdot x + (\beta - \frac{\gamma\mathbb{E}[x]}{\sqrt{\text{Var}[x] + \epsilon}})\\
&amp;\text{where }E[x] = E_\mathcal{B}[\mu_\mathcal{B}],\text{Var}[x] = \frac{m}{m-1}E_\mathcal{B}[\sigma_\mathcal{B}^2]
\end{aligned}\]</span>
<p><span class="math inline">\(\frac{m}{m-1}\)</span>은 unbiased estimate를 위하여 곱해진 값이며 <span class="math inline">\(E_{\mathcal{B}}\)</span>는 moving average 또는 exponential average를 의미한다. test에서의 normalization은 단순히 linear transform으로 볼 수 있는데 이는 평균과 분산을 구하는 것이 아닌 training에서 구해놓은 값을 단순히 averaging한 고정된(fixed)값을 활용하기 때문이다.</p>
</section>
<section id="introduction" class="level1">
<h1><span style="color:black"><strong>Introduction</strong></span></h1>
<p>SGD는 <strong>간단하고 효율적인 알고리즘</strong>이지만 그 자체로는 <strong>상당히 민감하고 까다로운</strong> 알고리즘이며 그 이유는 다음과 같다.</p>
<ul>
<li>requires care ful <strong>tuning of hyper-parameters</strong> (specifically the <strong>learning rate</strong>,<strong>initial values for the parameters</strong>)</li>
<li>The <strong>inputs to each layer</strong> are <strong>affected</strong> by the <strong>parameters of all preceeding layers</strong> =&gt; small changing to the network =&gt; amplification of gradient(<strong>vanish or slope</strong>)</li>
</ul>
<p>SGD로 최적화 하면 <strong>임의의 layer(sub-network)</strong>으로 들어가는 <strong>inputs의 distribution은 계속해서 변화</strong>하며 이는 <strong>각각의 레이어에 존재하는 파라미터</strong>를 <strong>각각의 입력들의 분포에 맞게 다시 학습</strong>해야 함을 의미한다.</p>
<p>만약 각 레이어에 들어가는 input의 분포가 고정되어 있다면 어떨까? 크게 다음과 같은 이점을 가진다고 논문에서는 설명한다.<br> - 입력의 분포가 변화하는 것에 대하여 파라미터를 재조정 할 필요가 없다.(따라서 유리하다.) - nonlinearity의 inputs이 saturated regime에 gradient vanish가 되는 현상을 막아준다. =&gt; 이는 batchnormalization을 추가하면 학습시간이 덜 들어가는 이유이다.</p>
<p>또한 fixed distribution은</p>
<ul>
<li>allows us to use much higer learning rates</li>
<li>less careful about initilization</li>
<li>acts as a regularizaer</li>
<li>in some cases eliminating the need for dropout</li>
</ul>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="hoyeon1234/sin-hoyeon" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script>
</div> <!-- /content -->



</body></html>