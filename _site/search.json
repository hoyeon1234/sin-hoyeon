[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "ì „ë¶ëŒ€í•™êµ ITì‘ìš©ì‹œìŠ¤í…œ ê³µí•™ê³¼ ì‹ í˜¸ì—° sinhoyeon0514@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HIHO",
    "section": "",
    "text": "[Probability & Statistics] Bias Variance Tradeoff\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] Appendix\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nimplementation\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nNamespace\n\n\n\n\n\n\n\n\n\n\n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nBellman Equation\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\n\n\n\n\n\n\n\n\n\n[Paper Study] AutoAugment : Learning Augmentation Strategies from Data\n\n\nfield : CV,RLunderstanding : ğŸ˜ƒğŸ˜ƒ\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\n[Paper Study] Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms\n\n\nfield : RLunderstanding : ğŸ˜ƒğŸ˜ƒ\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Algorithm & Datastructure]1-2. Naming, Styling and Comments\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Algorithm & Datastructure]1-2.Procedure-oriented program vs Object-oriented program\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Algorithm & Datastructure]1-1.Programming and Execution Environment\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\n[ì£¼ì €ë¦¬ì£¼ì €ë¦¬] ì‹œí—˜ë³´ê¸° 1ì£¼ì¼ ì „ ê¼­ ì½ì–´ì•¼ í•˜ëŠ” ê²ƒ\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Stochestic Process] 5.sigma field(2)\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\n[Stochestic Process] 4.sigma field(1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Stochestic Process] 3.ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸° ë¹„êµ\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\n[Stochestic Process] ë¶€ë¡ - ìê¾¸ ê±¸ë¦¬ëŠ” ì§‘í•©ê°œë… ì •ë¦¬\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Coading Test] ë¬¸ìì—´ ë‚˜ëˆ„ê¸°\n\n\nlevel : 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Stochestic Process] 2.Cardinality\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Coading Test] kë²ˆì§¸ìˆ˜\n\n\nlevel : 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Paper Study] Neural Machine Translation by jointly learning to align and translate\n\n\nfield : NLPunderstanding : ğŸ˜ƒğŸ˜ƒğŸ˜ƒ\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\n[ê°•í™”í•™ìŠµ] Simple Bandit Algorithm Implementation\n\n\nReinforcement Learning: An Introduction 2.4ì ˆ ì¤‘ Simple Bandit Algorithm psuedo code êµ¬í˜„\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Paper-study] Batch Normalization\n\n\nfield : coreunderstanding : ğŸ˜ƒğŸ˜ƒğŸ˜ƒ\n\n\n\n\n\n\n\n\n\n\n\n\n\n[pandas] map,apply,applymap ë¹„êµ\n\n\n\n\n\n\n\n\n\n\n\n\n\nSequence to Sequence Learning with Neural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuto-Encoding Variational Bayes(ì‘ì„±ì¤‘)\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\n[ê°•í™”í•™ìŠµ] Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenarative Adversarial Nets\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 6-2.EVD & Property\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 6-4.Quadratic Form\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 6-3.EVD of symmetric matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\nDQN\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 6-1.Eigenvalue & Eigenvector\n\n\n\n\n\n\n\n\n\n\n\n\n\n[ê°•í™”í•™ìŠµ] 2-1 Markov Decision process and property\n\n\nMDPì™€ ì¤‘ìš”í•œ property ì •ë¦¬\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\n[ê°•í™”í•™ìŠµ] 1 - ê°•í™”í•™ìŠµ ìš©ì–´ì •ë¦¬\n\n\nê°•í™”í•™ìŠµ ê³µë¶€í•˜ë©´ì„œ ìš©ì–´ì •ë¦¬\n\n\n\n\n\n\n\n\n\n\n\n\n\n[ê°•í™”í•™ìŠµ] 2-2 Reward & Return & State Value f & Action Value f\n\n\n\n\n\n\n\n\n\n\n\n\n\n[PRML ì½ê¸°] 1 - í™•ë¥ ë¡  ê°œìš”\n\n\nSum rule, Product rule, Bayeâ€™s rule,random variable independence\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\n[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)\n\n\nprobability density, expectation and covariance, Bayesian probabilities,Gausian distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimension Reduction using Auto Encoder with pytorch\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Coading Test] ì‹œì €ì•”í˜¸\n\n\nlevel : 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Coading Test] ì´ìƒí•œ ë¬¸ì ë§Œë“¤ê¸°\n\n\nlevel : 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Coading Test] í¬ê¸°ê°€ ì‘ì€ ë¶€ë¶„ ë¬¸ìì—´\n\n\nlevel : 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 5.Least Squares\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\ní™•ë¥ ë¡  ìš©ì–´ì •ë¦¬\n\n\nrandom variable,event,sample space,probability distribution,randomsample,realization\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinite Difference Method with np.gradient\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotly ì‹œê°í™” ëª¨ìŒ\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 4.Ax=bì˜ í•´ì˜ ê°¯ìˆ˜\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 3.rank & null space(kernel)\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 2.Linear Combination & Span\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 1.í–‰ë ¬ê³±ì— ëŒ€í•œ ì—¬ëŸ¬ê°€ì§€ ê´€ì \n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nMultinomial Logistic Regression & Softmax Regression\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\nì¹´í…Œê³ ë¦¬ ë¶„í¬\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLE & MAP (ì‘ì„±ì¤‘)\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\ní™•ë¥ ë¶„í¬ì—ì„œ ;ì™€|ì˜ ì‚¬ìš©\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\níŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\nÂ \n\n\n\npytorchë¡œ Rnnêµ¬í˜„í•˜ê¸°\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/coading test/programmerskë²ˆì§¸ìˆ˜.html",
    "href": "posts/coading test/programmerskë²ˆì§¸ìˆ˜.html",
    "title": "[Coading Test] kë²ˆì§¸ìˆ˜",
    "section": "",
    "text": "ë¬¸ì œ\n\n\n\në‚˜ì˜ í’€ì´\n\ndef drop(array,value):\n    for idx in range(len(array)):\n        if array[idx] == value:\n            i = idx\n            break\n    dropped_array = array[:i] + array[i+1:]\n    return dropped_array\n    \ndef sort(sub_list):\n    sorted_l = []\n    while len(sub_list)>0:\n        drop_element = min(sub_list)\n        sorted_l.append(drop_element)\n        sub_list = drop(sub_list,drop_element)\n    return sorted_l\n\ndef solution(array, commands):\n    answer = []\n    for el in range(len(commands)):\n        command = commands[el]\n        i = command[0];j=command[1];k=command[2]\n        sub_list = array[i-1:j]\n        sorted_list = sort(sub_list)\n        answer.append(sorted_list[k-1])\n    return answer\n\n\n\nê³ ì°°\n\nì˜¤ëœë§Œì— í•´ì„œ ê·¸ëŸ°ì§€ sortedí•¨ìˆ˜ë¥¼ êµ¬í˜„í•˜ëŠ” ê²ƒì„ ê¹Œë¨¹ì–´ ì˜¤ë˜ê±¸ë ¸ë‹¤.\nêµ¬í˜„ëœ ë‚´ì¥í•¨ìˆ˜ ì¤‘ ìì£¼ ë‚˜ì˜¤ëŠ” ê²ƒì€ ì™¸ì›Œì•¼ í•œë‹¤."
  },
  {
    "objectID": "posts/coading test/programmersë¶€ë¶„ë¬¸ìì—´.html",
    "href": "posts/coading test/programmersë¶€ë¶„ë¬¸ìì—´.html",
    "title": "[Coading Test] ë¬¸ìì—´ ë‚˜ëˆ„ê¸°",
    "section": "",
    "text": "ë¬¸ì œ\n\n\n\në‚˜ì˜ í’€ì´\n\ndef solution(s):\n    cnt = [0,0]\n    sep_num = 0\n    for idx in range(len(s)):\n        if cnt == [0,0]:\n            x = s[idx]\n            print(\"ì²«ë²ˆì§¸ ë¬¸ì:\",x)\n        if s[idx] == x:\n            cnt[0] += 1\n        else:\n            cnt[1] += 1\n        if cnt[0] == cnt[1]:\n            sep_num += 1\n            cnt = [0,0]\n        if idx == len(s) -1 and cnt[0] != cnt[1]:\n            sep_num +=1\n    return sep_num\n\n\n\nê³ ì°°\n\në¬¸ì œì—ì„œ â€œë¬¸ìì—´ì„ ë¶„ë¦¬í•©ë‹ˆë‹¤â€ë¼ê³  ë‚˜ì™€ ìˆì–´ì„œ ì²˜ìŒì— ë¬¸ìì—´ ë¶„ë¦¬í•˜ëŠ” ì½”ë“œë„ ì ë‹¤ê°€ ì‹œê°„ì´ˆê³¼ì˜¤ë¥˜ê°€ ë‚¬ìŒ .. ã…œã…œ\në¬¸ì œì„¤ëª…ì€ ì´í•´ë¥¼ ë•ê¸°ìœ„í•œ ì„¤ëª…ì¼ ë¿, ëª¨ë“  ê³¼ì •ì„ ì¼ì¼íˆ,ê·¸ëŒ€ë¡œ ë”°ë¼í•  í•„ìš”ëŠ” ì—†ìœ¼ë©° ë¬´ìŠ¨ë°©ë²•ì´ë˜ì§€ ì¨ì„œ ì…ì¶œë ¥ì˜ˆì‹œë¥¼ ì˜ ë§íˆëŠ”ë° ì´ˆì ì„ ë‘ì."
  },
  {
    "objectID": "posts/coading test/programmersì‹œì €ì•”í˜¸.html",
    "href": "posts/coading test/programmersì‹œì €ì•”í˜¸.html",
    "title": "[Coading Test] ì‹œì €ì•”í˜¸",
    "section": "",
    "text": "ë¬¸ì œ\n\n\n\në‚˜ì˜ í’€ì´\n\ndef solution(s, n):\n    upper_ch = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    answer = \"\"\n    for el_s in s:\n        #1.ë§¨ ë§ˆì§€ë§‰ ë¦¬í„´ì„ ìœ„í•´ ì›ë˜ ë¬¸ìê°€ ì†Œë¬¸ìì¸ì§€ ëŒ€ë¬¸ìì¸ì§€ ê¸°ì–µ and ëŒ€ë¬¸ìì—ì„œ ê²€ìƒ‰í• ê²ƒì´ê¸° ë•Œë¬¸ì— ëŒ€ë¬¸ìë¡œ ë³€í™˜\n        #ëŒ€ë¬¸ìë¡œ ë³€í™˜ëœ ë¬¸ìì—´ sì˜ ê°ê°ì˜ ë¬¸ì,ëŒ€ì†Œë¬¸ì ì—¬ë¶€\n        if el_s == \" \":\n            answer += \" \"\n        elif el_s.isupper() == True:\n            was_upper = True\n        else:\n            was_upper = False\n            el_s = el_s.upper()\n        #2.ì¸ë±ìŠ¤ ìˆ«ìê°€ upper_chì—ì„œ ë²—ì–´ë‚ ë•Œ ì•„ë‹ë•Œ ì²˜ë¦¬\n        for idx,up_ch in enumerate(upper_ch):\n            if el_s == up_ch and idx + n <= len(upper_ch)-1:\n                find_idx = idx+n\n                if was_upper == True:\n                    answer += upper_ch[find_idx]\n                else:\n                    answer += upper_ch[find_idx].lower()        \n            elif el_s == up_ch and idx + n > len(upper_ch)-1:\n                find_idx = n-len(upper_ch[idx:])\n                if was_upper == True:\n                    answer += upper_ch[find_idx]\n                else:\n                    answer += upper_ch[find_idx].lower()  \n    return answer\n\n\n\në‹¤ë¥¸ í’€ì´\n\ndef caesar(s, n):\n    lower_list = \"abcdefghijklmnopqrstuvwxyz\"  \n    #ì†Œë¬¸ìë„ ë¦¬ìŠ¤íŠ¸ë¡œ ë§Œë“¦ \n    #ì¢‹ì€ì ->ëŒ€ì†Œë¬¸ì ì—¬ë¶€ë¥¼ ê¸°ì–µí•˜ëŠ” ì½”ë“œ ë¶ˆí•„ìš”(ì¡°ê±´ë¬¸)\n    #ì•ˆì¢‹ì€ì ->ì†Œë¬¸ìë¡œ ì´ë¤„ì§„ ë¦¬ìŠ¤íŠ¸ë¥¼ ë§Œë“œëŠ”ë° ê·¸ë§Œí¼ì˜ ë©”ëª¨ë¦¬ í•„ìš”\n    upper_list = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \n    \n    \n    \n    result = [] \n    #ë¬¸ìì—´ë“¤ì„ ì €ì¥í•  listë¥¼ ë§Œë“¦\n    #mutableë¡œ ë¶™ì´ëŠ” ê²ƒê³¼ immutableë¡œ ë¶™ì´ëŠ” ê²ƒ ì°¨ì´\n    #ì†ë„ -> \n    #mutableì´ ë” ë¹ ë¦„,\n    #ê·¸ëŸ¬ë‚˜ garbage collectorë„ ê³ ë ¤ì‹œ ìŠ¤ìº”ë²”ìœ„ê°€ ë„ˆë¬´ì»¤ì„œ ëŠë ¤ì§ˆìˆ˜ë„?\n    #ë©”ëª¨ë¦¬ -> immutableì´ ë” ì ê²Œë“¤ì„ ë“¯(ë¦¬ìŠ¤íŠ¸ëŠ” ì´ì¤‘í¬ì¸í„°ê°™ì€ êµ¬ì¡°ë¼ì„œ ì´ë ‡ê²Œ ì˜ˆìƒë¨)\n    for i in s:\n        if i == \" \":\n            result.append(\" \")\n        elif i.islower() is True:\n            new_ = lower_list.find(i) + n\n            result.append(lower_list[new_ % 26]) \n            #ë‚˜ë¨¸ì§€ë¡œ ê³„ì‚°í•˜ëŠ” ë°©ì‹,ì´ê²Œ ë” ê°„ë‹¨í•˜ê³  ì¢‹ì€ë“¯\n            #ë°˜ë³µë¬¸ì´ ë¬¸ìì—´ì— ëŒ€í•´ì„œ ëŒë‹¤ë³´ë‹ˆ ë¬¸ìì—´ê³¼ ë¬¸ìì—´ì˜ ì¸ë±ìŠ¤ ìœ„ì£¼ë¡œ ë„ˆë¬´ ìƒê°í•¨\n            #ì–´ë–¤ ìˆ«ì(ì—¬ê¸°ì„œëŠ” ì¸ë±ìŠ¤)ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ì„ë•Œì— ë‹¤ì‹œ 0ë¶€í„° ì¤˜ì•¼í•˜ëŠ” ìƒí™©? -> ë‚˜ë¨¸ì§€ í™œìš©\n        else:\n            new_ = upper_list.find(i) + n\n            result.append(upper_list[new_ % 26])\n    return \"\".join(result)\n\në‚˜ì¤‘ì— ë³¼ ë§í¬ ë§í¬1 ë§í¬2 ë§í¬3"
  },
  {
    "objectID": "posts/coading test/programmersì´ìƒí•œë¬¸ìë§Œë“¤ê¸°.html",
    "href": "posts/coading test/programmersì´ìƒí•œë¬¸ìë§Œë“¤ê¸°.html",
    "title": "[Coading Test] ì´ìƒí•œ ë¬¸ì ë§Œë“¤ê¸°",
    "section": "",
    "text": "ë¬¸ì œ\n\n\n\në‚˜ì˜ í’€ì´\n\ndef solution(s):\n    words = s.split(\" \") #ìŠ¤í”Œë¦¿ í•¨ìˆ˜ í—·ê°ˆë¦¬ëŠ” ë¶€ë¶„ì´ ìˆì—ˆìŒ - ì •ë¦¬\n    answer = \"\"\n    for wd in words:\n        for idx,chr in enumerate(wd):\n            print(idx,chr)\n            if idx % 2 == 0:\n                chr = chr.upper()\n            else:\n                chr = chr.lower()\n            answer += chr\n        answer+= \" \"\n    return answer[:-1]\n\n\n\nsplit ë©”ì„œë“œ(í•¨ìˆ˜)\n\nt=\"sdsa  sd\"\nhelp(t.split)\n\nHelp on built-in function split:\n\nsplit(sep=None, maxsplit=-1) method of builtins.str instance\n    Return a list of the words in the string, using sep as the delimiter string.\n    \n    sep\n      The delimiter according which to split the string.\n      None (the default value) means split according to any whitespace,\n      and discard empty strings from the result.\n    maxsplit\n      Maximum number of splits to do.\n      -1 (the default value) means no limit.\n\n\n\nstringê°ì²´ì˜ ì¸ìŠ¤í„´ìŠ¤ ì¦‰,ë¬¸ìì—´ì— ëŒ€í•´ì„œë§Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë©”ì„œë“œ sepíŒŒë¼ë¯¸í„°ì— ì „ë‹¬í•œ ì¸ìˆ˜ë¥¼ êµ¬ë¶„ìë¡œ ì‚¬ìš©í•˜ì—¬ ë¬¸ìì—´ì•ˆì— ìˆëŠ” ë‹¨ì–´ë“¤ì„ ê°€ì ¸ì˜´. ê°€ì ¸ì˜¨ ë‹¨ì–´ë“¤ì€ ë¦¬ìŠ¤íŠ¸ì˜ ì›ì†Œê°€ ë˜ì–´ ë°˜í™˜ë¨ Parameter - sep - â€¦\nReturn - list[word1,word2,â€¦] (êµ¬ë¶„ìë¥¼ í†µí•˜ì—¬ êµ¬ë¶„ëœ ë‹¨ì–´ë“¤,ê¸°ë³¸ê°’ì€ ê³µë°±(space))\n\n\nì˜ˆì‹œ\n\n#ë¬¸ìì—´ ë‚´ì— êµ¬ë¶„ê¸°ì¤€ì´ ì—†ëŠ” ê²½ìš°?\n#sep = \" \"ìœ¼ë¡œ ê¸°ë³¸ê°’ìœ¼ë¡œ ì„¤ì •ë˜ì–´ ìˆìŒ. ê³µë°±ì´ ì—†ìœ¼ë¯€ë¡œ ê·¸ëƒ¥ ë¬¸ìì—´ì„ ë¦¬ìŠ¤íŠ¸ì— ë„£ì–´ì„œ ë°˜í™˜\n_t = \"abcde\"\nanswer = _t.split()\nprint(answer)\n\n['abcde']\n\n\n\n#ë¬¸ìì—´ ë‚´ì— êµ¬ë¶„ê¸°ì¤€ì´ ìˆëŠ” ê²½ìš°?\n_t = \"ab cd esda dg\"\nanswer = _t.split()\nprint(answer)\n\n['ab', 'cd', 'esda', 'dg']\n\n\n\n_t = \"abtcdtesdatdg\"\nanswer = _t.split(\"t\")\nprint(answer)\n\n['ab', 'cd', 'esda', 'dg']\n\n\n\n_t = \"ab;;cd;;esd;;atdg\"\nanswer = _t.split(\";;\")\nprint(answer)\n\n['ab', 'cd', 'esd', 'atdg']"
  },
  {
    "objectID": "posts/coading test/programmersí¬ê¸°ê°€ì‘ì€ë¶€ë¶„ë¬¸ìì—´.html",
    "href": "posts/coading test/programmersí¬ê¸°ê°€ì‘ì€ë¶€ë¶„ë¬¸ìì—´.html",
    "title": "[Coading Test] í¬ê¸°ê°€ ì‘ì€ ë¶€ë¶„ ë¬¸ìì—´",
    "section": "",
    "text": "ë¬¸ì œ\n\n\n\në‚˜ì˜ í’€ì´\n\ndef solution(t, p):\n    t_len = len(t);p_len = len(p)\n    answer = 0\n    p = int(p)\n    for idx in range(t_len-p_len+1):\n        num = int(t[idx:idx+p_len])\n        if num <= p:\n            answer+=1\n        else:\n            pass\n    return answer"
  },
  {
    "objectID": "posts/DL/Auto Encoder Dimension Reduction.html",
    "href": "posts/DL/Auto Encoder Dimension Reduction.html",
    "title": "Dimension Reduction using Auto Encoder with pytorch",
    "section": "",
    "text": "import torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nimport torch\ntest_path = \"./test.csv\"\ntrain_path = \"./train.csv\"\n\n\n# %pip install plotly (jupyter notebook)\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n#pio.renderers.default = 'iframe_connected'\n#pio.renderers.default = \"vscode\"\npio.renderers.default = \"plotly_mimetype+notebook\""
  },
  {
    "objectID": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension3",
    "href": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension3",
    "title": "Dimension Reduction using Auto Encoder with pytorch",
    "section": "encoding dimension=3",
    "text": "encoding dimension=3\n\ntraining autoencoder\n\ntorch.manual_seed(201711375)\nautoencoder_3 = AutoEncoder(47,3)\nloss_fn = torch.nn.MSELoss()\nrelu = torch.nn.LeakyReLU()\noptimizer = torch.optim.Adam(autoencoder_3.parameters(),lr=0.001)\n\n\nfor epoch in range(20000):\n    #1.yhat\n    out = autoencoder_3(X_train_ohe)\n    #2\n    loss = loss_fn(out,X_train_ohe)\n    #3\n    loss.backward()\n    if epoch % 10000 == 0:\n        print(f\"epoch:{epoch} loss:{loss.tolist()}\")\n    #4\n    optimizer.step()\n    optimizer.zero_grad()\n\nepoch:0 loss:0.5274774432182312\nepoch:10000 loss:0.11526338756084442\n\n\n\n\nvisualization\n\nclass_map_inv = {}\nfor key,value in class_map.items():\n    class_map_inv[value] = key\nclass_map_inv\n\n{0: 'A', 1: 'B', 2: 'C'}\n\n\n\ndt_dim3 = pd.DataFrame({\"class\":Y_train_ohe})\ndt_dim3 = pd.concat([pd.DataFrame(np.array(autoencoder_3.encoder(X_train_ohe).tolist())),dt_dim3],axis=1)\ndt_dim3 = dt_dim3.rename(columns = {0:\"x\",1:\"y\",2:\"z\"})\n\n\ncount = 0\ndata = []\nfor cl in dt_dim3[\"class\"].unique():\n    cond = dt_dim3[\"class\"] == cl\n    _data = dt_dim3.loc[cond,:]\n    x = _data.x.tolist()\n    y = _data.y.tolist()\n    z = _data.z.tolist()\n    if count == 0:\n        color = \"red\"\n    elif count == 1:\n        color = \"blue\"\n    else:\n        color = \"black\"\n    trace=go.Scatter3d(\n        x=x,\n        y=y,\n        z=z,\n        mode=\"markers\",\n        marker = dict(color = color,size=2),\n        name = str(class_map_inv[cl])\n        )\n    data.append(trace)\n    count+=1\n\nlayout = go.Layout(title=dict(text = \"3-dimension \"))\n\n#4. figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()"
  },
  {
    "objectID": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension10",
    "href": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension10",
    "title": "Dimension Reduction using Auto Encoder with pytorch",
    "section": "encoding dimension=10",
    "text": "encoding dimension=10\n\ntorch.manual_seed(201711375)\nautoencoder_3 = AutoEncoder(47,10)\nloss_fn = torch.nn.MSELoss()\nrelu = torch.nn.LeakyReLU()\noptimizer = torch.optim.Adam(autoencoder_3.parameters(),lr=0.001)\n\n\nfor epoch in range(40000):\n    #1.yhat\n    out = autoencoder_3(X_train_ohe)\n    #2\n    loss = loss_fn(out,X_train_ohe)\n    #3\n    loss.backward()\n    if epoch % 10000 == 0:\n        print(f\"epoch:{epoch} loss:{loss.tolist()}\")\n    #4\n    optimizer.step()\n    optimizer.zero_grad()\n\nepoch:0 loss:0.3828417658805847\nepoch:10000 loss:0.060432590544223785\nepoch:20000 loss:0.06043253839015961\nepoch:30000 loss:0.060432031750679016"
  },
  {
    "objectID": "posts/DL/Bias Variance tradeoff/bias variance trade off.html",
    "href": "posts/DL/Bias Variance tradeoff/bias variance trade off.html",
    "title": "[Probability & Statistics] Bias Variance Tradeoff",
    "section": "",
    "text": "supervised learningì˜ ëª©ì  ì¤‘ í•˜ë‚˜ëŠ” unknown dataì— ëŒ€í•´ì„œ generalizationì„ ì˜ í•˜ë„ë¡ ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì…ë‹ˆë‹¤.\nì´ëŠ” expected generalization errorì— í¬í•¨í•˜ëŠ” biasì™€ varianceë¥¼ ì¤„ì´ë„ë¡ í•™ìŠµí•˜ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤..\nê·¸ëŸ¬ë‚˜ í•™ìŠµê³¼ì •ì—ì„œ biasì™€ varianceëŠ” ì„œë¡œê°„ì— ìƒì¶©í•©ë‹ˆë‹¤.\ní’€ì–´ì“°ìë©´ biasê°€ ì‘ì•„ì§€ë©´ varianceëŠ” ì»¤ì§€ê³  varianceê°€ ì‘ì•„ì§€ë©´ biasê°€ ì»¤ì§€ëŠ” bias variance tradeoff í˜„ìƒì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤.\nbiasì™€ varianceë¥¼ ì¡°ê¸ˆ êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\nbias\n\nbiasëŠ” í•™ìŠµì•Œê³ ë¦¬ì¦˜ì—ì„œì˜ ì˜ëª»ëœ ê°€ì •ìœ¼ë¡œ ì¸í•´ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ì…ë‹ˆë‹¤.\nHigh biasëŠ” í•™ìŠµì•Œê³ ë¦¬ì¦˜ì´ featureì™€ targetê°„ì˜ regularities ë¥¼ ë†“ì¹˜ê²Œ í•©ë‹ˆë‹¤.(underfittingì˜ ì›ì¸)\nì´ëŠ” unknown dataì— ëŒ€í•´ì„œ generalization errorê°€ ë‚˜ì˜¤ê²Œ í•©ë‹ˆë‹¤.\n\nvairance\n\nvarianceëŠ” í›ˆë ¨ë°ì´í„°ì—ì„œì˜ ì‘ì€ ë³€ë™ì— í•™ìŠµì•Œê³ ë¦¬ì¦˜ì´ ë¯¼ê°í•˜ê²Œ ë°˜ì‘í•˜ì—¬ ë°œìƒí•˜ëŠ” ì˜¤ë¥˜ì…ë‹ˆë‹¤.\nHigh varianceëŠ” í•™ìŠµì•Œê³ ë¦¬ì¦˜ì´ featureì™€ targetê°„ì˜ regularities ë¥¼ ë„ˆë¬´ ê³¼í•˜ê²Œ í•™ìŠµí•´ì„œ ë°œìƒí•©ë‹ˆë‹¤.(overfittingì˜ ê²°ê³¼)\në§ˆì°¬ê°€ì§€ë¡œ ì´ëŠ” unknown dataì— ëŒ€í•´ì„œ generalization errorê°€ ë‚˜ì˜¤ê²Œ í•©ë‹ˆë‹¤.\n\n\n1ë²ˆê·¸ë¦¼\n\nbiasê°€ ì‘ì•„ì„œ ì•Œê³ ë¦¬ì¦˜ì´ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ í¸í–¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\nvarianceë˜í•œ ì‘ì•„ì„œ ë°ì´í„°ì˜ ë³€í™”ì— ì˜í•œ í•™ìŠµëœ ì•Œê³ ë¦¬ì¦˜ì˜ ë³€ë™,í¸ì°¨ëŠ” ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.\nì „ì²´ì ìœ¼ë¡œ bias,varianceê°€ ì‘ê¸° ë•Œë¬¸ì— í•™ìŠµëœ ì•Œê³ ë¦¬ì¦˜ì€ Trueê°’ì— ê°€ê¹ìŠµë‹ˆë‹¤.\n\n2ë²ˆê·¸ë¦¼\n\nbiasê°€ í¬ê¸° ë•Œë¬¸ì— ì˜¤ë¥¸ìª½ ë°©í–¥ìœ¼ë¡œ í¸í–¥ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\nvarianceëŠ” ì‘ì•„ì„œ ë°ì´í„°ì˜ ë³€í™”ì— ì˜í•œ í•™ìŠµëœ ì•Œê³ ë¦¬ì¦˜ì˜ ë³€ë™,í¸ì°¨ëŠ” ê±°ì˜ ì—†ìŠµë‹ˆë‹¤.\n\n3ë²ˆê·¸ë¦¼\n\nbiasê°€ ì‘ì•„ì„œ ì–´ë–¤ ë°©í–¥ìœ¼ë¡œ í¸í–¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\nvarianceëŠ” í¬ê¸°ë•Œë¬¸ì— ì‘ì€ ë°ì´í„°ì˜ ë³€í™”ì— ì˜í•œ í•™ìŠµëœ ì•Œê³ ë¦¬ì¦˜ì˜ ì„±ëŠ¥ë³€í™”ê°€ ë§¤ìš° ì‹¬í•©ë‹ˆë‹¤.\n\n\n\n\n\n\n\nìœ„í‚¤í”¼ë””ì•„ì˜ notationê³¼ ê°•ì˜ìë£Œ ì¤‘ notationì´ ë‹¤ë¥´ê³  ì´ì™¸ì—ë„ ë‹¤ë¥¸ë¶€ë¶„ì´ ë§ì´ í—·ê°ˆë¦¬ê¸°ì— notationì„ ë¨¼ì € ì •ë¦¬í•©ë‹ˆë‹¤.\n\\(F^*(x) = f(x) = f,\\hat{F}(x) = \\hat{f}(x;D) = \\hat{f}\\)\n\\(\\mathbb{E}_D = \\mathbb{E}\\)\n\n\n\n\n\ní†µê³„í•™ì—ì„œ ë°ì´í„°ëŠ” ë‹¤ìŒê³¼ ê°™ì€ additive error modelë¡œë¶€í„° samplingë©ë‹ˆë‹¤.\n\n\\[\\begin{aligned}\n&y = f(x) + \\epsilon ,\\,\\, \\epsilon \\overset{\\text{i.i.d}}{\\sim} \\mathcal{N}(0,\\sigma^2)\\\\\n&\\text{where} \\\\\n&f(x) : \\text{target function that we are trying to learn,but do not really know} \\\\\n&\\epsilon : \\text{statistical error}\n\\end{aligned}\\]\n\n\\(f(x)\\)ëŠ” ëª¨ì§‘ë‹¨ì—ì„œ ë‘ ë³€ìˆ˜ \\(x,y\\)ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëŒ€í‘œí•˜ëŠ” ê³¡ì„ ì´ë©° ìš°ë¦¬ê°€ í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ ì°¾ê³ ì í•˜ëŠ” target functionì…ë‹ˆë‹¤.\ntarget functionì„ ì°¾ê²Œë˜ë©´ ìš°ë¦¬ëŠ” unknown dataì— ëŒ€í•´ì„œë„ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.(good generalization)\n\n\nì¶œì²˜ : Pilsung Kang êµìˆ˜ë‹˜ ê¹ƒí—ˆë¸Œ\n\nëª¨ì§‘ë‹¨ìœ¼ë¡œë¶€í„° samplingë˜ëŠ” data setì— ì˜í•´ì„œ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì´ë¼ë„ ë‹¤ë¥´ê²Œ í•™ìŠµì´ ë©ë‹ˆë‹¤.\në§Œì•½ ì„œë¡œë‹¤ë¥¸ data setê°€ Nê°œê°€ ì¡´ì¬í•œë‹¤ë©´ true functionì¸ \\(f(x)\\)ì— ëŒ€í•´ ì„œë¡œë‹¤ë¥¸ Nê°œì˜ ê·¼ì‚¬ê°’ì¸ \\(\\hat{f}_1(x),\\hat{f}_2(x),\\dots,\\hat{f}_n(x)\\)ë¥¼ êµ¬í•©ë‹ˆë‹¤.\nê°œì¸ì ìœ¼ë¡œ ê°ê°ì˜ datapointëŠ” \\(p(x,y)\\)ë¥¼ ë”°ë¥´ë¯€ë¡œ dataset \\(D\\)ë„ í™•ë¥ ë³€ìˆ˜ì´ë©° ì´ë¡œë¶€í„° randomí•œ \\(\\hat{f}(x)\\)ë„ í™•ë¥ ë³€ìˆ˜ë¼ê³  ìƒê°í•©ë‹ˆë‹¤.\n\n\n\n\n\n\nbias-variance decompositionì€ bias,variance,irreducible errorë¡œ êµ¬ì„±ë˜ëŠ” í•™ìŠµì•Œê³ ë¦¬ì¦˜ì˜ expected generalization errorë¥¼ ì‚´í´ë´„ìœ¼ë¡œì„œ í•™ìŠµì•Œê³ ë¦¬ì¦˜ì„ ë¶„ì„í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Linear Regression.html",
    "href": "posts/DL/Linear Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "ì„ í˜•íšŒê·€ì— ëŒ€í•´ì„œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#assumption-modeling",
    "href": "posts/DL/Linear Regression.html#assumption-modeling",
    "title": "Linear Regression",
    "section": " Assumption & modeling",
    "text": "Assumption & modeling\n\n\nText(0.5, 1.0, 'n=200')\n\n\n\n\n\nìš°ë¦¬ê°€ ê°€ì§„ ë°ì´í„°ë¥¼ ê´€ì°°í•´ë´…ì‹œë‹¤. ê°€ì¥í¬ê²Œ ëˆˆì— ë„ëŠ” ì‚¬ì‹¤ì€ xì™€ yì‚¬ì´ì˜ ê´€ê³„ê°€ ì„ í˜•ì ì´ë¼ëŠ” ì ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ê°„ë‹¨í•œ ì„ í˜•ëª¨í˜•ìœ¼ë¡œ ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ìˆ˜ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ëª¨ë¸ë§í•  ìˆ˜ ìˆì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤. ì¡°ê¸ˆ ë” ì„¸ë¶€ì ìœ¼ë¡œ ë“¤ì–´ê°€ì„œ ê°ê°ì˜ xê°’ì— ëŒ€í•´ì„œ yê°’ì„ ê´€ì°°í•´ ë´…ì‹œë‹¤. ì²«ë²ˆì§¸ë¡œ ì•Œ ìˆ˜ ìˆëŠ” ì ì€ ë™ì¼í•œ xê°’ì— ëŒ€í•´ì„œë„ ì„œë¡œë‹¤ë¥¸ yê°’ì„ ê°€ì§€ëŠ” ì ë“¤ ë°ì´í„°ê°€ ë§ì´ ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ë¡œë¶€í„° \\(y\\)ê°€ \\(x\\)ë¿ë§Œì•„ë‹ˆë¼ ë˜ë‹¤ë¥¸ í™•ë¥ ë³€ìˆ˜ \\(\\epsilon\\)ì˜ ê°’ì— ì˜í•´ ê²°ì •ëœë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë‘ë²ˆì§¸ë¡œ xê°’ì— ì˜í•´ì„œ ì°íˆëŠ” ì ì˜ ìœ„ì¹˜(y)ì˜ ì–‘ìƒì´ ë‹¤ë¥´ë‹¤ëŠ” ì ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. xê°€ ì‘ìœ¼ë©´ ì¼ë°˜ì ìœ¼ë¡œ yëŠ” ë‚®ì€ìœ„ì¹˜ì—ì„œ ì ì´ ì°íˆê³  xê°€ í¬ë©´í´ìˆ˜ë¡ ì¼ë°˜ì ìœ¼ë¡œ ë†’ì€ ìœ„ì¹˜ì—ì„œ ì ì´ ì°íˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në°ì´í„°ë¥¼ ê´€ì°°í•˜ë©´ì„œ ì–»ì€ ì‚¬ì‹¤ë¡œë¶€í„° \\(x\\)ì™€ \\(y\\)ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ìˆ˜í•™ì ìœ¼ë¡œ ëª¨ë¸ë§ í•´ë³´ê² ìŠµë‹ˆë‹¤. ì²«ë²ˆì§¸ ì‚¬ì‹¤ë¡œë¶€í„° ìš°ë¦¬ëŠ” ë™ì¼í•œ \\(x\\)ë¼ë„ ê°ê°ì˜ ë°ì´í„°ì— ëŒ€í•´ì„œ ì–´ë–¤ ë˜ë‹¤ë¥¸ ê°’ì´ ë”í•´ì§ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ë½‘íë•Œë§ˆë‹¤ ê·¸ ê°’ì´ ë‹¤ë¥¸ ë³€ìˆ˜ëŠ” í™•ë¥ ë³€ìˆ˜ \\(\\epsilon_i\\)ë¥¼ ë”í•´ì¤Œìœ¼ë¡œì„œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ì˜¤ì°¨ê°€ ë”°ë¥´ëŠ” ë¶„í¬ì— ëŒ€í•´ì„œ ê°€ì •ì„ í•©ë‹ˆë‹¤. ì˜¤ì°¨ëŠ” í‰ê· ì´ 0ì´ê³  ë¶„ì‚°ì´ \\(\\sigma^2\\)ì¸ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜ë¼ê³  ê°€ì •í•©ë‹ˆë‹¤.\në˜í•œ ë‘ë²ˆì§¸ ì‚¬ì‹¤ë¡œë¶€í„° ìš°ë¦¬ëŠ” \\(x\\)ì™€ \\(y\\)ëŠ” ì»¤ì§€ë©´ ì»¤ì§€ê³  ì‘ì•„ì§€ë©´ ì‘ì•„ì§€ëŠ” ê´€ê³„ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ê´€ê³„ë¥¼ í‘œí˜„í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì§€ë§Œ ì„ í˜•íšŒê·€ì—ì„œëŠ” ì„ í˜•ìœ¼ë¡œ ì´ ê´€ê³„ë¥¼ í‘œí˜„í•©ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&Y_i = w_0 + w_1x_1 + \\dots + w_Mx_M + \\epsilon_i={\\bf{x_i}^\\intercal}w + \\epsilon_i\\\\\n&\\text{where , }{\\bf{{\\bf{x_i}^\\intercal}}} = \\begin {bmatrix}0,x_{i,1},\\dots,x_{i,M} \\end {bmatrix} \\in \\mathbb{R^{1 \\times (M+1)},{\\bf{w}} = \\begin {bmatrix} w_0,w_1,\\dots,w_M\\end{bmatrix}}^\\intercal \\in \\mathbb{R^{(M+1)\\times 1}}\\\\\n\\quad\\quad\\quad &\\epsilon_i \\overset{i.i.d}{\\sim} \\mathcal{N}(0,\\sigma^2)\\,\\,(\\text{for } i=1,2,\\dots,N)\n\\end{aligned}\\]\nì´ë ‡ê²Œ \\({\\bf{x_i}^\\intercal}\\)ì™€ \\(Y_i\\)ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì •í•  ê²½ìš° ê°ê°ì˜ \\(Y_i\\)ì•ˆì— í™•ë¥ ë³€ìˆ˜ \\(\\epsilon_i\\)ê°€ \\(Y_i\\)ë„ ë§ˆì°¬ê°€ì§€ë¡œ ì •ê·œë¶„ë¡œë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜ë¡œ ê·¸ ê°’ì´ \\(\\epsilon\\)ì— ì˜í•´ ë…ë¦½ë³€ìˆ˜ì˜ ê°’ì´ ë™ì¼í•˜ë”ë¼ë„ ì¶”ì¶œí• ë•Œë§ˆë‹¤ ë‹¤ë¥¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \\(Y_i\\)ë„ í™•ë¥ ë³€ìˆ˜ì´ë¯€ë¡œ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜ì´ë¯€ë¡œ ì¤‘ì‹¬ìœ„ì¹˜ì™€ ë³€ë™ì„ í™•ì¸í•˜ê¸° ìœ„í•´ ê¸°ëŒ“ê°’ê³¼ ë¶„ì‚°ì„ êµ¬í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ \\(x\\)ëŠ” ì£¼ì–´ì ¸ ìˆìœ¼ë¯€ë¡œ ê·¸ë•Œì˜ í™•ë¥ ë¶„í¬ì— ëŒ€í•´ì„œ ê³„ì‚°í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mathbb{E}[Y_i] = \\mathbb{E}[w_0 + \\dots + w_Mx_M+\\epsilon_i] = w_0 + w_1x_1 + \\dots + w_Mx_M={\\bf{x_i}^\\intercal}w \\\\\n&\\text{var}[Y_i] = \\text{var}[w_0 + \\dots + w_Mx_M+\\epsilon_i] = \\text{var}[\\epsilon_i] = \\sigma^2\n\\end{aligned}\\]\nìœ„ìˆ˜ì‹ìœ¼ë¡œë¶€í„° ê°ê°ì˜ \\(Y_i\\)ì— ëŒ€í•œ í™•ë¥ ë¶„í¬ëŠ” ì •ê·œë¶„í¬ì´ë©°(\\(\\epsilon_i\\)ì— ì˜í•´) ê¸°ëŒ“ê°’ì€ \\(x\\)ì™€ \\(w\\)ì— ì˜í•´ ì •í•´ì§€ì§€ë§Œ ë¶„ì‚°ì€ \\(\\sigma^2\\)ìœ¼ë¡œ í•­ìƒ ë™ì¼í•˜ë‹¤ëŠ” ì ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë‘ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ì§€ë§Œ ê¸°ëŒ“ê°’ë§Œ \\(x\\)ì— ì˜í•˜ì—¬ ë‹¬ë¼ì§‘ë‹ˆë‹¤. ì¦‰ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&Y_i|w;{\\bf{x_i}^\\intercal} \\sim \\mathcal{N}({\\bf{x_i}^\\intercal}{\\bf{w}},\\sigma^2)\\, \\text{ for } i = 1,2,\\dots,N\\\\\n&p(y_i|{\\bf{w}};{\\bf{x_i}^\\intercal}) = ì •ê·œë¶„í¬ì‹\n\\end{aligned}\\]\nì´ë¯¸ì§€\nì—¬ê¸°ê¹Œì§€ ìš°ë¦¬ê°€ ê°€ì§„ ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë…ë¦½ë³€ìˆ˜ì™€ ì¢…ì†ë³€ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì„ í˜•ëª¨í˜•ì„ ë§Œë“¤ì–´ë´¤ìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ê¶ê·¹ì ì¸ ëª©ì ì¸ unseen dataì— ì ì ˆí•˜ê²Œ ì¢…ì†ë³€ìˆ˜ì˜ ê°’ì„ ì˜ˆì¸¡í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œìš”?ë§Œì•½ í•™ìŠµë°ì´í„°ë¡œë¶€í„° ì ì ˆíˆ ê°€ì¤‘ì¹˜ì¸ \\(w\\)ë¥¼ êµ¬í• ìˆ˜ë§Œ ìˆë‹¤ë©´ ë…ë¦½ë³€ìˆ˜ê°€ ì…ë ¥ë˜ì—ˆë•Œ ëŒ€í•˜ì—¬ ì¢…ì†ë³€ìˆ˜ê°€ ë”°ë¥´ëŠ” í™•ë¥ ë¶„í¬(ì •ê·œë¶„í¬)ë¥¼ ì•Œ ìˆ˜ ìˆê³  ê·¸ ë¶„í¬ì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ ê³³ì˜ ì¢…ì†ë³€ìˆ˜ì˜ ê°’(ì •ê·œë¶„í¬ì—ì„œëŠ” xw)ì„ ì˜ˆì¸¡ê°’ìœ¼ë¡œ í•˜ë©´ ë©ë‹ˆë‹¤. ë˜ ë‹¤ë¥´ê²Œ ìƒê°í•˜ë©´ ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•œë‹¤ëŠ” ê²ƒì€ ë°ì´í„°ë¥¼ ê°€ì¥ ì˜ í‘œí˜„í•˜ëŠ” ì§ì„ (í‰ë©´,ì´ˆí‰ë©´)ì„ ì–»ì€ê²ƒì´ë¯€ë¡œ ì…ë ¥xì— ëŒ€í•˜ì—¬ ì§ì„ ì˜ ê°’ì„ ì½ì–´ì„œ ì˜ˆì¸¡ê°’ìœ¼ë¡œ í•˜ë©´ ë©ë‹ˆë‹¤. ì–´ì°Œëê±´ ë‘ ê²½ìš° ëª¨ë‘ ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•´ì•¼**í•˜ë¯€ë¡œ ìš°ë¦¬ì˜ ëª©ì ì€ ì´ì œ ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#point-estimation---mle",
    "href": "posts/DL/Linear Regression.html#point-estimation---mle",
    "title": "Linear Regression",
    "section": " Point Estimation - MLE",
    "text": "Point Estimation - MLE\nê·¸ë ‡ë‹¤ë©´ ê°€ì¥ ì ì ˆí•œ ê°€ì¤‘ì¹˜ëŠ” ë¬´ì—‡ì¼ê¹Œìš”? ë°ì´í„°ê°€ ì£¼ì–´ì§ˆë•Œ ê°€ì¤‘ì¹˜ì— í™•ë¥ ì´ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  í•´ë´…ì‹œë‹¤.\n\\[\\begin{aligned}\n&p({\\bf{w}}|D) = p_{{\\bf{w}}|Y_1,Y_2,\\dots,Y_N}(w|y_1,y_2,\\dots,y_N;{\\bf{X}})\\\\\n&\\text{where, } {\\bf{X}} =\n\\begin{bmatrix}\n--{\\bf{x_1^\\intercal}}--\\\\\n--{\\bf{x_2^\\intercal}}--\\\\\n\\vdots\\\\\n--{\\bf{x_N^\\intercal}}--\n\\end{bmatrix}\n\\end{aligned}\\]\nì˜¤ë¥¸ìª½ì‹ì€ ì™¼ìª½ì‹ì—ì„œì˜ ë°ì´í„°\\(D\\)ë¥¼ ì¢€ë” í’€ì–´ì ì€ ìˆ˜ì‹ì…ë‹ˆë‹¤. ë§Œì•½ ìœ„ì™€ ê°™ì€ í™•ë¥ ì„ ê³„ì‚°í–ˆì„ë•Œ ê·¸ ê°’ì´ ì‘ì€ ê°€ì¤‘ì¹˜ëŠ” ê°€ëŠ¥ì„±ì´ ë‚®ì€ ê°€ì¤‘ì¹˜ì´ë¯€ë¡œ ìš°ë¦¬ê°€ ì°¾ëŠ” ì ì ˆí•œ ê°€ì¤‘ì¹˜ëŠ” ì•„ë‹ê²ƒì…ë‹ˆë‹¤. ë°˜ëŒ€ë¡œ í™•ë¥ ì´ ë†’ì€ ê°€ì¤‘ì¹˜ëŠ” ê°€ëŠ¥ì„±ì´ ë†’ì€ ê°€ì¤‘ì¹˜ì´ê¸°ë•Œë¬¸ì— ìš°ë¦¬ê°€ ì°¾ëŠ” ê°€ì¤‘ì¹˜ë¼ê³  í•  ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´ ê·¸ëƒ¥ â€œì € í™•ë¥ ì„ ê°€ì¥ í¬ê²Œ í•˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ì°¾ìœ¼ë©´ ë˜ê² ë‹¤â€ë¼ê³  ìƒê°ì´ ë“¤ì§€ë§Œ ì•ˆíƒ€ê¹Œìš´ ì ì€ ìš°ë¦¬ëŠ” ìœ„ì™€ê°™ì€ (ì¡°ê±´ë¶€)í™•ë¥ (ë¶„í¬)ì„ ë°”ë¡œ ì•Œê¸°ê°€ ì–´ë µìŠµë‹ˆë‹¤.. ë”°ë¼ì„œ ë² ì´ì¦ˆì •ë¦¬ì˜ ë„ì›€ì„ ë°›ìŠµë‹ˆë‹¤. ë² ì´ì¦ˆ ì •ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&p({\\bf{w}}|D) = \\frac{p(D|{\\bf{w}})p({\\bf{w}})}{p(D)} \\propto (D|{\\bf{w}})p({\\bf{w}})\n\\end{aligned}\\]\nìˆ˜ì‹ì—ì„œ ë¶„ëª¨\\(p(D)\\)ëŠ” normalization constantë¼ í•˜ëŠ” ìƒìˆ˜ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ë¶„ìë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ êµ¬í•˜ë©´ ë˜ì§€ë§Œ MLE(maximum likelyhood estimation) ì—ì„œëŠ” likelyhoodì¸ \\(L = p(D|w)\\)ë§Œì„ ìµœëŒ€í™” í•˜ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n\\[\\begin{aligned}\nL = p_{Y_1,Y_2,\\dots,Y_N|{\\bf{w}}}(y_1,y_2,\\dots,y_N|w;X)  &= p_{Y_1,Y_2,\\dots,Y_N|{\\bf{w}}}(y_1,y_2,\\dots,y_N|w;X)\\\\\n&= p_{Y_1,{\\bf{w}}}(y_1|{w},{\\bf{X}})\\dots p_{Y_N,{\\bf{w}}}(y_N|{w},{\\bf{X}}) \\\\\n&= \\prod_{n = 1}^{N}p_{Y_n|{\\bf{W}}}(y_n|w;{\\bf{X}}) = \\prod_{n = 1}^{N}\\mathcal{N}(y_n|{\\bf{x_n}^\\intercal}{\\bf{w}},\\sigma^2) \\\\\n&= \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(y_n-{\\bf{x_n}}^\\intercal{\\bf{w}})^2}{2\\sigma^2} \\right\\}  \\\\\n\\end{aligned}\\]\nlog likelyhoodë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ë¡œê·¸ë¥¼ ì·¨í•˜ëŠ” ì´ìœ ëŠ” ìµœëŒ“ê°’ì„ ê°€ì§€ëŠ” \\({\\bf{w}}\\)ê°€ ë³€í•˜ì§€ ì•Šê³  ê³±ì…ˆì„ ë§ì…ˆì„ ë°”ê¿”ì„œ ê³„ì‚°í•˜ê¸° ë” í¸í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\\[\\begin{aligned}\n\\text{lnL} &= \\text{ln}\\prod_{n=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(y_n-{\\bf{x_n}}^\\intercal{\\bf{w}})^2}{2\\sigma^2} \\right\\}  \\\\\n&= \\sum_{n=1}^N\\text{ln} \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\text{exp}\\left\\{-\\frac{(y_n-{\\bf{x_n}}^\\intercal{\\bf{w}})^2}{2\\sigma^2} \\right\\} \\\\\n&= \\sum_{n=1}^N \\text{ln}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} - \\frac{1}{2\\sigma^2} \\sum_{n=1}^{N}(y_n-{\\bf{x_n}}^\\intercal{\\bf{w}})^2 \\\\\n&= C_1 - C_2\\sum_{n=1}^{N}(y_n-{\\bf{x_n}}^\\intercal{\\bf{w}})^2\n\\end{aligned}\\]\nìƒìˆ˜ë¥¼ ì œì™¸í•´ë„ ìµœëŒ“ê°’ì˜ ìœ„ì¹˜ëŠ” ë³€í•˜ì§€ ì•Šìœ¼ë¯€ë¡œ ì œì™¸í•˜ê³  loglikelyhoodë¥¼ ìµœëŒ€í™” í•˜ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ì–»ì€ê²ƒì´ ìš°ë¦¬ì˜ ëª©í‘œì…ë‹ˆë‹¤. ì´ë•Œì˜ ê°€ì¤‘ì¹˜ëŠ” ê°€ì¤‘ì¹˜ì— ëŒ€í•œ ì¶”ì •ëŸ‰ì´ë¯€ë¡œ \\(\\bf{\\hat{w}}\\)ë¡œ í‘œê¸°í•©ë‹ˆë‹¤.\nì—¬ê¸°ì„œ ì‹œê·¸ë§ˆì—ì„œë¶€í„° ë³´ë©´ í”íˆ ê²½ì‚¬í•˜ê°•ë²•ì—ì„œ ì“°ëŠ” MSEê°€ ë³´ì…ë‹ˆë‹¤. MSEëŠ” ì„ í˜•íšŒê·€ì˜ MLEì—ì„œ Negative log likelyhoodë¥¼ êµ¬í• ë•Œ ë‚˜ì˜¤ëŠ” í•­ì…ë‹ˆë‹¤.\npsuedo inverseì— ì˜í•œ í•´ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n\\hat{{\\bf{w}}} = (\\bf{X^TX}^{-1})X^Ty\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#ì„ í˜•íšŒê·€ì˜-loss-function",
    "href": "posts/DL/Linear Regression.html#ì„ í˜•íšŒê·€ì˜-loss-function",
    "title": "Linear Regression",
    "section": " ì„ í˜•íšŒê·€ì˜ Loss function",
    "text": "ì„ í˜•íšŒê·€ì˜ Loss function\n2ë²ˆì—ì„œ êµ¬í•œ ì¶”ì •ê°’ \\(\\hat{\\bf{W}}\\)ì´ ì–¼ë§ˆë‚˜ í‹€ë¦°ì§€,ë¶€ì •í™•í•œì§€ ì•Œë ¤ì£¼ëŠ” í•¨ìˆ˜ë¥¼ Loss function ë˜ëŠ” Cost functionì´ë¼ê³  í•©ë‹ˆë‹¤. ì„ í˜•íšŒê·€ì—ì„œì˜ Loss functionì€ ì¼ë°˜ì ìœ¼ë¡œ MSEë¥¼ ì‚¬ìš©í•˜ë©° ì£¼ì–´ì§„ ìƒ˜í”Œì—ì„œ ì”ì°¨(residual,\\(\\hat{y}_i-y\\))ë“¤ì„ ì „ë¶€ ì œê³±í•˜ì—¬ ë”í•œ ê°’ì…ë‹ˆë‹¤.\n(Loss function) \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2} = \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}}) = \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\)\nMSEì™€ ê°™ì€ Loss functionì€ ìš°ë¦¬ì˜ ì¶”ì •ì´ ì–¼ë§ˆë‚˜ í‹€ë ¸ëŠ”ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” \\(\\hat{\\bf{W}}\\)ì— ëŒ€í•œ í•¨ìˆ˜ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, loss functionì„ ê°€ì¥ ìµœì†Œí™” í•˜ëŠ” \\(\\bf{\\hat{W}}\\)ì„ ì°¾ì•„ë‚´ë©´ í™•ë¥ ë³€ìˆ˜ì‚¬ì´ì˜ ì„ í˜•ê´€ê³„ì¸ \\(\\bf{W}\\)ë¥¼ ì•Œì•„ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\nText(110, 15, 'residual')"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#parameter-update",
    "href": "posts/DL/Linear Regression.html#parameter-update",
    "title": "Linear Regression",
    "section": " Parameter update",
    "text": "Parameter update\nnê°œì˜ ë…ë¦½ë³€ìˆ˜ë¥¼ ê°€ì§€ëŠ” ë‹¤ë³€ìˆ˜ ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ì— ëŒ€í•œ GradientëŠ” ìˆ˜í•™ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\(\\nabla_{X}{f(x_1,x_2,...,x_n)} = (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\dots,\\frac{\\partial f}{\\partial x_n})\\) ë‹¤ë³€ìˆ˜ ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ì— ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ì·¨í•˜ë©´ ë²¡í„°ì…ë‹ˆë‹¤.ê·¸ëŸ¬ë¯€ë¡œ,ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ ë²¡í„°(ë‹¤ë³€ìˆ˜)ë¥¼ ì…ë ¥í–ˆì„ ë•Œ,ë²¡í„°ë¥¼ ì¶œë ¥ìœ¼ë¡œ í•˜ëŠ” ë²¡í„°í•¨ìˆ˜ë¼ê³  ìƒê°í•´ë„ ë¬´ë°©í•©ë‹ˆë‹¤.ì¤‘ìš”í•œ ì‚¬ì‹¤ì€ ì„ì˜ì˜ ê³µê°„ìƒì˜ ì„ì˜ì˜ point \\(X\\)ì—ì„œ ìŠ¤ì¹¼ë¼í•¨ìˆ˜ì— ëŒ€í•œ gradient of f = \\(-\\nabla_{X}{f}\\) ë°©í–¥ì€ ìŠ¤ì¹¼ë¼í•¨ìˆ˜ê°€ ê°€ì¥ ê¸‰ê²©í•˜ê²Œ ê°ì†Œí•˜ëŠ” ë°©í–¥ì´ë¼ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤.(ì¦ëª…ìƒëµ)\nìœ„ì˜ ì‚¬ì‹¤ì— ì˜í•˜ë©´,ìš°ë¦¬ëŠ” ì„ì˜ì˜ \\(\\hat{\\bf{W}}\\)ì—ì„œ Loss functionì´ ê°€ì¥ ê¸‰ê²©í•˜ê²Œ ê°ì†Œí•˜ëŠ” ë°©í–¥ì„ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ê°ì†Œí•˜ëŠ” ë°©í–¥ì„ ì°¾ê³  ì´ë™í•˜ê³  ê°ì†Œí•˜ëŠ” ë°©í–¥ì„ ì°¾ê³  ì´ë™í•˜ê³  ë°˜ë³µí•˜ë‹¤ë³´ë©´â€¦ ê¶ê·¹ì ì¸ ëª©ì ì¸ í‹€ë¦°ì •ë„ë¥¼ ìµœì†Œí™”í•˜ëŠ” ì¦‰,Loss functionê°’ì´ ê°€ì¥ ì‘ì€ \\(\\hat{\\bf{W}}\\)ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. \\(\\bf\\hat{W}\\)ë¥¼ ìˆ˜ì •í•˜ëŠ” êµ¬ì²´ì ì¸ ìˆ˜ì‹ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n(Gradient descent parameter update) \\(\\hat{\\bf{W}}_{t} = \\hat{\\bf{W}}_{t-1} - \\alpha\\times\\nabla_{W}{L}\\)\n\\(\\hat{\\bf{W}}_{t-1}\\)ì€ ìˆ˜ì •ë˜ê¸°ì „ì˜ ê°€ì¤‘ì¹˜(ë²¡í„°)ì´ë©° \\(\\hat{\\bf{W}_{t}}\\)ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ í•œë²ˆ ì—…ë°ì´íŠ¸ í•œ í›„ì˜ ê°€ì¤‘ì¹˜(ë²¡í„°)ì…ë‹ˆë‹¤. \\(t-1\\)ì˜ \\(\\hat{\\bf{W_{t-1}}}\\)ì— \\(-\\alpha\\times\\nabla_{W}{L}\\)ë¥¼ ë”í•´ì¤Œìœ¼ë¡œì„œ \\(\\hat{\\bf{W}}_{t-1}\\)ì€ loss functionì´ ê°€ì¥ ê¸‰ê²©íˆ(ë§ì´)ê°ì†Œí•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ì´ë™í•˜ë©° \\(\\hat{\\bf{W}}_{t}\\)ê°€ ë©ë‹ˆë‹¤. \\(\\alpha\\)ëŠ” í•™ìŠµë¥ (learning rate)ì…ë‹ˆë‹¤. \\(\\hat{\\bf{W}}_{t-1}\\)ê³¼ ê³±í•´ì ¸ì„œ ì–¼ë§ˆë‚˜ ë§ì´ ë˜ëŠ” ì ê²Œ ì›€ì§ì¼ì§€ë¥¼ ê²°ì •í•©ë‹ˆë‹¤. í•œë²ˆì— ì–¼ë§ˆë‚˜ ì´ë™í• ì§€ì— ë¹„ìœ í•œ â€œë³´í­â€ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nìš”ì•½í•˜ìë©´, ê²½ì‚¬í•˜ê°•ë²•ì„ í†µí•˜ì—¬ ìœ„ì™€ ê°™ì´ ê°€ì¤‘ì¹˜\\(\\hat{\\bf{W}}\\)ë¥¼ ì¬ê·€ì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•˜ë©´ loss function \\(L\\)ì´ ê°€ì¥ ìµœì†Œê°€ ë˜ëŠ” ì§€ì ì˜ \\(\\hat{\\bf{W}}\\)ë¥¼ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#mseì—-ëŒ€í•œ-ë”-ìƒì„¸í•œ-ì „ê°œ",
    "href": "posts/DL/Linear Regression.html#mseì—-ëŒ€í•œ-ë”-ìƒì„¸í•œ-ì „ê°œ",
    "title": "Linear Regression",
    "section": " MSEì— ëŒ€í•œ ë” ìƒì„¸í•œ ì „ê°œ",
    "text": "MSEì— ëŒ€í•œ ë” ìƒì„¸í•œ ì „ê°œ\nMSEë¥¼ ë” ìƒì„¸íˆ ì „ê°œí•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2}\\) \\(= \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}})\\) \\(= \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\) \\(= \\frac{1}{n}(\\bf{y^T - \\hat{\\bf{W}}^{T}\\bf{X}^{T})(\\bf{y} - \\bf{X}\\bf{\\hat{W}}})\\) \\(= \\frac{1}{n}(\\bf{y^Ty-y^TX\\hat{W}} - \\hat{W}X^Ty + \\hat{W}^TX^TX\\hat{W})\\)\nì—¬ê¸°ì„œ \\(\\bf{y^TX\\hat{W}} \\in \\bf{R}^{1 \\times 1}\\) ì´ë¯€ë¡œ \\(\\bf{y^TX\\hat{W}} = (\\bf{y^TX\\hat{W}})^T = (\\bf{\\hat{W}X^Ty})\\)ê°€ ì„±ë¦½í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ MSEë¥¼ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. (MSE) \\(MSE = \\frac{1}{n}(\\bf{y^Ty -2\\hat{W}X^Ty + \\hat{W}^TX^TX\\hat{W}})\\)"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#gradient-descentì—-ëŒ€í•œ-ë”-ìƒì„¸í•œ-ì „ê°œloss-mseì¼-ê²½ìš°",
    "href": "posts/DL/Linear Regression.html#gradient-descentì—-ëŒ€í•œ-ë”-ìƒì„¸í•œ-ì „ê°œloss-mseì¼-ê²½ìš°",
    "title": "Linear Regression",
    "section": " Gradient Descentì— ëŒ€í•œ ë” ìƒì„¸í•œ ì „ê°œ(\\(Loss\\) = MSEì¼ ê²½ìš°)",
    "text": "Gradient Descentì— ëŒ€í•œ ë” ìƒì„¸í•œ ì „ê°œ(\\(Loss\\) = MSEì¼ ê²½ìš°)\n(Gradient of MSE) \\(\\nabla{L} = MSE\\) \\(= \\bf{\\frac{1}{n}\\frac{\\partial}{\\partial \\hat{W}}(\\bf{y^Ty - 2\\hat{W}^TX^T + \\hat{W}^TX^TX\\hat{W}})}\\) \\(= \\bf{\\frac{1}{n}}(\\bf{\\frac{\\partial}{\\partial \\hat{W}}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2\\hat{W}^{T}X^{T}y + \\frac{\\partial}{\\partial\\hat{W}}\\hat{W}^{T}X^{T}X\\hat{W})\\) \\(= \\bf{\\frac{1}{n}(\\frac{\\partial}{\\partial \\hat{W}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2y^TX\\hat{W} + \\frac{\\partial}{\\partial\\hat{W}}\\hat{W}^TX^TX\\hat{W})}\\) \\(= \\bf{\\frac{1}{n}[0 - 2X^Ty + (X^TX + X^TX)\\hat{W}]}\\) \\(= \\bf{\\frac{2}{n}X^T(X\\hat{W} - y)}\\)\n(parameter update) \\(\\bf{\\hat{W}_{t} = \\hat{W}_{t-1} - \\alpha \\times \\frac{2}{n}X^T(X\\hat{W} - y)}\\)"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#ê²°ê³¼í•´ì„",
    "href": "posts/DL/Linear Regression.html#ê²°ê³¼í•´ì„",
    "title": "Linear Regression",
    "section": " ê²°ê³¼í•´ì„",
    "text": "ê²°ê³¼í•´ì„\n200ê°œì˜ ìƒ˜í”Œë¡œë¶€í„° \\(\\bf{w}\\)ë¥¼ ì¶”ì •í•˜ì—¬ \\(\\hat{\\bf{w}}= (0.125,0.969)\\)ë¥¼ ì–»ì—ˆìŠµë‹ˆë‹¤. population regression modelì˜ \\({\\bf{w}} = (w_0,w_1) = (0,1)\\)ì„ ì˜¬ë°”ë¥´ê²Œ ì¶”ì •í–ˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ì£¼ ì•½ê°„ì˜ ì°¨ì´ê°€ ì¡´ì¬í•˜ëŠ”ë° ì´ ì°¨ì´ëŠ” ëª¨ì§‘ë‹¨ì—ì„œ ìƒ˜í”Œì„ ë” ì–»ê±°ë‚˜ ë” ì„¸ë°€í•˜ê²Œ ì—…ë°ì´íŠ¸í•˜ë©´ ìµœì†Œí™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n#plt.title(\"w_1 : {} // w_0: {}\".format(round(W_hat[1].tolist()[0],3),round(W_hat[0].tolist()[0],3)))\nplt.title(\"Linear Regression\")\ntext=f\"What = ({round(t[0].tolist()[0],3)},{round(t[1].tolist()[0],3)})\"\nplt.plot(X[:,1],y,\"bo\",alpha=0.5)\nplt.plot(X[:,1],X@W_hat,\"r--\")\nplt.gca().axes.xaxis.set_visible(False)\nplt.gca().axes.yaxis.set_visible(False)\nplt.title(text)\n\nText(0.5, 1.0, 'What = (-0.125,0.969)')"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#appendix",
    "href": "posts/DL/Linear Regression.html#appendix",
    "title": "Linear Regression",
    "section": " Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#mleì˜-í•´ì™€-mseìœ ë„",
    "href": "posts/DL/Linear Regression.html#mleì˜-í•´ì™€-mseìœ ë„",
    "title": "Linear Regression",
    "section": " MLEì˜ í•´ì™€ MSEìœ ë„",
    "text": "MLEì˜ í•´ì™€ MSEìœ ë„"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#ì°¸ê³ ìë£Œ",
    "href": "posts/DL/Linear Regression.html#ì°¸ê³ ìë£Œ",
    "title": "Linear Regression",
    "section": " ì°¸ê³ ìë£Œ",
    "text": "ì°¸ê³ ìë£Œ\nMaximum Likelihood Estimation(MLE) & Maximum A Posterior(MAP)"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html",
    "href": "posts/DL/Logistic Regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "ë¡œì§€ìŠ¤í‹±íšŒê·€ì— ëŒ€í•´ì„œ ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#ê¸°ëŒ“ê°’ì—-ëŒ€í•œ-ê³ ì°°",
    "href": "posts/DL/Logistic Regression.html#ê¸°ëŒ“ê°’ì—-ëŒ€í•œ-ê³ ì°°",
    "title": "Logistic Regression",
    "section": "ê¸°ëŒ“ê°’ì— ëŒ€í•œ ê³ ì°°",
    "text": "ê¸°ëŒ“ê°’ì— ëŒ€í•œ ê³ ì°°\nê¸°ëŒ“ê°’ì€ ì‹¤í—˜ ë˜ëŠ” ì‹œí–‰ì„ ë¬´í•œíˆ ë°˜ë³µí–ˆì„ë•Œ í™•ë¥ ë³€ìˆ˜ê°€ ì·¨í•˜ëŠ” ê°’ì˜ í‰ê· ìœ¼ë¡œ(ë˜ëŠ” ìƒ˜í”Œë§ëœ ê°’ì˜ í‰ê· ) ê¸°ëŒ€ë˜ëŠ” ê°’ì…ë‹ˆë‹¤. í™•ë¥ ë³€ìˆ˜ê°€ ë² ë¥´ëˆ„ì´ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ê²½ìš° í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•œ ê¸°ëŒ“ê°’(\\(E\\,[y|x_{1,i},x_{2,i}\\.,\\dots,x_{m,i}]\\))ê³¼ ëª¨ìˆ˜\\((p_i)\\)ê°€ ê°™ì€ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ,ë§Œì•½ì— ì£¼ì–´ì§„ ìƒ˜í”Œë°ì´í„°ë¡œë¶€í„° ë² ë¥´ëˆ„ì´ë¶„í¬ì˜ ëª¨ìˆ˜ë¥¼ ì ì ˆíˆ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ë©´ ì£¼ì–´ì§„ ì¡°ê±´í•˜ì—ì„œ ì‹¤í—˜ ë˜ëŠ” ì‹œí–‰ì„ ë¬´í•œíˆ ë°˜ë³µí•  ê²½ìš° í™•ë¥ ë³€ìˆ˜ê°€ 1ì¸ì‚¬ê±´ê³¼ 0ì¸ì‚¬ê±´ì¤‘ ì–´ë–¤ ì‚¬ê±´ì´ ë” ë§ì´ ë°œìƒí• ì§€ ì•Œ ìˆ˜ ìˆê³  ì´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì¢…ì†ë³€ìˆ˜ Yì˜ ê°’ì„ ê²°ì •í•˜ëŠ” ê²ƒì€ íƒ€ë‹¹í•©ë‹ˆë‹¤. - e.g.\n\n\\(E\\,[y]\\, = \\hat{p_i}<0.5\\) => ë¬´í•œíˆ ì‹¤í–‰í–ˆì„ë•Œ 0ì¸ ê²½ìš°ê°€ ë” ë§ì„ ê²ƒì„ => ê´€ì¸¡ì¹˜ë¥¼ 0ìœ¼ë¡œ ì˜ˆì¸¡ \n\\(E\\,[y]\\, = \\hat{p_i}\\geq0.5\\)=>ë¬´í•œíˆ ì‹¤í–‰í–ˆì„ë•Œ 1ì¸ ê²½ìš°ê°€ ë” ë§ì„ ê²ƒì„ => ê´€ì¸¡ì¹˜ë¥¼ 1ë¡œ ì˜ˆì¸¡"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#concept",
    "href": "posts/DL/Logistic Regression.html#concept",
    "title": "Logistic Regression",
    "section": "concept",
    "text": "concept\nì„ í˜•íšŒê·€ì—ì„œ ì¶”ì •í•˜ê³ ìí•˜ëŠ” ë³€ìˆ˜\\(y\\)ëŠ” \\(x_0,x_1,...,x_m\\)ê³¼ \\(w_0,w_1,...,w_m\\)ê³¼ì˜ linear combinationì´ì˜€ìŠµë‹ˆë‹¤.ìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ íŠ¹ì •ìƒ˜í”Œì— ëŒ€í•œ ëª¨ìˆ˜ë¥¼ ì ì ˆí•˜ê²Œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ë©´ ê´€ì¸¡ì¹˜ê°€ ì–´ë–¤ í´ë˜ìŠ¤ì— ì†í• ì§€ í•©ë¦¬ì ìœ¼ë¡œ ì•Œ ìˆ˜ ìˆìœ¼ë¯€ë¡œ,ë¡œì§€ìŠ¤í‹±íšŒê·€ì—ì„œë„ ì„ í˜•íšŒê·€ì—ì„œì˜ ì•„ì´ë””ì–´ë¥¼ í•µì‹¬ì•„ì´ë””ì–´ë¥¼ ê°€ì§€ê³ ì™€ì„œ ì¶”ì •í•˜ê³ ì í•˜ëŠ” ëª¨ìˆ˜\\(p_i\\)ë¥¼ \\(x_0,x_1,...,x_m\\)ê³¼ \\(w_0,w_1,...,w_m\\)ì˜ linear combinationë¡œ í‘œí˜„í•˜ê³ ì í•©ë‹ˆë‹¤.\nì„ í˜•íšŒê·€ì˜ ì•„ì´ë””ì–´(linear combination) + ëª¨ìˆ˜ì— ëŒ€í•œ í‘œí˜„ì´ë¼ëŠ” ì¡°ê±´ì„ ë§Œì¡±í•˜ê¸° ìœ„í•´ì„œ ìµœì¢…ì ì¸ ì‹ì€ ë‹¤ìŒê³¼ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ ê²ƒì…ë‹ˆë‹¤. - \\((x_0,x_1,...,x_m)\\,,(w_0,w_1,..,w_m)\\)ì˜ linear combination ì‹ì— ìˆì–´ì•¼ í•¨. - linearcombination = ëª¨ìˆ˜(ì¶”ì •í•˜ê³ ìí•˜ëŠ”ê°’)ì—¬ì•¼ í•¨.\nwhy linear combination?"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#ë³¸ê²©ì ì¸-ìœ ë„",
    "href": "posts/DL/Logistic Regression.html#ë³¸ê²©ì ì¸-ìœ ë„",
    "title": "Logistic Regression",
    "section": "ë³¸ê²©ì ì¸ ìœ ë„",
    "text": "ë³¸ê²©ì ì¸ ìœ ë„\n\nëª¨ìˆ˜ë¥¼ ë¡œì§€ìŠ¤í‹±í•¨ìˆ˜ë¡œ ë°”ê¾¸ê¸°\n\n\\((x_0,x_1,...,x_m)\\,,(w_0,w_1,..,w_m)\\)ì˜ linear combinationì´ ì‹ì— ì¡´ì¬í•´ì•¼ í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì„ í˜•ë°©ì •ì‹ì„ í•˜ë‚˜ ë§Œë“­ë‹ˆë‹¤. \\[\\begin{align}\nf(i) = x_{1,i}w_0 + x_{2,i}w_1 + x_2w_2 + ... + x_{m,i}w_m = X_iW \\nonumber \\\\\nwhere,X_i = \\,[x_{1,i},x_{2,i},\\dots,x_{m,i}]\\, ,W = \\,[w_0,w_1,\\dots,w_m]^\\text{T} \\nonumber \\\\\n\\end{align}\\]\nì¢Œë³€ì€ ì˜ˆì¸¡í•˜ê³ ì í•˜ëŠ” ê°’ì¸ ëª¨ìˆ˜ì—¬ì•¼ í•©ë‹ˆë‹¤. ì¢Œë³€ì„ ë°”ê¿”ë´…ë‹ˆë‹¤. \\[p_i = WX_i\\]\nì¢Œë³€ì˜ ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ ëª¨ìˆ˜ \\(p_i\\)ëŠ” í™•ë¥ ë³€ìˆ˜ \\(y = 1\\)ì¸ ì‚¬ê±´ì´ ì¼ì–´ë‚  í™•ë¥ ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ \\([0,1]\\)ì´ë¼ëŠ” ë²”ìœ„ë¥¼ ê°€ì§€ëŠ” ë°˜ë©´ ìš°ë³€ì˜ ê°’\\(WX_i\\)ì€ \\(\\,[-\\infty,\\infty]\\,\\)ì— ë²”ìœ„ë¥¼ ê°€ì§‘ë‹ˆë‹¤. ì—¬ê¸°ì„œ Odss Ratioë¥¼ ì¨ì„œ ëª¨ìˆ˜ \\(p_i\\)ë¥¼ í¬í•¨í•˜ë©° ë” ë„“ì€ rangeë¥¼ ê°–ë„ë¡ ì¢Œë³€ì„ ìˆ˜ì •í•©ë‹ˆë‹¤. \\[\\text{Odds Ratio} = \\frac{p_i}{1-p_i} = WX_i\\]\nì¢Œë³€ì„ Odds Ratioë¡œ ìˆ˜ì •í–ˆì§€ë§Œ ì—¬ì „íˆ ì¢Œë³€ì˜ ë²”ìœ„ëŠ”\\(\\,[0,\\infty]\\,\\)ìœ¼ë¡œ ìš°ë³€ì— ë¹„í•´ ì¢ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ Odds Ratioì— ë¡œì§“ë³€í™˜ì„ ì·¨í•˜ì—¬ ì¢Œë³€ì˜ ë²”ìœ„ë¥¼ \\(\\,[-\\infty,\\infty]\\)ë¡œ ë„“í˜€ì¤ë‹ˆë‹¤. \\[\\text{logit}(p) = \\text{ln}\\frac{p_i}{1-p_i} = WX_i\\]\n\nìœ„ ì‹ì„ í•´ì„í•˜ê¸° ìœ„í•´ \\(X\\)ì˜ ì²«ë²ˆì§¸ ìš”ì†Œì¸ \\(x_1\\)ì— ëŒ€ì‘í•˜ëŠ” íšŒê·€ê³„ìˆ˜ \\(w_1\\)ì´ í•™ìŠµê²°ê³¼ 3ìœ¼ë¡œ ì •í•´ì¡Œë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤.ë§Œì•½ \\(x_1\\)ì˜ ê°’ì´ 1ì¦ê°€í•œë‹¤ë©´ ë¡œê·¸ì˜¤ì¦ˆë¹„ê°€ 3ì¦ê°€í•©ë‹ˆë‹¤.\n\nì´ì œ ì–‘ë³€ì˜ ë²”ìœ„ëŠ” ë§ì¶°ì¡Œìœ¼ë¯€ë¡œ ì¶”ì •í•˜ê³ ì í•˜ëŠ” ë³€ìˆ˜ \\(p_i\\)ê°€ ì¢Œë³€ì— ì˜¤ë„ë¡ ì •ë¦¬í•´ë´…ì‹œë‹¤. \\[p_i = \\frac{1}{\\,(1 + e^{-WX_i})\\, }\\] (ì „ê°œ) \\(\\frac{p_i}{1-p_i} = e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\,(1-p_i)\\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\,e^{WX_i}-p_ie^{WX_i}\\) \\(\\Longleftrightarrow p_i + p_ie^{WX_i} = \\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i\\,(1 + e^{WX_i})\\, = \\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\frac{\\,e^{WX_i}}{\\,(1 + e^{WX_i})\\, }\\) \\(\\Longleftrightarrow p_i = \\frac{1}{\\,(1 + e^{-WX_i})\\, }\\)\n\nìµœì¢…ì ìœ¼ë¡œ, ì•ì„œ ëª©ì ì´ì—ˆë˜ Xì™€ Wì˜ ì„ í˜•ì¡°í•©ì´ ìˆ˜ì‹ë‚´ë¶€ì— ì¡´ì¬í•˜ë„ë¡ ìƒˆë¡­ê²Œ í‘œí˜„í•œ ëª¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\[p_i(y) = Pr\\,(y = 1|X_i;W)\\, = \\frac{1}{\\,1 + e^{-WX_i}\\,}\\]\n\n\në² ë¥´ëˆ„ì´ ë¶„í¬ì˜ pmf ì •ë¦¬\në² ë¥´ëˆ„ì´ë¶„í¬ì˜ ëª¨ìˆ˜ \\(p_i\\)ê°€ ìƒˆë¡­ê²Œ í‘œí˜„ë˜ì—ˆìœ¼ë¯€ë¡œ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë„ ìƒˆë¡­ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§ˆì§€ë§‰ ìˆ˜ì‹ì€ ë² ë¥´ëˆ„ì´ ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ê°€ ëª¨ìˆ˜\\(W\\)ì— ê´€í•œ ì‹ìœ¼ë¡œ ë°”ë€Œì—ˆìŒì„ í‘œí˜„í•©ë‹ˆë‹¤.\n\\[\\begin{align}\nBern(y;p_i) = Pr\\,(Y_{i} = y|x_{1,i},x_{2,i},\\dots,x_{m,i};p_i) &=\n\\begin{cases}\np_i & \\text{if}\\,y=1 \\\\\n1-p_i & \\text{if}\\,y=0\n\\end{cases} \\\\\n&= p_i^{y}(1-p_i)^{1-y} \\\\\n&= \\frac{e^{yWX_i}}{1+e^{WX_i}}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#setting",
    "href": "posts/DL/Logistic Regression.html#setting",
    "title": "Logistic Regression",
    "section": "setting",
    "text": "setting\n\nimport torch\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n#sig = lambda z:torch.exp(z)/(1+torch.exp(z))\nsig = torch.nn.Sigmoid()"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#data",
    "href": "posts/DL/Logistic Regression.html#data",
    "title": "Logistic Regression",
    "section": "data",
    "text": "data\n\ntorch.manual_seed(2022)\nn=400\n\n#1 ëª¨ìˆ˜ Wê°€ì •\nW = torch.tensor(\n    [[-0.8467],\n    [0.041]]).float()\n\n#2 ê°ê°ì˜ ê´€ì¸¡ì¹˜(ë°ì´í„°ìš”ì†Œ)ì—ì„œì˜ ëª¨ìˆ˜ p_iì‹œê°í™”(ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ ì‹œê°í™”)\n_x = torch.linspace(-150,150,n).reshape(-1,1)\n_one = torch.ones((n,1))\nX = torch.concat((_one,_x),axis=1)\np_i = sig(X@W)\ny = torch.bernoulli(p_i)\nplt.xlim([-150,150])\nplt.plot(X[:,1],y,\"bo\",alpha=0.5)\nplt.plot(X[:,1],p_i,\"r--\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y,$p_i$\")\nplt.title(\"realizations $y_1,\\dots,y_{300}$ from $Bern(p_1),\\dots,Bern(p_{300})$\")\nplt.legend([\"y\",\"p\"])\n\n<matplotlib.legend.Legend at 0x1f2684bfe20>"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#gradient-descent-1",
    "href": "posts/DL/Logistic Regression.html#gradient-descent-1",
    "title": "Logistic Regression",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nimport torch\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n\ntorch.manual_seed(2022)\nn=400\n\n#2 ì„ì˜ì˜ Wì— ëŒ€í•œ estimated value(ì¶”ì •ì¹˜) What ì´ˆê¸°í™”\nWhat = torch.tensor(\n    [[0.],\n    [-0.03]],requires_grad=True)\n\n_x = torch.linspace(-150,150,n).reshape(-1,1)\n_one = torch.ones((n,1))\nX = torch.concat((_one,_x),axis=1)\nyhat = sig(X@What)\n\nplt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\nplt.plot(X[:,1].data,p_i,\"r\")\nplt.plot(X[:,1].data,yhat.data,\"g\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"realizations $y_1,\\dots,y_{60}$ from $Bern(p_1),\\dots,Bern(p_{60})$\")\nplt.legend([\"y\",\"$p_i$\",\"$\\hat{y}(\\hat{p_i})$\"])\n\n<matplotlib.legend.Legend at 0x1f268e11e50>\n\n\n\n\n\n\nloss_fn = torch.nn.BCELoss()\n\"\"\"\ndef BCE_Loss(yhat,y):\n    return torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat))\n\"\"\"\n\n'\\ndef BCE_Loss(yhat,y):\\n    return torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat))\\n'\n\n\n\n#custom sigmoid + torch.BCELoss ì“°ë©´ ì˜¤ë¥˜ ë°œìƒ. 0ê³¼ 1ì‚¬ì´ì˜ ë²”ìœ„ ì•„ë‹˜\n#torch.nn.Sigmoid + custom BCE Loss ì¨ë„ ì˜¤ë¥˜ë°œìƒ => nan\nplt.subplots(2,5,figsize=(20,8))\nplt.subplots_adjust(hspace=0.3)\ni=1\n\nfor epoch in range(200):\n    #1 yhat \n    yhat = sig(X@What)\n    #2 loss\n    loss = loss_fn(yhat,y)\n    if epoch % 20 == 0:\n        plt.subplot(2,5,i)\n        #plt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\n        plt.plot(X[:,1].data,p_i,\"r\")\n        plt.plot(X[:,1].data,yhat.data,\"g\")\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        #plt.title(\"realizations $y_1,\\dots,y_{60}$ from $Bern(p_1),\\dots,Bern(p_{60})$\")\n        plt.legend([\"p\",\"yhat\"])\n        title = \"loss : {}\".format(round(loss.tolist(),5))\n        plt.title(title)\n        i+=1\n    #3 derivative\n    loss.backward()\n    #4 update & clean\n    What.data = What.data - 0.00005 * What.grad\n    What.grad = None\n\n\n\n\n\nround(loss.tolist(),4)\n\n0.3109\n\n\n\nplt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\nplt.plot(X[:,1].data,p_i,\"r\")\nplt.plot(X[:,1].data,yhat.data,\"g\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Logistic Regression\")\nplt.legend([\"y\",\"$p_i$\",\"$\\hat{y}(\\hat{p_i})$\"])\nplt.gca().axes.xaxis.set_visible(False)\nplt.gca().axes.yaxis.set_visible(False)"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#ë² ë¥´ëˆ„ì´-ë¶„í¬-ì „ê°œ",
    "href": "posts/DL/Logistic Regression.html#ë² ë¥´ëˆ„ì´-ë¶„í¬-ì „ê°œ",
    "title": "Logistic Regression",
    "section": "1.ë² ë¥´ëˆ„ì´ ë¶„í¬ ì „ê°œ",
    "text": "1.ë² ë¥´ëˆ„ì´ ë¶„í¬ ì „ê°œ\n\\[\\begin{aligned}\np_i^y(1-p_i)^{1-y} &= \\ (\\frac{1}{\\,1 + e^{-WX_i}\\,})^y\\,\\,(1-\\frac{1}{\\,1 + e^{-WX_i}\\,})^{1-y} \\\\\n&= (\\frac{1}{1+e^{-WX_i}})^{y}(\\frac{e^{-WX_i}}{1+e^{-WXi}})^{1-y} \\\\\n&= (\\frac{1}{1+e^{-WX_i}})^{y}(\\frac{1}{1+e^{WXi}})^{1-y} \\\\\n&= (\\frac{1+e^{WX_i}}{1+e^{-WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= (\\frac{e^{WX_i}+e^{2WX_i}}{1+e^{WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= (\\frac{e^{WX_i}(1+e^{WX_i})}{1+e^{WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= e^{yWX_i}\\frac{1}{1+e^{WX_i}} \\\\\n&= \\frac{e^{yWX_i}}{1+e^{WX_i}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#nllì „ê°œwith-parameter-w",
    "href": "posts/DL/Logistic Regression.html#nllì „ê°œwith-parameter-w",
    "title": "Logistic Regression",
    "section": "2.NLLì „ê°œ(with parameter \\(W\\))",
    "text": "2.NLLì „ê°œ(with parameter \\(W\\))\n\\[\\begin{aligned}\nLL &= \\text{ln}(\\underset{i=1}{\\overset{n}{\\large{\\prod}}}\\,\\frac{e^{y_iWX_i}}{1+e^{WX_i}}) \\\\\n&=\\overset{n}{\\underset{i=1}{\\large{\\sum}}}(\\text{ln}\\frac{e^{y_iWX_i}}{1+e^{WX_i}}) \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}\\,[\\text{ln}e^{y_iWX_i} - \\text{ln}(1+e^{WX_i})]\\, \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}\\,[y_iWX_i - \\text{ln}(1+e^{WX_i})], \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}y_iWX_i - \\overset{n}{\\underset{i=1}{\\large{\\sum}}}ln(1+e^{WX_i}) \\\\\n\n\nNLL &= -\\overset{n}{\\underset{i=1}{\\large{\\sum}}}y_iWX_i + \\overset{n}{\\underset{i=1}{\\large{\\sum}}}ln(1+e^{WX_i}) \\\\\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#nllì „ê°œcross-entropy-ìœ ë„í•˜ê¸°",
    "href": "posts/DL/Logistic Regression.html#nllì „ê°œcross-entropy-ìœ ë„í•˜ê¸°",
    "title": "Logistic Regression",
    "section": "3.NLLì „ê°œ(Cross Entropy ìœ ë„í•˜ê¸°)",
    "text": "3.NLLì „ê°œ(Cross Entropy ìœ ë„í•˜ê¸°)\nì„ì˜ì˜ ië²ˆì§¸ í•­ì—ì„œì˜ í™•ë¥ ë³€ìˆ˜ \\(Y_i\\)ê°€ ë”°ë¥´ëŠ” ë² ë¥´ëˆ„ì´ ë¶„í¬ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \n\\[\\begin{aligned}\nBern(y|X_i;p_i) = (p_i)^y(1-p_i)^{y-1}\n\\end{aligned}\\]\nëª¨ìˆ˜ê°€ \\(p_i\\)ì¸ ê°ê°ì˜ ë² ë¥´ëˆ„ì´ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜ \\(Y_1,Y_2\\dots Y_n\\)ìœ¼ë¡œë¶€ nê°œì˜ realization(sample) \\(y_1,y_2\\dots y_n\\)ì— ëŒ€í•œ NLLëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \n\\[\\begin{aligned}\nNLL &= -\\text{ln}\\prod_{i=1}^{n}p_i^{y_i}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}\\text{ln}p_i^{y_i}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}\\text{ln}p_i^{y_i} + \\text{ln}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}y_i\\text{ln}p_i + (1-y_i)\\text{ln}(1-p_i)\n\\end{aligned}\\]\nì°¸ê³ ë§í¬ 1. ë¡œì§€ìŠ¤í‹± íšŒê·€ ì „ê°œ 2. ìœ„í‚¤í”¼ë””ì•„ - ë¡œì§€ìŠ¤í‹± íšŒê·€ 3. ratsgoâ€™s blog"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "",
    "text": "[Deep Learning Series - Part3]\nì•ˆë…•í•˜ì„¸ìš”!!ğŸ˜€ ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ë‹¤í•­ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ ì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€ì— ëŒ€í•´ì„œ ì •ë¦¬í•´ë³´ê³ ì í•©ë‹ˆë‹¤. ê³µë¶€í•˜ë©´ì„œ ìƒê°ë³´ë‹¤ ëª¨ë¥´ëŠ” ë‚´ìš©ì´ ë§ì•„ì„œ ë‹¤ì‹œ ì²˜ìŒë¶€í„° ê³µë¶€í•˜ê³  ë³µìŠµí•´ì•¼ í•˜ëŠ” ë‚´ìš©ì´ ë§ì•˜ë„¤ìš”. ì¡ë‹´ì€ ê·¸ë§Œí•˜ê³  ì‹œì‘í•´ë³´ê² ìŠµë‹ˆë‹¤!! ì½ì–´ì£¼ì…”ì„œ ê°ì‚¬í•´ìš”ğŸ˜"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#ê°€ì •-1",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#ê°€ì •-1",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "ê°€ì • (1)",
    "text": "ê°€ì • (1)\në¡œì§€ìŠ¤í‹±íšŒê·€ë¥¼ ë³µê¸°í•´ë³´ë©´â€¦ ì¢…ì†ë³€ìˆ˜ \\(y_i\\)ëŠ” ë² ë¥´ëˆ„ì´ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜\\(Y_i\\)ë¡œë¶€í„° ìƒ˜í”Œë§ëœ ê°’ìœ¼ë¡œ ê°€ì •í–ˆìŠµë‹ˆë‹¤. ë˜í•œ ë² ë¥´ëˆ„ì´ë¶„í¬ì˜ ëª¨ìˆ˜\\(W\\)ëŠ” ì£¼ì–´ì§„ ì¡°ê±´ì¸ \\(X_i\\)ì™€ íšŒê·€ê³„ìˆ˜(ê°€ì¤‘ì¹˜)\\(W\\)ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ ê°€ì •í–ˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ëª¨ìˆ˜ë¥¼ ê°€ì •í•˜ë©´ì„œ ë² ë¥´ëˆ„ì´ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ë„ ìƒˆë¡œìš´ ëª¨ìˆ˜\\(W\\)ë¥¼ ê°€ì§€ê²Œë˜ì—ˆê³  Wë¥¼ ì ì ˆíˆ ì¶”ì •í•˜ë©´ ë°ì´í„°ê°€ 0ë˜ëŠ”1ì— ì†í•  í™•ë¥ ì„ ì•Œì•„ë‚´ê²Œ ë˜ì–´ í™•ë¥ ì´ ë” ë†’ì€ í´ë˜ìŠ¤ë¥¼ ì£¼ì–´ì§„ë°ì´í„°ì— ëŒ€í•œ í´ë˜ìŠ¤ë¡œ ì˜ˆì¸¡í–ˆì—ˆìŠµë‹ˆë‹¤.ë‹¤í•­ë¡œì§€ìŠ¤í‹±íšŒê·€ì™€ ì†Œí”„íŠ¸ë§¥ìŠ¤íšŒê·€ì—ì„œë„ ì´ëŸ¬í•œ ê³¼ì • ì¦‰,ë¶„í¬ë¥¼ ê°€ì •í•˜ê³  ë°ì´í„°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ëª¨ìˆ˜ë¥¼ ì¶”ì •í•˜ì—¬ í™•ë¥ ë¶„í¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì˜ˆì¸¡í•˜ëŠ” ë§¤ì»¤ë‹ˆì¦˜ì€ ê±°ì˜ ê·¸ëŒ€ë¡œì…ë‹ˆë‹¤.\në¨¼ì € ë‹¤í•­ë¡œì§€ìŠ¤í‹±íšŒê·€ì™€ ì†Œí”„íŠ¸ë§¥ìŠ¤íšŒê·€ì—ì„œ ì¢…ì†ë³€ìˆ˜ì— ëŒ€í•œ ê°€ì •ì„ í•´ë³´ê² ìŠµë‹ˆë‹¤. ë‹¤í•­ë¡œì§€ìŠ¤í‹± íšŒê·€ì™€ ì†Œí”„íŠ¸ë§¥ìŠ¤íšŒê·€ì—ì„œ ëª¨ë‘ ê°ê°ì˜ ê´€ì¸¡ì¹˜(each observation)ì—ì„œ ì¢…ì†ë³€ìˆ˜ì˜ realizationì¸ \\(y_i\\)ëŠ” í™•ë¥ ë³€ìˆ˜\\(Y_i\\)ë¡œë¶€í„° í‘œë³¸ì¶”ì¶œ(sampling)ë˜ì—ˆë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤. ì´ë•Œ ê°ê°ì˜ ê´€ì¸¡ì¹˜ì—ì„œì˜ í™•ë¥ ë³€ìˆ˜ \\(Y_i\\)ê°€ ë”°ë¥´ëŠ” ë¶„í¬ëŠ” ì„¤ëª…ë³€ìˆ˜ \\(x_{1,i},x_{2,i},\\dots,x_{M,i}\\)ê°€ ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§ˆ ë•Œ, ê°ê°ì˜ ë²”ì£¼(í´ë˜ìŠ¤)ì— ì†í•  í™•ë¥ ë“¤ì„ ëª¨ìˆ˜ë¡œ ê°€ì§€ëŠ” ì¹´í…Œê³ ë¦¬ë¶„í¬ë¥¼ ë”°ë¦…ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\nY_i|x_{1,i},x_{2,i},\\dots,x_{M,i} \\sim \\text{Cat}(y|x_{1,i},x_{2,i},\\dots,x_{M,i};\\mu_i)\n& =\n\\begin{cases}\n\\mu_{1,i} \\text{ if } y = (1,0,\\dots,0,0) \\\\\n\\mu_{2,i} \\text{ if } y = (0,1,\\dots,0,0) \\\\\n\\quad\\quad \\vdots \\\\\n\\mu_{K,i} \\text{ if } y = (0,0,\\dots,0,1) \\\\\n\\end{cases} \\\\\n&= \\mu_{1,i}^{y_1}\\mu_{2,i}^{y_2},\\dots,\\mu_{K,i}^{y_K} \\\\\n&= \\prod_{K=1}^{K}\\mu_{K,i}y_{K,i} \\\\\n\\end{aligned} \\\\\n&\n\\begin{aligned}\n&\\text{where, }\\\\\n&\\mu_i = {\\mu_{1,i},\\mu_{2,i},\\dots,\\mu_{K,i}} \\\\\n&\\mu_{1,i} = Pr(Y_i = (1,0,\\dots,0)|x_{1,i},\\dots,x_{M,i}) \\\\\n&\\mu_{2,i} = Pr(Y_i = (0,1,\\dots,0)|x_{1,i},\\dots,x_{M,i}) \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\vdots \\\\\n&\\mu_{K,i} = Pr(Y_i = (0,0,\\dots,0,1)|x_{1,i},\\dots,x_{M,i}) \\\\\n\\end{aligned}\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#ê°€ì •-2",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#ê°€ì •-2",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "ê°€ì • (2)",
    "text": "ê°€ì • (2)\nê°ê°ì˜ ê´€ì¸¡ì¹˜ì—ì„œ í™•ë¥ ë³€ìˆ˜\\(Y_i\\)ê°€ ë”°ë¥´ëŠ” ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜\\(\\mu_i\\)ëŠ” ë°ì´í„°í¬ì¸íŠ¸ë§ˆë‹¤ ë‹¤ë¥¸ ì„¤ëª…ë³€ìˆ˜(X_i)ì™€ ì‹œí–‰ë§ˆë‹¤ ë³€í•˜ì§€ ì•ŠëŠ” ê³ ì •ëœ íšŒê·€ê³„ìˆ˜(W)ì˜ ì¼ì°¨ê²°í•©ì„ í¬í•¨í•˜ëŠ” ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. ì£¼ì–´ì§„ Xê°’ì„ Wì™€ ì¼ì°¨ê²°í•©í•˜ì—¬ ì¶”ì •í•˜ê³ ì í•˜ëŠ” ê°’ì„ í‘œí˜„í•˜ëŠ” ì„ í˜•íšŒê·€ì˜ í•µì‹¬ì•„ì´ë””ì–´ì´ì ëŒ€ë¶€ë¶„ì˜ íšŒê·€ë¬¸ì œì—ì„œ ì‚¬ìš©í•˜ëŠ” ì¤‘ìš”í•œ ì•„ì´ë””ì–´ ì…ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mu_{k,i}  = \\mu_{k,i}(X_i;W_{k,i}) = \\mu_{k,i}(X_iW_{k,i}) =  Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i)\\\\\n&\\text{where},\\\\\n&w_{m,k} : \\text{$k$ë²ˆì§¸ ëª¨ìˆ˜ë¥¼ í‘œí˜„í•˜ê¸°ìœ„í•´ $m$ë²ˆì§¸ ê°’ê³¼ ê³±í•´ì§€ëŠ” ê°€ì¤‘ì¹˜} \\\\\n&x_{m,i} : \\text{i-th ê´€ì¸¡ì¹˜ì˜ $m$ë²ˆì§¸ ë…ë¦½ë³€ìˆ˜ì˜ ê°’} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-thê´€ì¸¡ì¹˜ì˜ feature vector(ë‹¨,$x_{0,i}$ = 1)} \\\\\n&W_k = [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : ì¹´í…Œê³ ë¦¬ ë¶„í¬ì˜ ì„ì˜ì˜ k-th ëª¨ìˆ˜$\\mu_k$ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ì•„ë†“ì€ ë²¡í„°} \\\\\n&\\mu_{k,i} = \\text{i-th ê´€ì¸¡ì¹˜ì˜ $k$ë²ˆì§¸ ëª¨ìˆ˜}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xwì˜-ì„ í˜•ì¡°í•©ì„-í¬í•¨í•œ-ëª¨ìˆ˜ì˜-í‘œí˜„-ìœ ë„í•˜ê¸°",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xwì˜-ì„ í˜•ì¡°í•©ì„-í¬í•¨í•œ-ëª¨ìˆ˜ì˜-í‘œí˜„-ìœ ë„í•˜ê¸°",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "X,Wì˜ ì„ í˜•ì¡°í•©ì„ í¬í•¨í•œ ëª¨ìˆ˜ì˜ í‘œí˜„ ìœ ë„í•˜ê¸°",
    "text": "X,Wì˜ ì„ í˜•ì¡°í•©ì„ í¬í•¨í•œ ëª¨ìˆ˜ì˜ í‘œí˜„ ìœ ë„í•˜ê¸°\nìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ëŒ€ë¶€ë¶„ì˜ íšŒê·€ì—ì„œ ëª¨ë¸ë§ì˜ í•µì‹¬ì•„ì´ë””ì–´ëŠ” ì¶”ì •í•˜ê³ ì í•˜ëŠ” ëŒ€ìƒì„ ì„¤ëª…ë³€ìˆ˜ì™€ ê°€ì¤‘ì¹˜ì˜ ì¼ì°¨ê²°í•©(ì„ í˜•ì¡°í•©)ì´ í¬í•¨ë˜ë„ë¡ í‘œí˜„í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ë‹¤í•­ë¡œì§€ìŠ¤í‹±íšŒê·€ë„ ì¶”ì •í•˜ê³ ì í•˜ëŠ” ëª¨ìˆ˜\\(\\mu_i = (\\mu_{1,i},\\mu_{2,i},\\dots,\\mu_{K,i})\\)ë¥¼ ê°ê°ì„ ì„¤ëª…ë³€ìˆ˜ì™€ ê°€ì¤‘ì¹˜ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ í‘œí˜„í•´ì•¼ í•©ë‹ˆë‹¤.ì´ì§„ë¡œì§€ìŠ¤í‹±íšŒê·€ì™€ì—ì„œë„ ì´ë ‡ê²Œ ëª¨ìˆ˜ë¥¼ í‘œí˜„í–ˆì—ˆëŠ”ë° ë‹¤í•­ë¡œì§€ìŠ¤í‹±íšŒê·€ì—ì„œëŠ” ì¼ì°¨ê²°í•©ìœ¼ë¡œ í‘œí˜„í•´ì•¼í•  ëª¨ìˆ˜ê°€ ì¢€ ë” ë§ìŠµë‹ˆë‹¤. -_-;;\nì°¨ê·¼ì°¨ê·¼ í•œë²ˆ ìœ ë„í•´ë³´ê² ìŠµë‹ˆë‹¤. ì¼ë‹¨ Kê°œì˜ ëª¨ìˆ˜ë¥¼ í‘œí˜„í•˜ëŠ” ì¼ì°¨ê²°í•©ì„ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤. ì´ëŸ¬í•œ ì¼ì°¨ê²°í•©ì—ì„œ xëŠ” ê´€ì¸¡ì¹˜ë§ˆë‹¤ ì¡´ì¬í•˜ëŠ” ì„¤ëª…ë³€ìˆ˜ì˜ ê°’ì— ë”°ë¼ì„œ íšŒê·€ê³„ìˆ˜(ê°€ì¤‘ì¹˜)ì¸ WëŠ” ê´€ì¸¡ì¹˜ì— ë”°ë¼ì„œ ë³€í•˜ì§€ ì•ŠëŠ” ì¼ì •í•œ ê°’ì…ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i-\\text{ln}Z \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\\n&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i {\\text{ (ì„ì˜ì˜ kë²ˆì§¸ í•­)}}\\\\  \n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n&where,\\\\\n&w_{m,k} : \\text{$k$ë²ˆì§¸ ëª¨ìˆ˜ë¥¼ í‘œí˜„í•˜ê¸°ìœ„í•´ $m$ë²ˆì§¸ ê°’ê³¼ ê³±í•´ì§€ëŠ” ê°€ì¤‘ì¹˜} \\\\\n&x_{m,i} : \\text{i-th ê´€ì¸¡ì¹˜ì˜ $m$ë²ˆì§¸ ë…ë¦½ë³€ìˆ˜ì˜ ê°’} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-thê´€ì¸¡ì¹˜ì˜ feature vector(ë‹¨,$x_{0,i}$ = 1)} \\\\\n&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : ì¹´í…Œê³ ë¦¬ ë¶„í¬ì˜ ì„ì˜ì˜ k-th ëª¨ìˆ˜$\\mu_k$ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ì•„ë†“ì€ ë²¡í„°} \\\\\n\\end{aligned}\\]\n í•œ ê°€ì§€ ìœ ì˜í•´ì•¼ í•  ì ì€ ë§ˆì§€ë§‰ ëª¨ìˆ˜ëŠ” ì¼ì°¨ê²°í•©ìœ¼ë¡œ í‘œí˜„í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì¹´í…Œê³ ë¦¬ë¶„í¬ì—ì„œ ëª¨ìˆ˜ì˜ ì´í•©ì€ 1ì´ê¸° ë•Œë¬¸ì— ë§ˆì§€ë§‰ \\(K\\)ë²ˆì§¸ ëª¨ìˆ˜ëŠ” 1ì—ì„œ ì „ë¶€ ë¹¼ë©´ ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\nê·¸ëŸ°ë° ì„£ë¶ˆë¦¬ ì¼ì°¨ê²°í•©ì„ ë§Œë“¤ë‹¤ë³´ë‹ˆ â€¦ ì¢Œë³€ì— ìˆëŠ” ëª¨ìˆ˜ëŠ” \\([0,1]\\)ì˜ ë²”ìœ„ì´ê³  ìš°ë³€ì€ \\([-\\infty,\\infty]\\)ì˜ ë²”ìœ„ì´ë¯€ë¡œ ê°€ì§€ë¯€ë¡œ ì–‘ë³€ì˜ ë²”ìœ„ê°€ ì „í˜€ ë§ì§€ ì•ŠìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì¢Œë³€ì„ Odds Ratio(ì—„ë°€íˆ Odds RatioëŠ” ì•„ë‹ˆì§€ë§Œ í†µì¼ì„±ì„ ìœ„í•´ Odds Ratioë¼ê³  í•˜ê² ìŠµë‹ˆë‹¤.) + Logit transformì„ ì·¨í•˜ì—¬ ì¢Œë³€ì´ ìš°ë³€ê³¼ ê°™ì€ ë²”ìœ„ë¥¼ ê°€ì§ˆ ìˆ˜ ìˆë„ë¡ í™•ì¥í•˜ì—¬ ì¤ë‹ˆë‹¤. (ë¡œê·¸ì•ˆì— ìˆëŠ” ë¶„ëª¨ê°€ Kë²ˆì§¸ í´ë˜ìŠ¤ì— ëŒ€í•œ í•­ì„ì„ ìœ ì˜í•©ë‹ˆë‹¤.) \\[\\text{ln}\\frac{\\mu_{k,i}}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = \\text{ln}\\frac{Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i;W_k)}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = W_k^TX_i\\]\nì›ë˜ì˜ ëª©ì ì€ ëª¨ìˆ˜ì— ëŒ€í•œ ì¼ì°¨ê²°í•©ì´ í¬í•¨ëœ í•­ì„ ì–»ëŠ” ê²ƒì´ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì •ë¦¬í•˜ì—¬ ëª¨ìˆ˜ì— ëŒ€í•œ í‘œí˜„ì„ ì–»ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n\\mu_{k,i} = Pr(Y_i = (0,\\dots,0,1_{k-th},0,\\dots,0|X_i;W_k) = Pr(Y_i = K|X_i)e^{X_iW_k}\n\\end{aligned}\\]\nì—¬ê¸°ê¹Œì§€ í•´ì„œ ëª¨ìˆ˜ì— ëŒ€í•œ í‘œí˜„ì„ ì–»ì—ˆìŠµë‹ˆë‹¤. ë‹¤ë§Œ \\(Y_i\\)ê°€ \\(K\\)ë²ˆì§¸ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥ ì€ ì¹´í…Œê³ ë¦¬ë¶„í¬ì—ì„œì˜ ëª¨ìˆ˜ì— ëŒ€í•œ ì œì•½ì¡°ê±´ì„ í™œìš©í•˜ì—¬ ë” ê°„ë‹¨í•˜ê²Œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&Pr(Y_i = K|X_i) = 1- \\sum_{k=1}^{K-1}Pr(Y_i = K|X_i)e^{X_iW_k} = 1-Pr(Y_i = K|X_i)\\sum_{k=1}^{K-1}e^{X_iW_k} \\\\\n&\\Longleftrightarrow Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{k=1}^{K-1}e^{X_iW_k}}\n\\end{aligned}\\]\në” ê°„ë‹¨í•˜ê²Œ í‘œí˜„ëœ í•­ìœ¼ë¡œ ë‹¤ì‹œ ì •ë¦¬í•˜ì—¬ ì“°ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mu_{k,i}=Pr(Y_i = k|X_i) = Pr(Y_i = K|X_i)e^{X_iW_k} = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}}\\\\\n&\\text{ì¸ë±ìŠ¤ ê²¹ì¹˜ë¯€ë¡œ ì‹œê·¸ë§ˆì˜ $k \\rightarrow j$}\n\\end{aligned}\\]\n ìµœì¢…ì ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë¶„í¬ì˜ ëª¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì „ê°œí•˜ëŠ” ê³¼ì •ì´ ë§ˆì§€ë§‰ \\(K\\)ë²ˆì§¸ í•­ì€ ì œì™¸í•œì±„ ì§„í–‰ë˜ì—ˆìœ¼ë¯€ë¡œ Kë²ˆì§¸ í•­ì—ëŒ€í•œ í™•ë¥ ì€ ë”°ë¡œ ì¨ì¤ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mu_{k,i}=Pr(Y_i = k|X_i) = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}} \\text{(ë‹¨, $k != K$)}\\\\\n&\\mu_{K,i}=Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{j=1}^{K-1}e^{X_iW_k}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#estimation",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#estimation",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "Estimation",
    "text": "Estimation\në” ê³µë¶€í•´ ì˜¤ê² ìŠµë‹ˆë‹¤ ^__^;;"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#ê°€ì •",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#ê°€ì •",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "ê°€ì •",
    "text": "ê°€ì •\nì†Œí”„íŠ¸ë§¥ìŠ¤íšŒê·€ì˜ ê°€ì •ì€ ë¡œì§€ìŠ¤í‹±íšŒê·€ì˜ ê°€ì •ê³¼ ìŠµë‹ˆë‹¤. ê° datapointì—ì„œì˜ ì¢…ì†ë³€ìˆ˜ì˜ ê°’ì€ ì¹´í…Œê³ ë¦¬ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜ì—ì„œ ìƒ˜í”Œë§ë˜ì—ˆìœ¼ë©° ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜ëŠ” ê° datapointë§ˆë‹¤ ë³€í•˜ëŠ” ì„¤ëª…ë³€ìˆ˜ì™€ íšŒê·€ê³„ìˆ˜(ê°€ì¤‘ì¹˜)ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xwì˜-ì„ í˜•ì¡°í•©ì„-í¬í•¨í•œ-ëª¨ìˆ˜ì˜-í‘œí˜„-ìœ ë„í•˜ê¸°-1",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xwì˜-ì„ í˜•ì¡°í•©ì„-í¬í•¨í•œ-ëª¨ìˆ˜ì˜-í‘œí˜„-ìœ ë„í•˜ê¸°-1",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "X,Wì˜ ì„ í˜•ì¡°í•©ì„ í¬í•¨í•œ ëª¨ìˆ˜ì˜ í‘œí˜„ ìœ ë„í•˜ê¸°",
    "text": "X,Wì˜ ì„ í˜•ì¡°í•©ì„ í¬í•¨í•œ ëª¨ìˆ˜ì˜ í‘œí˜„ ìœ ë„í•˜ê¸°\nì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€ë§ˆì°¬ê°€ì§€ë¡œ ì¶”ì •í•˜ê³ ì í•˜ëŠ” ëª¨ìˆ˜ë¥¼ ì„¤ëª…ë³€ìˆ˜ì™€ ê°€ì¤‘ì¹˜ì˜ ì¼ì°¨ê²°í•©ì´ í¬í•¨ëœ í•­ìœ¼ë¡œ í‘œí˜„í•©ë‹ˆë‹¤.\në¨¼ì € ì„¤ëª…ë³€ìˆ˜ì™€ ê°€ì¤‘ì¹˜ì˜ ì¼ì°¨ê²°í•©í˜•íƒœë¡œ ëª¨ìˆ˜ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì„ì˜ì˜ ië²ˆì§¸ ê´€ì¸¡ì¹˜ê°€ ê°ê°ì˜ ë²”ì£¼ì— \\((1,2,...,K)\\) ì†í•  í™•ë¥ ì„ ì˜ë¯¸í•˜ëŠ” ëª¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i-\\text{ln}Z \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\\n&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i \\\\  \n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n&where,\\\\\n&w_{m,k} : \\text{$k$ë²ˆì§¸ ëª¨ìˆ˜ë¥¼ í‘œí˜„í•˜ê¸°ìœ„í•´ $m$ë²ˆì§¸ ê°’ê³¼ ê³±í•´ì§€ëŠ” ê°€ì¤‘ì¹˜} \\\\\n&x_{m,i} : \\text{i-th ê´€ì¸¡ì¹˜ì˜ $m$ë²ˆì§¸ ë…ë¦½ë³€ìˆ˜ì˜ ê°’} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-thê´€ì¸¡ì¹˜ì˜ feature vector(ë‹¨,$x_{0,i}$ = 1)} \\\\\n&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : ì¹´í…Œê³ ë¦¬ ë¶„í¬ì˜ ì„ì˜ì˜ k-th ëª¨ìˆ˜$\\mu_k$ë¥¼ êµ¬í•˜ê¸° ìœ„í•œ ê°€ì¤‘ì¹˜ë¥¼ ëª¨ì•„ë†“ì€ ë²¡í„°} \\\\\n\\end{aligned}\\]\nì´ë ‡ê²Œ ë‚˜íƒ€ë‚´ê³  ë³´ë‹ˆ ì¢Œë³€ê³¼ 0~1ì‚¬ì´ì˜ ìˆ˜ë§Œ ê°–ì§€ë§Œ ìš°ë³€ì€ ì–´ë–¤ ìˆ˜ë˜ì§€ ë‚˜ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë²”ìœ„ë¥¼ ë§ì¶° ì£¼ê¸° ìœ„í•´ì„œ ì¢Œë³€ì— ë¡œê·¸ë¥¼ ì”Œì›Œ ë¡œê·¸í™•ë¥ ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤. ì¶”ê°€ì ìœ¼ë¡œ ìš°ë³€ì— \\(-lnZ\\)ë¼ëŠ” normalizating factorë¥¼ ë”í•´ì¤ë‹ˆë‹¤. ë‹¤ìŒê³¼ì •ì—ì„œ ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜ì˜ í•©ì´ 1ì´ë˜ë„ë¡ í•˜ëŠ” í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ì˜ íŠ¹ì§•ì„ ìœ ì§€í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\text{ln}\\mu_{1,i} = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\text{ln}\\mu_{2,i} = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\text{ln}\\mu_{k,i} = w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{M,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\text{ln}\\mu_{K-1,i} = w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i-\\text{ln}Z \\\\\n\\\\\n\\end{aligned}\\]\në”°ë¼ì„œ,ì„ì˜ì˜ \\(k\\)ë²ˆì§¸ ëª¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\[\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th}|X_i;W_k) = \\frac{1}{Z}e^{W_k^TX_i}\\]\nì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ì œì•½ì¡°ê±´ ì¦‰,ëª¨ìˆ˜ëŠ” ê°ê°ì˜ ë²”ì£¼ì— ì†í•  í™•ë¥ ì„ ë‚˜íƒ€ë‚´ë¯€ë¡œ ì´í•©ì´ 1ì„ì„ í™œìš©í•©ë‹ˆë‹¤. ì´ë¥¼ í™œìš©í•˜ì—¬ Zë¥¼ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\sum_{k=1}^{K}{\\mu_{k,i}} =\\sum_{k=1}^{K}{Pr(Y_i=k)}= \\frac{1}{Z}\\sum_{k=1}^{K}e^{W_k^TX_i} = 1\\\\\n&\\Longleftrightarrow Z = \\sum_{k=1}^{K}e^{W_k^TX_i}\n\\end{aligned}\\]\nìµœì¢…ì ìœ¼ë¡œ, ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. - ì¶”ì •í•˜ê³ ìí•˜ëŠ” ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜ëŠ” \\(\\mu_k\\)ëŠ” \\(W_k\\)ì™€ \\(X_i\\)ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ í‘œí˜„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì†Œí”„íŠ¸ë§¥ìŠ¤ í•¨ìˆ˜ì´ë¯€ë¡œ ì†Œí”„íŠ¸ë§¥ìŠ¤ íšŒê·€ë¼ëŠ” ì´ë¦„ì´ ë¶™ì—ˆìŠµë‹ˆë‹¤. \\[\\mu_{c,i}(X_i;W) = Pr(Y_i = (0,0,\\dots,1_{c-th},0,\\dots,0)|X_i;W_k) = \\frac{e^{W_c^TX_i}}{\\sum_{k=1}^{K}e^{W_k^TX_i}} = softmax(c,W_1^TX_i,W_2^TX_i,\\dots,W_K^TX_i)\\] - ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ìœ„ì—ì„œ êµ¬í•œ ëª¨ìˆ˜ë¡œ ë‹¤ì‹œ ì •ë¦¬í•˜ë©´ í™•ë¥ ì§ˆëŸ‰ í•¨ìˆ˜ëŠ” ìƒˆë¡œìš´ ëª¨ìˆ˜ \\(W_1,W_2,\\dots,W_K\\)ë¥¼ ê°€ì§‘ë‹ˆë‹¤.(ì¸ë±ìŠ¤ \\(k->j,c->k\\)) \\[Y_i \\sim Cat(y|X_i;W_1,W_2,\\dots,W_K) = \\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#mle",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#mle",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "MLE",
    "text": "MLE\nì—¬ê¸°ê¹Œì§€ì˜ ê³¼ì •ìœ¼ë¡œë¶€í„° ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜ëŠ” ì„¤ëª…ë³€ìˆ˜ì™€ ê°€ì¤‘ì¹˜(íšŒê·€ê³„ìˆ˜)ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ í‘œí˜„ë˜ë©° ë˜í•œ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ê°€ ìƒˆë¡œìš´ ëª¨ìˆ˜ \\(W = (W_1,W_2,\\dots,W_K)\\)ë¡œ í‘œí˜„ë˜ì—ˆìŠµë‹ˆë‹¤.ë§Œì•½ ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜ë§Œ ì¶”ì •í•  ìˆ˜ ìˆë‹¤ë©´ ìš°ë¦¬ëŠ” ë°ì´í„°í¬ì¸íŠ¸ê°€ ì–´ë–¤ ë²”ì£¼ì— ì†í•  í™•ë¥ ì´ ê°€ì¥ ë†’ì€ì§€ ì•Œ ìˆ˜ ìˆìœ¼ë©° ë²”ì£¼ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œëŠ” ì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ëª¨ìˆ˜\\(W\\)ë¥¼ MLEë¡œ ì¶”ì •í•©ë‹ˆë‹¤.\ní™•ë¥ ë¶„í¬ì—ì„œ ì„ì˜ì˜ ëª¨ìˆ˜\\(W = (W_1,W_2,\\dots,W_K)\\)ë¥¼ ê°€ì •í•  ë•Œ, í™•ë¥ ë³€ìˆ˜ \\(Y_1,Y_2,\\dots,Y_N\\)ìœ¼ë¡œë¶€í„° realizationì¸ \\(y_1,y_2,\\dots,y_N\\)ì´ ë‚˜ì˜¬ ê°€ëŠ¥ë„ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\nL({W};X_i|y_1,y_2,\\dots,y_n) &= Pr_{Y_1,Y_2,\\dots,Y_N}(y1,y2,\\dots,y_n|X_i;W)\\\\\n&= \\prod_{i=1}^{N}Pr_{Y_i}(Y_i=y_i|X_i;W) \\\\\n&= \\prod_{i=1}^{N}\\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\\\\\n\\end{aligned}\n\\\\\n&\\text{where } \\{W\\} = \\{W1,W2,\\dots,W_N\\}\n\\end{aligned}\\]\nìœ„ì™€ ê°™ì€ ê°€ëŠ¥ë„ë¥¼ ìµœì†Œí™” í•˜ëŠ” \\(W\\)ë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤.ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤\n\\[\\begin{aligned}\n\\overset{*}{\\{W\\}} = \\underset{\\{W\\}}{\\text{argmax}} \\prod_{i=1}^{N}\\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn êµ¬í˜„.html",
    "href": "posts/DL/Pytorch Rnn êµ¬í˜„.html",
    "title": "pytorchë¡œ Rnnêµ¬í˜„í•˜ê¸°",
    "section": "",
    "text": "hi?hi!ê°€ ë°˜ë³µë˜ëŠ” í…ìŠ¤íŠ¸ ë°ì´í„°ì—ì„œ ë‹¤ìŒ ë¬¸ìê°€ ë­ê°€ ë‚˜ì˜¬ì§€ ì˜ˆì¸¡í•˜ëŠ” RNNëª¨í˜• ë§Œë“¤ê¸°"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn êµ¬í˜„.html#vectorization",
    "href": "posts/DL/Pytorch Rnn êµ¬í˜„.html#vectorization",
    "title": "pytorchë¡œ Rnnêµ¬í˜„í•˜ê¸°",
    "section": "vectorization",
    "text": "vectorization\n\nì—¬ëŸ¬ê°€ì§€ ë°©ë²•ì´ ìˆìœ¼ë‚˜(tf-idf,dense vector,one-hot encoding ë“±ë“±â€¦) ì—¬ê¸°ì„œëŠ” ì›í•«ì¸ì½”ë”© ì‚¬ìš©\n\n\ndef mapping(txt,map_dict):\n    return [map_dict[chr]for chr in txt]\ntxt_mapped = mapping(txt,map_dict)\nprint(txt_mapped[:10])\n\ndef onehot_encoding(txt_mapped):\n    seq_encoded = torch.nn.functional.one_hot(torch.tensor(txt_mapped))\n    return seq_encoded.float()\nsequence_data_encoded = onehot_encoding(txt_mapped)\nprint(sequence_data_encoded[:10])\n\n[2, 3, 0, 2, 3, 1, 2, 3, 0, 2]\ntensor([[0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.]])\n\n\në°ì´í„° ì‚´ì§ ë³€í˜• í•˜ë‚˜ì˜ ê¸´ sequence dataë¥¼ RNNì˜ ì…ë ¥ìœ¼ë¡œ í•´ë„ ë˜ì§€ë§Œ ì²˜ë¦¬ì†ë„,ì„±ëŠ¥ì„ ê³ ë ¤í–ˆì„ ë•Œ ìê·¸ë§ˆí•œ sequencedataë¡œ ë¶„ë¦¬í•˜ì—¬ ì…ë ¥í•´ì£¼ëŠ”ê²Œ ë” ì¢‹ì€ ë°©ë²•ì„. ë¶„ë¦¬í•˜ëŠ” ë°©ë²•ë„ ì—¬ëŸ¬ê°€ì§€ê°€ ìˆì„ ìˆ˜ ìˆê² ëŠ”ë° ì—¬ê¸°ì„œëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë¶„ë¦¬í•¨ raw sequence data : hi?hi!hi?hi!hi?hi! â€¦â€¦â€¦.. sequence1 : (x,y) = (hi?,h) sequence2 : (x,y) = (i?h,i) sequence3 : (x,y) = (?hi,!) â€¦\n\ndef create_seqdataset(seq_data,seq_length):\n    #x = seq_data[:-1]\n    #y = seq_data[1:]\n    seqs_x = []\n    seqs_y = []\n    for idx in range(0,len(seq_data)-seq_length):\n        seqs_x.append(seq_data[idx:idx+seq_length])\n        seqs_y.append(seq_data[idx+seq_length])\n    return torch.stack(seqs_x),torch.stack(seqs_y)\n    #return seq_x,seq_y\n\nx_data,y_data = create_seqdataset(sequence_data_encoded,3)\nprint(x_data.shape,y_data.shape)\n\ntorch.Size([57, 3, 4]) torch.Size([57, 4])\n\n\n\nì™œ ì €ëŸ° shapeì„ ë§ì¶° ì£¼ëŠ”ê°€?\nì—¬ê¸°ì„œ ë‚˜ì˜¤ëŠ” x_data.shape = \\((57,3,4)\\)ê°€ ì‚´ì§ ë‚œí•´í•¨.  íŒŒì´í† ì¹˜ ê³µì‹ë¬¸ì„œì— ë”°ë¥´ë©´ batch_first = Trueë¡œ ì„¤ì •í•  ê²½ìš°,rnnê³„ì—´ì˜ ëª¨ë¸ì— ë„£ì–´ì¤˜ì•¼ í•˜ëŠ” í…ì„œì˜ shapeì€ \\((N,L,H_{in})\\) = (batch size,sequnce length,input_size)ì´ê³  dataloaderë¼ëŠ” ì¼ì¢…ì˜ ë°ì´í„° ì¤‘ê°„ê´€ë¦¬ì?ë¥¼ í•œ ë²ˆ ê±°ì³ì„œ ëª¨ë¸ì— ì…ë ¥ë¨. dataloaderì—ì„œ ë‚˜ì˜¤ëŠ” output.shape = \\((N,L,H_{in})\\)ì´ ë˜ê¸° ìœ„í•´ì„œëŠ” input.shape = \\((D,L,H_{in}\\)(DëŠ” ë¶„ë¦¬ëœ ì‹œí€€ìŠ¤ì˜ ê°¯ìˆ˜)ì´ì–´ì•¼ í•¨(ì¦‰ ì…ë ¥í…ì„œì˜ ì°¨ì›ì´ 3ê°œì—¬ì•¼ ì¶œë ¥í…ì„œì˜ ì°¨ì›ë„3ê°œì´ê³  ì°¨ì›ì´ ë‚˜ì˜¤ëŠ” ìˆœì„œë„ ì €ëŸ°ì‹ì´ ë˜ì–´ì•¼ í•¨). ë”°ë¼ì„œ ì €ë ‡ê²Œ ì„¤ì •í•¨.\n\n\níŒŒë¼ë¯¸í„° ì ê¹ ì„¤ëª…\nbatch sizeëŠ” ë°°ì¹˜ì˜ ì´ ê°¯ìˆ˜(ë°°ì¹˜ì•ˆì— ìˆëŠ” ì›ì†Œì˜ ê°¯ìˆ˜ ì•„ë‹˜!), sequnce lengthëŠ” ì‹œí€€ìŠ¤ë°ì´í„°ì˜ ê¸¸ì´ì´ì timestemp(ì‹œì )ì˜ ì´ ê°¯ìˆ˜(ê¸¸ì´), \\(H_{in}\\)ì€ each timestep(ê° ì‹œì )ë§ˆë‹¤ ì…ë ¥ë˜ëŠ” ë²¡í„°ì˜ ê¸¸ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŒ. ìœ„ì²˜ëŸ¼ ì›í•«ì¸ì½”ë”©ì„ í•œ ê²½ìš° \\(H_{in}\\)ì€ ì‹œí€€ìŠ¤ë°ì´í„°ì— ìˆëŠ” ë¬¸ìì˜ ê°¯ìˆ˜ë¡œ ê²°ì •ë˜ë¯€ë¡œ 4ì´ê³  Lì€ create_seqdatasetí•¨ìˆ˜ì—ì„œ ì¸ìˆ˜ë¡œ ë„£ì–´ì¤€ 3(sequnce_length)ì´ê³  ë§ˆì§€ë§‰ìœ¼ë¡œ N(batch_size)ì€ torch.utils.data.DataLoaderì•ˆì— ì¸ìˆ˜ë¡œ ë„£ì–´ì£¼ëŠ” batch_sizeë¡œ ì¸í•´ì„œ ì¼ì •í•œ ê°¯ìˆ˜ë¡œ ë°°ì¹˜ë¥¼ ë‚˜ëˆ„ì—ˆì„ë•Œ ë‚˜ì˜¤ëŠ” ë°°ì¹˜ë“¤ì˜ ì´ ìˆ«ìì„.rnn ë¬¸ì„œì—ì„œ ì„¤ëª…í•˜ëŠ” batch_sizeëŠ” torch.utils.dada.DataLoaderì—ì„œ ì„¤ì •í•œ batch_sizeì˜ ê°¯ìˆ˜ë§Œí¼ ë°ì´í„°ë¥¼ ëª¨ì•„ì„œ ì—¬ëŸ¬ê°œì˜ ë°°ì¹˜ë¡œ ë§Œë“¤ì—ˆì„ë•Œ ë‚˜ì˜¤ëŠ” ë°°ì¹˜ì˜ ì´ ê°¯ìˆ˜ë¼ê³  ë³´ë©´ë¨.(í—·ê°ˆë¦¬ëŠ” ë¶€ë¶„â€¦.)"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn êµ¬í˜„.html#í•™ìŠµ-ì¤€ë¹„í•˜ê¸°",
    "href": "posts/DL/Pytorch Rnn êµ¬í˜„.html#í•™ìŠµ-ì¤€ë¹„í•˜ê¸°",
    "title": "pytorchë¡œ Rnnêµ¬í˜„í•˜ê¸°",
    "section": "í•™ìŠµ ì¤€ë¹„í•˜ê¸°",
    "text": "í•™ìŠµ ì¤€ë¹„í•˜ê¸°\n\ndefine architecture,loss,optimizer\ndata check\n\n\n#architecture,loss,optimizer \ntorch.manual_seed(2022)\nrnn = torch.nn.RNN(4,20,batch_first = True)\nlinr = torch.nn.Linear(20,4)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=1e-3)\n\n\nds = torch.utils.data.TensorDataset(x_data,y_data)\ndl = torch.utils.data.DataLoader(ds,batch_size=8,drop_last=True)\n\nfor idx,(x,y) in enumerate(dl):\n    if idx ==5:\n        break\n    print(x.shape,y.shape)\n\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\n\n\nìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ë°ì´í„°ë¡œë”ë¥¼ ê±°ì³ì„œ ë‚˜ì˜¤ëŠ” í…ì„œëŠ” RNNì— ë°”ë¡œ ì…ë ¥ë  ê²ƒì„. input.shape = \\((N,L,H_{in}) = (8,3,4)\\)"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn êµ¬í˜„.html#ëª¨í˜•í•™ìŠµ",
    "href": "posts/DL/Pytorch Rnn êµ¬í˜„.html#ëª¨í˜•í•™ìŠµ",
    "title": "pytorchë¡œ Rnnêµ¬í˜„í•˜ê¸°",
    "section": "ëª¨í˜•í•™ìŠµ",
    "text": "ëª¨í˜•í•™ìŠµ\n\nfor epoch in range(0,101):\n    for tr_x,tr_y in dl:\n        #1 output\n        hidden,hT = rnn(tr_x)\n        #print(hidden.shape)\n        output = linr(hT[-1])\n        #2 loss\n        loss = loss_fn(output,tr_y)\n        #3 derivative\n        loss.backward()\n        #4 update & clean\n        optimizer.step()\n        optimizer.zero_grad()\n    if epoch % 10 == 0:\n        print(f'epoch : {epoch},loss : {round(loss.tolist(),5)}')\n\nepoch : 0,loss : 1.31779\nepoch : 10,loss : 0.69453\nepoch : 20,loss : 0.19338\nepoch : 30,loss : 0.05891\nepoch : 40,loss : 0.02861\nepoch : 50,loss : 0.01791\nepoch : 60,loss : 0.0126\nepoch : 70,loss : 0.00947\nepoch : 80,loss : 0.00744\nepoch : 90,loss : 0.00602\nepoch : 100,loss : 0.00499\n\n\npytorchì˜ rnnì„ ê±°ì³ì„œ ë‚˜ì˜¤ëŠ” outputì€ ë‘ ê°€ì§€ì„. - hidden : ê°€ì¥ ê¹Šì´ ìœ„ì¹˜í•œ íˆë“ ë ˆì´ì–´ì˜ ê°ê°ì˜ ì‹œì ì—ì„œì˜ ì¶œë ¥ê°’ì„ ëª¨ì•„ë†“ì€ í…ì„œ - hT : ëª¨ë“  íˆë“ ë ˆì´ì–´ì—ì˜ ë§ˆì§€ë§‰ ì‹œì (ì‹œì T)ì—ì„œì˜ ì¶œë ¥ê°’ì„ ëª¨ì•„ë†“ì€ í…ì„œ - ì™¸ìš°ê¸°! ìœ„ì¹˜ : ê°€ì¥ê¹Šì€ <=> ëª¨ë“  , ì‹œì  : ê°ê°ì˜ <=> ë§ˆì§€ë§‰\nìœ„ì™€ê°™ì€ ì„¤ì •ì—ì„œëŠ” ê°€ì¥ ê¹Šì´ ìœ„ì¹˜í•œ íˆë“ ë ˆì´ì–´ì˜ ë§ˆì§€ë§‰ì‹œì ì—ì„œì˜ ì¶œë ¥ê°’ë§Œì´ ìš°ë¦¬ëŠ” ë‹¤ìŒì—ì˜¬ ë¬¸ìì—´ì„ ì˜ˆì¸¡í•  ë•Œ í•„ìš”í•˜ë¯€ë¡œ hT[-1]ì„ í•˜ì—¬ ê·¸ ê°’ì„ ê°€ì ¸ì˜´."
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory.html",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory.html",
    "title": "[PRML ì½ê¸°] 1 - í™•ë¥ ë¡  ê°œìš”",
    "section": "",
    "text": "Figure1.9\n\n\në¹¨ê°•ìƒ‰,íŒŒë‘ìƒ‰ ìƒì ì¤‘ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ê³  ì„ íƒí•œ ìƒìì•ˆì—ì„œ ì‚¬ê³¼(ì´ˆë¡) ë˜ëŠ” ì˜¤ë Œì§€(ì£¼í™©)ë¥¼ êº¼ë‚¸ë‹¤ê³  í•©ì‹œë‹¤. ì—¬ëŸ¬ë²ˆ ë°˜ë³µí–ˆì„ ë•Œ, ë¹¨ê°•ìƒ‰ ìƒìì™€ íŒŒë‘ìƒ‰ ìƒìê°€ ì„ íƒëœ ë¹„ìœ¨ì´ ê°ê° 40%,60%ë¼ê³  ì•Œë ¤ì ¸ ìˆëŠ” ìƒíƒœì…ë‹ˆë‹¤. ë¯¸ë˜ì— ì„ íƒëœ ìƒìë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ë¥¼ Bë¼ê³  í•˜ë©´ ì‹¤ì œë¡œ ìƒìë¥¼ ì„ íƒí•˜ê¸° ì „ê¹Œì§€ëŠ” ê°ê°ì˜ ìƒìë¥¼ ë½‘ì„ í™•ë¥ (ê°€ëŠ¥ì„±)ë§Œì´ ì¡´ì¬í•˜ë¯€ë¡œ ë³€ìˆ˜BëŠ” í™•ë¥ ë³€ìˆ˜ ì…ë‹ˆë‹¤. ì´ í™•ë¥ ë³€ìˆ˜ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ì€ r ë˜ëŠ” bë¡œ ë‘ ê°€ì§€ ì…ë‹ˆë‹¤. ë§ˆì°¬ê°€ì§€ë¡œ ë¯¸ë˜ì— ì„ íƒëœ ê³¼ì¼ì„ ë‚˜íƒ€ë‚´ëŠ” í™•ë¥ ë³€ìˆ˜ Fë¥¼ ë†“ì„ ìˆ˜ ìˆê³  Fê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ì„ a ë˜ëŠ” oë¡œ ë†“ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në¹ˆë„ì£¼ì˜ ê´€ì ì—ì„œ ì–´ë– í•œ ì‚¬ê±´ì´ ë°œìƒí•  í™•ë¥ ì€ ë§¤ìš° ì—¬ëŸ¬ë²ˆ ì‹œí–‰ì„ ë°˜ë³µí–ˆì„ë•Œ, ì–´ë–¤ ì‚¬ê±´ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ì˜ ë¹„ìœ¨(fraction,ratio)ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì£¼ì‚¬ìœ„ ëˆˆì´ 3ì´ë‚˜ì˜¬ í™•ë¥ ì´ 50%ë¼ê³  í•˜ë©´ 100ë²ˆ ë˜ì¡Œì„ë•Œ 50ë²ˆì •ë„ëŠ” 3ì´ ë‚˜ì˜¤ëŠ” ê²ƒìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ì˜ ë¬¸ì œì—ì„œ ì—¬ëŸ¬ë²ˆ ë°˜ë³µí–ˆì„ë•Œ ë¹¨ê°•ìƒ‰ ë˜ëŠ” íŒŒë‘ìƒ‰ìƒìì¸ ì‚¬ê±´ì´ ë°œìƒí•˜ëŠ” ê²½ìš°ê°€ ê°ê° ì „ì²´ì—ì„œ 40%,60%ì˜€ë‹¤ê³  í–ˆìœ¼ë¯€ë¡œ ì´ëŠ” í™•ë¥ ì…ë‹ˆë‹¤. ë˜í•œ í™•ë¥ ë³€ìˆ˜ Bê°€ rì„ ì·¨í•˜ëŠ” ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ê³¼ í™•ë¥ ë³€ìˆ˜ Bê°€ bë¥¼ ì·¨í•˜ëŠ” ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ì´ë¼ê³  ë§í•˜ë©° ë‹¤ìŒê³¼ ê°™ì´ ì ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\np(B = r) = 0.4 \\\\\np(B = b) = 0.6\n\\end{aligned}\\]\nê°ê°ì˜ ìƒìë¥¼ ì„ íƒí•˜ëŠ” ì‚¬ê±´ì˜ í™•ë¥ ì€ í™•ë¥ ì˜ ì •ì˜ì— ì˜í•´ì„œ [0,1]ì‚¬ì´ì˜ êµ¬ê°„ì—ë§Œ ì¡´ì¬í•©ë‹ˆë‹¤. ë˜í•œ ê°ê°ì˜ ìƒìë¥¼ ì„ íƒí•˜ëŠ” ì‚¬ê±´ì€ ìƒí˜¸ë² íƒ€ì ì´ë©´ì„œ ì‹œí–‰ìœ¼ë¡œë¶€í„° ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë“  ê²°ê³¼ë“¤ ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ í™•ë¥ ì˜ í•©ì€ 1ì…ë‹ˆë‹¤.\nëª‡ ê°€ì§€ ê¶ê¸ˆí•œ ì ì´ ìƒê²¼ìŠµë‹ˆë‹¤. â€œì‚¬ê³¼(ì´ˆë¡)ì´ë‚˜ ì˜¤ë Œì§€(ì£¼í™©)ê°€ ë‚˜ì˜¬ í™•ë¥ ì€?â€ ë˜ëŠ” â€œì‚¬ê³¼ë¥¼ ë½‘ì•˜ì„ ë•Œ ì–´ë–¤ ìƒìë¥¼ ì„ íƒí•  ê°€ëŠ¥ì„±ì´ ë†’ì€ì§€?â€ì— ëŒ€í•´ì„œ ê¶ê¸ˆí•©ë‹ˆë‹¤. ì´ëŠ” sum ruleê³¼ product ruleì„ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤.\n\n\n\n\n\n\nFigure1.9\n\n\ní™•ë¥ ë³€ìˆ˜ \\(X,Y\\)ê°€ ì¡´ì¬í•˜ê³  \\(X\\)ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ì€ \\(x_i(i=1,2,\\dots,M)\\) \\(Y\\)ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ì€ \\(y_j(j=1,2,\\dots,L)\\)ë¼ê³  í•©ì‹œë‹¤. ì´ \\(N\\)ë²ˆì„ ì‹œí–‰í–ˆë‹¤ê³  í•  ë•Œ,ì‹œí–‰ì˜ ê²°ê³¼ ì¤‘ \\(X = x_i\\)ì´ë©´ì„œ ë™ì‹œì— \\(Y=y_i\\)ì¸ ê²½ìš°ëŠ” \\(n_{ij}\\)ë²ˆ ë‚˜ì™”ìœ¼ë©° \\(X = x_i\\)ì¸ ê²½ìš°ëŠ” \\(c_i\\)ë²ˆ ë‚˜ì™”ê³  í™•ë¥ ë³€ìˆ˜ \\(Y = y_j\\)ì¸ ê²½ìš°ëŠ” \\(r_j\\)ë²ˆì´ ë‚˜ì™”ìŠµë‹ˆë‹¤.\ní™•ë¥ ë³€ìˆ˜ \\(X = x_i\\)ì´ê³  \\(Y=y_j\\)ì¸ ì‚¬ê±´ì´ ë™ì‹œì— ë°œìƒí•  í™•ë¥ ì„ \\(X=x_i,Y=y_j\\)ì¼ ë•Œì˜ ê²°í•©í™•ë¥ (joint probability)ë¼ê³  í•©ë‹ˆë‹¤.\n\\[p(X = x_i,Y = y_j)\\]\nê²°í•©í™•ë¥ ì€ ë§¤ìš° ì—¬ëŸ¬ë²ˆ ì‹œí–‰í–ˆì„ ë•Œ, \\(X=x_i,Y=y_j\\)ì¸ ì‚¬ê±´ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ì˜ \\(n_{ij}\\) ë¹„ìœ¨ì…ë‹ˆë‹¤.\n\\[N \\rightarrow \\infty,\\,\\, p(X=x_i,Y=y_j) = \\frac{n_{ij}}{N}\\]\nì‹œí–‰ìœ¼ë¡œë¶€í„° í™•ë¥ ë³€ìˆ˜ \\(X = x_i\\)ì¸ ì‚¬ê±´ì´ ëª‡ ë²ˆ ë‚˜ì™”ëŠ”ì§€ ì•Œê¸°ìœ„í•´ì„œëŠ” \\(c_i\\)ëŠ” \\(X = x_i,Y = y_j(\\text{for } j=1,2,\\dots,L)\\)ì¸ ì‚¬ê±´ì´ ë°œìƒí•˜ëŠ” ëª¨ë“  ê²½ìš°ë¥¼ ì „ë¶€ ë‹¤ ë”í•´ì•¼ í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ì„œ ì‚¬ê³¼ë¥¼ ì„ íƒí•˜ëŠ” \\(F = a\\)ì´ ê²½ìš°ëŠ” ì‚¬ê³¼ë¥¼ ì„ íƒí•˜ê³  ìƒìê°€ íŒŒë‘ìƒ‰ ìƒìì¸ \\(F=a,B=r\\) ì‚¬ê±´ì´ ë°œìƒí•œ ê²½ìš°ì™€ ì‚¬ê³¼ë¥¼ ì„ íƒí•˜ê³  ìƒìê°€ ë¹¨ê°•ìƒ‰ ìƒìì¸\\(F=a,B=b\\) ì‚¬ê±´ì´ ë°œìƒí•œ ê²½ìš°ì´ë¯€ë¡œ ì‹œí–‰ìœ¼ë¡œë¶€í„° ë‘ ê°€ì§€ ì¼€ì´ìŠ¤ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  ê²½ìš°ë¥¼ ëª¨ë‘ ì„¸ì–´ì•¼ í•©ë‹ˆë‹¤.\n\\[c_i = \\sum_{j=1}^{L}n_{ij}\\]\nê²°ê³¼ì ìœ¼ë¡œ ,\\(X = x_i\\)ì¸ ì‚¬ê±´ì˜ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\np(X = x_i) &= \\frac{c_{i}}{N} \\\\\n&=\\frac{\\sum_{j=1}^{L}n_{ij}}{N}\\\\\n&= p(X=x_i,Y=y_1) + p(X=x_i,Y=y_2) + \\dots + p(X=x_i,Y=y_L) \\\\\n&= \\sum_{j=1}^{L}p(X=x_i,Y=y_j) \\\\\n\\end{aligned}\\]\nìœ„ì™€ ê°™ì´ í•˜ë‚˜ì˜ í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•œ í™•ë¥ ì„ êµ¬í•  ë•Œ, ë‹¤ë¥¸ í™•ë¥ ë³€ìˆ˜ì™€ì˜ ëª¨ë“  ê²°í•©í™•ë¥ ì„ ë”í•˜ì—¬ êµ¬í•˜ëŠ” ë²•ì¹™ì„ sum rule of probabilityë¼ê³  í•©ë‹ˆë‹¤. ì´ë•Œ ë‹¤ë¥¸ í™•ë¥ ë³€ìˆ˜ì™€ ê²°í•©í™•ë¥ ì„ marginalizing ë˜ëŠ” summing outí•˜ì—¬ êµ¬í•˜ë¯€ë¡œ marginal probabilityë¼ê³  í•©ë‹ˆë‹¤.\nì‹œí–‰ì˜ ê²°ê³¼ê°€ \\(X=x_i\\)ì¸ íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ê²½ìš°ì— ëŒ€í•´ì„œë§Œ ê³ ë ¤í•´ë³¸ë‹¤ê³  í•©ì‹œë‹¤. ì¡°ê±´ì— ë§ëŠ” ê²½ìš°ì•ˆì—ì„œ \\(Y = y_j\\)ì¸ ì‚¬ê±´ì´ ë‚˜ì˜¤ëŠ” íšŸìˆ˜ì˜ ë¹„ëŠ” \\(p(Y=y_j|X=x_i)\\)ë¼ê³  í‘œê¸°í•˜ë©° ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[p(Y=y_j|X=x_i) = \\frac{n_{ij}}{c_j}\\]\nì´ë¥¼ \\(X=x_i\\)ë¡œ ì£¼ì–´ì¡Œì„ ë•Œ, \\(Y=y_j\\)ì¸ ì‚¬ê±´ì— ëŒ€í•œ ì¡°ê±´ë¶€í™•ë¥ ì´ë¼ê³  í•©ë‹ˆë‹¤. ì¡°ê±´ë¶€ í™•ë¥ ì˜ ê²½ìš° ê¸°í˜¸\\(|\\) ë‹¤ìŒì— ì¡°ê±´ì´ ì˜¤ë©° ë¶„ëª¨ì—ëŠ” ì¡°ê±´ì— í•´ë‹¹í•˜ëŠ” ì‚¬ê±´ì´ ë‚˜ì˜¤ëŠ” ê²½ìš°ê°€ ëª‡ ë²ˆì¸ì§€ ê·¸ íšŸìˆ˜ì— ê°’ì´ ì˜¤ì§€ë§Œ ì¡°ê±´ë¶€í™•ë¥ ì´ ì•„ë‹Œ ê·¸ëƒ¥ í™•ë¥ ì˜ ê²½ìš° ë¶„ëª¨ëŠ” ëª‡ ë²ˆ ì‹œí–‰í–ˆëŠ”ì§€ ì…ë‹ˆë‹¤. ì´ëŸ¬í•œ ì´ìœ ëŠ” ì¡°ê±´ë¶€í™•ë¥ ì€ ì¼ë°˜ì ì¸ í™•ë¥ ê³¼ ë‹¤ë¥´ê²Œ ì–´ë–¤ íŠ¹ì •í•œ ì¡°ê±´ì•ˆì—ì„œ ë‹¤ë¥¸ì‚¬ê±´ì´ ë‚˜ì˜¤ëŠ” ë¹„ìœ¨ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\nìœ„ì—ì„œ ì •ì˜í•œ ì¡°ê±´ë¶€í™•ë¥ ë¡œ ê²°í•©í™•ë¥ ì„ ë‹¤ì‹œ ì ì–´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[p(X=x_i,Y=y_j) = \\frac{n_{ij}}{N} = \\frac{n_{ij}}{c_i}\\frac{c_i}{N} = p(Y=y_j|X=x_i)p(X = x_i)\\]\nì¦‰ ê²°í•©í™•ë¥ ì€ \\(X=x_i\\)ì¸ ì‚¬ê±´ì´ ë°œìƒí•œ í™•ë¥ ê³¼ ë°œìƒí•œ ì‚¬ê±´ì„ ì¡°ê±´\\(X=x_i\\)ìœ¼ë¡œí•˜ê³  \\(Y=y_j\\)ê°€ ë°œìƒí•  í™•ë¥ ì˜ ê³±ê³¼ ê°™ìŠµë‹ˆë‹¤. ì´ëŠ” ì–´ëŠì •ë„ ì§ê´€ê³¼ ì¼ì¹˜í•œë‹¤ê³  ë³¼ ìˆ˜ ìˆëŠ”ë° ì˜ˆë¥¼ ë“¤ìë©´ ë¹¨ê°•ìƒ‰ìƒìì—ì„œ ì‚¬ê³¼ë¥¼ ë½‘ì„ ê°€ëŠ¥ì„±ì€ ë¨¼ì € ë¹¨ê°•ìƒ‰ìƒìë¥¼ ê³ ë¥´ê³  ê·¸ ë‹¤ìŒ ì‚¬ê³¼ë¥¼ ë½‘ì„ ê°€ëŠ¥ì„±ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\n\n\n\nìœ„ì—ì„œ ì ì€ ê²°í•©í™•ë¥ ë¡œë¶€í„° ë‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ì–»ì–´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)}\\]\nì´ë¥¼ ë² ì´ì¦ˆì •ë¦¬ ë¼ê³  í•©ë‹ˆë‹¤. ë² ì´ì¦ˆ ì •ë¦¬ì—ì„œ \\(p(Y|X)\\)ëŠ” posterior probability(ì‚¬í›„í™•ë¥ )ë¡œ ì–´ë–¤ ì¡°ê±´ ë˜ëŠ” ì¦ê±°ê°€ ë°œê²¬ë˜ì—ˆì„ë•Œì˜ í™•ë¥ ì…ë‹ˆë‹¤. \\(p(Y)\\)ëŠ” prior probabilityë¡œ ì–´ë–¤ ì¦ê±° ë˜ëŠ” ì¡°ê±´ì´ ë°œê²¬ë˜ê¸°ì „ì˜ í™•ë¥ ì…ë‹ˆë‹¤. ë² ì´ì¦ˆ ì •ë¦¬ë¡œë¶€í„° ìš°ë¦¬ëŠ” posteriorì™€ priorì˜ ê´€ê³„ ì¦‰,ì¡°ê±´ ë˜ëŠ” ì¦ê±°ê°€ ë°œê²¬ë˜ê¸° ì „,í›„ì˜ í™•ë¥ ì‚¬ì´ì˜ ìˆ˜ì‹ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì‚¬ì „í™•ë¥ ì„ ì•Œê³  ìˆë‹¤ë©´ ê·¸ í™•ë¥ ì„ í†µí•˜ì—¬ ì‚¬í›„í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)} = \\frac{p(X|Y)p(Y)}{\\sum_{Y}p(X,Y)} = \\frac{p(X|Y)p(Y)}{\\sum_{Y}p(X|Y)p(Y)}\\]\në¶„ëª¨ë¥¼ sum ruleê³¼ product ruleì— ì˜í•˜ì—¬ ë” ì „ê°œí•˜ë©´ ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤. ì‚¬í›„í™•ë¥  \\(p(Y|X)\\)ëŠ” \\(X\\)ë¼ëŠ” ì¦ê±°,ì¡°ê±´ì´ ì£¼ì–´ì§ˆ ë•Œ ê²°ê³¼ \\(Y\\)ì— ëŒ€í•œ í™•ë¥ ì´ì—ˆìŠµë‹ˆë‹¤. ìœ„ì˜ ìˆ˜ì‹ì„ ê³°ê³°íˆ ë³´ë©´ â€¦ \\(X\\)ê°€ ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§ˆë•Œ ê²°ê³¼\\(Y\\)ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì„ êµ¬í•˜ê¸° ìœ„í•˜ì—¬ \\(Y\\)ê°€ ì£¼ì–´ì§ˆë•Œì˜ \\(X\\)ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ë¡œ ê³„ì‚°í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë² ì´ì¦ˆ ì •ë¦¬ì—ì„œ í•µì‹¬ì€ ì–´ë–¤ ì¡°ê±´ì´ ì£¼ì–´ì§€ê³  ê²°ê³¼ì— ëŒ€í•œ í™•ë¥ ì„ êµ¬í•  ë•Œ, ê²°ê³¼ë¥¼ ì¡°ê±´ìœ¼ë¡œ ì¡°ê±´ì„ ê²°ê³¼ë¡œ ì—­ìœ¼ë¡œ ë°”ê¾¼ í™•ë¥ ì„ ì‚¬ìš©í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ì¦‰ \\(X\\)ê°€ ì¡°ê±´ì¼ ë•Œ, \\(Y\\)ì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ì´ ì˜ êµ¬í•´ì§€ì§€ ì•ŠëŠ”ë‹¤ë©´ ì´ë¥¼ ë’¤ì§‘ì–´ì„œ \\(Y\\)ê°€ ì¡°ê±´ì´ê³  \\(X\\)ê°€ ê²°ê³¼ì¼ë•Œì˜ í™•ë¥ ì„ ì´ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në² ì´ì¦ˆ ì •ë¦¬ì—ì„œ ë¶„ëª¨ëŠ” normalization ìƒìˆ˜ë¡œ ëª¨ë“  \\(Y\\)ì— ëŒ€í•˜ì—¬ í™•ë¥ ì˜ í•©ì´ 1ì´ ë˜ë„ë¡ í•©ë‹ˆë‹¤.\nì˜ˆì‹œë¡œ ëŒì•„ê°€ì„œ ì˜¤ë Œì§€ ë˜ëŠ” ì‚¬ê³¼ê°€ ë‚˜ì˜¬ í™•ë¥ ê³¼ ì‚¬ê³¼ë¥¼ ê³¨ëì„ ë•Œ ì–´ë–¤ ìƒìë¥¼ ê³¨ëì„ í™•ë¥ ì´ ë†’ì€ì§€ë¥¼ ê³„ì‚°í•´ë´…ì‹œë‹¤. ê°ê°ì˜ ê²½ìš°ì— ëŒ€í•´ í™•ë¥ ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{align}\n&p(B = r) = 0.4 \\\\\n&p(B = b) = 0.6 \\\\\n&p(F = a | B = r) = \\frac{1}{4}\\\\\n&p(F = o | B = r) = \\frac{3}{4}\\\\\n&p(F = a | B = b) = \\frac{3}{4}\\\\\n&p(F = o | B = b) = \\frac{1}{4}\\\\\n\\end{align}\\]\n(3)(4),(5)(6) ê°ê°ì˜ í•©ì€ normalization constantë¡œ ì¸í•´ í•©ì´ 1ì´ ë˜ëŠ”ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ì–´ì„œ ì›ë˜ ê¶ê¸ˆí–ˆë˜ ì²«ë²ˆì§¸ ë¬¸ì œì¸ ì‚¬ê³¼ ë˜ëŠ” ì˜¤ë Œì§€ê°€ ë‚˜ì˜¬ í™•ë¥ ì„ Product ruleë¡œ ê³„ì‚°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\np(F = a) &= p(F=a,B=r) + p(F=a,B=b) \\\\\n&=p(F=a|B=r)p(B=r) + p(F=a|B=b)p(B=b) \\\\\n&=\\frac{1}{4}\\times\\frac{4}{10} + \\frac{3}{4}\\times\\frac{6}{10}\\\\\n&= \\frac{11}{20}\\\\\n\\end{aligned}\n\\\\\n&p(F=b) = 1-\\frac{11}{20}= \\frac{9}{20}\n\\end{aligned}\\]\në˜ë‹¤ë¥¸ ë¬¸ì œì¸ ì˜¤ë Œì§€ ë˜ëŠ” ì‚¬ê³¼ë¥¼ ë½‘ì•˜ì„ë•Œ ì–´ë–¤ ë°•ìŠ¤ë¥¼ ì„ íƒí–ˆëŠ”ì§€ ì•Œê³  ì‹¶ìŠµë‹ˆë‹¤. ì¦‰ ì•Œê³ ì‹¶ì€ í™•ë¥ ì€ \\(p(B|F=a)\\) ë˜ëŠ” \\(p(B|F=o)\\)ì…ë‹ˆë‹¤. ê·¸ëŸ°ë° ìš°ë¦¬ì—ê²Œ ì£¼ì–´ì§„ í™•ë¥ ë“¤ì€ ì‚¬ì „í™•ë¥ ì¸ \\(p(F)\\)ì™€ ì¡°ê±´ê³¼ ê²°ê³¼ê°€ ì—­ìœ¼ë¡œ ë’¤ì§‘íŒ í™•ë¥ ì¸ \\(p(F|B)\\)ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ,Bayes Ruleì„ ì‚¬ìš©í•˜ì—¬ ì›í•˜ëŠ” í™•ë¥ ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\\begin{align}\n&p(B|F) = \\frac{p(F|B)p(F)}{p(B)} \\\\\n&p(B=r|F=o) = \\frac{p(F=o|B=r)p(B=r)}{p(F=o)} = \\frac{2}{3}\\\\\n&\\leftrightarrow p(B=b|F=o) = 1-\\frac{2}{3} = \\frac{1}{3}\n\\end{align}\\]\nì˜¤ë Œì§€ë¥¼ í™•ì¸í•˜ê¸°ì „ê¹Œì§€ëŠ” ë¹¨ê°•ìƒ‰ ë°•ìŠ¤ì¼ í™•ë¥ ì´ ì ˆë°˜ì´ ì•ˆë˜ëŠ” 0.4 ì˜€ëŠ”ë° ì˜¤ë Œì§€ë¥¼ í™•ì¸í•˜ê³ ëŠ” \\(\\frac{2}{3}\\)ë¡œ í™•ë¥ ì´ ìƒìŠ¹í–ˆìŠµë‹ˆë‹¤. ì´ëŠ” ì£¼ì–´ì§€ëŠ” ì •ë³´,ì£¼ê±´ì´ í™•ë¥ ì— í° ì˜í–¥ì„ ë¯¸ì¹˜ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë˜í•œ êµ¬í•œ í™•ë¥ ì´ ì§ê´€ì ìœ¼ë¡œ ê·¸ë¦¼ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆëŠ” ì‚¬ì‹¤ê³¼ë„ ì¼ì¹˜í•¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\në‘ í™•ë¥ ë³€ìˆ˜ê°€ ë…ë¦½ì´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\text{X and Y are independent random variables} \\iff p(X,Y) = p(X)p(Y)\\]\nì˜ˆì‹œì—ì„œ í™•ì¸í•´ë´…ì‹œë‹¤.\n\\[\\begin{aligned}\n&p(B=r|F=o) = \\frac{2}{3} ,p(B=r) = \\frac{4}{10}\\\\\n&p(B=r|F=o) \\not = p(B=r) \\Longleftrightarrow \\text{X and Y are dependent}\n\\end{aligned}\\]\në§Œì•½ ë‘ ë°•ìŠ¤ì•ˆì— ë“¤ì–´ìˆëŠ” ì˜¤ëœì§€ì™€ ì‚¬ê³¼ê°€ ê°™ì€ ë¹„ìœ¨ë¡œ ë“¤ì–´ìˆë‹¤ê³  í•œë‹¤ë©´â€¦\n\\[\\begin{aligned}\np(F=o | B=r) = p(F = o) \\\\\np(F=a | B=r) = p(F = a)  \\\\\np(F=o | B=b) = p(F = o)  \\\\\np(F=a | B=b) = p(F = a)\n\\end{aligned}\\]\në”°ë¼ì„œ ìƒìì•ˆì— ìˆëŠ” ê³¼ì¼ì˜ ê°¯ìˆ˜ê°€ ê°™ì„ ê²½ìš°, ë‘ í™•ë¥ ë³€ìˆ˜ëŠ” ë…ë¦½ì…ë‹ˆë‹¤.\n\n\n\n\në¹ˆë„ì£¼ì˜ ê´€ì ì—ì„œ í™•ë¥ ì€ ë§¤ìš°ì—¬ëŸ¬ë²ˆ ì‹œí–‰í–ˆì„ ë•Œ, ì–´ë–¤ ì‚¬ê±´ì´ ë°œìƒí•˜ëŠ”(ë‚˜ì˜¤ëŠ”) ë¹„ìœ¨(ratio)ì…ë‹ˆë‹¤.\nSum ruleì€ ì—¬ëŸ¬í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•œ ê²°í•©í™•ë¥ ì´ ì£¼ì–´ì§ˆ ë•Œ, íŠ¹ì •í•œ í•˜ë‚˜ì˜ ë³€ìˆ˜ì— ëŒ€í•œ í™•ë¥ (marginal probability)ì„ êµ¬í•¨\n\n\\[p(X) = \\sum_Yp(X,Y)\\]\n\nProduct ruleì€ ì¡°ê±´ë¶€í™•ë¥ ê³¼ ì£¼ë³€í™•ë¥ ì‚¬ì´ì˜ ê³±ìœ¼ë¡œ ê²°í•©í™•ë¥ ì„ êµ¬í•¨\n\n\\[p(X,Y) = p(Y|X)p(X) = p(X|Y)p(Y)\\]\n\në² ì´ì¦ˆì •ë¦¬ëŠ” posterior(ì‚¬í›„í™•ë¥ )ë¥¼ prior(ì‚¬ì „í™•ë¥ )ê³¼ ì¡°ê±´ê³¼ ê²°ê³¼ê°€ ë°”ë€Œì—ˆì„ë•Œì˜ í™•ë¥ ì„ í†µí•´ì„œ êµ¬í•©ë‹ˆë‹¤. ë˜í•œ ì‚¬ì „í™•ë¥ ê³¼ ì‚¬í›„í™•ë¥ ì‚¬ì´ì˜ ê´€ê³„(ìˆ˜ì‹)ì…ë‹ˆë‹¤.\n\n\\[p(X|Y) = \\frac{p(Y|X)p(Y)}{p(X)} = \\frac{p(Y|X)p(Y)}{\\sum_Yp(X,Y)p(Y)}\\]\n\në‘ í™•ë¥ ë³€ìˆ˜ê°€ ë…ë¦½ì¼ ê²½ìš° , ê²°í•©í™•ë¥ (ë¶„í¬)ëŠ” ê°ê°ì˜ ë³€ìˆ˜ì— ëŒ€í•œ (ì£¼ë³€)í™•ë¥ ì˜ ê³±ì…ë‹ˆë‹¤\n\n\\[\\text{X and Y are independent random variables} \\iff p(X,Y) = p(X)p(Y)\\]\n\n\n\n\n\ní™•ë¥ ë³€ìˆ˜ê°€ ë…ë¦½ì´ë¼ë©´ \\(Y\\)ì˜ ì¡°ê±´ë¶€í™•ë¥ ì€ ì¡°ê±´\\(X\\)ê°€ ì–´ë–¤ ê°’ì´ë˜ê°„ì— ì „í˜€ ì˜í–¥ì´ ì—†ìŠµë‹ˆë‹¤(ë…ë¦½ì ì…ë‹ˆë‹¤).\n\\[\\begin{aligned}\n&p(Y|X) = p(Y) \\\\\n&\\leftrightarrow p(Y|X) = \\frac{p(X,Y)}{p(X)} = p(Y)\\\\\n&\\leftrightarrow p(X,Y) = p(X)p(Y)\n\\end{aligned}\\]\në°˜ëŒ€ë¡œ í•´ë„ ì„±ë¦½í•©ë‹ˆë‹¤.\n\n\n\n\nê°€ì¥ ì‰½ê²Œ ì´í•´í•˜ëŠ” ë² ì´ì¦ˆ ì •ë¦¬(Bayesâ€™ Law)\nIndependent Random Variables\nPRML 1.2-probability theory"
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html",
    "title": "[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)",
    "section": "",
    "text": "PRMLì„ ì½ê³  ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#probability-density-function",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#probability-density-function",
    "title": "[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)",
    "section": "Probability density function",
    "text": "Probability density function\ní™•ë¥ ë°€ë„í•¨ìˆ˜ëŠ” ì—°ì†í™•ë¥ ë³€ìˆ˜ê°€ ë¯¸ì†Œêµ¬ê°„ì•ˆì— ì†í•˜ëŠ” ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ì„ ë¯¸ì†Œêµ¬ê°„ì˜ ê¸¸ì´ë¡œ ë‚˜ëˆˆ í™•ë¥ ë°€ë„ê°’ì„ í•¨ìˆ«ê°’ìœ¼ë¡œ ê°€ì§€ëŠ” í™•ë¥ í•¨ìˆ˜ë¡œ ì •ì˜í•©ë‹ˆë‹¤.\n\\[p(x) \\overset{\\Delta}{=} \\lim_{\\Delta x \\rightarrow 0}\\frac{p(x<X\\leq x+\\Delta x))}{\\Delta x}\\]\ní™•ë¥ ë°€ë„í•¨ìˆ˜ë¥¼ ì •ì ë¶„í•˜ë©´ í™•ë¥ ë³€ìˆ˜ê°€ ì„ì˜ì˜ êµ¬ê°„ì•ˆì— ì†í•˜ëŠ” ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ì¦ëª…)\n\\[\\begin{aligned}\nP(a < X \\leq b) = \\int_{a}^{b}p(u)d(u)\n\\end{aligned}\\]\ní™•ë¥ ë°€ë„í•¨ìˆ˜ëŠ” ë‹¤ìŒì˜ ë‘ ê°€ì§€ ì¡°ê±´ì„ ë§Œì¡±í•´ì•¼ í•©ë‹ˆë‹¤. ì²«ë²ˆì§¸ ì‹ì€ í™•ë¥ (ë°€ë„)ëŠ” ë°˜ë“œì‹œ 0ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë‘ë²ˆì§¸ ì‹ì—ì„œ í™•ë¥ ë³€ìˆ˜ëŠ” ë°˜ë“œì‹œ \\((-\\infty,\\infty]\\)ì¸ êµ¬ê°„ì•ˆì— ì†í•¨ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n\\[\\begin{align}\np(x) \\geq 0 \\\\\n\\int_{-\\infty}^{\\infty}f(t)dt = 1\n\\end{align}\\]"
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#probabiltiy-variable-transform",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#probabiltiy-variable-transform",
    "title": "[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)",
    "section": "Probabiltiy variable transform",
    "text": "Probabiltiy variable transform\nì´ ë¶€ë¶„ì˜ ë‚´ìš©ì€ PRMLì— ìˆëŠ” ë‚´ìš©ì„ ê°ìƒ‰í•œ ë¶€ë¶„ì…ë‹ˆë‹¤. í‹€ë¦°ë¶€ë¶„ì´ ìˆë‹¤ë©´ ì•Œë ¤ì£¼ì„¸ìš”!!\nì—°ì†í™•ë¥ ë³€ìˆ˜ \\(X\\)ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ë¥¼ \\(p_X(x)\\)ë¼ í•  ë•Œ, ë³€ìˆ˜ë¥¼ ë³€í™˜í•˜ì—¬ \\(X\\)ë¥¼ \\(Y\\)ì— ê´€í•œ ì‹\\(X = g(Y)\\)ë¡œ í‘œí˜„í–ˆë‹¤ê³  í•´ë´…ì‹œë‹¤. ëª©ì ì€ í™•ë¥ ë³€ìˆ˜ \\(Y\\)ì˜ í™•ë¥ ë°€ë„í•¨ìˆ˜ \\(p_Y(y)\\)ë¥¼ ì–»ëŠ” ê²ƒì…ë‹ˆë‹¤. \\(\\Delta x\\rightarrow 0 \\Delta y \\rightarrow 0\\)ì´ë¼ê³  í•œë‹¤ë©´ ë‹¤ìŒì´ ì„±ë¦½í•©ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\lim_{\\Delta x \\rightarrow 0}\\frac{p(x < X \\leq x + \\Delta x)}{\\Delta x} \\times \\Delta x \\overset{\\sim}{=} \\lim_{\\Delta y \\rightarrow 0}\\frac{p(y < Y \\leq y + \\Delta y)}{\\Delta y} \\times \\Delta y \\\\\n&\\Longleftrightarrow p_X(x)dx \\overset{\\sim}{=} p_Y(y)dy\n\\end{aligned}\\]\nìœ—ì‹ì€ Jacobian factorì— ì˜í•´ ë“±ì‹ìœ¼ë¡œ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\np_Y(y) &= p_X(x) \\begin {vmatrix} \\frac{dx}{dy} \\end {vmatrix} \\\\\n&= p_X(g(y))|g^{'}(y)|\n\\end{aligned}\\]\ní™•ë¥ ë³€ìˆ˜ì˜ ë³€í™˜ì€ í™•ë¥ ë¶„í¬í•¨ìˆ˜ë¥¼ ìµœëŒ€í™” í•˜ëŠ” ë¬¸ì œì—ì„œ ìœ ìš©í•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. ë³€í™˜í•  ë³€ìˆ˜ë¥¼ ì„ íƒí•˜ë©´ ìµœëŒ€í™”í•´ì•¼í•˜ëŠ” í™•ë¥ í•¨ìˆ˜ë¥¼ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#sum-rule-product-rule-of-continuous-variable",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#sum-rule-product-rule-of-continuous-variable",
    "title": "[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)",
    "section": "Sum rule & Product rule of continuous variable",
    "text": "Sum rule & Product rule of continuous variable\nì´ì‚°í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•´ì„œëŠ” Sum ruleê³¼ Product ruleì„ ì‚´í´ë´¤ì—ˆì§€ë§Œ ì—°ì†í™•ë¥ ë³€ìˆ˜ ëŒ€í•´ì„œëŠ” ë³´ì§€ ì•Šì•˜ì—ˆìŠµë‹ˆë‹¤. ì—°ì†í™•ë¥ ë³€ìˆ˜ì˜ ê²½ìš° ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì—„ë°€í•œ ì¦ëª…ì€ measure theroyë¡œ ì¦ëª…í•´ì•¼ í•˜ë¯€ë¡œ .. ìƒëµí•˜ê² ìŠµë‹ˆë‹¤.(ê°„ëµí•œ ì¦ëª…)\n\\[\\begin{aligned}\n&p(x) = \\int_y f(x,y)dy \\\\\n&p(x,y) = p(y|x)p(x)\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#expectations-and-variances",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#expectations-and-variances",
    "title": "[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)",
    "section": "Expectations and Variances",
    "text": "Expectations and Variances\ní•¨ìˆ˜ì˜ ê¸°ëŒ“ê°’(ë˜ëŠ” í‰ê· )ì€ í•¨ìˆ«ê°’ì´ ì–´ë–¤ ê°’ì„ ì¤‘ì‹¬ìœ¼ë¡œ ë¶„í¬í•˜ëŠ”ì§€ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤. ê°€ëŠ¥í•œ ëª¨ë“ \\(x\\)ì— ëŒ€í•˜ì—¬ í•¨ìˆ«ê°’ê³¼ ê·¸ë•Œì˜ í™•ë¥ ë¶„í¬ì˜ ê°’ì„ ê³±í•˜ì—¬ ì–»ì€ ê°€ì¤‘í‰ê· ì…ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mathbb{E}[f] = \\sum_x p(x)f(x) \\quad \\text{If X is a discrete R.V} \\\\\n&\\mathbb{E}[f] = \\int_x p(x)f(x)dx \\quad \\text{If X is a continuous R.V}\n\\end{aligned}\\]\ní‘œë³¸ì˜ í¬ê¸°ê°€ ë¬´í•œí•  ê²½ìš°, í‘œë³¸ìœ¼ë¡œ ë¶€í„° êµ¬í•œ í•¨ìˆ«ê°’ì˜ í‰ê· ê³¼ ê¸°ëŒ“ê°’ì€ ê°’ì´ ê°™ìŠµë‹ˆë‹¤. ì´ë¥¼ í†µí•´ì„œ í™•ë¥ ë¶„í¬ì˜ ê¸°ëŒ“ê°’ì„ ì•Œ ìˆ˜ ìˆë‹¤ë©´ í‘œë³¸ì´ ì ë‹¹íˆ í¬ê¸°ê°€ í´ ê²½ìš° í•¨ìˆ«ê°’ì´ ì–´ëŠì •ë„ ì¼ì§€ ëŒ€ëµì ìœ¼ë¡œ ì˜ˆì¸¡í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[\\mathbb{E}[f] = \\lim_{N \\rightarrow \\infty}\\frac{1}{N}\\sum_{n=1}^{N}f(x_n)\\]\në‹¤ë³€ìˆ˜í•¨ìˆ˜ëŠ” ì—¬ëŸ¬ê°œì˜ ë³€ìˆ˜ë¥¼ ê°€ì§€ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ê°ê°ì˜ ë³€ìˆ˜ê°€ ë”°ë¥´ëŠ” í™•ë¥ ë¶„í¬ì¤‘ì—ì„œ í•˜ë‚˜ë¥¼ ì„ íƒí•˜ì—¬ ê·¸ë•Œì˜ í™•ë¥ ë¶„í¬ì™€ í•¨ìˆ«ê°’ì˜ ê¸°ëŒ“ê°’ì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ ê¸°ëŒ“ê°’ì€ ë‚˜ë¨¸ì§€ í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•œ í•¨ìˆ˜ê°€ ë©ë‹ˆë‹¤.\n\\[\\mathbb{E}_x[f(x,y)] = f(y)\\]\ní•¨ìˆ˜ì˜ ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì€ ì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬ì™€ì˜ ê°€ì¤‘í‰ê· ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \\(y\\)ê°€ ì¡°ê±´ìœ¼ë¡œ ì£¼ì–´ì§ˆ ë•Œ, \\(x\\)ì˜ ì¡°ê±´ë¶€ ê¸°ëŒ“ê°’ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\mathbb{E}_x[f|y] = \\sum_x{p(x|y)}{f(x)}\\]\ní™•ë¥ ë³€ìˆ˜ \\(f(x)\\)ì˜ ë¶„ì‚°(variance)ëŠ” í•¨ìˆ˜ê°€ ê¸°ëŒ“ê°’ì„ ì¤‘ì‹¬ìœ¼ë¡œ ì–¼ë§ˆë‚˜ í¼ì ¸ìˆëŠ”ì§€ ì•Œë ¤ì¤ë‹ˆë‹¤. í¸ì°¨ì œê³±ì˜ ê¸°ëŒ“ê°’(í‰ê· )ìœ¼ë¡œ ì •ì˜í•©ë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\mathbb{E}[x] = \\int_xxp(x)dx \\text{ or } \\sum_xxp(x)\\\\\n&\n\\begin{aligned}\n\\text{var}[x]\n&= \\mathbb{E}[(x - \\mathbb{E}[x])^2] \\\\\n&= \\mathbb{E}[x^2] - \\mathbb{E}[x]^2\\\\\n\\end{aligned}\n\\end{aligned}\\]\në‘ ê°œì˜ í™•ë¥ ë³€ìˆ˜ì— ëŒ€í•´ì„œ ê³µë¶„ì‚°ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.(2ë²ˆì§¸ ì‹ì— ëŒ€í•œ ì „ê°œ)\n\\[\\begin{align}\n\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}] \\\\\n&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}[x]\\mathbb{E}[y]\n\\end{align}\\]"
  },
  {
    "objectID": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#appendix",
    "href": "posts/DL/[PRML ì •ë…í•˜ê¸°] Probability Theory2.html#appendix",
    "title": "[PRML ì½ê¸°] 2 - í™•ë¥ ë¡  ê°œìš”(ì‘ì„±ì¤‘ â€¦)",
    "section": "Appendix",
    "text": "Appendix\n\ní™•ë¥ ë°€ë„í•¨ìˆ˜ì— ê´€í•œ ì—¬ëŸ¬ê°€ì§€ ì¦ëª…\nëˆ„ì ë¶„í¬í•¨ìˆ˜ëŠ” ì—°ì†í™•ë¥ ë³€ìˆ˜ê°€ \\((-\\infty,x]\\)ì¸ êµ¬ê°„ì•ˆì— ì†í•  í™•ë¥ ì…ë‹ˆë‹¤.\n\\[F(x) = P(-\\infty<X\\leq x)\\]\në”°ë¼ì„œ,ì—°ì†í™•ë¥ ë¶„í¬ì˜ ë¶„ìë¥¼ ëˆ„ì ë¶„í¬í•¨ìˆ˜ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ëˆ„ì ë¶„í¬í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ê°€ í™•ë¥ ë°€ë„í•¨ìˆ˜ì´ë©° ëˆ„ì ë¶„í¬í•¨ìˆ˜ì˜ ê¸°ìš¸ê¸°,ë³€í™”ìœ¨ì´ í™•ë¥ ë°€ë„í•¨ìˆ˜ì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n\\[p(x) = \\lim_{\\Delta x \\rightarrow 0}\\frac{p(x<X\\leq x+\\Delta x))}{\\Delta x} = \\lim_{\\Delta x \\rightarrow 0}\\frac{F(x+\\Delta x) - F(x)}{\\Delta x} = \\frac{dF}{dx}\\]\nëˆ„ì ë¶„í¬í•¨ìˆ˜ì˜ ë„í•¨ìˆ˜ê°€ í™•ë¥ ë°€ë„í•¨ìˆ˜ì´ë¯€ë¡œ í™•ë¥ ë°€ë„í•¨ìˆ˜ì˜ ì ë¶„ì€ ëˆ„ì ë¶„í¬í•¨ìˆ˜ì…ë‹ˆë‹¤.\n\\[\\int_{-\\infty}^{x}f(t)dt = F(x) = P(-\\infty<X\\leq x)\\]\nì„ì˜ì˜ êµ¬ê°„ \\((a,b]\\)ì‚¬ì´ì— í™•ë¥ ë³€ìˆ˜ \\(X\\)ê°€ ì†í•˜ëŠ” ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\nP(a < X \\leq b) &= P(-\\infty < X \\leq b) - P(-\\infty < X \\leq a) \\\\\n&= F(b) - F(a) \\\\\n&= \\int_{-\\infty}^{b}f(t)dt - \\int_{-\\infty}^{a}f(t)dt \\\\\n&= \\int_{a}^{b}f(t)dt\n\\end{aligned}\\]\n\n\nê³µë¶„ì‚° ì „ê°œí•˜ê¸°\n\\[\\begin{aligned}\n\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}] \\\\\n&=\\mathbb{E}_{x,y}[xy - x\\mathbb{E}[y] - y\\mathbb{E}[x] + \\mathbb{E}[x]\\mathbb{E}[y]]\\\\\n&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}_{x,y}[x\\mathbb{E}[y]] - \\mathbb{E}_{x,y}[y\\mathbb{E}[x]] + \\mathbb{E}[x]\\mathbb{E}[y]]\\\\\n\\end{aligned}\\]\nì—¬ê¸°ì„œ \\(\\mathbb{E}_{x,y}[x\\mathbb{E}[y]]\\)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\int_{\\infty}^{\\infty}\\int_{\\infty}^{\\infty}x\\mathbb{E}[y]p(y)p(x)dydx &=  \\int_{\\infty}^{\\infty}x\\mathbb{E}[y]p(x)\\bigg(\\int_{\\infty}^{\\infty}p(y)dy\\bigg)dx \\\\\n&= \\int_{\\infty}^{\\infty}x\\mathbb{E}[y]p(x)dx \\\\\n&= \\mathbb{E}[y]\\int_{\\infty}^{\\infty}xp(x)dx \\\\\n&= \\mathbb{E}[y]\\mathbb{E}[x]\n\\end{aligned}\\]\në§ˆì°¬ê°€ì§€ë¡œ \\(\\mathbb{E}_{x,y}[y\\mathbb{E}[x]]\\)ë„ ê°™ì€ ê°’ì„ ê°€ì§„ë‹¤. ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[xy] - \\mathbb{E}_{x,y}[x\\mathbb{E}[y]] - \\mathbb{E}_{x,y}[y\\mathbb{E}[x]] + \\mathbb{E}[x]\\mathbb{E}[y]]\\\\\n&= \\mathbb{E}_{x,y}[xy] - \\mathbb{E}[y]\\mathbb{E}[x] - \\mathbb{E}[x]\\mathbb{E}[y] + \\mathbb{E}[x]\\mathbb{E}[y] \\\\\n&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}[x]\\mathbb{E}[y]\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/jujurery/ì‹œí—˜ë³´ê¸° 1ì£¼ì¼ ì „ ì²´í¬í•´ì•¼ í•  ê²ƒ.html",
    "href": "posts/jujurery/ì‹œí—˜ë³´ê¸° 1ì£¼ì¼ ì „ ì²´í¬í•´ì•¼ í•  ê²ƒ.html",
    "title": "[ì£¼ì €ë¦¬ì£¼ì €ë¦¬] ì‹œí—˜ë³´ê¸° 1ì£¼ì¼ ì „ ê¼­ ì½ì–´ì•¼ í•˜ëŠ” ê²ƒ",
    "section": "",
    "text": "ì´ ê¸€ì€ í™•ë¥ ê³¼ì •ë¡  ì‹œí—˜ì„ ë³´ê³  ì¶©ê²©ë°›ì•„ì„œ ë‹¤ì‹œëŠ” ì´ëŸ° ì‹¤ìˆ˜ë¥¼ ë°˜ë³µí•˜ì§€ ì•Šê¸°ìœ„í•´ ì“°ëŠ” ê¸€ì´ë‹¤. ì–¸ì  ê°€ ë˜ ì´ ê¸€ì„ ë³¼ ë¯¸ë˜ì˜ ë‚˜ì—ê²Œ ë§í•´ì£¼ìë©´ ë§Œì•½ ì´ëŸ¬í•œ ê²ƒë“¤ ì¤‘ í•˜ë‚˜ë¼ë„ ë”± í•˜ë‚˜ë¼ë„ ì§€ê¸ˆ ë„ˆê°€ í•˜ê³  ìˆë‹¤ë©´ ê·¸ ì‹œí—˜(ì‹œí—˜,ë°œí‘œ ë“±ë“± ê³ ë‚œ)ì€ ë§¤ìš° ìœ„í—˜í•˜ë©° ê²°ì½” ì„±ê³µí•˜ì§€ ëª»í•  ê²ƒì´ë‹¤. ê·¸ ìˆœê°„ ë„Œ ì´ë¯¸ ìì‹ ì—ê²Œ ì§€ë‚˜ì¹˜ê²Œ ê´€ëŒ€í•œ ì¸ê°„ì´ë©° ì—´ì‹¬íˆ í•˜ëŠ” ì²™ ì—°ê¸°í•˜ëŠ” ì‚¬ëŒì´ë‹¤.\n\nì ˆëŒ€ë¡œ ë‹¤ ì•ˆë‹¤ê³  ìƒê°í•˜ì§€ ë§ê¸°(ìë§Œ,ê°€ì¥ ìœ„í—˜í•œ ìƒê°)\n\nì•„ë§ˆ ê³µë¶€ë¥¼ ì¢€ í•´ë´¤ì„ ìˆ˜ë„ ìˆë‹¤.\nê·¸ë˜ì„œ â€œì´ ì •ë„ë©´ ì—´ì‹¬íˆ í–ˆë‹¤.â€,â€œì´ ì •ë„ë©´ ë‹¤ ì•Œì§€â€,â€œê·¸ê±´ ì¢€ ì‰½ì–ì•„â€ ë¼ëŠ” ìƒê°ì´ ë“¤ ìˆ˜ ìˆë‹¤.\nê°€ì¥ ìœ„í—˜í•œ ìƒê°ì´ë‹¤.\nì´ëŸ° ìƒê°ì€ ë‚´ ë§ˆìŒì†ì— ë“¤ì–´ê°€ìˆëŠ” ìˆœê°„ë¶€í„° ì•ìœ¼ë¡œ ë§¤ ìˆœê°„ ìµœì„ ì„ ë‹¤ í•˜ì§€ ëª»í•˜ê²Œ ë§Œë“¤ê¸° ë•Œë¬¸ì´ë‹¤.\nì œì¼ ë²„ë ¤ì•¼ í•  ìƒê°ì´ë©° ê°€ì¥ ì“¸ëª¨ì—†ë‹¤.\nëŒ€ì¶©,ì–´ëŠì •ë„ë§Œ í•˜ê²Œ ë§Œë“¤ë©° ê³„ì†í•´ì„œ ìŒ“ì´ë‹¤ê°€ ê²°êµ­ ë‚˜ì¤‘ì— í° ë¬¸ì œê°€ ë˜ì–´ í„°ì§€ê³  ë§Œë‹¤.\në˜í•œ ê°€ì¥ ê°€ì†Œë¡œìš´ ìƒê°ì´ê¸°ë„ í•˜ë‹¤.\nâ€œê·¸ê±´ ì¢€ ì‰½ì–ì•„?â€ ë‚´ê°€ ë­˜ ì•ˆë‹¤ê³  ê·¸ê±¸ í‰ê°€í•˜ëŠ”ê°€? ë‚´ê°€ ë­˜ í‰ê°€í•  ìˆ˜ì¤€ì€ ë˜ëŠ”ê°€?\nâ€œë” ì´ìƒ í• ê±° ì—†ëŠ”ë°â€ë¼ê³  ìƒê°ì´ ë“¤ ìˆ˜ ìˆë‹¤. ê·¸ëŸ´ë•ŒëŠ” ì´ë²ˆì— ì™„ì „ ì²˜ìŒ ê³µë¶€í•´ ë³¸ ê°œë… or ë§ì´ ë§‰í˜”ë˜ ë‚´ìš© or ë°”ë¡œ ëŒ€ë‹µ ëª»í•˜ê³  ë¨¸ë¦¿ì†ì„ í•œ ë²ˆ ê±°ì³ì•¼ í•˜ëŠ” ë‚´ìš© or ë°‘ë°”ë‹¥ì´ ë˜ëŠ” ë‚´ìš© or ë¬¸ì œ ë§Œë“¤ì–´ì„œ í’€ê¸° ë“±ë“±â€¦ ëª¨ë“  ìˆ˜ë‹¨ì„ ê°€ë™í•´ì„œ ë” ê³µë¶€í•´ì•¼ í•  ë¶€ë¶„ì„ ì°¾ëŠ”ë‹¤,\n\n\n\nì‹œí—˜ê¸°ê°„ì€ ì›ë˜ ì§€ì˜¥ì´ë‹¤.\n\nì‹œí—˜ê¸°ê°„ì€ ëª¨ë‘ì—ê²Œ ë‹¤ í˜ë“  ì‹œê°„ì´ë‹¤. ê·¼ë° ìê¾¸ ë²—ì–´ë‚˜ì„œ í¸í•˜ê²Œ í•˜ë ¤í•˜ì§€ ë§ì.\nì‹œí—˜ê¸°ê°„ì¸ë° í¸í•˜ê³  ì‹¶ì–´? ê·¸ëŸ¼ ì‹œí—˜ ëë‚˜ê³ ê°€ ì§€ì˜¥ì¼ ê±°ì•¼â€¦\nê·¸ëƒ¥ ë¯¸ë¦¬ ë§ëŠ”ê±°ë¼ ìƒê°í•˜ì. ê·¸ë˜ë„ ì„±ì  ì˜ ë§ê³  ì§€ì˜¥ì¸ê²Œ ì¢‹ì–ì•„?\n\n\n\ní—ˆìš©ëœ ë¶€ë¶„í•˜ì—ì„œ ìµœëŒ€í•œ ì‰½ê²Œ ì‹œí—˜ ë§Œë“¤ê¸°\n\në­ë“ ì§€ ê°„ì— ì–´ëŠì •ë„ ê¹Œì§€ í—ˆìš©ëœ ë¶€ë¶„ì´ ìˆì„ ê²ƒì´ë‹¤.(í™•ë¥ ê³¼ì •ë¡ ì˜ ê²½ìš° ë‚´ê°€ 20ë¶„ì„ ì†Œìš”í–ˆë˜ 1,2ë²ˆì´ ê·¸ëƒ¥ ì§‘ì—ì„œ ì ì–´ì˜¬ ìˆ˜ ìˆëŠ” ë¬¸ì œì˜€ë‹¤.)\ní—ˆìš©ëœ ë¶€ë¶„ì„ ìµœëŒ€í•œ í™œìš©í•˜ì. ìµœëŒ€í•œ ì‰½ê²Œ ì‹œí—˜ì„ ì‰½ê²Œ ë§Œë“¤ì.\nì–´ë””ê¹Œì°Œ í—ˆìš©ì´ ë˜ëŠ”ì§€ ì„¸ì„¸íˆ ì•Œê¸° ìœ„í•´ì„œë¼ë„ í‰ì†Œì— ì—´ì‹¬íˆ ë“£ëŠ” ê²ƒì´ ì¤‘ìš”í•˜ë‹¤. ìë§Œì€ ê¸ˆë¬¼ì´ë©° ì‹œê°„ì„ í—›ë˜ì´ ë³´ë‚´ì§€ ë§ì\n\n\n\n ëª‡ ë²ˆì”© ë´¤ëŠ”ì§€ ì²´í¬í•˜ê¸°\n\nì–´ë””ë¥¼ ëª‡ ë²ˆì”© ë³´ë©° ì²´í¬í•´ì•¼ í• ì§€ê°€ ì¢€ ì¶”ìƒì ì¼ ìˆ˜ ìˆë‹¤.\nì‹œí—˜ì´ë¼ë©´ ë´ì•¼í•  ë²”ìœ„ì— ìˆëŠ” ì—¬ëŸ¬ ë‹¨ì›,ë¶€ë¶„,ë¬¸ì œê°€ ìˆê³  ë°œí‘œì¤€ë¹„ë¼ë©´ pptì˜ ì—¬ëŸ¬ ìŠ¬ë¼ì´ë“œê°€ ë  ìˆ˜ ìˆê³  ë“±ë“± ë­ëƒì— ë”°ë¼ì„œ ì—¬ëŸ¬ê°€ì§€ ì„¹ì…˜ë³„ë¡œ ë‚˜ëˆ ì§„ë‹¤.\nê·¸ ì¤‘ ê°€ì¥ ìµœì†Œê°€ ë˜ëŠ” ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ë°˜ë“œì‹œ ì²´í¬í•œë‹¤.\nì´ë ‡ê²Œ í•´ì•¼ ì–´ë””ë¥¼ ë§ì´ ë´¤ê³  ì–´ë””ë¥¼ ì ê²Œ ë´¤ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.\në˜í•œ ì˜ í–ˆë˜ ê²ƒê³¼ ëª»í–ˆë˜ ê²ƒ ë‚˜ëˆ ì„œ ë‹¤ë¥¸ ëª¨ì–‘ìœ¼ë¡œ ì²´í¬í•˜ì. ì´ë ‡ê²Œ í•˜ë©´ ì–´ë””ê°€ ë§ì´ ë§‰í˜”ëŠ”ì§€,ì˜ ì•ˆë°›ì•„ ë“¤ì—¬ì§€ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤.\n\n\n\n 3~4ì¼ì „ ë¬´ì¡°ê±´ ì „ìê¸°ê¸° ë©€ë¦¬í•˜ê¸°\n\në‡Œë¥¼ ê³µë¶€ì´ì™¸ì˜ ë‹¤ë¥¸ ê²ƒë“¤ë¡œë¶€í„° ë¹„ìš°ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤.\nì‹œí—˜ë•Œ ìê¾¸ ë…¸ë˜ ìƒê°ë‚˜ì„œ ë§í•¨(HOT ìº”ë”” í™•ë¥ ê³¼ì •ë¡  ì‹œí—˜ ë•Œ ìê¾¸ ìƒê°ë‚˜ì„œ ë©¸ë§â€¦)\nìœ íŠœë¸Œê°€ ë¬¸ì œì¸ ë“¯ í•¨. ìœ íŠœë¸Œì—ì„œ ë…¸ë˜ê°€ ìš°ì—°íˆ ë‚˜ì™€ê°€ì§€ê³  ì¢‹ì•„ì„œ ê³„ì†ë“¤ìŒ\nê·¸ëƒ¥ ì•„ì˜ˆ ì „ìê¸°ê¸°ë“¤ ìì²´ë¥¼ ë©€ë¦¬ í•˜ì.\nì“¸ ìˆ˜ ìˆëŠ” ê²½ìš°ëŠ” ê°•ì˜ ë“¤ì„ ë•Œì´ê±°ë‚˜ ì—°ë½ì˜¨ ê²ƒ ì²´í¬í•´ì•¼ í• ë•Œ(ì´ê²ƒë„ ë°˜ë“œì‹œ ì‹œê°„ì„ ì •í•´ë†“ì)\nì‚¬ì‹¤ 3~4ì¼ ì´ë¼ê³  í•´ë†¨ì§€ë§Œ ì¤‘ìš”í•˜ê²Œ ê¸‰íˆ ì²˜ë¦¬í•´ì•¼ í•  ì¼ì´ ìˆì„ë•ŒëŠ” ê·¸ëƒ¥ ë©€ë¦¬í•˜ëŠ”ê²Œ ë‹µì´ë‹¤.\n\n\n\n í‰ì†Œì— ê°€ì¥ ì‹œì•¼ì— ë­”ê°€ ì—†ëŠ” ê³³ì— ì•‰ì\n\në’· ìë¦¬ë‚˜ ì‚¬ëŒë“¤ì´ ìˆëŠ” ê³³ì— ì•‰ì§€ë§ˆë¼. ì• ìë¦¬ì— ì•‰ì•„ë¼\nì§‘ì¤‘ë ¥ì„ ë¶„ì‚°ì‹œí‚¤ê³  íŠ¹íˆ ì‹œí—˜ë•Œ ë‹¤ë¥¸ì‚¬ëŒì´ ë¹¨ë¦¬ í’€ê±°ë‚˜ í•˜ëŠ” ê²ƒì„ ë³´ë©´ ì¡°ê¸‰í•´ì§€ê¸° ë•Œë¬¸ì´ë‹¤.\nìµœëŒ€í•œ ì•„ë¬´ë„ ì—†ëŠ” ì˜¨ì „íˆ ì§‘ì¤‘í•  ìˆ˜ ìˆëŠ” ê³³ì„ ì°¾ì•„ì„œ ê·¸ ì¥ì†Œì—ì„œ í’€ì.(ì˜ í‘¸ëŠ” ì‚¬ëŒ ì˜†ì´ë‚˜ ë’¤ëŠ” íŠ¹íˆ ë°˜ë“œì‹œ í”¼í•˜ì.)"
  },
  {
    "objectID": "posts/Linear Algebra/Ax = b/Ax = b.html",
    "href": "posts/Linear Algebra/Ax = b/Ax = b.html",
    "title": "[Linear Algebra] 4.Ax=bì˜ í•´ì˜ ê°¯ìˆ˜",
    "section": "",
    "text": "ìœ íˆ¬ë¸Œ - í˜íœí•˜ì„ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ê°•ì˜ë¥¼ ì •ë¦¬í•˜ê¸° ìœ„í•´ ì‘ì„±í•œ ê¸€ì…ë‹ˆë‹¤.Rankì™€ Null spaceë¡œ Ax=bì˜ í•´ì˜ ê°¯ìˆ˜ë¥¼ íŒŒì•…í•©ë‹ˆë‹¤.\nì—°ë¦½ì¼ì°¨ë°©ì •ì‹ì€ í–‰ë ¬ Ax=bë¡œ ë‚˜íƒ€ë‚´ê³  í–‰ë ¬ì˜ ë­í¬ì™€ ì—´ê³µê°„ì„ ê¸°ë°˜ìœ¼ë¡œ í•´ì˜ ê°¯ìˆ˜ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì—´ê³µê°„ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ Ax=bì˜ í•´ì„\ní–‰ë ¬\\(A = \\begin{bmatrix}a_1 & a_2 & \\dots & a_n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\)ì™€ ë²¡í„°\\(x = \\begin{bmatrix}x_1,x_2,\\dots,x_n\\end{bmatrix}^T\\in \\mathbb{R}^{n \\times 1}\\)ì˜ ê³±ì„ ìƒê°í•´ë³´ì. í–‰ë ¬ê³¼ ë²¡í„°ì˜ ê³±ì€ ë‹¤ìŒê³¼ ê°™ì´ ì—´ë²¡í„°ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ ë°”ë¼ë³¼ ìˆ˜ ìˆë‹¤.\n\\[\\begin{aligned}\nAx = \\begin{bmatrix}a_1 & a_2 & \\dots & a_n\\end{bmatrix}\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots\\\\\nx_n\n\\end{bmatrix}\n= a_1x_1 + a_2x_2 + \\dots a_nx_n\n\\end{aligned}\\]\ní–‰ë ¬ Aì˜ ì—´ê³µê°„ì€ ì—´ë²¡í„°ë“¤ì˜ ì„ í˜•ìƒì„±ì´ë‹¤.\n\\[\\begin{aligned}\n&\\text{C}(A) = \\text{span}(a_1,a_2,\\dots,a_n) = \\{c_1a_1 + c_2a_2 + \\dots c_na_n|c_1,c_2,\\dots,c_n \\in K\\}\n\\end{aligned}\\]\nê·¸ëŸ¬ë¯€ë¡œ, í–‰ë ¬ \\(A\\)ì˜ ì—´ê³µê°„ì€ ì„ì˜ì˜ \\(x\\)ì— ëŒ€í•˜ì—¬ ê°€ëŠ¥í•œ \\(Ax\\)ì˜ ëª¨ë“  ì§‘í•©ê³¼ ê°™ë‹¤. ê°€ëŠ¥í•œ ëª¨ë“  ìŠ¤ì¹¼ë¼\\(x_1,x_2,\\dots,x_n\\)ë¡œ ë²¡í„°ì˜ ì¼ì°¨ê²°í•©ì´ ì´ë£¨ëŠ” ì§‘í•©ì€ ìƒì„±(span)ê³¼ ê°™ê¸° ë•Œë¬¸ì´ë‹¤.\n\\[\\text{C}(A) = \\text{span}(a_1,a_2,\\dots,a_n) = \\{Ax |x\\in K^n\\}\\]\në§Œì•½ \\(Ax = b\\)ì¸ ë°©ì •ì‹ì˜ í•´ \\(x\\)ë¥¼ êµ¬í•˜ë ¤ í•œë‹¤ í•˜ì. \\(\\text{C}(A)\\)ëŠ” ê°€ëŠ¥í•œ \\(Ax\\)ì˜ ëª¨ë“  ì§‘í•©ì´ì˜€ìœ¼ë¯€ë¡œ ë§Œì•½ \\(\\text{C}(A)\\)ê°€ \\(b\\)ë¥¼ í¬í•¨í•œë‹¤ë©´(ì¦‰,\\(b\\)ê°€ ì—´ê³µê°„ì˜ ì›ì†Œë¼ë©´) \\(Ax=b\\)ì¸ \\(x\\)ê°€ ì¡´ì¬í•˜ì—¬ ë°©ì •ì‹ì˜ í•´ê°€ ì¡´ì¬í•˜ê³  \\(\\text{C}(A)\\)ê°€ \\(b\\)ë¥¼ í¬í•¨í•˜ì§€ ì•ŠëŠ”ë‹¤ë©´ \\(Ax = b\\)ì¸ \\(x\\)ê°’ì´ ì¡´ì¬í•˜ì§€ ì•Šê³  ë”°ë¼ì„œ ë°©ì •ì‹ì˜ í•´ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n\nCase1 : Aê°€ full column rankì¼ ë•Œ\në¬¸ì œ : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) ì¼ë•Œ, \\(Ax = b\\)ë¥¼ ë§Œì¡±í•˜ëŠ” xì˜ ê°¯ìˆ˜ëŠ”?\n ìœ„ì˜ ë¬¸ì œì—ì„œ AëŠ” full column rankì´ë‹¤. full column rankì¼ ê²½ìš° \\(Ax\\)ì˜ ëª¨ì–‘ì€ ìœ„ì™€ ê°™ê³  ì—´ê³µê°„ì€ mì°¨ì› ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì´ì nì°¨ì› ë²¡í„°ê³µê°„ì´ë‹¤.ì•ì—ì„œ â€œmì°¨ì› ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì¸ nì°¨ì› ë²¡í„°ê³µê°„â€ ì´ë¼ í–ˆëŠ”ë° ê·¸ ì´ìœ ëŠ” (ì—´)ë²¡í„°ê°€ mì°¨ì› ë²¡í„°ê³µê°„ì˜ ì›ì†Œ(ë²¡í„°)ì´ê¸° ë•Œë¬¸ì´ë‹¤. mì°¨ì› ë²¡í„°ê³µê°„ì— ìˆëŠ” (ì—´)ë²¡í„° nê°œë¥¼ ê¸°ì €ë¡œ ìƒì„±ëœ ê³µê°„ì´ê¸° ë•Œë¬¸ì— mì°¨ì› ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì´ë©´ì„œ ë™ì‹œì— nì°¨ì› ë²¡í„°ê³µê°„ì´ ëœë‹¤. \nìœ„ì—ì„œ ì–¸ê¸‰í–ˆë“¯ì´ ì—´ê³µê°„ì€ ì„ì˜ì˜ xì— ëŒ€í•˜ì—¬ ê°€ëŠ¥í•œ \\(Ax\\)ì˜ ì§‘í•©ì´ê¸°ì— ë°©ì •ì‹ì˜ í•´ê°€ ì¡´ì¬í•˜ë ¤ë©´ bê°€ ì—´ê³µê°„ì•ˆì— ìˆìœ¼ë©´ ë˜ê³  ì•„ë‹ˆë©´ ë°”ê¹¥ì— ìˆìœ¼ë©´ ëœë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ ê²½ìš° bì˜ ìœ„ì¹˜ëŠ” ì–´ë–»ê²Œ ë ê¹Œ?\n\nê°€ëŠ¥í•œ bì˜ ìœ„ì¹˜ëŠ” 2ê°€ì§€ì´ë‹¤. 1ì˜ ê²½ìš°ëŠ” ì—´ê³µê°„ì•ˆì— bê°€ ì—†ëŠ” ê²½ìš°ë‹¤. ì´ ê²½ìš° bëŠ” mì°¨ì› ë²¡í„°ê³µê°„ì—ëŠ” ìˆìœ¼ë©´ì„œ(\\(b \\in R^{m \\times 1}\\)ì´ê¸°ì— ë‹¹ì—°í•˜ë‹¤) ë¶€ë¶„ê³µê°„ì¸ nì°¨ì› ì—´ê³µê°„ì•ˆì—ëŠ” ì—†ê²Œëœë‹¤. ë”°ë¼ì„œ ì´ ê²½ìš° í•´ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. 2ì˜ ê²½ìš°ëŠ” ì—´ê³µê°„ì•ˆì— bê°€ ìˆëŠ” ê²½ìš°ë‹¤. ì´ ê²½ìš° bëŠ” mì°¨ì› ë²¡í„°ê³µê°„ì— ì†í•˜ë©´ì„œ ë™ì‹œì— ë¶€ë¶„ê³µê°„ì¸ nì°¨ì› ë²¡í„°ê³µê°„ì—ë„ ì†í•œë‹¤.\nì´ë ‡ê²Œ ìƒê°í•˜ë©´ ëë‚œ ê²ƒ ê°™ì€ë° í•œê°€ì§€ ë” ìƒê°í•´ì•¼ í•  ê²ƒì´ ìˆë‹¤. ë°”ë¡œ null spaceì´ë‹¤. ë§Œì•½ Ax = bë¥¼ ë§Œì¡±í•˜ëŠ” í•˜ë‚˜ì˜ í•´ì¸ \\(x_{particular}\\)ê°€ ìˆë‹¤ê³  í•´ë³´ì. ì´ë•Œ \\(Ax=0\\)ì„ ë§Œì¡±í•˜ëŠ” ë„ê³µê°„ì˜ ì„ì˜ì˜ ì›ì†Œì¸ \\(x\\)ë¥¼ \\(x_{null}\\in N(A)\\)ì´ë¼ê³  í•˜ë©´ ë‹¤ìŒì´ ì„±ë¦½í•œë‹¤.\n\\[A(x_{particular} + x_{null}) = Ax_{particular} + Ax_{null} = b\\]\nìœ—ì‹ì— ì˜í•´ì„œ null spaceì˜ ì›ì†Œì¸ \\(x_{null}\\)ê³¼ \\(x_{particular}\\)ì˜ í•©ë„ ë°©ì •ì‹ì˜ í•´ì´ë¯€ë¡œ í•´ë¥¼ êµ¬í• ë•Œ null spaceë„ í™•ì¸í•´ì•¼ í•œë‹¤. null spaceì— ëŒ€í•œ xë¥¼ ì¶”ê°€í•œ ì™„ì „í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[x_{complete} = x_{particular} + x_{null}\\]\nê·¸ë ‡ë‹¤ë©´ x_{null}ì„ ì–´ë–»ê²Œ í™•ì¸í•  ìˆ˜ ìˆì„ê¹Œ? ë­í¬-ë„ë¦¬í‹° ì •ë¦¬ë¡œ í™•ì¸í•  ìˆ˜ ìˆëŠ”ë° ìœ„ì™€ê°™ì´ full column rankì¸ ê²½ìš°ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&\\text{rank}(A) + \\text{nulity(A)} = n\\\\\n&\\Longleftrightarrow \\text{nulity(A)} = n - rank(A) = n - n = 0\n\\end{aligned}\\]\në­í¬-ë„ë¦¬í‹° ì •ë¦¬ì— ì˜í•˜ì—¬ null spaceì˜ ì°¨ì›ì´ 0ì„ì„ í™•ì¸í–ˆë‹¤. ì°¨ì›ì´ 0ì¸ ë„ê³µê°„(ë²¡í„°ê³µê°„)ì€ \\(\\{\\bf{0}\\}\\)ì´ë¯€ë¡œ \\(x_{null} \\in N(A)\\)ì¸ \\(x_{null} ={\\bf 0}\\)ì´ë‹¤. ë”°ë¼ì„œ 2ë²ˆì˜ ê²½ìš°, \\(x_{particular}\\)ê°€ ì¡´ì¬í•˜ì—¬ \\(Ax = 0\\)ì¼ ê²½ìš°ì˜ ì™„ì „í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\\therefore  x_{total} = x_{particular} + x_{null} = x_{particular} + {\\bf 0} = x_{particular}\\]\nê²°ë¡ ì ìœ¼ë¡œ, full column rankì¼ ê²½ìš° í•´ê°€ ì¡´ì¬í•˜ì§€ ì•Šê±°ë‚˜ ë‹¨ í•˜ë‚˜ ì¡´ì¬í•œë‹¤.\n\n\nCase2 : Aê°€ full row rankì¼ ë•Œ\në¬¸ì œ : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = m<n\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) ì¼ë•Œ, \\(Ax = b\\)ë¥¼ ë§Œì¡±í•˜ëŠ” xì˜ ê°¯ìˆ˜ëŠ”?\n ìœ„ì˜ ë¬¸ì œì—ì„œ AëŠ” full row rankì´ë‹¤. ìœ„í•´ì„œ í•œ ê²ƒì²˜ëŸ¼ ë¨¼ì € \\(C(A)\\)ì™€ \\(b\\)ê°€ ì–´ë–»ê²Œ ìœ„ì¹˜í•˜ê³  ìˆì„ì§€ íŒŒì•…í•˜ê³  null spaceë¥¼ ë”°ì ¸ ì™„ì „í•´ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ë¨¼ì € \\(C(A)\\)ë¥¼ ìƒê°í•´ë³´ì. \\(C(A)\\)ëŠ” í–‰ë ¬ì˜ ë­í¬ê°€ mì´ê¸°ì— nê°œì˜ (ì—´)ë²¡í„° ì¤‘ ì„ í˜•ë…ë¦½ì¸ column vectorëŠ” mê°œ ë¿ì´ë‹¤. ë”°ë¼ì„œ ë²¡í„°ì˜ spanì¸ ì—´ê³µê°„ì€ mì°¨ì› ë²¡í„°ê³µê°„ì´ë‹¤. (nê°œì˜ ì—´ë²¡í„°ê°€ ì¡´ì¬í•˜ì§€ë§Œ,ì„ í˜•ì¢…ì†ì´ê¸°ë–„ë¬¸ì´ë‹¤.)\nì´ì „ë¬¸ì œì™€ ë‹¤ë¥¸ì ë„ ì¡´ì¬í•˜ëŠ”ë° ë°”ë¡œ ì—´ê³µê°„ì´ (ì—´)ë²¡í„°ê°€ ì¡´ì¬í•˜ëŠ” ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì´ ì•„ë‹ˆë¼ëŠ” ì ì´ë‹¤. ì´ ë¬¸ì œì—ì„œ ê°ê°ì˜ ì—´ë²¡í„°ëŠ” mì°¨ì› ê³µê°„ì˜ ë²¡í„°ì´ê³  ì—´ë²¡í„°ì˜ ìƒì„±ë„ mì°¨ì› ë²¡í„°ê³µê°„ì´ê¸°ë•Œë¬¸ì— ì—´ë²¡í„°ê°€ ì¡´ì¬í•˜ëŠ” ë°”ë¡œ ê·¸ ê³µê°„ì´ ì—´ê³µê°„ì´ë‹¤.\nìœ„ì™€ ê°™ì´ ì—´ê³µê°„ì— ëŒ€í•´ì„œ ìƒê°í–ˆìœ¼ë©´ ì´ì œ bì— ëŒ€í•´ì„œ ìƒê°í•´ë³¼ ì°¨ë¡€ë‹¤. bì˜ ìœ„ì¹˜ëŠ” ì–´ë–»ê²Œ ë ê¹Œ? bëŠ” Aì˜ ì—´ê³µê°„ì— ì†í•˜ëŠ” ë²¡í„°ì¼ê¹Œ ì•„ë‹ê¹Œ?\n\nbì˜ ê²½ìš° \\(b \\in \\mathbb{R}^{m \\times 1}\\)ì´ê¸°ì— ì—´ê³µê°„ì— ì†í•˜ëŠ” ë²¡í„°ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ full row rankì¸ ê²½ìš°ëŠ” ë°˜ë“œì‹œ í•´ê°€ ì¡´ì¬í•œë‹¤.\nì—¬ê¸°ì„œ ì™„ì „í•´ë¥¼ êµ¬í•˜ê¸° ìœ„í•´ì„œ null spaceë„ ìƒê°í•´ì•¼í•œë‹¤. ë­í¬-ë„ë¦¬í‹° ì •ë¦¬ì— ì˜í•´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&rank(A) + nulity(A) = n \\\\\n&\\Longleftrightarrow nulity(A) = n - rank(A) =  n - m\n\\end{aligned}\\]\në­í¬ì •ë¦¬ì— ì˜í•´ì„œ null spaceëŠ” n-mì°¨ì›ì˜ ë²¡í„°ê³µê°„ì´ë‹¤. ì´ ê²½ìš° ê°€ëŠ¥í•œ \\(x_{null}\\)ì€ ë¬´í•œíˆ ë§ì´ ì¡´ì¬í•˜ë¯€ë¡œ í•´ê°€ ë¬´ìˆ˜íˆ ë§ë‹¤. ì™„ì „í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\therefore\\,\\,  &x_{complete} = x_{particular} + x_{null}\\\\\n&\\text{where, } x_{null} \\in N(A) = \\mathbb{R}^{n-m}\n\\end{aligned}\\]\nê²°ë¡ ì ìœ¼ë¡œ, full row rankì˜ ê²½ìš° í•´ëŠ” ë¬´ìˆ˜íˆ ë§ë‹¤.\n\n\nCase3 : Aê°€ full rankì¼ ë•Œ\në¬¸ì œ : \\(A \\in \\mathbb{R}^{m \\times m},\\text{rank}(A) = m\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) ì¼ë•Œ, \\(Ax = b\\)ë¥¼ ë§Œì¡±í•˜ëŠ” xì˜ ê°¯ìˆ˜ëŠ”?\n\nì‚¬ì‹¤ ì´ ë¬¸ì œì˜ í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[x = A^{-1}b\\] ê·¸ëŸ¬ë¯€ë¡œ,full rankì¸ ê²½ìš° í•´ì˜ ê°¯ìˆ˜ê°€ 1ê°œì´ë‹¤.\nìœ„ì²˜ëŸ¼ ê°„ë‹¨í•˜ê²Œ í•´ë¥¼ êµ¬í•  ìˆ˜ ìˆì§€ë§Œ â€¦ ê·¸ë˜ë„ ê¸°í•˜í•™ì ìœ¼ë¡œ ìƒê°í•´ë³´ê¸° ìœ„ì—ì„œ í–ˆë˜ ê²ƒì²˜ëŸ¼ ë”°ì ¸ë³´ì. column spaceëŠ” mì°¨ì› ë²¡í„°ê³µê°„ì´ë‹¤. column vectorëŠ” column space ê·¸ ìì²´ì¸ mì°¨ì› ë²¡í„°ê³µê°„ì˜ ì›ì†Œì´ë‹¤. \\(b\\in \\mathbb{R}^{m \\times 1}\\)ì˜ ê²½ìš° mì°¨ì› ë²¡í„°ê³µê°„ì˜ ì›ì†Œì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, column spaceëŠ” ë°˜ë“œì‹œ bë¥¼ í¬í•¨í•˜ë©° ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\në„ê³µê°„ì„ ë”°ì§€ê¸° ìœ„í•´ ë­í¬-ë„ë¦¬í‹° ì •ë¦¬ë¥¼ ì‚¬ìš©í•´ ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&\\text{rank}(A)+\\text{nulity}(A) = m\\\\\n&\\Longleftrightarrow \\text{nulity}(A) = m - \\text{rank}(A) = m - m = 0\n\\end{aligned}\\]\nnull spaceëŠ” ì°¨ì›ì´ 0ì´ë¯€ë¡œ \\(N(A) = \\{{\\bf 0}\\}\\)ì´ê³  ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[x_{complete} = x_{particular} + x_{null} = x_{particular} + {\\bf 0} = x_{particular}\\]\nê²°ê³¼ì ìœ¼ë¡œ, full rankì¸ ê²½ìš° í•´ëŠ” ë°˜ë“œì‹œ ì¡´ì¬í•˜ë©° ê°¯ìˆ˜ëŠ” 1ê°œì´ë‹¤.\n\n\nCase4 : Aê°€ rank deficient ì¼ ë•Œ\në¬¸ì œ : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = \\alpha < \\text{min}(m,n)\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) ì¼ë•Œ, \\(Ax = b\\)ë¥¼ ë§Œì¡±í•˜ëŠ” xì˜ ê°¯ìˆ˜ëŠ”?\n\nìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” Aê°€ rank deficientì¸ ê²½ìš° ì¤‘, \\(m<n\\) ì¸ ê²½ìš°ë¥¼ ìƒê°í•´ë³´ì. í–‰ë ¬ Aì˜ column spaceëŠ” mì°¨ì› ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì´ì \\(\\alpha\\)ì°¨ì›ì˜ ë²¡í„°ê³µê°„ì´ë‹¤.\\(b \\in \\mathbb{R}^{m \\times 1}\\)ì´ë¯€ë¡œ ê°€ëŠ¥í•œ ê²½ìš°ëŠ” ì•„ë˜ì™€ ê°™ë‹¤.\n\n1ë²ˆì˜ ê²½ìš°ë¼ë©´ bëŠ” column spaceì˜ ì›ì†Œê°€ ì•„ë‹ˆë¯€ë¡œ ë°©ì •ì‹ì˜ í•´ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. ë§Œì•½ 2ë²ˆì˜ ê²½ìš°ë¼ë©´ í•´ê°€ ì¡´ì¬í•œë‹¤. ì´ë•Œì—ëŠ” null spaceë¥¼ ê³ ë ¤í•œ ì™„ì „í•´ë¥¼ ë”°ì ¸ì•¼í•˜ë¯€ë¡œ ë­í¬-ë„ë¦¬í‹° ì •ë¦¬ë¥¼ í™•ì¸í•œë‹¤.\n\\[\\begin{aligned}\n&\\text{rank}(A) + \\text{nulity}(A) = \\alpha + \\text{nulity}(A) = n \\\\\n&\\Longleftrightarrow \\text{nulity}(A) = n - \\alpha > 0\n\\end{aligned}\\]\në­í¬ì •ë¦¬ì— ì˜í•´ì„œ null spaceëŠ” \\(n - \\alpha\\)ì°¨ì›ì˜ ë²¡í„°ê³µê°„ì´ë‹¤. ì´ ê²½ìš° null spaceì˜ ì„ì˜ì˜ ì›ì†Œì¸ \\(x_{null}\\)ëŠ” ë¬´í•œíˆ ë§ì´ ì¡´ì¬í•˜ë¯€ë¡œ í•´ê°€ ë¬´ìˆ˜íˆ ë§ë‹¤. ì™„ì „í•´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\therefore\\,\\,  &x_{complete} = x_{particular} + x_{null}\\\\\n&\\text{where, } x_{null} \\in N(A) = \\mathbb{R}^{n-\\alpha}\n\\end{aligned}\\]\nê²°ë¡ ì ìœ¼ë¡œ, rank deficientì˜ ê²½ìš° í•´ëŠ” ë¬´ìˆ˜íˆ ë§ê±°ë‚˜ í•´ëŠ” ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤.\n\n\nì •ë¦¬\n\n\n\n\n\n\n\n\nrank type\nexpression\ní•´ì˜ ê°¯ìˆ˜\n\n\n\n\nfull column rank\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) = n<m\\)\ní•´ê°€ ì—†ê±°ë‚˜ í•´ê°€ í•œê°œë§Œ ì¡´ì¬í•œë‹¤.\n\n\nfull row rank\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) = m<n\\)\ní•´ê°€ ë¬´ìˆ˜íˆ ë§ë‹¤.\n\n\nfull rank\n\\(A \\in \\mathbb{R}^{m \\times m}\\),\\(\\text{rank}(A) = m\\)\ní•´ê°€ í•œê°œë§Œ ì¡´ì¬í•œë‹¤.\n\n\nrank deficient\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) < \\text{min}(m,n)\\)\ní•´ê°€ ì—†ê±°ë‚˜ í•´ê°€ ë¬´ìˆ˜íˆ ë§ë‹¤.\n\n\n\n\n\nì°¸ê³ ìë£Œ**\ní˜íœí•˜ì„ - [ì„ ëŒ€] 2-11ê°•. Ax=b ì˜ í•´ì˜ ìˆ˜ ì•Œì•„ë‚´ê¸° í”„ë¦°í‚¤í”¼ì•„"
  },
  {
    "objectID": "posts/Linear Algebra/derivative/ë²¡í„°,í–‰ë ¬ë¯¸ë¶„.html",
    "href": "posts/Linear Algebra/derivative/ë²¡í„°,í–‰ë ¬ë¯¸ë¶„.html",
    "title": "[Linear Algebra] Appendix",
    "section": "",
    "text": "ë‹¤ë³€ìˆ˜ ìŠ¤ì¹¼ë¼í•¨ìˆ˜ë¥¼ ìƒê°í•´ë³´ì. ì¦‰ 2ê°œì´ìƒì˜ ë³€ìˆ˜ê°€ inputì´ë©° ouputì€ í•˜ë‚˜ì˜ ìŠ¤ì¹¼ë¼ì¸ í•¨ìˆ˜ì´ë‹¤.\në§Œì•½ í•¨ìˆ˜ë¥¼ ì´ë£¨ëŠ” ëª¨ë“  ë³€ìˆ˜ê°€ ê°ê° ì¡°ê¸ˆì”© ë°”ë€ë‹¤ë©´? \\(\\to\\) í•¨ìˆ«ê°’ë„ ì‘ì§€ë§Œ ì•„ì£¼ ì¡°ê¸ˆ ë³€í™”í•  ê²ƒì´ë‹¤.\nì „ë¯¸ë¶„ì€ ìœ„ì™€ ê°™ì€ ìƒí™© ì¦‰, ë‹¤ë³€ìˆ˜ í•¨ìˆ˜ì˜ ëª¨ë“  ë³€ìˆ˜ê°€ ì¡°ê¸ˆì”© ë³€í™”í–ˆì„ë•Œ í•¨ìˆ«ê°’ì˜ ë³€í™”ëŸ‰ì„ ê·¼ì‚¬í•˜ëŠ” ì–‘ì´ë‹¤.(from wikipedia)\n\në‹¤ë³€ìˆ˜í•¨ìˆ˜ì˜ ë¯¸ì†Œì¦ë¶„\n\n\n\n\n\n\n\n\nDefinition of total derivative\n\n\n\n\\[\\begin{aligned}\n&df = \\frac{\\partial{f}}{\\partial{x_1}}dx_1+\\frac{\\partial{f}}{\\partial{x_2}}dx_2+\\dots+\\frac{\\partial{f}}{\\partial{x_n}}dx_n \\\\\n&\\text{where } f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\n\\end{aligned}\\]\n\n\nì¶”ê°€ì„¤ëª…(ì—„ë°€í•œ notation ì œì™¸)\n\ní¸ë¯¸ë¶„ \\(\\frac{\\partial{f}}{\\partial{x}}\\)ëŠ” ì„ì˜ì˜ ë³€ìˆ˜ xì˜ ë‹¨ìœ„ë³€í™”ëŸ‰ì´ë‹¤.\n\nxë¥¼ ë§¤ìš° ë¯¸ì†Œí•˜ê²Œ ì¦ê°€ì‹œí‚¤ê³  : \\(x \\overset{+dx}{\\to} x+dx\\)\nì´ì— ë”°ë¼ì„œ fê°€ ì–¼ë§ˆë‚˜ ë³€í™”í–ˆëŠ”ì§€ ê·¸ ì–‘ì„ ì¸¡ì •í•œ ë’¤ : \\(f(x+dx) - f(x)\\)\nâ€œxê°€ 1ë§Œí¼ ì¦ê°€í•˜ë©´ ì–¼ë§ˆë‚˜ fëŠ” ë³€í™”í•´?â€ë¥¼ ì¸¡ì •í•œ ê°’(ë¹„ìœ¨)ì´ë‹¤. : \\(\\frac{f(x+dx) - f(x)}{dx}\\)\n\ní•˜ë‚˜ì˜ ë³€ìˆ˜ xì˜ ë§¤ìš° ì¡°ê·¸ë§ˆí•˜ê³  ìˆœê°„ì ì¸ ë³€í™”ì— ì˜í•´ í•¨ìˆ«ê°’ì˜ ë³€í™”ê°€ ì•„ì£¼ ì¡°ê¸ˆ ë³€í•  ë•Œ\nì´ëŸ¬í•œ ì¡°ê·¸ë§ˆí•œ ë³€í™”ê°€ ë³€ìˆ˜ê°€ 1ë§Œí¼ ì¦ê°€í•˜ëŠ” ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ ì‚¼ëŠ” ê²½ìš°ì—ëŠ” ì–¼ë§ˆì •ë„ ì¼ì§€ ì¸¡ì •í•œ ë‹¨ìœ„(ê¸¸ì´ì— ëŒ€í•œ)ë³€í™”ëŸ‰ì´ë‹¤.\nëŠë‚Œ : ì´ëŸ° ìˆœê°„ì ì¸ ë³€í™”ë¥¼ 1ë§Œí¼ ê³„ì† ìœ ì§€í•˜ë©´ ì–¼ë§ˆë‚˜ ë³€í™”í• ê¹Œ?\n\nì •ë¦¬í•˜ìë©´ ì„ì˜ì˜ ë‹¨ì¼ë³€ìˆ˜ì˜ ë¯¸ì†Œë³€í™”ì— ëŒ€í•œ ë‹¨ìœ„ë³€í™”ëŸ‰ì´ë‹¤.(ë”±ë”±í•œ ì •ì˜)\nì—¬ëŸ¬ê°œì˜ ë³€ìˆ˜ê°€ ìˆì§€ë§Œ í•˜ë‚˜ì˜ ë³€ìˆ˜ë§Œ ì¡°ê¸ˆ ë³€í™”ì‹œí‚¨ë’¤ ì¸¡ì •í•œë‹¤.(ê·¸ë˜ì„œ í¸ë¯¸ë¶„ì´ë‹¤.)\nì‚¬ì‹¤ ì‹ì´ ì—„ë°€í•˜ì§„ ì•Šë‹¤.ìœ„ì˜ í‘œí˜„ì€ ìƒë¯¸ë¶„ì— ëŒ€í•œ í‘œí˜„ì´ë‹¤.\nìì„¸í•œ í‘œí˜„ì‹ì€ wikië¥¼ ì°¸ì¡°í•˜ì.\n\në‹¨ìœ„ë³€í™”ëŸ‰ì„ ì–´ë–¤ ê°’ê³¼ ê³±í•´ì£¼ë©´ ë³€í™”ëŸ‰ì´ ë‚˜ì˜¨ë‹¤.\n\në°¥ì„ 100gë¨¹ìœ¼ë©´ ëª¸ë¬´ê²Œê°€ 5kg ì°ë‹¤ê³  í•˜ì.(ã…œã…œì–µìš¸í•œ ìƒí™©ì´ë‹¤.)\nê·¸ë ‡ë‹¤ëŠ” ê²ƒì€ 1gë¨¹ìœ¼ë©´ 0.05kgì´ ì°ë‹¤ëŠ” ê²ƒì„ ì•”ì‹œí•˜ë©°\nì´ëŠ” ë°¥(x) 1gì— ëŒ€í•œ ëª¸ë¬´ê²Œ(f)ì˜ ë‹¨ìœ„ë³€í™”ëŸ‰ì€ 0.05kgì´ë‹¤. \\(\\Longleftrightarrow= \\frac{\\partial f}{\\partial x} = 0.05\\)\n\në°¥ 300g ë¨¹ìœ¼ë©´? \\(\\frac{\\partial f}{\\partial x}300 = 0.05 \\times 300 = 15\\)\në°¥ 0.03g ë¨¹ìœ¼ë©´? \\(\\frac{\\partial f}{\\partial x}0.03 = 0.05 \\times 0.03 = 0.0015\\)\në°¥ \\(db\\)ë§Œí¼(ì§„ì§œì¡°ê¸ˆ) ë¨¹ìœ¼ë©´? \\(\\frac{\\partial f}{\\partial x}db = 0.05 \\times db\\)\n\n\në³€í™”ë¥¼ ì‹œí‚¤ëŠ” ì—¬ëŸ¬ê°€ì§€ ìš”ì¸ì„ ì „ë¶€ ê³ ë ¤í•˜ë©´?\n\në°¥(\\(x_1\\))ë„ ì¡°ê¸ˆë¨¹ê³  ë¬¼ë„ ì¡°ê¸ˆ(\\(x_2\\))ë¨¹ê³  ì¡°ê¸ˆ ìë‹¤ê°€(\\(x_3\\)) ìš´ë™ë„ ì¡°ê¸ˆ(\\(x_4\\))í•˜ë©´ ëª¸ë¬´ê²Œ(f)ê°€ ì–´ë–»ê²Œ ë ê¹Œ??\n\\(df = \\frac{\\partial{f}}{\\partial{x_1}}dx_1 + \\frac{\\partial{f}}{\\partial{x_2}}dx_2 + \\frac{\\partial{f}}{\\partial{x_3}}dx_3 + \\frac{\\partial{f}}{\\partial{x_4}}dx_4\\)\n\n\n\n\n\nì¶”í›„ ì •ë¦¬"
  },
  {
    "objectID": "posts/Linear Algebra/derivative/ë²¡í„°,í–‰ë ¬ë¯¸ë¶„.html#definition",
    "href": "posts/Linear Algebra/derivative/ë²¡í„°,í–‰ë ¬ë¯¸ë¶„.html#definition",
    "title": "[Linear Algebra] Appendix",
    "section": "Definition",
    "text": "Definition\n\në‹¤ìŒê³¼ ê°™ì´ ê¸¸ì´ê°€ 2ì¸ ë²¡í„°ê°€ inputì´ê³  í•˜ë‚˜ì˜ ê°’ì„ outputí•˜ëŠ” ìŠ¤ì¹¼ë¼í•¨ìˆ˜ë¥¼ ìƒê°í•´ë³´ì.\nì´ í•¨ìˆ˜ëŠ” ë‹¤ë¥¸ ì‹œê°ìœ¼ë¡œ 2ê°œì˜ ë³€ìˆ˜ë¥¼ inputìœ¼ë¡œ ë°›ëŠ” ë‹¤ë³€ìˆ˜ ìŠ¤ì¹¼ë¼ í•¨ìˆ˜ë¡œ ìƒê°í•  ìˆ˜ë„ ìˆë‹¤.\n\n\\[\\begin{aligned}\nf\\big(\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}\\big)\n= f(x_1,x_2)\n\\end{aligned}\\]\n\nìœ„ì™€ ê°™ì€ ìŠ¤ì¹¼ë¼í•¨ìˆ˜ì˜ ë²¡í„°ì— ëŒ€í•œ ë¯¸ë¶„ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ëœë‹¤.\n\n\n\n\n\n\n\në‹¤ë³€ìˆ˜ ìŠ¤ì¹¼ë¼í•¨ìˆ˜ì˜ ë²¡í„°ë¯¸ë¶„ì— ëŒ€í•œ ì •ì˜\n\n\n\n\\[\\begin{aligned}\n&\\frac{\\partial{f}}{\\partial{x^T}} := \\begin{bmatrix}\\frac{\\partial{f}}{\\partial{x_1}} & \\frac{\\partial{f}}{\\partial{x_2}} & \\dots & \\frac{\\partial{f}}{\\partial{x_n}}\\end{bmatrix}\\\\\n&\\text{where } f:\\mathbb{R}^n \\to \\mathbb{R}\\,,x = \\begin{bmatrix}x_1,x_2,\\dots,x_n\\end{bmatrix}^T\n\\end{aligned}\\]\n\n\n\nì™œ \\(\\partial{x^T}\\)ì¼ê¹Œ?\n\ninputì€ column vectorì¸ë° outputì€ row vectorì´ë‹¤.\nê·¸ë˜ì„œ ëˆ•í˜€ì„œ transpose\n\nê·¸ëŸ°ë° ì‚¬ì‹¤ ë§ì€ ê³³ì—ì„œ notationì„ í—·ê°ˆë¦¬ê²Œ í•œë‹¤.\n\në¯¸ë¶„ì˜ ê²°ê³¼ë¥¼ column vectorë¡œ ì ëŠ” ê³³ë„ ë§ê³ \n\\(\\partial{x^T}\\)ëŒ€ì‹  \\(\\partial{x}\\)ë¡œ ì ëŠ” ê³³ë„ ë§ë‹¤.\nê·¸ë˜ì„œ ê·¸ëƒ¥ í•¨ìˆ˜ë¥¼ ì´ë£¨ëŠ” ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•´ì„œ ë¯¸ë¶„í•´ë³¸ë’¤ ëª¨ì•„ë†“ì€ ê±°êµ¬ë‚˜ ì´ë ‡ê²Œ ê¸°ì–µí•´ë†“ëŠ”ê²Œ ì¢‹ë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/derivative/ë²¡í„°,í–‰ë ¬ë¯¸ë¶„.html#ì‰½ê²Œ-ë¯¸ë¶„-êµ¬í•˜ê¸°",
    "href": "posts/Linear Algebra/derivative/ë²¡í„°,í–‰ë ¬ë¯¸ë¶„.html#ì‰½ê²Œ-ë¯¸ë¶„-êµ¬í•˜ê¸°",
    "title": "[Linear Algebra] Appendix",
    "section": "â˜…ì‰½ê²Œ ë¯¸ë¶„ êµ¬í•˜ê¸°â˜…",
    "text": "â˜…ì‰½ê²Œ ë¯¸ë¶„ êµ¬í•˜ê¸°â˜…\n\nì •ì„ì ìœ¼ë¡œ êµ¬í•˜ë ¤ë©´ ìœ„ì˜ ì •ì˜ì— í•´ë‹¹í•˜ëŠ” ëª¨ë“  componentë¥¼ ê°ê° í¸ë¯¸ë¶„ ì·¨í•´ì„œ êµ¬í•´ì¤˜ì•¼ í•œë‹¤.\nì´ê±´ ë„ˆë¬´ ë²ˆê±°ë¡œìš´ ê³¼ì •ì´ë©° ë” ì‰½ê²Œ êµ¬í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.\n\\(df\\)ëŠ” ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤\n\n\\[\\begin{aligned}\n&df := f(x+ dx) - f(x)\\\\\n&\\text{where } f:\\mathbb{R}^n \\to \\mathbb{R},x = \\begin{bmatrix}x_1,x_2,\\dots,x_n\\end{bmatrix}^T,dx = \\begin{bmatrix}dx_1,dx_2,\\dots,dx_n\\end{bmatrix}^T\n\\end{aligned}\\]\n\në˜í•œ \\(df\\)ì˜ total derivativeëŠ” ì •ì˜ì— ì˜í•´ ë‹¤ìŒê³¼ ê°™ë‹¤.(ë‘ ë²ˆì§¸ ì •ì˜ë¼ê³  ìƒê°í•´ë„ ë¬´ë°©í•˜ë‹¤.)\n\n\\[\\begin{aligned}\ndf &:= \\frac{\\partial{f}}{\\partial{x_1}}dx_1+\\frac{\\partial{f}}{\\partial{x_2}}dx_2+\\dots+\\frac{\\partial{f}}{\\partial{x_n}}dx_n \\\\\n&= \\begin{bmatrix}\\frac{\\partial{f}}{\\partial{x_1}} & \\frac{\\partial{f}}{\\partial{x_2}} & \\dots & \\frac{\\partial{f}}{\\partial{x_n}}\\end{bmatrix}\n\\begin{bmatrix}dx_1\\\\dx_2\\\\ \\vdots \\\\ dx_n \\end{bmatrix}\\\\\n&= \\frac{\\partial{f}}{\\partial{x}^T}dx\n\\end{aligned}\\]\n\në‘ ì •ì˜ë¥¼ ëª¨ì•„ì„œ ì •ë¦¬í•´ë³´ì.\n\n\\[\\begin{align}\ndf &= f(x+ dx) - f(x) \\\\\n&= \\frac{\\partial{f}}{\\partial{x}^T}dx\n\\end{align}\\]\n\nê·¸ë˜ì„œ ì–´ë–»ê²Œ êµ¬í• ê¹Œ?\n\n(1)ì„ í†µí•´ì„œ \\(df\\)ë¥¼ ê³„ì‚°í•œ í›„\n(2)ì™€ ê°™ì€ í˜•ì‹ì„ ì–´ë–»ê²Œë“  ë§Œë“¤ê³ \ndxì•ì— ìˆëŠ” í•­ì´ ì›í•˜ëŠ” ë¯¸ë¶„ì´ë‹¤.\n\nì£¼ì˜\n\n\\(f(x+dx)\\)ë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì—ì„œ \\(dx^Tdx,dx_1dx_2\\)ì™€ ê°™ì€ 2ì°¨ ì´ìƒì˜ í•­ì´ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤.\nì´ê±´ ê·¸ëƒ¥ ì œì™¸ì‹œí‚¤ë©´ ëœë‹¤. ì¦‰, 1ì°¨í•­ë§Œì„ ë‚¨ê¸°ê³ ëŠ” ì‹¹ ì§€ìš°ë©´ ëœë‹¤.\nì™œ ê·¸ëŸ´ê¹Œ?(ì—„ë°€í•˜ì§€ ì•Šì€ ì¦ëª…)\n\n\\(df = f(x+dx) - f(x) = dx^Tdx = dx^2\\)ë¼ í•´ë³´ì.(dx^2ì²˜ëŸ¼ ì“¸ ìˆ˜ ìˆë‹¤.)\nì—¬ê¸°ì„œ \\(dx\\)ê°€ ë²¡í„°ê°€ ì•„ë‹ˆë¼ ìŠ¤ì¹¼ë¼ë¼ê³  ìƒê°í•´ë³´ë©´ \\(dx\\)ë¡œ ì–‘ë³€ì„ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤.\nì–‘ë³€ì„ \\(dx\\)ë¡œ ë‚˜ëˆ„ê³  (2)ì— ì˜í•˜ì—¬ \\(\\frac{df}{dx} = dx = \\frac{\\partial f}{\\partial x^T}\\)ì´ë‹¤.\në¯¸ë¶„ì˜ ì •ì˜ì— ì˜í•´ \\(dx \\to 0\\) ì´ë¯€ë¡œ \\(dx^2\\)ì™€ ë¹„ìŠ·í•œ 2ì°¨ì´ìƒì˜ í•­ì— ëŒ€í•œ \\(\\frac{\\partial{f}}{\\partial{x^T}} = 0\\)ì´ë‹¤.\në”°ë¼ì„œ 2ì°¨ì´ìƒì˜ í•­ì´ ë‚˜ì˜¤ë©´ \\(\\frac{\\partial{f}}{\\partial{x^T}} = 0\\)ì´ ë  ê²ƒì´ë¯€ë¡œ ì œì™¸ë¥¼ ì‹œì¼œì¤˜ë„ ëœë‹¤.\nì‚¬ì‹¤ ë²¡í„°ë¯¸ë¶„ì´ê¸° ë•Œë¬¸ì— ë‚˜ëˆ ì¤„ ìˆ˜ ì—†ì–´ì„œ ì—„ë°€í•œ ì¦ëª…ì€ ì•„ë‹ˆë‹¤.\n\n\n\n\nì˜ˆì‹œ\n\n\\(f(x) = (y-Ax)^T(y-Ax)\\)ì¼ë•Œ \\(\\frac{\\partial{f}}{\\partial{x^T}}\\)ë¥¼ êµ¬í•´ë³´ì."
  },
  {
    "objectID": "posts/Linear Algebra/eigendecomposition.html",
    "href": "posts/Linear Algebra/eigendecomposition.html",
    "title": "[Linear Algebra] 6-2.EVD & Property",
    "section": "",
    "text": "ìœ íˆ¬ë¸Œ - í˜íœí•˜ì„ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ê°•ì˜ë¥¼ ì •ë¦¬í•˜ê¸° ìœ„í•´ ì‘ì„±í•œ ê¸€ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/eigendecomposition.html#ëŒ€ê°í™”-ê´€ë ¨-ë™ì¹˜ëª…ì œ-ì •ë¦¬",
    "href": "posts/Linear Algebra/eigendecomposition.html#ëŒ€ê°í™”-ê´€ë ¨-ë™ì¹˜ëª…ì œ-ì •ë¦¬",
    "title": "[Linear Algebra] 6-2.EVD & Property",
    "section": "ëŒ€ê°í™” ê´€ë ¨ ë™ì¹˜ëª…ì œ ì •ë¦¬",
    "text": "ëŒ€ê°í™” ê´€ë ¨ ë™ì¹˜ëª…ì œ ì •ë¦¬\n\n\\(A \\in \\mathbb{R}^{n \\times n}\\)ì´ë¼ í–ˆì„ë•Œ ë‹¤ìŒì˜ ëª…ì œëŠ” ë™ì¹˜ì´ë‹¤.\n\n\\[\\begin{aligned}\n&A\\text{ê°€ diagonalizableí•˜ë‹¤.} \\\\\n&\\longleftrightarrow A \\text{ê°€ eigen decompositionì´ ê°€ëŠ¥í•˜ë‹¤.}\\\\\n&\\longleftrightarrow V^{-1}\\text{ë¥¼ êµ¬í•  ìˆ˜ ìˆë‹¤.} \\\\\n&\\longleftrightarrow \\text{det(V)ê°€ ì¡´ì¬í•œë‹¤.} \\\\\n&\\longleftrightarrow \\text{Vì— ì¡´ì¬í•˜ëŠ” nê°œì˜ vectorëŠ” ì„ í˜•ë…ë¦½ì´ë‹¤.} \\\\\n&\\longleftrightarrow A \\text{ì— ëŒ€í•œ ì„ í˜•ë…ë¦½ì¸ eigenvectorê°€ nê°œ ì¡´ì¬í•œë‹¤.}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Linear Algebra/eigenvalue eigenvector/eigenvalue eigenvector.html",
    "href": "posts/Linear Algebra/eigenvalue eigenvector/eigenvalue eigenvector.html",
    "title": "[Linear Algebra] 6-1.Eigenvalue & Eigenvector",
    "section": "",
    "text": "ìœ íŠœë¸Œ - í˜íœí•˜ì„ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ ê°•ì˜ ì •ë¦¬ìš© ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/eigenvalue eigenvector/eigenvalue eigenvector.html#example",
    "href": "posts/Linear Algebra/eigenvalue eigenvector/eigenvalue eigenvector.html#example",
    "title": "[Linear Algebra] 6-1.Eigenvalue & Eigenvector",
    "section": "Example",
    "text": "Example\n\n\nê·¸ë¦¼ê³¼ ê°™ì€ ì„ í˜•ë³€í™˜ì„ ìƒê°í•´ë³´ì.\nëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì„ í˜•ë³€í™˜ëœ ë‹¤ìŒ ê·¸ ë°©í–¥,í¬ê¸°ëŠ” ì œë©‹ëŒ€ë¡œ ë³€í•œë‹¤.\nê·¸ëŸ¬ë‚˜ ë³´ë¼ìƒ‰ë²¡í„°ì™€ ì´ˆë¡ìƒ‰ë²¡í„°ëŠ” ê°ê° 3,1ë§Œí¼ í¬ê¸°ë§Œ ëŠ˜ì–´ë‚˜ë©° ë°©í–¥ì´ ë°”ë€Œì§€ ì•ŠëŠ” ë‹¨ìˆœí•œ ìŠ¤ì¹¼ë¼ë°°ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.\në”°ë¼ì„œ ë³´ë¼ìƒ‰ë²¡í„° \\([-1,1]^T\\),ì´ˆë¡ìƒ‰ë²¡í„° \\([1,1]^T\\)ëŠ” \\(A\\)ì˜ eigenvectorì´ë©° ëŒ€ì‘í•˜ëŠ” eigenvalueëŠ” ê°ê° 3,1ì„ì„ ì•Œ ìˆ˜ ìˆë‹¤.\nìœ„ì—ì„œëŠ” ì´ˆë¡ìƒ‰ ë³´ë¼ìƒ‰ë§Œ ì°¾ì•˜ì§€ë§Œ ì‚¬ì‹¤ ì´ ê²½ìš° eigenvectorëŠ” ë¬´í•œíˆ ë§ë‹¤.\në˜ë‹¤ë¥¸ ì–´ë–¤ ì„ í˜•ë³€í™˜ì˜ ê²½ìš°ì—ëŠ” ì•„ì˜ˆ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ê²½ìš°ë„ ìˆë‹¤.\nê·¸ë¦¼ìœ¼ë¡œ ë³´ë©´ ì•„ë˜ì™€ ê°™ë‹¤.(ì‚¬ì‹¤ ì˜ìƒìœ¼ë¡œ ë³´ëŠ”ê²Œ í›¨ì”¬ ì´í•´ê°€ ì˜ëœë‹¤. í˜íœí•˜ì„ë‹˜ ìœ íŠœë¸Œ ê°€ì„œ í•œë²ˆ ë³´ê¸¸ ì¶”ì²œí•´ìš”)\n\n\n\n\n\n\n\nbefore\n\n\n\n\n\n\n\nafter"
  },
  {
    "objectID": "posts/Linear Algebra/Least Squares/Least Squares.html",
    "href": "posts/Linear Algebra/Least Squares/Least Squares.html",
    "title": "[Linear Algebra] 5.Least Squares",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m,x \\in \\mathbb{R}^{n \\times 1},b \\in \\mathbb{R}^{m \\times 1}\\) ê°€ ì£¼ì–´ì§€ê³  ë°©ì •ì‹ \\(Ax = b\\)ë¥¼ ë§Œì¡±í•˜ëŠ” í•´ì¸ \\(x\\)ë¥¼ êµ¬í•  ìˆ˜ ì—†ì„ ë•Œ, \\(Ax\\)ê°€ \\(b\\)ì™€ ê°€ì¥ ë¹„ìŠ·í•˜ê²Œ í•˜ëŠ” \\(x\\)ë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/Least Squares/Least Squares.html#projection-matrix",
    "href": "posts/Linear Algebra/Least Squares/Least Squares.html#projection-matrix",
    "title": "[Linear Algebra] 5.Least Squares",
    "section": "projection matrix",
    "text": "projection matrix\nìœ„í•´ì„œ êµ¬í•œ \\(\\hat{x}\\)ë¥¼ \\(A\\hat{x}\\)ì— ëŒ€ì…í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\[A\\hat{x} = (A^TA)^{-1}A^Tb\\]\nìœ„ ì‹ì€ ìš°ë³€ì˜ \\(b\\)ì— \\(A(A^TA)^{-1}A^T\\)ë¥¼ ê³±í•˜ì—¬ \\(C(A)\\)ì—ì„œ \\(b\\)ì™€ ê°€ì¥ ë¹„ìŠ·í•˜ë©´ì„œ(ê±°ë¦¬ê°€ ê°€ì¥ ê°€ê¹Œìš°ë©´ì„œ) \\(b\\)ë¥¼ \\(C(A)\\)ì— ì •ì‚¬ì˜(projection) í•œ ë²¡í„° \\(A\\hat{x}\\)ì„ ì–»ìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ë”°ë¼ì„œ \\(A(A^TA)^{-1}A^T\\)ë¥¼ projection matrixë¼ ë¶€ë¥´ê³  \\(p_A\\)ë¡œ í‘œê¸°í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/nmf/non-negative-matrix-factorization.html",
    "href": "posts/Linear Algebra/nmf/non-negative-matrix-factorization.html",
    "title": "HIHO",
    "section": "",
    "text": "# Import basic library\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nfrom os import listdir\nimport time\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn import cluster\nfrom sklearn import decomposition\nfrom numpy.random import RandomState\nimport matplotlib.pyplot as plt\n\n\nrng = RandomState(0)\nfaces, _ = fetch_olivetti_faces(return_X_y=True, shuffle=True, random_state=rng)\nn_samples, n_features = faces.shape\n\ndownloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to C:\\Users\\22668\\scikit_learn_data\n\n\n\nfaces.shape\n\n(400, 4096)\n\n\n\nimg_shape = (64, 64)\nplt.imshow(faces[0].reshape(img_shape),cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x1e86be33160>\n\n\n\n\n\n\nimplementation\n\ndef nmf(X,p,n_iter=None):\n    m = X.shape[0]; n = X.shape[1]\n    W = np.random.rand(m,p) #W.shape = (m,p),randí•¨ìˆ˜ ì‚¬ìš©í•´ì„œ ë°˜ë“œì‹œ 0ë³´ë‹¤ í° ê°’ìœ¼ë¡œ ì´ˆê¸°í™” í•´ì•¼í•¨!\n    H = np.random.rand(p,n) #H.shape = (p,n)\n    #print(X.shape);#print(W.shape);#print(H.shape)\n    if n_iter is not None:\n        for i in range(n_iter):\n            W = W * ((X@H.T)/(W@H@H.T))\n            H = H * ((W.T@X)/(W.T@W@H))\n            \n    return X,H,W\n    \nX,H,W = nmf(faces,p = 40,n_iter = 2000)\n\n\nplt.imshow(H[16].reshape(img_shape),cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x1e8701cfca0>\n\n\n\n\n\n\nplt.imshow(X[0].reshape(img_shape),cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x1e8517cd1f0>\n\n\n\n\n\n\n_t = np.zeros(img_shape[0]*img_shape[1])\nfor i in range(len(W[0])):\n    _t = _t + W[0][i] * H[i]\n\nplt.imshow(_t.reshape(img_shape),cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x1e851836430>"
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html",
    "href": "posts/Linear Algebra/orthonormal matrix.html",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "",
    "text": "ìˆ˜íŠœë¸Œë‹˜ì˜ ì§êµëŒ€ê°í™”ê°€ëŠ¥(orthogonal diagonalizability)ì„ ë³´ê³  ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#orthonormal-matrix",
    "href": "posts/Linear Algebra/orthonormal matrix.html#orthonormal-matrix",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "Orthonormal matrix",
    "text": "Orthonormal matrix\nIn linear algebra, an orthogonal matrix, or orthonormal matrix, is a real square matrix whose columns and rows are orthonormal vectors. (from wikipedia)\n\ní–‰ë²¡í„°ë“¤ê³¼ ì—´ë²¡í„°ë“¤ì´ orthogonal(ì§êµ) + normal(í¬ê¸°ê°€ 1)ì¸ ì •ì‚¬ê°í–‰ë ¬ì„ orthonormal matrixë¼ í•œë‹¤.\northogonal matrix ë˜ëŠ” orthonormal matrixëŠ” ê°™ì€ í‘œí˜„ì´ë‹¤.\nê·¸ëŸ¬ë‚˜ ì§êµ + í¬ê¸°ê°€ 1ì¸ ë²¡í„°ê°€ ì—´ë²¡í„°ì´ê¸°ì— orthogonal + normal = orthonormalì´ ë” ì˜ë¯¸ë¥¼ ì‚´ë¦¬ëŠ” ë“¯í•˜ì—¬ ì—¬ê¸°ì„œëŠ” orthonormal matrixìš©ì–´ë¥¼ ì‚¬ìš©í•œë‹¤.\n\n\nexpression 1\n\northonormal matrix \\(Q\\)ì„ ë‚˜íƒ€ë‚´ëŠ” ì²« ë²ˆì§¸ í‘œí˜„ì€ ì•„ë˜ì™€ ê°™ë‹¤.\n\n\\[\\begin{aligned}\n&Q^TQ = QQ^T = I \\\\\n&\\text{where } I \\text{ is the identity matrix}\n\\end{aligned}\\]\n\nì™œ ì €ëŸ° í‘œí˜„ì‹ì´ ë‚˜ì˜¬ê¹Œ?\nì´ëŠ” ì •ì˜ë¥¼ í•¨ì¶•í•˜ê³  ì•„ì£¼ ì˜ í•¨ì¶•í•˜ê³  ìˆëŠ” í‘œí˜„ì´ë‹¤.\nì„ì˜ì˜ \\(Q \\in \\mathbb{R}^{n \\times n}\\)ê°€ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  í•´ë³´ì.\n\n\\[\\begin{aligned}\n&Q =\n\\begin{bmatrix}\nq_1 & q_2 & \\dots q_n\n\\end{bmatrix}\n\\end{aligned}\\]\n\nì—¬ê¸°ì„œ \\(q_i \\in \\mathbb{R}^{n \\times 1}(i=1,2,\\dots,n)\\)ì€ \\(Q\\)ì˜ column vectorì´ë‹¤.\në¨¼ì € \\(Q^TQ = I\\)ë¥¼ í™•ì¸í•´ë³´ì.\n\n\\[\\begin{aligned}\nQ^TQ =\n\\begin{bmatrix}\nq_1^T \\\\ q_2^T \\\\ \\vdots \\\\ q_n^T\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1 & q_2 & \\dots & q_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nq_1^T,q_1 & q_1^Tq_2 & \\dots & q_1^Tq_n \\\\\nq_2^T,q_1 & q_2^Tq_2 & \\dots & q_2^Tq_n \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nq_n^T,q_1 & q_n^Tq_2 & \\dots & q_n^Tq_n\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 & \\dots & 0\\\\\n0 & 1 & \\dots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1\\\\\n\n\n\\end{bmatrix}\n\\end{aligned}\\]\n\nì¢Œìš°ë³€ì´ ê°™ê¸°ë•Œë¬¸ì— ì•„ë˜ê°€ ìœ ë„ëœë‹¤.\n\n\\[\\begin{aligned}\nq_i^Tq_j =\n\\begin{cases}\n1 \\quad (i = j)\\\\\n0 \\quad (\\text{otherwise})\n\\end{cases}\n\\text{for all }i \\in (1,\\dots,n),\\,j \\in (1,\\dots,n)\n\\end{aligned}\\]\n\nì´ëŠ” ê°™ì€ ë²¡í„°ë¼ë¦¬ ë‚´ì ì„ í•˜ë©´ 1ì´ê³  ë‹¤ë¥¸ë²¡í„°ë¼ë¦¬ ë‚´ì í•˜ë©´ 0ì„ì„ ì˜ë¯¸í•œë‹¤.\nì¦‰, ì„ì˜ì˜ ë²¡í„°\\(q_i\\)ì˜ í¬ê¸°ëŠ” 1ì¸ unit vectorì´ë©° ì—´ë²¡í„°ë“¤ì€ orthogornal í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤.\n\n\\[\\begin{aligned}\n&\\text{for all }i \\in (1,...,n), j \\in (1,...,n) \\\\\n&|q_i| = \\sqrt{q_i} = \\sqrt{dot(q_i,q_i)} = 1 \\\\\n&dot(q_i,q_j)=0 \\longleftrightarrow q_i\\perp q_j\n\n\\end{aligned}\\]\n\në™ì¼í•œ ê³¼ì •ì„ í–‰ë²¡í„°ì— ëŒ€í•´ì„œë„ ìˆ˜í–‰í•˜ë©´ í–‰ë²¡í„°ë“¤ë„ orthonormalí•˜ë‹¤ëŠ” ê²ƒì„ ë³´ì¼ ìˆ˜ ìˆë‹¤.\n\n\n\nexpression 2\n\nexpression1ìœ¼ë¡œë¶€í„° orthonormal matrixì— ê´€í•œ ë‹¤ìŒì˜ ì‹ë„ ëŒì–´ë‚¼ ìˆ˜ ìˆë‹¤.\n\n\\[\\begin{aligned}\n&Q^T = Q^{-1} \\\\\n\\end{aligned}\\]\n\nì¦‰,ì–´ë–¤ í–‰ë ¬ì˜ transposeê°€ ê·¸ê²ƒì˜ inverseì™€ ê°™ìœ¼ë©´ orthonormal matrixë¼ëŠ” ê²ƒì´ë©° ì´ê²ƒë˜í•œ orthonormal matrixì˜ ë‹¤ë¥¸ í‘œí˜„ì´ë‹¤.\nì™œ ê·¸ëŸ°ê°€ í•˜ë©´ expression1ì—ì„œ \\(Q^TQ = QQ^T = I\\)ì˜€ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ \\(Q\\)ì˜ ì—­í–‰ë ¬\\(Q^{-1}\\)ëŠ” \\(Q^T\\)ì™€ ê°™ë‹¤. (Qì˜ ì—­í–‰ë ¬ì€ \\(Q^{-1}Q = QQ{-1} = I\\)ë¥¼ ë§Œì¡±í•˜ëŠ” í–‰ë ¬ì´ë‹¤.)"
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#orthogonally-similar",
    "href": "posts/Linear Algebra/orthonormal matrix.html#orthogonally-similar",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "orthogonally similar",
    "text": "orthogonally similar\n\në‹¤ìŒê³¼ ê°™ì€ ì‹ì„ ë§Œì¡±í•˜ë©´ CëŠ” Aì™€ orthogonally similar(ì§êµë‹®ìŒ)ì´ë‹¤.\n\n\\[\\begin{aligned}\n&C = Q^{-1}AQ \\\\\n&\\text{where } Q^{-1} = Q^T\n\\end{aligned}\\]\n\nì—¬ê¸°ì„œ \\(Q^{-1} = Q^T\\)ëŠ” orthonomal matrixì˜ ì •ì˜ì´ë‹¤.\nìœ— ì‹ì— ì˜í•´ì„œ Aë„ Cì™€ orthogonally similarì„ì„ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆë‹¤.\n\n\\[\\begin{aligned}\n&A = QCQ^{-1} \\\\\n&\\text{where } Q^{-1} = Q^T\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#orthogonally-diagonalize",
    "href": "posts/Linear Algebra/orthonormal matrix.html#orthogonally-diagonalize",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "orthogonally diagonalize",
    "text": "orthogonally diagonalize\n\nì„ì˜ì˜ ì •ì‚¬ê°í–‰ë ¬ \\(A,D\\)ì— ëŒ€í•´ì„œ ë‹¤ìŒì´ ë§Œì¡±í•œë‹¤ê³  í•´ë³´ì.\n\n\\[\\begin{aligned}\n&\n\\begin{aligned}\nD &= Q^{-1}AQ \\\\\n&= Q^TAQ \\\\\n\\end{aligned}\n\\\\\n&\\text{where } D = diag(d_1,d_2,\\dots,d_n) , Q^{-1} = Q^T\n\\end{aligned}\\]\n\nì´ì™€ ê°™ì´ \\(A\\)ì™€ ì§êµë‹®ìŒì¸ ëŒ€ê°í–‰ë ¬ \\(D\\)ê°€ ì¡´ì¬í• ë•Œ í–‰ë ¬\\(A\\)ê°€ ì§êµëŒ€ê°í™” ë˜ì—ˆë‹¤ ë¼ê³  í•œë‹¤.\në˜í•œ ì´ë•Œ ì •ì‚¬ê°í–‰ë ¬ \\(A\\)ëŠ” \\(P^{-1}AP\\)ê°€ ëŒ€ê°í–‰ë ¬\\(D\\)ê°€ ë˜ê²Œí•˜ëŠ” ê°€ì—­í–‰ë ¬ì´ì orthornomal matrixì¸ \\(P\\)ê°€ ì¡´ì¬í•˜ê¸°ë•Œë¬¸ì— â€œí–‰ë ¬\\(A\\)ëŠ” orthogonally diagonalizableí•˜ë‹¤â€ë¼ê³  ë§í•œë‹¤.\n\\(Q^T\\)ì™€ \\(Q\\)ì˜ ìë¦¬ë¥¼ ë°”ê¿”ì„œ ì¨ë„ ëœë‹¤. ì¦‰ \\(Q^{-1}\\)ë¥¼ ìƒˆë¡œìš´ Që¡œ ë³¸ë‹¤ë©´ ë‹¤ìŒê³¼ ê°™ì´ ì ì„ ìˆ˜ë„ ìˆë‹¤.\n\n\\[\\begin{aligned}\nD &= QAQ^{-1}\\\\\n&= QAQ^T\\\\\n&\\text{where } D = \\text{diag}(d_1,d_2,\\dots,d_n),Q^{-1} = Q^T\n\\end{aligned}\\]\n\në‹¨ì§€ notationì˜ ì°¨ì´ì¼ ë¿ì´ë‹¤. ì—­í–‰ë ¬ì˜ ê´€ê³„ì´ê¸° ë•Œë¬¸ì´ë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#ëª…ì œ1-aê°€-orthogonally-diagonalizable-rightarrow-a-at",
    "href": "posts/Linear Algebra/orthonormal matrix.html#ëª…ì œ1-aê°€-orthogonally-diagonalizable-rightarrow-a-at",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "ëª…ì œ1 : \\(A\\)ê°€ orthogonally diagonalizable \\(\\rightarrow A = A^T\\)",
    "text": "ëª…ì œ1 : \\(A\\)ê°€ orthogonally diagonalizable \\(\\rightarrow A = A^T\\)\n\ní–‰ë ¬\\(A\\)ê°€ orthogonally diagonalizableí•˜ë‹¤ê³  í•˜ì. ì¦‰ \\(D = P^TAP\\)ë¥¼ ë§Œì¡±í•˜ëŠ” ì§êµí–‰ë ¬ \\(P\\)ê°€ ì¡´ì¬í•œë‹¤. ì´ë•Œ \\(A^T\\)ë¥¼ ì „ê°œí•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n\\[\\begin{aligned}\n&A = PDP^{-1} = PDP^T\\\\\n&A^T = (PDP^T)^T = PD^TP^T = PDP^T = A\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#ëª…ì œ2-a-at-rightarrow-aëŠ”-orthogonally-diagonalizable",
    "href": "posts/Linear Algebra/orthonormal matrix.html#ëª…ì œ2-a-at-rightarrow-aëŠ”-orthogonally-diagonalizable",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "ëª…ì œ2 : \\(A = A^T \\rightarrow\\) \\(A\\)ëŠ” orthogonally diagonalizable",
    "text": "ëª…ì œ2 : \\(A = A^T \\rightarrow\\) \\(A\\)ëŠ” orthogonally diagonalizable\n\nìˆ˜í•™ì  ê·€ë‚©ë²•ìœ¼ë¡œ ì•„ë˜ì˜ ë‘ ê°€ì§€ë¥¼ ì¦ëª…í•˜ë©´ ëœë‹¤.\n\n\\(A \\in \\mathbb{R}^{1 \\times 1}\\) ì¼ë•Œ ì§êµëŒ€ê°í™”ê°€ ê°€ëŠ¥í•˜ë‹¤.(1)\n\\(A \\in \\mathbb{R}^{(n-1)\\times (n-1)}\\)ë•Œ ì§êµëŒ€ê°í™”ê°€ ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •í•˜ê³  \\(A \\in \\mathbb{R}^{n \\times n}\\)ë•Œ ì§êµëŒ€ê°í™”ê°€ ê°€ëŠ¥í•¨ì„ ë³´ì¸ë‹¤.(2)\nì„ì˜ì˜ \\(n\\) ê·¸ë¦¬ê³  \\(n-1\\)ì— ëŒ€í•´ì„œ ì„±ë¦½í•¨ì„ ì¦ëª…í–ˆê³  1ì—ëŒ€í•´ì„œ ì¦ëª…í–ˆê¸° ë•Œë¬¸ì— â€¦\n\n\n\\[\\begin{aligned}\n&\\mathbb{R}^{1 \\times 1} \\rightarrow \\mathbb{R}^{2 \\times 2}\\\\\n&\\mathbb{R}^{2 \\times 2} \\rightarrow \\mathbb{R}^{3 \\times 3}\\\\\n&\\mathbb{R}^{3 \\times 3} \\rightarrow \\mathbb{R}^{4 \\times 4}\\\\\n&\\quad\\quad\\quad\\vdots\\\\\n&\\mathbb{R}^{n-1 \\times n-1} \\rightarrow \\mathbb{R}^{n \\times n}\\\\\n&\\quad\\quad\\quad\\vdots\\\\\n\n\\end{aligned}\\]\n\nì´ë ‡ê²Œ ì¦ëª…í•˜ë©´ ëª¨ë“  \\(n\\) ê²°êµ­ì€ ëª¨ë“  ëŒ€ê°í–‰ë ¬ì— ëŒ€í•´ì„œ ì§êµëŒ€ê°í™”ê°€ ê°€ëŠ¥í•¨ì„ ë³´ì¼ ìˆ˜ ìˆë‹¤.\nëŠë‚Œ\n\ní•˜ë‚˜ì˜ ì¦ëª…ì„ ìœ„í•´ì„œ ëª¨ë“  ìì—°ìˆ˜ì— ëŒ€í•´ ë”¸ë ¤ìˆëŠ” ì¦ëª…ë“¤ì„ í•´ì•¼ë˜ë„¤?! \\(\\rightarrow\\) ì‘ì€ ê²ƒë¶€í„° ì°¨ë¡€ì°¨ë¡€ ì“°ëŸ¬ëœ¨ë ¤ë³´ì! \\(\\rightarrow\\) ì‘ì€ê±° í•˜ë‚˜í•˜ë‚˜ ë‹¤í•˜ê¸° í˜ë“œë‹ˆê¹Œ ì„ì˜ì˜ nì— ëŒ€í•´ì„œ í•´ë³´ì.!\n(1)ì„ ì¦ëª…í•˜ê³  (2)ë§Œ ì¦ëª…í•˜ë©´ ëª¨ë“  nì— ëŒ€í•´ì„œ ì¦ëª…í•  ìˆ˜ ìˆì–´ì„œ (2)ë¥¼ ì¦ëª…í•´ì„œ ëš«ëŠ” ëŠë‚Œ.\në„ë¯¸ë…¸ : í•˜ë‚˜ë¥¼ ì“°ëŸ¬ëœ¨ë¦¬ë©´ ê·¸ ë‹¤ìŒì´ ì“°ëŸ¬ì§€ëŠ” ê²ƒì„ ì•Œê³ ìˆì–´(ì¦ëª…í–ˆì–´). ì¦ëª…í•´ì•¼ ë  ë¬¸ì œëŠ” ë§¨ ì²˜ìŒë¶€í„° ë§¨ ë§ˆì§€ë§‰ê¹Œì§€ ì „ë¶€ ì“°ëŸ¬ëœ¨ë¦¬ë©´ ë˜ëŠ”ë¬¸ì œì•¼. ë§¨ ì²˜ìŒì´ ë¬´ë„ˆì§„ë‹¤ë©´ ë‹¤ìŒì— ì˜¤ëŠ” ëª¨ë“  ê²ƒë“¤ì´ ë¬´ë„ˆì§ˆêº¼ì•¼.\n\n\në¨¼ì € \\(A \\in \\mathbb{R}^{1 \\times 1}\\) ì¼ë•Œ ì§êµëŒ€ê°í™”ê°€ ê°€ëŠ¥í•¨ì„ ë³´ì¸ë‹¤. ë¨¼ì € \\(A\\)ì™€ \\(P\\)í–‰ë ¬ì„ ì¡ëŠ”ë‹¤.\n\\[\\begin{aligned}\n&A = \\begin{bmatrix}a\\end{bmatrix}\n\\in \\mathbb{R}^{1 \\times 1} \\\\\n&P = \\begin{bmatrix}1\\end{bmatrix}\n\\in \\mathbb{R}^{1 \\times 1}\n\\end{aligned}\\]\nì—¬ê¸°ì„œ \\(A=A^T\\)ì¸ ëŒ€ì¹­í–‰ë ¬ì´ë©° \\(P\\)ì˜ ê²½ìš° \\(P^{-1} = P^T\\)ê°€ ì„±ë¦½í•˜ëŠ” orthonomal matrixì´ë‹¤.\n\\[\\begin{aligned}\nD = P^{-1}AP = \\begin{bmatrix}a\\end{bmatrix}\n\\end{aligned}\\]\nDëŠ” ëŒ€ê°í–‰ë ¬ì´ë‹¤. ë”°ë¼ì„œ \\(D = P^{-1}AP\\)ë¥¼ ë§Œì¡±í•˜ëŠ” ê°€ì—­í–‰ë ¬ì´ì ì§êµí–‰ë ¬ì¸\\(P\\)ì™€ ëŒ€ê°í–‰ë ¬\\(D\\)ê°€ ì¡´ì¬í•˜ë¯€ë¡œ í–‰ë ¬\\(A \\in \\mathbb{R}^{1 \\times 1}\\)ëŠ” orthogonally diagonalizableí•˜ë‹¤.\n\nDëŠ” ëŒ€ê°í–‰ë ¬ì´ë‹¤. ë”°ë¼ì„œ \\(D = P^-1AP\\)ë¥¼ ë§Œì¡±í•˜ëŠ” ê°€ì—­í–‰ë ¬ì´ì ì§êµí–‰ë ¬ì¸\\(P\\)ì™€ ëŒ€ê°í–‰ë ¬\\(D\\)ê°€ ì¡´ì¬í•˜ë¯€ë¡œ í–‰ë ¬\\(A \\in \\mathbb{R}^{1 \\times 1}\\)ëŠ” orthogonally diagonalizableí•˜ë‹¤.\nì •ë¦¬2 ìì„¸í•œ ì¦ëª… ìƒëµâ€¦\në”°ë¼ì„œ ì–´ë–¤ í–‰ë ¬ì´ ëŒ€ì¹­í–‰ë ¬ \\(\\Longleftrightarrow\\)ì§êµëŒ€ê°í™”ê°€ ê°€ëŠ¥í•˜ë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#symmetric-matrixì˜-evdì™€-orthogonally-diagonalizableì˜-ì—°ê´€ì„±",
    "href": "posts/Linear Algebra/orthonormal matrix.html#symmetric-matrixì˜-evdì™€-orthogonally-diagonalizableì˜-ì—°ê´€ì„±",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "Symmetric matrixì˜ EVDì™€ orthogonally diagonalizableì˜ ì—°ê´€ì„±",
    "text": "Symmetric matrixì˜ EVDì™€ orthogonally diagonalizableì˜ ì—°ê´€ì„±\n\nì ê¹ ë„˜ì–´ê°€ê¸° ì „ì— ì¤‘ìš”í•œ ì¤‘ìš”í•œ ì‚¬ì‹¤ í•˜ë‚˜ë¥¼ ì§šê³  ë„˜ì–´ê°€ì.\nìš°ë¦¬ëŠ” ì •ë¦¬ë¥¼ í†µí•´ì„œ Symmetric matrixëŠ” ì§êµëŒ€ê°í™”ê°€ ê°€ëŠ¥í•˜ë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œê³  ìˆë‹¤.\ní•˜ì§€ë§Œ ì´ëŸ¬í•œ ì‚¬ì‹¤ë§Œ ì•Œê³ ìˆì„ë¿ ì‹¤ì œë¡œ ì–´ë–»ê²Œ \\(P\\)ì™€ \\(D\\)ë¥¼ êµ¬í•˜ëŠ”ì§€ëŠ” ì•Œì§€ ëª»í–ˆë‹¤.\nëŒ€ì¹­í–‰ë ¬ì˜ ê²½ìš° ì§êµëŒ€ê°í™”ëŠ” EVDë¥¼ í†µí•´ êµ¬í˜„í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ì •ë°©í–‰ë ¬ì˜ ê²½ìš°ì™€ ìœ ì‚¬í•˜ë‹¤.(ì •ë°©í–‰ë ¬ì˜ EVDì™€ orthogonally diagonalizable ì°¸ê³ )\n\nì •ë°©í–‰ë ¬ì˜ ê²½ìš° ëŒ€ê°í™”ëŠ” EVDë¥¼ í†µí•´ì„œ êµ¬í˜„í•  ìˆ˜ ìˆì—ˆë‹¤.\nëŒ€ì¹­í–‰ë ¬ì˜ ê²½ìš° ì§êµëŒ€ê°í™”ëŠ” EVDì—ì„œ \\(V^{-1} = V^T\\)ì¸ Vê°€ êµ¬í•´ì§€ê¸°ì— êµ¬í˜„í•  ìˆ˜ ìˆë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#ì‹ ê¸°í•˜ê³ -ì¤‘ìš”í•œ-ì‚¬ì‹¤ë“¤",
    "href": "posts/Linear Algebra/orthonormal matrix.html#ì‹ ê¸°í•˜ê³ -ì¤‘ìš”í•œ-ì‚¬ì‹¤ë“¤",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "ì‹ ê¸°í•˜ê³  ì¤‘ìš”í•œ ì‚¬ì‹¤ë“¤",
    "text": "ì‹ ê¸°í•˜ê³  ì¤‘ìš”í•œ ì‚¬ì‹¤ë“¤\n\nì‹ ê¸°í•œ ì‚¬ì‹¤1 : ëŒ€ì¹­í–‰ë ¬ AëŠ” rank1 ë§¤íŠ¸ë¦­ìŠ¤ë“¤ì˜ ê°€ì¤‘í•©ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\nEVDë¥¼ ê³„ì†í•´ì„œ ì „ê°œí•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n\\[\\begin{aligned}\nA &= V\\Lambda V^T =\n\\begin{bmatrix}\nq_1 & q_2 & q_3\n\\end{bmatrix}\n\\begin{bmatrix}\n\\lambda_1 & 0 & 0 \\\\\n0 & \\lambda_2 & 0 \\\\\n0 &  0 & \\lambda_3\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1^T \\\\ q_2^T \\\\ q_3^T\n\\end{bmatrix}\n=\n&\\begin{bmatrix}\n\\lambda_1q_1 & \\lambda_2q_2 & \\lambda_3q_3\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1^T \\\\ q_2^T \\\\ q_3^T\n\\end{bmatrix}\\\\\n&= \\lambda_1q_1q_1^T + \\lambda_2q_2q_2^T + \\lambda_3q_3q_3^T =\n\\sum_{i=1}^3 \\lambda_iq_iq_i^T\n\\end{aligned}\\]\n\nì—¬ê¸°ì„œë¶€í„° ì¬ë¯¸ë‚œ ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\\(q_iq_i^T\\)ëŠ” rankê°€ 1ì¸ matrixì´ë‹¤.\në§ˆì§€ë§‰ ìˆ˜ì‹ì€ ì´ëŸ¬í•œ rank-1 matrixë“¤ì„ ì ì ˆí•˜ê²Œ ìƒìˆ˜ë°°í•˜ê³  ë”í•˜ì—¬ í•©ì„ ì·¨í•˜ë©´ \\(A\\)ê°€ ë‚˜ì˜¨ë‹¤ëŠ” ê²ƒì´ë‹¤. ìƒìˆ˜ê°€ ê°ê°ì— ëŒ€ì‘í•˜ëŠ” rank-1 matrix \\(q_iq_i^T\\)ì˜ ì¤‘ìš”ë„,ê¸°ì—¬ë„ë¥¼ ê³ ë ¤í•œ ê°€ì¤‘ì¹˜ë¼ê³  ìƒê°í•œë‹¤ë©´?\në‹¤ìŒê³¼ ê°™ì€ í•´ì„ì„ í•  ìˆ˜ ìˆë‹¤.\n\nì–´ë–¤ ì •ë°©í–‰ë ¬\\(A\\)ëŠ” ì—¬ëŸ¬ê°œì˜ rank-1 matrixë“¤ì´ ì„ì¸ê±°ì•¼.\nê·¸ëŸ°ë° ê·¸ëƒ¥ ì„ì¸ê±´ ì•„ë‹ˆê³  ì¤‘ìš”í•œê±´ ë§ì´ ì•ˆì¤‘ìš”í•œê±´ ì ê²Œ ì„ì€ ê²ƒì´ ì •ë°©í–‰ë ¬ì´ì•¼!\nê·¸ë•Œì˜ ì¤‘ìš”ë„ëŠ” \\(\\lambda_i\\)ê°€ ì•Œë ¤ì¤˜.\n\nê·¸ë¦¼ìœ¼ë¡œ ë³´ë©´ ì´ëŸ°ëŠë‚Œì´ë‹¤.! ê·¸ë¦¼\në§Œì•½ì— ë°ì´í„°ì˜ í¬ê¸°ê°€ ë„ˆë¬´ ì»¤ì„œ ì••ì¶•ì„ í•´ì•¼í•˜ëŠ” ìƒí™©ì´ë¼ë©´ ì´ë¥¼ ì‘ìš©í•  ìˆ˜ ìˆë‹¤.\n\nì´ë¯¸ì§€ì˜ í¬ê¸°ê°€ ë„ˆë¬´í¬ë‹¤. ì¡°ê¸ˆ ì‚¬ì´ì¦ˆë¥¼ ì¤„ì´ê³  ì‹¶ë‹¤.\nì¤‘ìš”ë„ \\(\\lambda_i\\)ê°€ ìƒëŒ€ì ìœ¼ë¡œ ì‘ì€ matrixë“¤ì€ ì¡°ê¸ˆ ì œì™¸ë¥¼ í•´ë„ ê´œì°®ê² ì§€?\nì‹¤ì œë¡œ ëœë‹¤. ì¡°ê¸ˆ ì¤„ì—¬ë„ ì—¬ì „íˆ ì¸ì‹í•  ìˆ˜ ìˆëŠ” ì •ë„ì´ë‹¤.(ì¡°ê¸ˆ í™”ì§ˆì´ ì•ˆì¢‹ì•„ì§€ê¸´ í•œë‹¤.)\n\n\n\n\nì‹ ê¸°í•œ ì‚¬ì‹¤2 : ì„ í˜•ë³€í™˜ì´ ëŒ€ì¹­í–‰ë ¬ì´ë¼ë©´ ì…ë ¥ë²¡í„°ëŠ” ì§êµí•˜ëŠ” ê³ ìœ ë²¡í„°ë¡œ ë¶„í•´í•˜ê³  ì ì ˆí•˜ê²Œ ì¤„ì´ê³  ëŠ˜ë ¤ì„œ ì¬ì¡°í•©í•œ ëœë‹¤.\n\n\nìœ„ì˜ ê·¸ë¦¼ê³¼ ê°™ì´ ì„ í˜•ë³€í™˜ì´ Symmetric matrix \\(A = A^T\\)ì¸ ê²½ìš°ë¥¼ ê³ ë ¤í•´ë³´ì.\n\\(A\\)ë¥¼ EVDí•˜ê³  rank-1 matrixì˜ ê°€ì¤‘í•©ìœ¼ë¡œ í‘œí˜„í–ˆì„ë•Œ \\(x\\)ì— ëŒ€í•œ ì„ í˜•ë³€í™˜ \\(Ax\\)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n\\[Ax = \\lambda_1q_1q_1^Tx + \\lambda_2q_2q_2^Tx + \\lambda_3q_3q_3^Tx\\]\n\n\\(x \\in \\mathbb{R}^3\\)ì— ëŒ€í•˜ì—¬ \\(q_i^Tx\\)ëŠ” ê°ê°ì˜ ì§êµí•˜ëŠ” 3ì°¨ì› ê³µê°„ì˜ ë˜ë‹¤ë¥¸ ì •ê·œì§êµê¸°ì €ì— ëŒ€í•œ \\(x\\)ì˜ ì„±ë¶„ì„ ì˜ë¯¸í•œë‹¤.\n\\(q_i(i=1,2,3)\\)ë¡œ í‘œí˜„ëœ \\(x\\)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n\\[\\begin{aligned}\nx = (q_1^Tx)q_1 + (q_2^Tx)q_2 + (q_3^Tx)q_3 = \\begin{bmatrix}q_1^Tx \\\\ q_2^Tx \\\\ q_3^Tx\\end{bmatrix}\n\\end{aligned}\\]\n\nê°„ë‹¨í•˜ê²Œ ë§í•´ì„œ \\(q_1,q_2,q_3\\)ë¥¼ í¬í•¨í•˜ë„ë¡ \\(x\\)ë¥¼ ë¶„í•´í–ˆë‹¤ê³  ë³´ë©´ëœë‹¤.\n\n\n\nê·¸ëŸ¼ \\(Ax\\)ëŠ”?? \\(\\to Ax\\)ëŠ” ì§êµê¸°ì €ë¡œ \\(x\\)ë¥¼ ë¶„í•´í•˜ê³  ê° ì„±ë¶„ì— ëŒ€í•´ ì ì ˆí•œ ìŠ¤ì¹¼ë¼ë°°ë¥¼ ì·¨í•´ì„œ ë‹¤ì‹œ ì¡°í•©í•˜ì—¬ ì„ì€ ê²ƒìœ¼ë¡œ ì´í•´í•  ìˆ˜ ìˆë‹¤.\n\nì²«ë²ˆì§¸ ì„±ë¶„ì¸ \\((q_1^Tx)q_1\\)ì€ \\(\\lambda_1\\)ë§Œí¼ ê³±í•œë‹¤. \\(\\to \\lambda_1q_1q_1^Tx\\)\në‘ë²ˆì§¸ ì„±ë¶„ì¸ \\((q_2^Tx)q_2\\)ì€ \\(\\lambda_2\\)ë§Œí¼ ê³±í•œë‹¤. \\(\\to \\lambda_2q_2q_2^Tx\\)\nì„¸ë²ˆì§¸ ì„±ë¶„ì¸ \\((q_3^Tx)q_3\\)ì€ \\(\\lambda_3\\)ë§Œí¼ ê³±í•œë‹¤. \\(\\to \\lambda_3q_3q_3^Tx\\)\nê³±í•œê²ƒë“¤ì„ +í•´ì„œ ì„ìœ¼ë©´ \\(x\\)ë¥¼ ëŒ€ì¹­í–‰ë ¬ë¡œ ì„ í˜•ë³€í™˜í•œ \\(Ax\\)ì´ë‹¤.\n\nì¦‰, \\(Ax = \\lambda_1q_1q_1^Tx + \\lambda_2q_2q_2^Tx + \\lambda_3q_3q_3^Tx\\)ì´ë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "",
    "text": "ë”¥ëŸ¬ë‹ê³µë¶€ë°©,soohee410ë‹˜ì˜ ë¸”ë¡œê·¸ì„ ì½ê³  ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html#definition",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html#definition",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "Definition",
    "text": "Definition\në²¡í„°\\({\\bf{x}}\\)ì˜ ì´ì°¨í˜•ì‹(Quadratic form)ì€ ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•œë‹¤.\n\n\n\n\n\n\nDefinition of quadratic form\n\n\n\n\\[\\begin{aligned}\n&Q({\\bf{x}}) := {\\bf{x}}^TA{\\bf{x}}\\\\\n&\\text{where }\n\\begin{aligned}\n&A = A^T \\in \\mathbb{R}^{n \\times n},x \\in \\mathbb{R}^{n \\times 1}\n\\end{aligned}\n\\end{aligned}\\]\n\n\n\ní–‰ë ¬\\(A\\)ë¥¼ ì´ì°¨í˜•ì‹ì˜ í–‰ë ¬(matrix of quadratic form)ì´ë¼ê³  í•˜ë©° symmetric matrixì´ë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html#example",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html#example",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "Example",
    "text": "Example\n\\({\\bf{x}} = [x_1,x_2]^T\\) ,\\(A = \\begin{bmatrix}3 & -2 \\\\ -2 & 7\\end{bmatrix}\\)ì¼ë•Œ, quadratic formì„ êµ¬í•´ë³´ì.\n\\[\\begin{aligned}\n&Q({\\bf{x}}) = {\\bf{x}}^TA{\\bf{x}} =\n\\begin{bmatrix}\nx_1 & x_2\n\\end{bmatrix}\n\\begin{bmatrix}3 & -2 \\\\ -2 & 7\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2\n\\end{bmatrix}\n= \\begin{bmatrix}\nx_1 & x_2\n\\end{bmatrix}\n\\begin{bmatrix}\n3x_1-2x_2 \\\\ -2x_1 + 7x_2\n\\end{bmatrix}\n= 3x_1^2 -4x_1x_2 + 7x_2^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html#problem-setting",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html#problem-setting",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "Problem Setting",
    "text": "Problem Setting\n\nìœ„ì˜ ê·¸ë¦¼ì€ ì´ì°¨í˜•ì‹ì„ ì¢Œí‘œê³„ì—ì„œ í‘œí˜„í•œ ê²ƒì´ë‹¤. ê°ê°ì˜ ì´ì°¨í˜•ì‹ì—ì„œ ì›ì ìœ¼ë¡œë¶€í„° ê±°ë¦¬ê°€ ê°€ì¥ ë¨¼ ì¢Œí‘œë¥¼ ì°¾ì•„ë³´ì.\n\nì´ˆë¡ìƒ‰ quadtratic formì˜ ê²½ìš° ì§ê´€ì ìœ¼ë¡œ \\(x_2 = 0\\)ì„ ëŒ€ì…í•˜ì—¬ ê±°ë¦¬ê°€ ê°€ì¥ ë¨¼ ì¢Œí‘œë¥¼ ë°”ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤.\nê·¸ëŸ¬ë‚˜ ë¹¨ê°•ìƒ‰ quadtratic formì˜ ê²½ìš° ê±°ë¦¬ê°€ ê°€ì¥ ë¨¼ ê³³ì˜ ì¢Œí‘œë¥¼ êµ¬í•˜ê¸°ê°€ ì‰½ì§€ì•Šë‹¤.\ncrossproduct termì¸ \\(4x_1x_2\\)ê°€ ì¡´ì¬ë¡œ ì¸í•´ì„œ í˜„ì¬ì˜ ì¢Œí‘œì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ íšŒì „ëœ íƒ€ì›ì´ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\n\n\në§Œì•½ ë¹¨ê°•ìƒ‰ quadratic formì´ ìœ„ì™€ ê°™ì´ ìƒˆë¡œìš´ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ í‘œí˜„ëœë‹¤ë©´ ì–´ë–¨ê¹Œ?\n\nìƒˆë¡œìš´ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ ë¹¨ê°•ìƒ‰ íƒ€ì›ì€ íšŒì „ë˜ì§€ ì•Šì€ í˜•íƒœì´ê¸°ì—\ncross productí•­ì´ ì œê±°ë˜ì–´ ê°€ì¥ ë¨¼ ê³³ì˜ ì¢Œí‘œë¥¼ ì§ê´€ì ìœ¼ë¡œ \\(y2=0\\)ì„ ëŒ€ì…í•˜ì—¬ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.\nì´ì™€ ê°™ì´ ì¶•ì„ ë°”ê¿”ì„œ í•˜ì—¬ cross productí•­ì„ ì œê±°í•˜ë©´ ìµœì†Œ ë˜ëŠ” ìµœëŒ€ê°€ ë˜ëŠ” ì¢Œí‘œë¥¼ ë°”ë¡œ ë³´ë‹¤ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆê²Œ í•´ì¤˜ì„œ ìµœì í™” ê´€ì ì—ì„œ ì´ì ì´ ë§ë‹¤.\n\nê·¸ë ‡ë‹¤ë©´ ì£¼ì–´ì§„ ì‹ì„ ìƒˆë¡œìš´ ì¶•ì„ ê¸°ì¤€ìœ¼ë¡œ ê°„ë‹¤í•˜ê²Œ ë°”ê¾¸ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ ë­˜ê¹Œ?\n\nì´ëŠ” ì´ì°¨í˜•ì‹ì˜ ë³€ìˆ˜ë³€í™˜ì„ í™œìš©í•˜ë©´ ëœë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html#method",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html#method",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "Method",
    "text": "Method\n\nì´ì°¨í˜•ì‹ì˜ ë³€ìˆ˜ë³€í™˜ì—ì„œ \\(\\bf{x}\\)ëŠ” \\(\\bf{{\\bf{y}}}\\)ì™€ ëŒ€ì¹­í–‰ë ¬ \\(A\\)ì˜ ê³ ìœ ë²¡í„°ì˜ ëª¨ìŒì¸ \\(P\\)ì˜ ê³±ì´ë¼ê³  ê°€ì •í•œë‹¤.\nì´ë•Œ \\(P\\)ëŠ” í–‰ë ¬\\(A\\)ê°€ ëŒ€ì¹­í–‰ë ¬ì´ë©° ë”°ë¼ì„œ ì§êµëŒ€ê°í™”ê°€ ê°€ëŠ¥í•˜ë¯€ë¡œ \\(P\\)ëŠ” \\(P^{-1} = P^T\\)ë¥¼ ë§Œì¡±í•˜ëŠ” orthonormal matrixì´ë‹¤.\n\n\\(A=A^T\\)ì¸ ëŒ€ì¹­í–‰ë ¬ì€ ì§êµëŒ€ê°í™”ê°€ ê°€ëŠ¥í•˜ë©° ê³ ìœ ë²¡í„°ë“¤ì€ ì„œë¡œê°„ì— ì§êµí•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ê¸°ì–µí•˜ì.\n\n\n\\[\\begin{aligned}\n&{\\bf{x}} = P{\\bf{{\\bf{y}}}} \\\\\n&{\\bf{{\\bf{y}}}}=P^{-1}{\\bf{x}}\\\\\n&\\text{where }P^{-1} = P^T\n\\end{aligned}\\]\n\nê³ ìœ ë²¡í„°ë“¤ì˜ ì§‘í•©ì¸ \\(B = \\{p_1,p_2,\\dots,p_n\\}\\)ë¥¼ êµ¬ì„±í•´ë³´ì.\n\\(B\\)ëŠ” \\(\\mathbb{R}^n\\)ê³µê°„ì˜ í‘œì¤€ê¸°ì € \\(\\{e_1,e_2,\\dots,e_n\\}\\)ê°€ ì•„ë‹Œ ë˜ë‹¤ë¥¸ ì •ê·œì§êµê¸°ì €ì´ë‹¤.(Bì˜ spanì€ \\(\\mathbb{R}^n\\)ì´ê¸° ë•Œë¬¸ì´ë‹¤.)\n\nê·¸ë ‡ë‹¤ë©´ ì—¬ê¸°ì„œ \\({\\bf{y}}\\)ëŠ” ë­˜ê¹Œ?\n\n\\({\\bf{x}}\\)ë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ì •ê·œì§êµê¸°ì €ì´ì \\(A\\)ì˜ ê³ ìœ ë²¡í„° ì§‘í•©ì¸ \\(B\\)ë¡œ í‘œí˜„í•œ ì¢Œí‘œ\\([{\\bf{x}}]_B\\)ì´ë‹¤.\n\n\\[\\begin{aligned}\n&{\\bf{x}} = P{\\bf{y}}\\\\\n&\\Longleftrightarrow {\\bf{x}} = p_1y_1 + p_2y_2 + \\dots + p_ny_n\\\\\n&\\Longleftrightarrow {\\bf{y}} = [{\\bf{x}}]_B\n\\end{aligned}\\]\nìœ„ì—ì„œ ê°€ì •í•œ ì‚¬ì‹¤ì„ ì´ì°¨í˜•ì‹ì— ëŒ€ì…í•´ë³´ì\n\\[\\begin{aligned}\n&{\\bf{x}}^TA{\\bf{x}} = (P{\\bf{y}})^TA(P{\\bf{y}}) = {\\bf{y}}^TP^TAP{\\bf{y}} = {\\bf{y}}^TD{\\bf{y}}\\\\\n&\\text{where } D = P^TAP,P^T = P^{-1}\n\\end{aligned}\\]\n\n\\(A\\)ëŠ” symmetricí•˜ë¯€ë¡œ ì§êµëŒ€ê°í™”\\(D = P^TAP\\)ê°€ ê°€ëŠ¥í•˜ë‹¤.\në”°ë¼ì„œ ë²¡í„°\\({\\bf{x}}\\)ì˜ quadratic formì€ ë²¡í„°\\({\\bf{y}}\\)ì˜ quadratic formì´ ë˜ê³ \n\\(A\\)ì˜ ê³ ìœ³ê°’ì„ ëª¨ì•„ë†“ì€ ëŒ€ê°í–‰ë ¬ \\(D\\)ê°€ quadratic formì˜ matrixê°€ ë¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\n\\[\\begin{aligned}\n{\\bf{x}}^TA{\\bf{x}}= {\\bf{y}}^TD{\\bf{y}} =\n\\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_n^T\n\\end{bmatrix}\n\\begin{bmatrix}\n\\lambda_1 & 0 &\\dots & 0 \\\\\n0 & \\lambda_2 &\\dots & 1 \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\lambda_n \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 & y_2 & \\dots & y_n\n\\end{bmatrix}\n= \\lambda_1y_1^2 + \\lambda_2y_2^2 + \\dots + \\lambda_ny_n^2\n\\end{aligned}\\]\n\nì¡°ê¸ˆ ë” ì „ê°œí•´ë³´ë©´ ìœ„ì™€ ê°™ì€ \\(y\\)ì— ê´€í•œ ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚œë‹¤.\n\në³€í™˜ê²°ê³¼ \\(D\\)ê°€ ëŒ€ê°í–‰ë ¬ì´ê¸°ì— í–‰ë ¬ê³±ì˜ ê²°ê³¼ë¡œ crossproduct termì´ ë‚˜íƒ€ë‚˜ì§€ ì•Šì•˜ìœ¼ë©°\n\\(B = \\{p_1,p_2,\\dots,p_n\\}\\)ë¥¼ ê¸°ì €ë¡œ í•˜ëŠ” ìƒˆë¡œìš´ ì¢Œí‘œì¶•ì—ì„œ yì— ê´€í•œ ë°©ì •ì‹ì´ í‘œí˜„ë˜ì—ˆë‹¤.\n\në”°ë¼ì„œ ì›ì ì—ì„œ ê±°ë¦¬ê°€ ê°€ì¥ ë¨¼ ì§€ì ì„ ë¹„êµì  ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.\n\n\\({\\bf{y}}^TD{\\bf{y}} = \\lambda_1y_1^2 + \\lambda_2y_2^2\\)ì™€ ê°™ì€ íƒ€ì›ì´ë¼ë©´\n$y_2 = 0 $ì„ ëŒ€ì…í•˜ì—¬ ì‰½ê²Œ êµ¬í•  ìˆ˜ ìˆë‹¤.\në¬¼ë¡  ê³ ìœ³ê°’,ê³ ìœ³ë²¡í„°ë¥¼ êµ¬í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•¨ì„ ìŠì§€ë§ì."
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html#example-orthogonally-diagonalize",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html#example-orthogonally-diagonalize",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "Example : orthogonally diagonalize",
    "text": "Example : orthogonally diagonalize\n\\(A =\\begin{bmatrix}4 & 2 & 2 \\\\ 2 & 4 & 2 \\\\ 2 & 2 & 4\\end{bmatrix}\\)ì¸ symmetric matrixë¥¼ ì§êµëŒ€ê°í™”í•´ë¼.\në¨¼ì € í–‰ë ¬ Aì˜ ê³ ìœ³ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ êµ¬í•´ë³´ì. ë‹¤ìŒê³¼ ê°™ì€ ë°©ì •ì‹ì„ ë§Œì¡±í•˜ëŠ” \\(\\lambda\\)ê°€ ê³ ìœ³ê°’ì´ë‹¤.\n\\[\\begin{aligned}\n&\\text{det}(\\lambda I - A) = 0 \\\\\n&\\Longleftrightarrow\n\\begin{vmatrix}\n\\lambda -4 & -2 & -2 \\\\\n-2 & \\lambda -4 & -2 \\\\\n-2 & -2 & \\lambda -4\n\\end{vmatrix} = 0 \\\\\n&\\Longleftrightarrow(\\lambda-2)^2(\\lambda-8) = 0\n\\end{aligned}\\]\në”°ë¼ì„œ ê³ ìœ³ê°’ì€ 2ì™€ 8ì´ë‹¤. ê°ê°ì˜ ê³ ìœ³ê°’ì— ëŒ€í•œ ë‹¨ìœ„ê³ ìœ ë²¡í„°ë¥¼ êµ¬í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nì´ë•Œ \\(p_2\\)ê°€ ë°”ë¡œ êµ¬í•´ì§€ì§€ëŠ” ì•Šìœ¼ë©° ì•„ì§ ê³µë¶€í•˜ì§€ ëª»í•œ ë°©ë²•ì´ë‹¤.\nì„œë¡œê°„ì— ì§êµì¸ ê³ ìœ ë²¡í„°ë“¤ì„ êµ¬ì„±í•˜ëŠ” ë°©ë²•ì€ ë‚˜ì¤‘ì— ê³µë¶€ë¥¼ ë” í•´ì•¼í•  ë“¯ í•˜ë‹¤.(ê·¸ëŒìŠˆë¯¸íŠ¸?ì§êµì—¬ê³µê°„?)\n\n\\[\\begin{aligned}\n&\n\\lambda=2 : p_1 = \\begin{bmatrix}-1/\\sqrt{2} & 1/\\sqrt{2}& 0\\end{bmatrix}^T,p_2 = \\begin{bmatrix}-1/\\sqrt{6}&-1/\\sqrt{6}&2/\\sqrt{6}\\end{bmatrix}^T \\\\\n&\\lambda=8 : p_3 = \\begin{bmatrix}1/\\sqrt{3} & 1/\\sqrt{3} & 1/\\sqrt{3}\\end{bmatrix}^T\n\\end{aligned}\\]\nëŒ€ê°í–‰ë ¬ Dë¥¼ êµ¬í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\nD = P^{-1}AP = \\text{diag}(2,2,8)\n\\end{aligned}\\]\n\nimport torch\nA = torch.FloatTensor([\n    [4,2,2],\n    [2,4,2],\n    [2,2,4]\n])\np_1 = torch.FloatTensor(\n    [[-1/2**(1/2),1/2**(1/2),0]]\n).T\np_2 = torch.FloatTensor(\n    [[-1/6**(1/2),-1/6**(1/2),2/6**(1/2)]]\n).T\np_3 = torch.FloatTensor(\n    [[1/3**(1/2),1/3**(1/2),1/3**(1/2)]]\n).T\nP = torch.concat([p_1,p_2,p_3],axis=1)\nP_inv = P.inverse()\n#ëŒ€ê°í™”\nD = P_inv @ A @ P\nprint(\"ëŒ€ê°í–‰ë ¬ D\\n\",D.round())\n\nëŒ€ê°í–‰ë ¬ D\n tensor([[2., 0., -0.],\n        [-0., 2., -0.],\n        [0., 0., 8.]])"
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html#example-change-of-variable-in-quadratic-form",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html#example-change-of-variable-in-quadratic-form",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "Example : Change of Variable in Quadratic Form",
    "text": "Example : Change of Variable in Quadratic Form\nìœ„ì—ì„œ êµ¬í•œ \\({\\bf{x}} = [x_1,x_2]^T\\) ,\\(A = \\begin{bmatrix}3 & -2 \\\\ -2 & 7\\end{bmatrix}\\)ì¼ë•Œ, quadratic formì„ ë³€ìˆ˜ë³€í™˜í•´ë³´ì.\n\\[\\begin{aligned}\n&Q({\\bf{x}}) = {\\bf{x}}^TA{\\bf{x}}\n= 3x_1^2 -4x_1x_2 + 7x_2^2\n\\end{aligned}\\]\në¨¼ì € ê³ ìœ³ê°’ê³¼ ê³ ìœ ë²¡í„°ë¥¼ êµ¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nì´ë•Œ ê³ ìœ ë²¡í„°ëŠ” ëª¨ë‘ ë‹¨ìœ„ë²¡í„°ì´ì orthonormalí•œ ë²¡í„°ë¡œ ì¡ì.\n\n\\[\\begin{aligned}\n&\n\\lambda=3 : p_1 = \\begin{bmatrix}2/\\sqrt{5} & -1/\\sqrt{5}\\end{bmatrix}^T\\\\\n&\\lambda=-7 : p_2 = \\begin{bmatrix}1/\\sqrt{5} & 2/\\sqrt{5} \\end{bmatrix}^T\n\\end{aligned}\\]\nëŒ€ê°í™”ë¥¼ ìœ„í•´ \\(A\\)ë¥¼ ëŒ€ê°í™”ì‹œí‚¤ëŠ” í–‰ë ¬ \\(P\\)ë¥¼ êµ¬ì„±í•˜ì.ì´ë•Œ \\(P\\)ì— ëŒ€í•´ì„œ ëª‡ ê°€ì§€ë¥¼ ê¸°ì–µí•˜ì.\n\n\\(P\\)ëŠ” \\(A\\)ì˜ ê³ ìœ ë²¡í„°ë“¤ì„ ëª¨ì•„ë†“ì€ í–‰ë ¬ì´ì\n\\(P^{-1} = P^T\\)ë¥¼ ë§Œì¡±í•˜ëŠ” orthonormal matrixì´ë‹¤.(Aê°€ symmetric matrixì´ê¸°ì— ê°€ëŠ¥í•˜ë‹¤.)\n\n\\[P = \\begin{bmatrix}2/\\sqrt{5} & 1/\\sqrt{5}\\\\-1\\sqrt{5} & 2/\\sqrt{5}\\end{bmatrix}\\]\në”°ë¼ì„œ ëŒ€ê°í™”ë¥¼ í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[D = P^{-1}AP = \\begin{bmatrix}3 & 0 \\\\ 0 & -7\\end{bmatrix}\\]\nì´ì œ ìœ„ì—ì„œ êµ¬í•œ \\(P\\)ë¡œ ë³€ìˆ˜ë³€í™˜ì„ í•  ìˆ˜ ìˆë‹¤. \\({\\bf{x}}\\)=\\(P{\\bf{y}}\\)ë¥¼ quadratic formì— ëŒ€ì…í•´ë³´ì.\n\\[\\begin{aligned}\nQ({\\bf{x}}) &= x^TAx = (Py)^TA(Py) = y^TP^TAPy = y^TP^TAPy\\\\\n&=y^TDy = 3y_1^2 - 7y_2^2\n\\end{aligned}\\]\nì´ëŸ¬í•œ ë³€ìˆ˜ë³€í™˜ì„ ì‹œê°í™” í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  í•œë‹¤.\n\n\nxì— ëŒ€í•œ quadratic formì¸ \\({\\bf{x}}^TA{\\bf{x}}\\)ê°€ ì¡´ì¬í•˜ë©° ê·¸ ê°’ì€ 16ì´ë‹¤.\n\\({\\bf{x}} = P{\\bf{y}}\\)ë¥¼ xì— ëŒ€í•œ 2ì°¨í˜•ì‹ì— ëŒ€ì…í•˜ì—¬ yì— ëŒ€í•œ quadratic formìœ¼ë¡œ ë³€ìˆ˜ë¥¼ ë°”ê¿¨ìœ¼ë©° ê·¸ ê°’ë„ ë§ˆì°¬ê°€ì§€ë¡œ 16ì´ë‹¤.\n\\({\\bf{x}}^TA{\\bf{x}} = {\\bf{y}}^TD{\\bf{y}} = 16\\)ìœ¼ë¡œ ê°™ì€ ê°’ì— mappingë¨ì„ ê¸°ì–µí•˜ì.(ë³€ìˆ˜ë³€í™˜ì„ í•´ë„ ë‚˜íƒ€ë‚´ëŠ” í˜•íƒœ(íƒ€ì›)ëŠ” ê·¸ëŒ€ë¡œì´ë‹¤.)"
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html",
    "title": "[Linear Algebra] 3.rank & null space(kernel)",
    "section": "",
    "text": "ìœ íˆ¬ë¸Œ - í˜íœí•˜ì„ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ê°•ì˜ë¥¼ ì •ë¦¬í•˜ê¸° ìœ„í•´ ì‘ì„±í•œ ê¸€ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#ì˜ˆì œ",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#ì˜ˆì œ",
    "title": "[Linear Algebra] 3.rank & null space(kernel)",
    "section": "ì˜ˆì œ",
    "text": "ì˜ˆì œ\n\\[\\begin{pmatrix}\n1&2&3\\\\\n0&0&0\n\\end{pmatrix}\\]\nìœ„ì— ìˆëŠ” í–‰ë ¬ì—ì„œ ì„ í˜•ë…ë¦½ì¸ ì—´ë²¡í„°ì˜ ê°¯ìˆ˜ = 1ì´ë‹¤. ë˜í•œ ì„ í˜•ë…ë¦½ì¸ ì—´ë²¡í„°ì˜ ê°¯ìˆ˜ = ì„ í˜•ë…ë¦½ì¸ í–‰ë²¡í„°ì˜ ê°¯ìˆ˜ = ì—´ê³µê°„ì˜ ì°¨ì› = í–‰ê³µê°„ì˜ ì°¨ì›ì´ë¯€ë¡œ ë­í¬ì •ë¦¬ë„ ì„±ë¦½í•œë‹¤.\nìœ„ì™€ ê°™ì€ \\(2 \\times 3\\) í–‰ë ¬ì€ ë­í¬ë³´ë‹¤ í–‰,ì—´ì˜ ê°¯ìˆ˜ê°€ ë§ì´ ë¶€ì¡±í•˜ë¯€ë¡œ rank-deficient ë¼ê³  í•œë‹¤.\n\\[\\begin{pmatrix}\n1&0&1\\\\\n0&1&1\n\\end{pmatrix}\\]\nìœ„ì— ìˆëŠ” í–‰ë ¬ì—ì„œ ì„ í˜•ë…ë¦½ì¸ ì—´ë²¡í„°ì˜ ê°¯ìˆ˜ = 2ì´ë‹¤. ë˜í•œ ì„ í˜•ë…ë¦½ì¸ ì—´ë²¡í„°ì˜ ê°¯ìˆ˜ = ì„ í˜•ë…ë¦½ì¸ í–‰ë²¡í„°ì˜ ê°¯ìˆ˜ = ì—´ê³µê°„ì˜ ì°¨ì› = í–‰ê³µê°„ì˜ ì°¨ì›ì´ë¯€ë¡œ ë­í¬ì •ë¦¬ë„ ì„±ë¦½í•œë‹¤.\nìœ„ì™€ ê°™ì€ í–‰ë ¬ì€ í–‰ì˜ ê°¯ìˆ˜ë§Œí¼ ë­í¬ê°€ ë‹¤ ì°¨ìˆìœ¼ë¯€ë¡œ full row rankë¼ í•œë‹¤."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#ìš©ì–´ì •ë¦¬",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#ìš©ì–´ì •ë¦¬",
    "title": "[Linear Algebra] 3.rank & null space(kernel)",
    "section": "ìš©ì–´ì •ë¦¬",
    "text": "ìš©ì–´ì •ë¦¬\n\\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m \\Rightarrow\\) ì—´ë²¡í„°ê°€ ëª¨ë‘ ì„ í˜•ë…ë¦½,full column rank,ìœ„ì•„ë˜ë¡œ ê¸¸ì­‰í•˜ê³  ì–‘ì˜†ì€ ì¢ì€ ì§ì‚¬ê°í–‰ë ¬ \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n>m \\Rightarrow\\) í–‰ë²¡í„°ê°€ ëª¨ë‘ ì„ í˜•ë…ë¦½,full row rank,ìœ„ì•„ë˜ê°€ ì¢ê³  ì¢Œìš° ì–‘ì˜†ìœ¼ë¡œ ê¸¸ì­‰í•œ ì§ì‚¬ê°í–‰ë ¬ \\(A \\in \\mathbb{R}^{n \\times n},\\text{rank}(A) = n \\Rightarrow\\) í–‰ë°±í„°,ì—´ë²¡í„°ê°€ ëª¨ë‘ ì„ í˜•ë…ë¦½,full rank,ìœ„,ì•„ë˜,ì–‘,ì˜†ì˜ ê¸¸ì´ê°€ ëª¨ë‘ ê°™ì€ ì •ì‚¬ê°í–‰ë ¬ \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) < \\text{min}(n,m)\\) = ì„ í˜•ì¢…ì†ì¸ í–‰ë²¡í„°,ì—´ë²¡í„° ë°˜ë“œì‹œ ì¡´ì¬,rank deficient"
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#ì˜ˆì œ1",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#ì˜ˆì œ1",
    "title": "[Linear Algebra] 3.rank & null space(kernel)",
    "section": "ì˜ˆì œ1)",
    "text": "ì˜ˆì œ1)\ní–‰ë ¬ A = \\(\\begin{pmatrix}1&0&1 \\\\0&1&1\\end{pmatrix}\\)ì¼ ë•Œ, í–‰ë ¬Aì˜ ì˜ê³µê°„ì€?\në¨¼ì € ë§Œì¡±í•˜ëŠ” \\(x\\)ë¥¼ ì°¾ì•„ë³´ì. ì–´ë–¤ ë°©ë²•ì´ë˜ ì‚¬ìš©ê°€ëŠ¥í•˜ì§€ë§Œ ë¬¸ì œê°€ ê°„ë‹¨í•˜ë¯€ë¡œ ì§ê´€ì ìœ¼ë¡œ í’€ì´í•œë‹¤.\n\\[\\begin{aligned}\n&Ax = x_1\\begin{pmatrix}1\\\\0\\end{pmatrix} + x_2\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + x_3\\begin{pmatrix}1\\\\1\\end{pmatrix} = {\\bf 0} \\\\\n&\\Longleftrightarrow\n\\begin{cases}\nx_1 + x_3 = 0 \\\\\nx_2 + x_3 = 0\n\\end{cases}\n\\\\\n&\\Longleftrightarrow x_3 = t,x_2 = -t,x_1 = t \\\\\n&\\Longleftrightarrow x = t\\begin{pmatrix}1\\\\-1\\\\1\\end{pmatrix} \\\\\n&\\therefore \\text{N}(A) = \\Bigg\\{t\\begin{pmatrix}1\\\\-1\\\\1 \\end{pmatrix}|t \\in \\mathbb{R}\\Bigg\\}\n\\end{aligned}\\]\në°©ì •ì‹ì˜ í•´ë¥¼ êµ¬í•´ë³´ë‹ˆ 1)\\([1,-1,1]^T\\)ì˜ spanì´ ì˜ê³µê°„ì´ë©° 2)ì˜ê³µê°„ì€ í–‰ë ¬ê³±ì— ì˜í•´ì„œ(ì¤‘ê°„ì—ì„œ ì°¨ì›ì¼ì¹˜ \\(2 \\times 3 ê³¼ 3 \\times 1\\)) í–‰ë²¡í„°ê°€ ì¡´ì¬í•˜ëŠ” ì°¨ì›ì¸ 3ì°¨ì› ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì¸ 1ì°¨ì› ë²¡í„°ê³µê°„ì„ ìƒì„±í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.\nì˜ê³µê°„ì˜ ì›ì†Œ(ë°©ì •ì‹ì˜ í•´)ëŠ” ë¬´ìˆ˜íˆ ë§ìŒì´ ìëª…í•œë° ì™œëƒí•˜ë©´ \\(Ax = 0\\)ì„ ë§Œì¡±í•˜ëŠ” ì„ì˜ì˜ xë²¡í„°ì— ëŒ€í•œ ìŠ¤ì¹¼ë¼ë°°ì— ëŒ€í•´ ë‹¤ìŒì´ ì„±ë¦½í•˜ê¸° ë•Œë¬¸ì´ë‹¤.\n\\[\\begin{aligned}\n&Ax = {\\bf 0} \\\\\n&\\Longleftrightarrow cAx = c{\\bf 0} = {\\bf 0} \\\\\n&\\Longleftrightarrow A(cx) ={\\bf 0} \\\\\n\\end{aligned}\\]\në”°ë¼ì„œ \\(x\\)ì˜ ìŠ¤ì¹¼ë¼ë°°ì¸ \\(cx\\)ë„ \\(A\\)ì™€ ê³±í•´ì ¸ì„œ \\({\\bf 0}\\) ë§Œë“¤ê¸° \\(x\\)ì˜ ìŠ¤ì¹¼ë¼ë°°ë„ ì˜ê³µê°„ì˜ ì›ì†Œì´ë‹¤. \nrank-nulity theoreomë„ ì„±ë¦½í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì˜ê³µê°„ì˜ ì°¨ì›ì€ ì˜ê³µê°„ì˜ ê¸°ì €ì˜ ê°¯ìˆ˜ì¸ë° \\([1,-1,1]^T\\)ì´ ê¸°ì €ì˜ ì¡°ê±´ì¸ 1)ì„ í˜•ìƒì„± = ë²¡í„°ê³µê°„ 2)ì„ í˜•ë…ë¦½ ì´ë¼ëŠ” ë‘ ì¡°ê±´ì„ ë§Œì¡±í•˜ë¯€ë¡œ ì°¨ì›ì€ 1ì´ë‹¤.ë­í¬ëŠ” ë‹¤ë¥¸ë°©ì‹ìœ¼ë¡œë„ êµ¬í•  ìˆ˜ ìˆì§€ë§Œ í–‰ë ¬ì´ ê°„ë‹¨í•´ì„œ ë°”ë¡œ2ì„ì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ë‹¤ \\[\\text{nulity}(A) + \\text{rank}(A)= n   \\Longleftrightarrow  1 + 2 = 3\\]"
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#ì˜ˆì œ2",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#ì˜ˆì œ2",
    "title": "[Linear Algebra] 3.rank & null space(kernel)",
    "section": "ì˜ˆì œ2)",
    "text": "ì˜ˆì œ2)\ní–‰ë ¬ A = \\(\\begin{pmatrix}1&2&3 \\\\0&0&0\\end{pmatrix}\\)ì¼ ë•Œ, í–‰ë ¬Aì˜ ì˜ê³µê°„ì€?\në§ˆì°¬ê°€ì§€ë¡œ ë§Œì¡±í•˜ëŠ” \\(x\\)ë¥¼ ë¨¼ì € ì°¾ì•„ë³´ì.\n\\[\\begin{aligned}\n&Ax = x_1\\begin{pmatrix}1\\\\0\\end{pmatrix} + x_2\\begin{pmatrix}2 \\\\ 0\\end{pmatrix} + x_3\\begin{pmatrix}3\\\\0\\end{pmatrix} = {\\bf 0} \\\\\n&\\Longleftrightarrow\nx_1 + 2x_2 + 3x_3 = 0 \\\\\n&\\Longleftrightarrow x_3 = t,x_2 = q,x_1 = -2q -3t \\\\\n&\\Longleftrightarrow x = \\begin{pmatrix}-2q-3t\\\\q\\\\t\\end{pmatrix} = q\\begin{pmatrix}-2\\\\1\\\\0\\end{pmatrix} + t\\begin{pmatrix}-3\\\\0\\\\1\\end{pmatrix} \\\\\n&\\therefore \\text{N}(A) = \\Bigg\\{q\\begin{pmatrix}-2\\\\1\\\\0 \\end{pmatrix} + t\\begin{pmatrix}-3\\\\0\\\\1\\end{pmatrix}|t,q \\in \\mathbb{R}\\Bigg\\}\n\\end{aligned}\\]\në°©ì •ì‹ì˜ í•´ë¥¼ êµ¬í•´ë³´ë‹ˆ 1)ë‘ ë²¡í„°ì˜ spanì´ ì˜ê³µê°„ì´ë©° 2)ìƒì„±ëœ ì˜ê³µê°„ì€ í–‰ë ¬ê³±ì— ì˜í•´ì„œ(ì¤‘ê°„ì—ì„œ ì°¨ì›ì¼ì¹˜ \\(2 \\times 3\\) ê³¼ \\(3 \\times 1\\)) í–‰ë²¡í„°ê°€ ì¡´ì¬í•˜ëŠ” ì°¨ì›ì¸ 3ì°¨ì› ë²¡í„°ê³µê°„ì•ˆì—ì„œ ë¶€ë¶„ê³µê°„ì¸ 2ì°¨ì› ë²¡í„°ê³µê°„ì„ ìƒì„±í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.\n\\(Ax = 0\\)ì„ ë§Œì¡±í•˜ëŠ” 2ê°œì˜ xì˜ ì„ í˜•ì¡°í•©ì´ ëª¨ë‘ ì˜ê³µê°„ì˜ ì›ì†Œì„ì„ í™•ì¸í•´ë³´ì.\n\\[\\begin{aligned}\n&Ax_1 = {\\bf 0},Ax_2 = {\\bf 0} \\\\\n&\\Longleftrightarrow cAx_1 = c{\\bf 0}={\\bf 0},cAx_2 = c{\\bf 0}={\\bf 0} \\\\\n&\\Longleftrightarrow A(cx_1) + A(cx_2) = A(cx_1 + cx_2) = {\\bf 0} \\\\\n\\end{aligned}\\]\në”°ë¼ì„œ \\(x\\) ë°©ì •ì‹ì„ ë§Œì¡±í•˜ëŠ” ë‘ ë²¡í„°ì˜ ì„ í˜•ì¡°í•©ì¸ \\(c(x_1 + cx_2)\\)ë„ \\(A\\)ì™€ ê³±í•´ì ¸ì„œ ì˜ê³µê°„ì˜ ì›ì†Œì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. \në§ˆì°¬ê°€ì§€ë¡œ rank-nulity theoreomë„ ì„±ë¦½í•¨ì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ì˜ê³µê°„ì˜ ì°¨ì›ì€ ì˜ê³µê°„ì˜ ê¸°ì €ì˜ ê°¯ìˆ˜ì´ê³  \\([-2,1,0]^T,[-3,0,1]^T\\)ê°€ ê¸°ì €ì˜ ì¡°ê±´ì¸ 1)ì„ í˜•ìƒì„± = ë²¡í„°ê³µê°„(ì—¬ê¸°ì„œ ì˜ê³µê°„) 2)ì„ í˜•ë…ë¦½ ì´ë¼ëŠ” ë‘ ì¡°ê±´ì„ ë§Œì¡±í•˜ë¯€ë¡œ ì˜ê³µê°„ì˜ ê¸°ì €ëŠ” \\([-2,1,0]^T,[-3,0,1]^T\\)ì´ê³  ì°¨ì›ì€ 2ì´ë‹¤.ë­í¬ëŠ” ë‹¤ë¥¸ë°©ì‹ìœ¼ë¡œë„ êµ¬í•  ìˆ˜ ìˆì§€ë§Œ í–‰ë ¬ì´ ê°„ë‹¨í•´ì„œ ë°”ë¡œ1ì„ì„ í™•ì¸í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ë‹¤ \\[\\text{nulity}(A) + \\text{rank}(A)= n   \\Longleftrightarrow  2 + 1 = 3\\]"
  },
  {
    "objectID": "posts/Linear Algebra/spanê³¼ columns space.html",
    "href": "posts/Linear Algebra/spanê³¼ columns space.html",
    "title": "[Linear Algebra] 2.Linear Combination & Span",
    "section": "",
    "text": "ìœ íˆ¬ë¸Œ - í˜íœí•˜ì„ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ê°•ì˜ë¥¼ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤.\n\nLinear Combination\në²¡í„° \\(\\bf{v_1,v_2,\\dots,v_n}\\)ì˜ ì„ í˜•ê²°í•©(ë˜ëŠ” ì¼ì°¨ê²°í•©)ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[w_1{\\bf v_1} + w_2{\\bf v_2} + \\dots + w_n{\\bf v_n}\\] ì„ í˜•ê²°í•©ì˜ ê²°ê³¼ëŠ” ì—¬ëŸ¬ê°€ì§€ ë²¡í„°ë“¤ì˜ ê²°í•©ì´ë‹¤. ì—¬ê¸°ì„œ ê°ê°ì˜ \\(w_1,w_2,\\dots,w_n\\)ì€ ìŠ¤ì¹¼ë¼ì´ë©° ëŒ€ì‘í•˜ëŠ” \\(x_1,x_2,\\dots,x_n\\)ë¥¼ ê²°í•©ì— ì‚¬ìš©í•˜ëŠ” ì •ë„ë¥¼ ì˜ë¯¸í•œë‹¤. ë§Œì•½ \\(w_1 = 0.0001\\)ì´ë©´ ì—¬ëŸ¬ ë²¡í„°ë“¤ì„ ê²°í•©í•˜ì§€ë§Œ ê·¸ ê²°í•© ì¤‘ \\(\\bf v_1\\)ì´ ì•„ì£¼ ì‚¬ìš©í•˜ì—¬ ê²°í•©í•˜ëŠ” ê²ƒì´ê³  \\(w_2 = 120\\)ì´ë¼ë©´ ê²°í•©ì—ì„œ \\(\\bf v_2\\)ë¥¼ ì•„ì£¼ ë§ì´ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤.\n\n\nspan\në²¡í„° \\(v_1,v_2,\\dots,v_n\\)ì˜ ì„ í˜•ê²°í•©ì—ì„œ \\(w_1,w_2,\\dots,w_n\\)(ìŠ¤ì¹¼ë¼,ê²°í•©ì— ì‚¬ìš©í•˜ëŠ” ì •ë„)ë¥¼ ë°”ê¿¨ì„ë•Œ ê°€ëŠ¥í•œ ëª¨ë“  ë²¡í„°ë“¤ì˜ ì§‘í•©ì´ë©° ì¦‰,ë²¡í„°ì˜ ì„ í˜•ê²°í•©ìœ¼ë¡œ ê°€ëŠ¥í•œ ëª¨ë“  ì§‘í•©ë“¤ì´ë©° ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\\text{span}({\\bf{v_1,v_2,\\dots,v_n}}) := \\{w_1{\\bf{v_1}} + w_2{\\bf{v_2}} + \\dots + w_n{\\bf{v_n}}:w_1,w_2,\\dots,w_n \\in K\\}\\]  spanì€ ì„ í˜•ê²°í•©ìœ¼ë¡œ ë§Œë“¤ì–´ì§€ëŠ” ë˜ë‹¤ë¥¸ ë²¡í„°ê³µê°„ì´ë‹¤. ì—¬ê¸°ì„œ KëŠ” fieldë¥¼ ì˜ë¯¸í•˜ëŠ”ë° ìŠ¤ì¹¼ë¼ëŠ” fieldë¼ëŠ” ë˜ë‹¤ë¥¸ ì§‘í•©ìœ¼ë¡œë¶€í„° ê°€ì ¸ì˜¨ ì›ì†Œì´ê¸°ë•Œë¬¸ì— ê·¸ë ‡ë‹¤.ì„ì˜ì˜ ë²¡í„°ë“¤ì˜ spanì€ ì–´ë–¨ê¹Œ? ì•„ë˜ì˜ ê·¸ë¦¼ì„ í™•ì¸í•´ë³´ì.\n<ì°¸ê³ > spanì€ ë™ì‚¬ë¡œë„ ì‚¬ìš©í•œë‹¤. ex : ë²¡í„°ê³µê°„ì„ ìƒì„±í•œë‹¤. ì—´ê³µê°„ì€ ì—´ë²¡í„°ë“¤ì´ ìƒì„±í•˜ëŠ” ê³µê°„ì´ë‹¤.(=ì—´ê³µê°„ì€ ì—´ë²¡í„°ì˜ ìƒì„±ì´ë‹¤.).ê¸°ì €ë“¤ì´ ë²¡í„°ê³µê°„ì„ ìƒì„±í•œë‹¤.\n:ëŠ” ì¡°ê±´ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nplt.style.use(\"ggplot\")\n\ndef linrcmb(v1,v2):\n    linrcomb_x= []\n    linrcomb_y= []\n\n    _w = np.linspace(-50,50,300).tolist()\n    for i in range(2000):\n    #w1,w2ë¥¼ -100~100ì‚¬ì´ì˜ ì„ì˜ì˜ ìˆ«ìë¡œ\n        w1 = random.sample(_w,1)\n        w2 = random.sample(_w,1)\n        #print(w1,w2)\n    #ì„ í˜•ê²°í•© ê³„ì‚°\n        linrcomb = w1 * v1 + w2 * v2\n    #ì‹œê°í™”ë¥¼ ìœ„í•´ ì„ í˜•ê²°í•©ì˜ xê°’ yê°’ ë”°ë¡œ ëª¨ì•„ë†“ê¸°\n        linrcomb_x.append(linrcomb[0][0])\n        linrcomb_y.append(linrcomb[1][0])\n    return linrcomb_x,linrcomb_y\n\nv1 = np.array([[0],[0]])\nv2 = np.array([[0],[0]])\nx,y = linrcmb(v1,v2)\nfig,ax = plt.subplots(figsize=(30,5))\nplt.subplot(1,5,1)\nplt.title(\"$v_1 = [0,0]^{T},v_2 = [0,0]^T$\")\nplt.scatter(x,y,color=\"black\")\n\nv1 = np.array([[1],[0]])\nv2 = np.array([[-3],[0]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,2)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [-3,0]^T$\")\nplt.scatter(x,y)\n\nv1 = np.array([[1],[1]])\nv2 = np.array([[0],[0]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,3)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [0,0]^T$\")\nplt.scatter(x,y,color=\"green\",alpha=1)\n\n\nv1 = np.array([[1],[0]])\nv2 = np.array([[0],[1]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,4)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [0,1]^T$\")\nplt.scatter(x,y,color=\"purple\",alpha=1)\n\ndef linrcmb2(v1,v2):\n    linrcomb_x= []\n    linrcomb_y= []\n\n    _w = (np.linspace(-250,250,300)).tolist()\n    for i in range(2000):\n        w1 = random.sample(_w,1)[0] * 100 #ê·¸ë˜í”„ë¥¼ ê·¸ë¦¬ê¸° ìœ„í•œ ê°’ ì¡°ì ˆ\n        w2 = random.sample(_w,1)[0] \n        #ì„ í˜•ê²°í•© ê³„ì‚°\n        linrcomb = w1 * v1 + w2 * v2\n        #ì‹œê°í™”ë¥¼ ìœ„í•´ ì„ í˜•ê²°í•©ì˜ xê°’ yê°’ ë”°ë¡œ ëª¨ì•„ë†“ê¸°\n        linrcomb_x.append(linrcomb[0][0])\n        linrcomb_y.append(linrcomb[1][0])\n    \n    return linrcomb_x,linrcomb_y\nlinrcmb2(v1,v2)\n\nv1 = np.array([[-1],[0]])\nv2 = np.array([[2],[2]])\nx,y = linrcmb2(v1,v2)\nplt.subplot(1,5,5)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [2,2]^T$\")\nplt.scatter(x,y,color=\"blue\",alpha=1)\nplt.subplots_adjust(wspace=0.4,hspace=0.5)\nplt.suptitle(\"span$(v_1,v_2)$\",y=1.02,fontsize=20)\n\n\nText(0.5, 1.02, 'span$(v_1,v_2)$')\n\n\n\n\n\nì  í•˜ë‚˜ëŠ” ë²¡í„° í•˜ë‚˜ë¥¼ ë‚˜íƒ€ë‚¸ë‹¤. 1ë²ˆì§¸ ê·¸ë¦¼ì—ì„œ ë²¡í„°ë“¤ì˜ ìƒì„±(span)ì€ 2ì°¨ì› ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì´ì 0ì°¨ì› ë²¡í„°ê³µê°„(ì )ì´ë©° 2,3ë²ˆì§¸ ê·¸ë¦¼ì—ì„œ ë²¡í„°ë“¤ì˜ ìƒì„±(span)ì€ 2ì°¨ì› ë²¡í„°ê³µê°„ì˜ ë¶€ë¶„ê³µê°„ì´ì 1ì°¨ì› ë²¡í„°ê³µê°„(ì§ì„ )ì´ë‹¤. ì´ì™€ëŠ” ë‹¤ë¥´ê²Œ 3ë²ˆì§¸ ê·¸ë¦¼ì—ì„œì˜ ë²¡í„°ë“¤ì˜ ìƒì„±(span)ì€ 3ì°¨ì› ë²¡í„°ê³µê°„ ê·¸ ìì²´ì¸ë° ê·¸ ì´ìœ ëŠ” 3ì°¨ì› ë²¡í„°ê³µê°„ì˜ ëª¨ë“  ì ì„ í‘œí˜„í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤.\n\n\ncolumn Space\nì—´ê³µê°„(columns space)ì€ í–‰ë ¬ì˜ ì—´ë²¡í„°ë“¤ì˜ span ì¦‰, í–‰ë ¬ì˜ ì—´ë²¡í„°ë“¤ë¡œ ê°€ëŠ¥í•œ ëª¨ë“  ì„ í˜•ì¡°í•©(ë²¡í„°)ì˜ ì§‘í•©ì…ë‹ˆë‹¤. \\(v_1,v_2,\\dots,v_n\\)ì´ í–‰ë ¬Aì˜ ì—´ë²¡í„°ë“¤ì´ë¼ê³  í•  ë•Œ, ì—´ê³µê°„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\[\\text{C}(A) = \\text{span}(v_1,v_2,\\dots,v_n) = \\{w_1{\\bf v_1} + w_2{\\bf v_2} + \\dots + w_n{\\bf v_n}:w_1,w_2,\\dots,w_n \\in K\\}\\] ì—´ê³µê°„ì€ ë°©ì •ì‹ \\(Ax = b\\)ì˜ í•´ì˜ ê°¯ìˆ˜ë¥¼ íŒŒì•…í•˜ëŠ”ë° ì“°ì…ë‹ˆë‹¤.\n\n\nì°¸ê³ ìë£Œ\n[ì„ í˜•ëŒ€ìˆ˜] ë²¡í„°ê³µê°„(vector space), ë²¡í„° ë¶€ë¶„ê³µê°„(vector subspace), ìƒì„±ê³µê°„(span), ì°¨ì›(dimension) ìœ„í‚¤í”¼ë””ì•„-Linear span ìœ„í‚¤í”¼ë””ì•„-Row and columns spaces í˜íœí•˜ì„ - [ì„ ëŒ€] 2-6ê°•. span ê³¼ column space (ì—´ê³µê°„) ì§ê´€ì  ì„¤ëª…"
  },
  {
    "objectID": "posts/Linear Algebra/í–‰ë ¬ê³±ì— ëŒ€í•œ ì—¬ëŸ¬ê°€ì§€ ì‹œê°.html",
    "href": "posts/Linear Algebra/í–‰ë ¬ê³±ì— ëŒ€í•œ ì—¬ëŸ¬ê°€ì§€ ì‹œê°.html",
    "title": "[Linear Algebra] 1.í–‰ë ¬ê³±ì— ëŒ€í•œ ì—¬ëŸ¬ê°€ì§€ ê´€ì ",
    "section": "",
    "text": "ìœ íŠœë¸Œ - í˜íœí•˜ì„ë‹˜ì˜ ì„ í˜•ëŒ€ìˆ˜í•™ì„ ì°¸ê³ í–ˆìŠµë‹ˆë‹¤.\n\ní–‰ë ¬ê³±ì€ ë‚´ì ì´ë‹¤.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},B \\in \\mathbb{R}^{n \\times p} \\\\\n&AB =\n\\begin{bmatrix}\na_1^T\\\\\na_2^T\\\\\n\\vdots\\\\\na_m^T\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 & b_2 & \\dots & b_p\n\\end{bmatrix}=\n\\begin{bmatrix}\na_1^Tb_1 & a_1^Tb_2 & \\dots & a_1^Tb_p \\\\\na_2^Tb_1 & a_2^Tb_2 & \\dots & a_2^Tb_p \\\\\n\\vdots & \\vdots  & \\vdots & \\vdots \\\\\na_{m}^Tb_1 & a_m^Tb_2 & \\dots & a_m^Tb_p \\\\\n\\end{bmatrix}\n\\end{aligned}\\]\n\n\ní–‰ë ¬ê³±ì€ rank-1 matrixì˜ í•©ì´ë‹¤.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},B \\in \\mathbb{R}^{n \\times p} \\\\\n&AB =\n\\begin{bmatrix}\na_1 & a_2 & \\dots & a_n\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1^T \\\\\nb_2^T \\\\\n\\vdots \\\\\nb_n^T\n\\end{bmatrix}\n= a_1b_1^T + a_2b_2^T + \\dots + a_nb_n^T\n\\end{aligned}\\]\n\n\ní–‰ë ¬ê³¼ ë²¡í„°ì˜ ê³±ì€ ì—´ê³µê°„ì— ì†í•œ ì„ì˜ì˜ ë²¡í„°ì´ë‹¤.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},x \\in \\mathbb{R}^{n \\times 1} \\\\\n&Ax =\n\\begin{bmatrix}\na_1 & a_2 & \\dots & a_n\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n= a_1x_1 + a_2x_2 + \\dots + a_nx_n\n\\end{aligned}\\]\n\n\\(a_1,a_2,\\dots,a_n\\)ì€ ë²¡í„° \\(x_1,x_2,\\dots,x_n\\)ì€ ìŠ¤ì¹¼ë¼ì´ë‹¤.\ní–‰ë ¬ê³±ì€ ìœ„ì™€ê°™ì´ ì—´ê³µê°„ì˜ ê¸°ì €(í–‰ë ¬\\(A\\)ì˜ ì—´ë²¡í„°)ì™€ ìŠ¤ì¹¼ë¼(ë¯¸ì§€ìˆ˜ë²¡í„°\\(x\\)ì˜ ì›ì†Œ)ì™€ì˜ ì¼ì°¨ê²°í•©ì´ë¯€ë¡œ ê¸°ì €ì¸ ì—´ë²¡í„°ê°€ ìƒì„±(span)í•˜ëŠ” ì—´ê³µê°„ì˜ ì›ì†Œì´ë‹¤. ì´ëŠ” ë°©ì •ì‹ \\(Ax=b\\)ì˜ í•´ì˜ ê°¯ìˆ˜ë¥¼ ì•Œì•„ë‚´ëŠ”ë°ì— ì‚¬ìš©í•˜ëŠ” ì¤‘ìš”í•œ ê°œë…ì´ë‹¤.(ì°¸ê³  : ë°©ì •ì‹ Ax = bì˜ í•´ì˜ ê°¯ìˆ˜ ì•Œì•„ë‚´ê¸°)\nì—´ê³µê°„(column space) : í–‰ë ¬ì—ì„œ (ì—´)ë²¡í„°ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ë²¡í„°ê³µê°„. ì—´ë²¡í„°ì˜ span\n\n\n\ní–‰ë²¡í„°ì™€ í–‰ë ¬ì˜ ê³±ì€ row space(í–‰ê³µê°„)ì— ì†í•œ ì„ì˜ì˜ ë²¡í„°ì´ë‹¤.\n\\[\\begin{aligned}\n&\\text{Let }x \\in \\mathbb{R}^{1 \\times n},X \\in \\mathbb{R}^{n \\times p} \\\\\n&xA =\n\\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\n\\vdots \\\\\na_n\n\\end{bmatrix}\n= x_1a_1^T + x_2a_2^T + \\dots + x_na_n^T\n\\end{aligned}\\]\n\n\\(a_1^T,a_2^T,\\dots,a_n^T\\)ì€ ë²¡í„° \\(x_1,x_2,\\dots,x_n\\)ì€ ìŠ¤ì¹¼ë¼ì´ë‹¤. í–‰ë ¬ê³±ì€ ìœ„ì™€ê°™ì´ í–‰ê³µê°„ì˜ ê¸°ì €(í–‰ë ¬\\(A\\)ì˜ í–‰ë²¡í„°)ì™€ ìŠ¤ì¹¼ë¼(\\(x\\)ì˜ ì›ì†Œ)ì™€ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ ê¸°ì €ì¸ í–‰ë²¡í„°ê°€ ìƒì„±í•˜ëŠ” í–‰ê³µê°„ì˜ ì›ì†Œì´ë‹¤.\ní–‰ê³µê°„ : í–‰ë ¬ì—ì„œ í–‰ë²¡í„°ì˜ ì¼ì°¨ê²°í•©ìœ¼ë¡œ ìƒì„±ë˜ëŠ” ë²¡í„°ê³µê°„. í–‰ë²¡í„°ì˜ span\n\n\n\nì°¸ê³ ë¬¸í—Œ\ní˜íœí•˜ì„ - ì„ ëŒ€ 2-5ê°•. í–‰ë ¬ì˜ ê³±ì…ˆê³¼ ë„¤ ê°€ì§€ ê´€ì  (ì—´ê³µê°„ (column space) ë“±)"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html",
    "title": "[pandas] map,apply,applymap ë¹„êµ",
    "section": "",
    "text": "sdasdaksldjklsdjklasjdklj dasmkdlmsakldmklsadmkls\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#data",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#data",
    "title": "[pandas] map,apply,applymap ë¹„êµ",
    "section": " Data",
    "text": "Data\n\ns = pd.Series([\"cat\",\"dog\",np.nan,\"rabbit\"])"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#argê°€-dictì¸-ê²½ìš°",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#argê°€-dictì¸-ê²½ìš°",
    "title": "[pandas] map,apply,applymap ë¹„êµ",
    "section": " argê°€ dictì¸ ê²½ìš°",
    "text": "argê°€ dictì¸ ê²½ìš°\n\ns.map({\"cat\":\"kitten\",\"dog\":\"puppy\"})\n\n0    kitten\n1     puppy\n2       NaN\n3       NaN\ndtype: object\n\n\n\nSeriesì˜ ê°’ ì¤‘ dictì˜ keyì™€ ì¼ì¹˜í•˜ëŠ” ê°’ì´ ì—†ìœ¼ë©´ NaNìœ¼ë¡œ ë°”ë€œ."
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#argê°€-seriesì¸-ê²½ìš°",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#argê°€-seriesì¸-ê²½ìš°",
    "title": "[pandas] map,apply,applymap ë¹„êµ",
    "section": " argê°€ Seriesì¸ ê²½ìš°",
    "text": "argê°€ Seriesì¸ ê²½ìš°\n\ns2 = pd.Series([\"lion\",\"elephant\",\"dog\",np.nan])\ns.map(s2)\n\n0    NaN\n1    NaN\n2    NaN\n3    NaN\ndtype: object"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#argê°€-functionì¸-ê²½ìš°",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#argê°€-functionì¸-ê²½ìš°",
    "title": "[pandas] map,apply,applymap ë¹„êµ",
    "section": " argê°€ functionì¸ ê²½ìš°",
    "text": "argê°€ functionì¸ ê²½ìš°\n\ns.map(lambda x : \"I am a {}\".format(x))\n\n0       I am a cat\n1       I am a dog\n2       I am a nan\n3    I am a rabbit\ndtype: object"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#ì˜ëª»ëœ-ì‚¬ìš©-ì˜ˆì‹œ",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#ì˜ëª»ëœ-ì‚¬ìš©-ì˜ˆì‹œ",
    "title": "[pandas] map,apply,applymap ë¹„êµ",
    "section": " ì˜ëª»ëœ ì‚¬ìš© ì˜ˆì‹œ",
    "text": "ì˜ëª»ëœ ì‚¬ìš© ì˜ˆì‹œ\ns = pd.Series([0.1,3,2,0.4])\ns.map(lambda x : sum(x))\n>>> TypeError: 'float' object is not iterable\n\nê°ê°ì˜ ê°’(ì—¬ê¸°ì„  float)ì´ ë”°ë¡œë”°ë¡œ í•¨ìˆ˜(mapping)ì´ ì…ë ¥ëœë‹¤. í•˜ì§€ë§Œ sumí•¨ìˆ˜ëŠ” floatí˜•ì„ inputìœ¼ë¡œ í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì˜¤ë¥˜ ë°œìƒí•¨.(floatí˜•ì€ iterableí•˜ì§€ ì•ŠìŒ)\n\n\ns = pd.Series([0.3,2,0.4,5])\ns.map(np.var)\n\n0    0.0\n1    0.0\n2    0.0\n3    0.0\ndtype: float64\n\n\n\nì˜¤ë¥˜ê°€ ëœ¨ì§€ ì•Šìœ¼ë‚˜ ì›í•˜ë˜ ë™ì‘ì´ ì•„ë‹ˆë‹¤. ê°ê°ì˜ ë³€ìˆ˜ì— ëŒ€í•´ì„œ í‰ê· ê°’ì„ êµ¬í•˜ë ¤ í–ˆê² ì§€ë§Œ ê°ê°ì˜ ê°’ì— meanì´ ë˜ë¯€ë¡œ ê·¸ ê°’ ìì²´ì´ë‹¤."
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#data-1",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#data-1",
    "title": "[pandas] map,apply,applymap ë¹„êµ",
    "section": " Data",
    "text": "Data\n\nn1 = pd.Series([0.1,4,0.35,2])\nn2 = pd.Series([0.3,0.4,3,-5])\ndf = pd.DataFrame({\"n1\":n1,\"n2\":n2})\ndf\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.10\n      0.3\n    \n    \n      1\n      4.00\n      0.4\n    \n    \n      2\n      0.35\n      3.0\n    \n    \n      3\n      2.00\n      -5.0"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#columnì—-function-ì ìš©",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#columnì—-function-ì ìš©",
    "title": "[pandas] map,apply,applymap ë¹„êµ",
    "section": " columnì— function ì ìš©",
    "text": "columnì— function ì ìš©\n\ncolumn vectorì— í•¨ìˆ˜ ì ìš©í•œë‹¤ê³  ìƒê°í•˜ì.\n\n\ndf.apply(func = lambda x : sum(x)/len(x),axis=0)\n\nn1    1.6125\nn2   -0.3250\ndtype: float64\n\n\n\ndf.apply(np.mean,axis=0)\n\nn1    1.6125\nn2   -0.3250\ndtype: float64\n\n\n\n#vectorì— elementwise**2\n#np.arrayë¥¼ **2í•˜ë©´ ëª¨ë“  ì›ì†Œê°€ ì œê³±ë˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.\ndf.apply(func = lambda x : x ** 2,axis=0)\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.0100\n      0.09\n    \n    \n      1\n      16.0000\n      0.16\n    \n    \n      2\n      0.1225\n      9.00\n    \n    \n      3\n      4.0000\n      25.00\n    \n  \n\n\n\n\n\n#vectorì— elementwiseë¡œ sinì·¨í•¨.\ndf.apply(func = lambda x : np.sin(x),axis=0)\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.099833\n      0.295520\n    \n    \n      1\n      -0.756802\n      0.389418\n    \n    \n      2\n      0.342898\n      0.141120\n    \n    \n      3\n      0.909297\n      0.958924"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#rowì—-function-ì ìš©",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#rowì—-function-ì ìš©",
    "title": "[pandas] map,apply,applymap ë¹„êµ",
    "section": " rowì— function ì ìš©",
    "text": "rowì— function ì ìš©\n\nrow vectorì— í•¨ìˆ˜ ì ìš©.\n\n\ndf.apply(sum,axis=1)\n\n0    0.40\n1    4.40\n2    3.35\n3   -3.00\ndtype: float64\n\n\n\ndf.apply(np.sin,axis=1)\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.099833\n      0.295520\n    \n    \n      1\n      -0.756802\n      0.389418\n    \n    \n      2\n      0.342898\n      0.141120\n    \n    \n      3\n      0.909297\n      0.958924\n    \n  \n\n\n\n\n\n#vectorì— elementwise**2\n#np.arrayë¥¼ **2í•˜ë©´ ëª¨ë“  ì›ì†Œê°€ ì œê³±ë˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.\ndf.apply(func = lambda x : x ** 2,axis=1)\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.0100\n      0.09\n    \n    \n      1\n      16.0000\n      0.16\n    \n    \n      2\n      0.1225\n      9.00\n    \n    \n      3\n      4.0000\n      25.00"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#data-2",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#data-2",
    "title": "[pandas] map,apply,applymap ë¹„êµ",
    "section": " Data",
    "text": "Data\n\nn1 = pd.Series([0.1,4,0.35,2])\nn2 = pd.Series([0.3,0.4,3,-5])\ndf = pd.DataFrame({\"n1\":n1,\"n2\":n2})\ndf\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.10\n      0.3\n    \n    \n      1\n      4.00\n      0.4\n    \n    \n      2\n      0.35\n      3.0\n    \n    \n      3\n      2.00\n      -5.0"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#ì˜¬ë°”ë¥¸-ì‚¬ìš©",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#ì˜¬ë°”ë¥¸-ì‚¬ìš©",
    "title": "[pandas] map,apply,applymap ë¹„êµ",
    "section": " ì˜¬ë°”ë¥¸ ì‚¬ìš©",
    "text": "ì˜¬ë°”ë¥¸ ì‚¬ìš©\n\ndf.applymap(lambda x : \"ê°’ì€ {}\".format(x))\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      ê°’ì€ 0.1\n      ê°’ì€ 0.3\n    \n    \n      1\n      ê°’ì€ 4.0\n      ê°’ì€ 0.4\n    \n    \n      2\n      ê°’ì€ 0.35\n      ê°’ì€ 3.0\n    \n    \n      3\n      ê°’ì€ 2.0\n      ê°’ì€ -5.0\n    \n  \n\n\n\n\n\ndf.applymap(lambda x : len(str(x)))\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      3\n      3\n    \n    \n      1\n      3\n      3\n    \n    \n      2\n      4\n      3\n    \n    \n      3\n      3\n      4\n    \n  \n\n\n\n\n\ndf.applymap(lambda x : x ** 2)\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.0100\n      0.09\n    \n    \n      1\n      16.0000\n      0.16\n    \n    \n      2\n      0.1225\n      9.00\n    \n    \n      3\n      4.0000\n      25.00"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#ì˜ëª»ëœ-ì‚¬ìš©",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#ì˜ëª»ëœ-ì‚¬ìš©",
    "title": "[pandas] map,apply,applymap ë¹„êµ",
    "section": " ì˜ëª»ëœ ì‚¬ìš©",
    "text": "ì˜ëª»ëœ ì‚¬ìš©\ndf.applymap(lambda x : sum(x))\n>>> TypeError: 'float' object is not iterable\n\nSeriesì˜ map methodì™€ ë§ˆì°¬ê°€ì§€ë¡œ DataFrameì˜ applymap methodë„ ê°ê°ì˜ ê°’(ì—¬ê¸°ì„  float)ì´ ë”°ë¡œë”°ë¡œ í•¨ìˆ˜(mapping)ì— ì…ë ¥ëœë‹¤. ê·¸ëŸ¬ë¯€ë¡œ sumí•¨ìˆ˜ì—ì„œ ì˜¤ë¥˜ê°€ ë°œìƒí•œë‹¤.\n\n\ndf.applymap(lambda x : np.mean(x))\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.10\n      0.3\n    \n    \n      1\n      4.00\n      0.4\n    \n    \n      2\n      0.35\n      3.0\n    \n    \n      3\n      2.00\n      -5.0\n    \n  \n\n\n\n\n\nì˜¤ë¥˜ê°€ ëœ¨ì§€ ì•Šìœ¼ë‚˜ ì›í•˜ë˜ ë™ì‘ì´ ì•„ë‹ˆë‹¤. ê°ê°ì˜ ë³€ìˆ˜ì— ëŒ€í•´ì„œ í‰ê· ê°’ì„ êµ¬í•˜ë ¤ í–ˆê² ì§€ë§Œ ê°ê°ì˜ ê°’ì— meanì´ ë˜ë¯€ë¡œ ê·¸ ê°’ ìì²´ì´ë‹¤."
  },
  {
    "objectID": "posts/numpy pandas/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html",
    "href": "posts/numpy pandas/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html",
    "title": "Finite Difference Method with np.gradient",
    "section": "",
    "text": "\\({\\bf x} =\\begin{bmatrix}x_1&x_2&\\dots&x_m\\end{bmatrix}^T\\)ì¼ ë•Œ, \\(x\\)ì— ëŒ€í•œ ë‹¤ë³€ìˆ˜í•¨ìˆ˜ \\(f({\\bf x})\\)ì˜ gradientëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&\\text{gradient of }f({\\bf{x}}) = \\frac{\\partial f}{\\partial {\\bf x}} = \\nabla f(\\bf{x}) =\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_m}\n\\end{pmatrix}\n\\end{aligned}\\]\ní•¨ìˆ˜ê°€ ê°€ì§€ëŠ” ëª¨ë“  ë³€ìˆ˜ì— ëŒ€í•´ì„œ í¸ë¯¸ë¶„ í•œ ë’¤ ëª¨ì•„ë†“ì€ ë²¡í„°ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. í•¨ìˆ˜ì˜ ìˆ˜ì‹ì„ ì•Œê³  ë¯¸ë¶„ì´ ê°€ëŠ¥í•˜ë©´ ìš°ë¦¬ëŠ” í•´ì„ì ìœ¼ë¡œ ë¯¸ë¶„í•´ì„œ(ë¯¸ë¶„ê³µì‹ì¨ì„œ) ê·¸ë ˆë””ì–¸íŠ¸ë¥¼ êµ¬í•˜ê³  ê°ê°ì˜ ì–´ë–¤ pointì—ì„œì˜ í¸ë¯¸ë¶„ê³„ìˆ˜ë“¤ë„ êµ¬í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ìš°ë¦¬ê°€ ì£¼ì–´ì§„ ë°ì´í„°ëŠ” í•¨ìˆ˜fì˜ í•¨ìˆ«ê°’ë“¤ì´ ì£¼ì–´ì§„ë‹¤. ì˜ˆë¥¼ ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nimport numpy as np\nf = np.array([1,2,4,7,11,17],dtype = float)\nprint(\"f(x)\")\nprint(f)\n\nf(x)\n[ 1.  2.  4.  7. 11. 17.]\n\n\nìœ„ì™€ ê°™ì€ í•¨ìˆ«ê°’ë“¤ë§Œ ì£¼ì–´ì§ˆë•Œì—ëŠ” ì›ë˜ì˜ í•¨ìˆ˜ë¥¼ ì•Œê¸°ëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤. ë”°ë¼ì„œ ë„í•¨ìˆ˜ë¥¼ í†µí•œ ì •í™•í•œ ë¯¸ë¶„ê³„ìˆ˜ë¥¼ êµ¬í•˜ê¸°ê°€ ë¶ˆê°€ëŠ¥í•˜ë¯€ë¡œ ì£¼ì–´ì§„ ë°ì´í„°ë¡œ \\(\\bf x\\)ì—ì„œì˜ ë¯¸ë¶„ê³„ìˆ˜ì˜ ê°’ì„ ê·¼ì‚¬ì ìœ¼ë¡œ êµ¬í•  ìˆ˜ ìˆëŠ”ë° ì´ë¥¼ ìˆ˜ì¹˜ë¯¸ë¶„ì´ë¼ í•œë‹¤."
  },
  {
    "objectID": "posts/numpy pandas/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì°¨ì›-ë°°ì—´ì˜-ê²½ìš°",
    "href": "posts/numpy pandas/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì°¨ì›-ë°°ì—´ì˜-ê²½ìš°",
    "title": "Finite Difference Method with np.gradient",
    "section": "1ì°¨ì› ë°°ì—´ì˜ ê²½ìš°",
    "text": "1ì°¨ì› ë°°ì—´ì˜ ê²½ìš°\n\nEx) \\(dx\\) = 1\në„˜íŒŒì´ 1ì°¨ì› ë°°ì—´ì´ ë‹¤ìŒê³¼ ê°™ì´ ì£¼ì–´ì ¸ ìˆë‹¤ê³  í•˜ì.\n\nfrom IPython.display import display, Markdown\nf = np.array([1,2,4,7,11,16],dtype=float)\nprint(f)\n\n[ 1.  2.  4.  7. 11. 16.]\n\n\nnp.gradientí•¨ìˆ˜ëŠ” 1ì°¨ì› ë°°ì—´ì˜ ë‚´ë¶€ì— ìˆëŠ” ê°ê°ì˜ ê°’ë“¤ì€ \\(x\\)ê°’ì´ ê±°ë¦¬ê°€ \\(dx\\)=1ì”© ë³€í™”í• ë•Œë§ˆë‹¤ì˜ í•¨ìˆ«ê°’\\(f(x)\\)ë“¤ë¡œ ì´í•´í•œë‹¤. ì¦‰,ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nfor i in range(len(f)):\n    display(Markdown(rf'$x_{i+1}$ì—ì„œì˜ í•¨ìˆ«ê°’ $f(x_{i+1})$ = {f[i]}'))\n\n\\(x_1\\)ì—ì„œì˜ í•¨ìˆ«ê°’ \\(f(x_1)\\) = 1.0\n\n\n\\(x_2\\)ì—ì„œì˜ í•¨ìˆ«ê°’ \\(f(x_2)\\) = 2.0\n\n\n\\(x_3\\)ì—ì„œì˜ í•¨ìˆ«ê°’ \\(f(x_3)\\) = 4.0\n\n\n\\(x_4\\)ì—ì„œì˜ í•¨ìˆ«ê°’ \\(f(x_4)\\) = 7.0\n\n\n\\(x_5\\)ì—ì„œì˜ í•¨ìˆ«ê°’ \\(f(x_5)\\) = 11.0\n\n\n\\(x_6\\)ì—ì„œì˜ í•¨ìˆ«ê°’ \\(f(x_6)\\) = 16.0\n\n\nìœ„ì—ì„œ ë„˜íŒŒì´ì˜ ê·¸ë˜ë””ì–¸íŠ¸ëŠ” ëê°’ì„ ì œì™¸í•œ ë‚´ë¶€ì˜ ìš”ì†Œì—ëŠ” ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬ ì–‘ëê°’ì— ëŒ€í•´ì„œëŠ” í›„í–¥ì°¨ë¶„ ë˜ëŠ” ì „í–¥ì°¨ë¶„ì„ ì‚¬ìš©í•œë‹¤ê³  ì–¸ê¸‰í–ˆì—ˆë‹¤. ê³„ì‚°í•œ,\\(x_2,x_5\\)ì—ì„œ ë¯¸ë¶„ê³„ìˆ˜ì˜ 2ì°¨ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&f^{'}(x_2) \\overset{\\sim}{=} \\frac{f(x_3)-f(x_1)}{2h} = \\frac{4-1}{2} = 1.5 \\\\\n&f^{'}(x_5) \\overset{\\sim}{=} \\frac{f(x_6)-f(x_4)}{2h} = \\frac{16-7}{2} = 4.5 \\\\\n&\\text{where, } h = x_3-x_2 = x_2-x_1 = 1\n\\end{aligned}\\]\n1ì°¨ì› ë°°ì—´ì˜ ê°€ì¥ ì²˜ìŒì— ì˜¤ëŠ” ê°’ì— ì „í–¥ì°¨ë¶„ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•˜ê³  ê°€ì¥ ë§ˆì§€ë§‰ì— ì˜¤ëŠ” ê°’ì—ì„œëŠ” í›„í–¥ì°¨ë¶„ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•œë‹¤.\n\\[\\begin{aligned}\n&f^{'}(x_1) = \\frac{f(x_2) - f(x_1)}{h} = \\frac{2-1}{1} = 1 \\\\\n&f^{'}(x_6) = \\frac{f(x_6) - f(x_5)}{h} = \\frac{16-11}{1} = 5\n\\end{aligned}\\]\nê³„ì‚°í•œ ê°’ê³¼ ì‹¤ì œë¡œ ì¼ì¹˜í•˜ëŠ”ì§€ í™•ì¸.\n\nnum_diff = np.gradient(f)\nprint(\"np.gradientì˜ ì¶œë ¥ê°’\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradientì˜ ì¶œë ¥ê°’\n[1.  1.5 2.5 3.5 4.5 5. ]\n\n\n\\(x_1\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 1.0\n\n\n\\(x_2\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 1.5\n\n\n\\(x_3\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 2.5\n\n\n\\(x_4\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 3.5\n\n\n\\(x_5\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 4.5\n\n\n\\(x_6\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 5.0\n\n\n\n\nEx) \\(dx \\not = 1\\) (defaultê°€ ì•„ë‹ ê²½ìš°)\nê±°ë¦¬\\(dx=2\\)ì¼ë•Œ ê³„ì‚°í•œ,\\(x_2,x_5\\)ì—ì„œ ë¯¸ë¶„ê³„ìˆ˜ì˜ 2ì°¨ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&f^{'}(x_2) \\overset{\\sim}{=} \\frac{f(x_3)-f(x_1)}{2h} = \\frac{4-1}{4} = 0.75 \\\\\n&f^{'}(x_5) \\overset{\\sim}{=} \\frac{f(x_6)-f(x_4)}{2h} = \\frac{16-7}{4} = 2.25 \\\\\n&\\text{where, } h = x_3-x_1 = x_6-x_4 = 2\n\\end{aligned}\\]\nê±°ë¦¬ê°€ \\(dx=2\\)ì¼ë•Œ ë°°ì—´ì˜ ì–‘ ëê°’ì—ì„œ ì „í–¥,í›„í–¥ì°¨ë¶„ê·¼ì‚¬ë¥¼ í†µí•œ ë¯¸ë¶„ê³„ìˆ˜ì˜ ê°’ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&f^{'}(x_1) = \\frac{f(x_2) - f(x_1)}{h} = \\frac{2-1}{2} = 0.5 \\\\\n&f^{'}(x_6) = \\frac{f(x_6) - f(x_5)}{h} = \\frac{16-11}{2} = 2.5\n\\end{aligned}\\]\nxê°’ ì‚¬ì´ì˜ ê±°ë¦¬\\(dx\\)ë¥¼ ë°”ê¾¸ê³  ì‹¶ë‹¤ë©´? => ë‘ë²ˆì§¸ ì¸ìˆ˜ì— ìŠ¤ì¹¼ë¼ ëŒ€ì…í•˜ë©´ ëœë‹¤.\n\ndx = 2\nnum_diff = np.gradient(f,dx)\nprint(\"np.gradientì˜ ì¶œë ¥ê°’\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradientì˜ ì¶œë ¥ê°’\n[0.5  0.75 1.25 1.75 2.25 2.5 ]\n\n\n\\(x_1\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 0.5\n\n\n\\(x_2\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 0.75\n\n\n\\(x_3\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 1.25\n\n\n\\(x_4\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 1.75\n\n\n\\(x_5\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 2.25\n\n\n\\(x_6\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 2.5\n\n\n\n\nEx) xê°’ì˜ ì¢Œí‘œë¥¼ ì§ì ‘ ì •í•´ì£¼ëŠ” ê²½ìš°\nì´ì „ì—ëŠ” ê°ê°ì˜ ì¸ë±ìŠ¤ê°„ì˜ ê±°ë¦¬ëŠ” ëª¨ë‘ ë™ì¼í•˜ê²Œ ê¸°ë³¸ê°’ 1ì´ê±°ë‚˜ ë‹¤ë¥¸ê°’ì„ ì‚¬ìš©í–ˆë‹¤. ê·¸ëŸ¬ì§€ ì•Šê³  \\(x_1,x_2,\\dots,x_6\\)ì˜ ì¢Œí‘œë¥¼ ì§ì ‘ ì§€ì •í•´ì£¼ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤. í•¨ìˆ˜ì˜ 2ë²ˆì¬ ì¸ìˆ˜ì— ì¢Œí‘œë¥¼ ì§ì ‘ ë„£ì–´ì£¼ë©´ ëœë‹¤.\në¨¼ì € xê°’ì˜ ì¢Œí‘œë¥¼ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  í•´ë³´ì.\n\nx = np.array([0., 1., 1.5, 3.5, 4., 6.], dtype=float)\nf = np.array([1,2,4,7,11,16],dtype=float)\nprint(\"ê°ê°ì˜ ì¢Œí‘œì™€ í•¨ìˆ«ê°’\")\nfor i in range(len(x)):\n    display(Markdown(rf'$x_{i+1}$ = {x[i]}, $f(x_{i+1})$ = {f[i]}'))\n\nê°ê°ì˜ ì¢Œí‘œì™€ í•¨ìˆ«ê°’\n\n\n\\(x_1\\) = 0.0, \\(f(x_1)\\) = 1.0\n\n\n\\(x_2\\) = 1.0, \\(f(x_2)\\) = 2.0\n\n\n\\(x_3\\) = 1.5, \\(f(x_3)\\) = 4.0\n\n\n\\(x_4\\) = 3.5, \\(f(x_4)\\) = 7.0\n\n\n\\(x_5\\) = 4.0, \\(f(x_5)\\) = 11.0\n\n\n\\(x_6\\) = 6.0, \\(f(x_6)\\) = 16.0\n\n\nê°ê°ì˜ ì¢Œí‘œì—ì„œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ì„ êµ¬í•˜ë©´ ì•„ë˜ì™€ ê°™ë‹¤.(ìˆ˜ì‹ ê³„ì‚°ì€ ì˜ ëª¨ë¥´ê² ë„¤ìš” â€¦ ì¶”í›„ì— ë” ê³µë¶€í•˜ê² ìŠµë‹ˆë‹¤!)\n\nnum_diff = np.gradient(f,x)\nprint(\"np.gradientì˜ ì¶œë ¥ê°’\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradientì˜ ì¶œë ¥ê°’\n[1.  3.  3.5 6.7 6.9 2.5]\n\n\n\\(x_1\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 1.0\n\n\n\\(x_2\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 2.9999999999999996\n\n\n\\(x_3\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 3.5\n\n\n\\(x_4\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 6.700000000000001\n\n\n\\(x_5\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 6.899999999999999\n\n\n\\(x_6\\)ì—ì„œì˜ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 2.5"
  },
  {
    "objectID": "posts/numpy pandas/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì°¨ì›-ë°°ì—´ì˜-ê²½ìš°-1",
    "href": "posts/numpy pandas/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì°¨ì›-ë°°ì—´ì˜-ê²½ìš°-1",
    "title": "Finite Difference Method with np.gradient",
    "section": "2ì°¨ì› ë°°ì—´ì˜ ê²½ìš°",
    "text": "2ì°¨ì› ë°°ì—´ì˜ ê²½ìš°\n2ì°¨ì› ë°°ì—´ì˜ ê²½ìš° axis=0(ì„¸ë¡œì¶•)ê³¼ axis=1(ê°€ë¡œì¶•) ë‘ ì¶•ë°©í–¥ìœ¼ë¡œ ê³„ì‚°í•œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ì„ ë°˜í™˜í•œë‹¤.axis=0ì¼ ê²½ìš° ê°ê°ì˜ ì—´ë§ˆë‹¤ ë”°ë¡œë”°ë¡œ ë…ë¦½ì ìœ¼ë¡œ \\(x_1,x_2...\\)ì— ëŒ€í•œ í•¨ìˆ«ê°’\\(f(x_1),f(x_2),\\dots\\)ì´ ìˆë‹¤ê³  ìƒê°í•˜ë©´ ë˜ê³  axis=1ì¼ ê²½ìš° ê°ê°ì˜ í–‰ë§ˆë‹¤ ë”°ë¡œë”°ë¡œ ë…ë¦½ì ìœ¼ë¡œ \\(x_1,x_2...\\)ì— ëŒ€í•œ í•¨ìˆ«ê°’\\(f(x_1),f(x_2),\\dots\\)ì´ ìˆë‹¤ê³  ìƒê°í•˜ë©´ ëœë‹¤.ë˜í•œ 1ì°¨ì› ë°°ì—´ê³¼ ìœ ì‚¬í•˜ê²Œ ê°ê°ì˜ í–‰,ì—´ì˜ ëê°’ì—ëŠ” ì „í–¥orí›„í–¥ì°¨ë¶„ê·¼ì‚¬ë¥¼ í–‰,ì—´ì˜ ë‚´ë¶€ì— ìˆëŠ” ê°’ì€ ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬ë¥¼ ì‚¬ìš©í•œë‹¤.\n\nEx) \\(dx=1,dy=1\\)\n2ì°¨ì› ë°°ì—´ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nnp.array([[1, 2, 6], [3, 4, 5]], dtype=float)\n\narray([[1., 2., 6.],\n       [3., 4., 5.]])\n\n\n\nax0_difcoef,ax1_difcoef= np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float))\nprint(f'axis = 0 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \\n{ax0_difcoef}')\nprint(f'axis = 1 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \\n{ax1_difcoef}')\n\naxis = 0 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \n[[ 2.  2. -1.]\n [ 2.  2. -1.]]\naxis = 1 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \n[[1.  2.5 4. ]\n [1.  1.  1. ]]\n\n\n\n\nEx) \\(dx \\not = 1,dy \\not = 1\\) (defaultê°€ ì•„ë‹Œ ê²½ìš°)\nê°ê°ì˜ í–‰,ì—´ë§ˆë‹¤ ê±°ë¦¬ë¥¼ ë”°ë¡œ ì„¤ì •í•´ì£¼ê³  ì‹¶ì€ ê²½ìš°? => ìŠ¤ì¹¼ë¼ 2ê°œ ì¸ìˆ˜ë¡œ ì „ë‹¬.\n\ndx = 2;dy = 2\nax0_difcoef,ax1_difcoef= np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float),dx,dy)\nprint(f'axis = 0 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \\n{ax0_difcoef}')\nprint(f'axis = 1 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \\n{ax1_difcoef}')\n\naxis = 0 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \n[[ 1.   1.  -0.5]\n [ 1.   1.  -0.5]]\naxis = 1 ë°©í–¥ìœ¼ë¡œ ë„í•¨ìˆ˜ì˜ ê·¼ì‚¿ê°’ ê³„ì‚° \n[[0.5  1.25 2.  ]\n [0.5  0.5  0.5 ]]"
  },
  {
    "objectID": "posts/numpy pandas/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì „í–¥ì°¨ë¶„ê·¼ì‚¬-ìœ ë„",
    "href": "posts/numpy pandas/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì „í–¥ì°¨ë¶„ê·¼ì‚¬-ìœ ë„",
    "title": "Finite Difference Method with np.gradient",
    "section": "ì „í–¥ì°¨ë¶„ê·¼ì‚¬ ìœ ë„",
    "text": "ì „í–¥ì°¨ë¶„ê·¼ì‚¬ ìœ ë„\n\\(x_1,x_2,\\dots,x_{i-1},x_i,x_{i+1},\\dots,x_n\\)ê³¼ ê°ê°ì— ëŒ€ì‘í•˜ëŠ” í•¨ìˆ«ê°’ \\(f(x_1),f(x_2),\\dots,f(x_{i-1}),f(x_i),f(x_{i+1}),\\dots,f(x_n)\\) ì£¼ì–´ì§„ ë°ì´í„°ë¼ê³  ê°€ì •í•˜ì. ëª©ì ì€ x_iì—ì„œì˜ ë¯¸ë¶„ê³„ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. \\(a = x_i\\)ì—ì„œ í•¨ìˆ˜\\(f(x)\\)ì˜ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ ê·¼ì‚¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f(x) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x-x_i) + \\frac{f^{''}(x_i)}{2!}(x-x_i)^2 + \\dots \\]\n\\(x=x_{i+1}\\)ì—ì„œì˜ í•¨ìˆ«ê°’ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f(x_{i+1}) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x_{i+1}-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x_{i+1}-x_i) + \\frac{f^{''}(x_i)}{2!}(x_{i+1}-x_i)^2 + \\dots \\]\n\\(f'(x_i)\\)ê°€ í¬í•¨ëœí•­ë§Œ ë‚¨ê²¨ë‘ê³  ë‚˜ë¨¸ì§€ëŠ” ì´í•­í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f^{'}(x_i)(x_{i+1}-x_i) = f(x_{i+1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}(x_{i+1}-x_i)^2 + \\dots\\]\n\\(h = x_{i+1}-x_i\\)ë¡œ ë‘ê³  ì–‘ë³€ì„ hë¡œ ë‚˜ëˆ„ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f'(x_i) = \\frac{f(x_{i+1})}{h} - \\frac{f(x_i)}{h} - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!}\\]\nì—¬ê¸°ì„œ ìš°ë³€ì˜ ë‘ê°œì˜ í•­ë§Œ ë‚¨ê²¨ë‘ê³  \\(O(h) = - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\)ë¼ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i+1})-f(x_i)}{h}\\\\\n&O(h) = - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\\\\n&\\text{where, } h = x_{i+1} - x_i \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/numpy pandas/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#í›„í–¥ì°¨ë¶„ê·¼ì‚¬-ìœ ë„",
    "href": "posts/numpy pandas/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#í›„í–¥ì°¨ë¶„ê·¼ì‚¬-ìœ ë„",
    "title": "Finite Difference Method with np.gradient",
    "section": "í›„í–¥ì°¨ë¶„ê·¼ì‚¬ ìœ ë„",
    "text": "í›„í–¥ì°¨ë¶„ê·¼ì‚¬ ìœ ë„\n\\(x_1,x_2,\\dots,x_{i-1},x_i,x_{i+1},\\dots,x_n\\)ê³¼ ê°ê°ì— ëŒ€ì‘í•˜ëŠ” í•¨ìˆ«ê°’ \\(f(x_1),f(x_2),\\dots,f(x_{i-1}),f(x_i),f(x_{i+1}),\\dots,f(x_n)\\) ì£¼ì–´ì§„ ë°ì´í„°ë¼ê³  ê°€ì •í•˜ì. ëª©ì ì€ x_iì—ì„œì˜ ë¯¸ë¶„ê³„ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. \\(a = x_i\\)ì—ì„œ í•¨ìˆ˜\\(f(x)\\)ì˜ í…Œì¼ëŸ¬ ê¸‰ìˆ˜ ê·¼ì‚¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f(x) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x-x_i) + \\frac{f^{''}(x_i)}{2!}(x-x_i)^2 + \\dots \\]\n\\(f(x_i)\\)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f(x_{i-1}) = \\sum_{n=0}^{\\infty}\\frac{f^n(x_i)}{n!}(x_{i-1}-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x_{i-1}-x_i) + \\frac{f^{''}(x_i)}{2!}(x_{i-1}-x_i)^2+\\dots \\]\n1ì°¨ë¯¸ë¶„ì´ í¬í•¨ëœ í•­ë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ì´í•­í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f^{'}(x_i)(x_{i-1}-x_i) = f(x_{i-1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}(x_{i-1}-x_i)^2-\\frac{f^{'''}(x_i)}{3!}(x_{i-1}-x_i)^3-\\dots \\]\n\\(h = x_{i} - x_{i-1}\\)ë¡œ ë†“ìœ¼ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f^{'}(x_i)(-h) = f(x_{i-1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}h^2+\\frac{f^{'''}(x_i)}{3!}h^3-\\dots \\]\nì–‘ë³€ì„ \\(-h\\)ë¡œ ë‚˜ëˆ„ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\nf^{'}(x_i) &= \\frac{f(x_{i-1})}{-h} + \\frac{f(x_i)}{h} + \\frac{f^{''}(x_i)}{2!}h-\\frac{f^{'''}(x_i)}{3!}h^2+\\dots \\\\\n&=\\frac{f(x_i)-f(x_{i-1}) }{h} +  \\frac{f^{''}(x_i)}{2!}h-\\frac{f^{'''}(x_i)}{3!}h^2+\\dots\n\\end{aligned}\\]\në§ˆì°¬ê°€ì§€ë¡œ ìš°ë³€ì˜ ë‘ê°œ í•­ë§Œ ë‚¨ê²¨ë‘ê³  \\(O(h) = \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} + \\dots\\)ë¼ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i})-f(x_{i-1})}{h}\\\\\n&O(h) = \\frac{h}{2!}f^{''}(x_i) - \\frac{h^2}{3!}f{'''}(x_i)+\\dots \\\\\n&\\text{where, } h = x_{i} - x_{i-1}, \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/numpy pandas/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬-ìœ ë„",
    "href": "posts/numpy pandas/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient/ìˆ˜ì¹˜ë¯¸ë¶„ & np.gradient.html#ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬-ìœ ë„",
    "title": "Finite Difference Method with np.gradient",
    "section": "ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬ ìœ ë„",
    "text": "ì¤‘ì•™ì°¨ë¶„ê·¼ì‚¬ ìœ ë„\nì „í–¥ì°¨ë¶„ê·¼ì‚¬ì™€ í›„í–¥ì°¨ë¶„ê·¼ì‚¬ì˜ ìœ ë„ê³¼ì •ì—ì„œì˜ í…Œì¼ëŸ¬ ì „ê°œì‹ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[f'(x_i) = \\frac{f(x_{i+1})}{h} - \\frac{f(x_i)}{h} - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - ...\\] \\[f'(x_i) = \\frac{f(x_{i})}{h} - \\frac{f(x_{i-1})}{h} + \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} + ...\\]\në‘ ì‹ì„ ë”í•´ì£¼ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&2f^{'}(x_i) = \\frac{f(x_{i+1})-f(x_{i-1})}{h} - \\frac{2h^2f^{'''}(x_i)}{3!} \\\\\n&\\Leftrightarrow f^{'}(x_i) = \\frac{f(x_{i+1})-f(x_{i-1})}{2h} - \\frac{h^2f^{'''}(x_i)}{3!} - ...\n\\end{aligned}\\]\në§ˆì°¬ê°€ì§€ë¡œ ìš°ë³€ì˜ ë‘ê°œ í•­ë§Œ ë‚¨ê²¨ë‘ê³  ì ˆë‹¨ì˜¤ì°¨\\(O(h^2) = -\\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\)ë¼ í•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i+1})-f(x_{i-1})}{2h}\\\\\n&O(h) = - \\frac{h^2}{3!}f{'''}(x_i)+\\dots \\\\\n&\\text{where, } h = x_{i} - x_{i-1}\\,\\,\\text{or}\\,\\, h = x_{i+1} - x_i\\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html",
    "href": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html",
    "title": "Auto-Encoding Variational Bayes(ì‘ì„±ì¤‘)",
    "section": "",
    "text": "variational auto-encoderëŠ” generative modelë¡œì„œ intractableí•œ posteriorë¥¼ í¬í•¨í•˜ëŠ” latent variableì´ ìˆê³  large datasetì—ë„ ì˜ ë™ì‘í•œë‹¤. ì´ëŠ” 1)variational lower boundë¥¼ reparamet-erizationì„ í†µí•´ ìƒ˜í”Œë§í•˜ê³  SGDë¥¼ ì‚¬ìš©í•˜ë©° 2)posteriorë¥¼ variational inferenceë¡œ êµ¬í•œ ë’¤ encoder(inference model)ë¡œ í•™ìŠµì‹œí‚¤ê¸°ì— ê°€ëŠ¥í•˜ë‹¤. ì´ë²ˆ í¬ìŠ¤íŠ¸ì—ì„œëŠ” ë…¼ë¬¸ì—ì„œ ì‚¬ìš©ëœ ë°©ë²•ë“¤ì— ëŒ€í•´ì„œ ìì„¸í•˜ê²Œ ìˆ˜ì‹ì ìœ¼ë¡œ ì„¤ëª…í•œë‹¤."
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#latent-variable-model",
    "href": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#latent-variable-model",
    "title": "Auto-Encoding Variational Bayes(ì‘ì„±ì¤‘)",
    "section": "Latent Variable Model",
    "text": "Latent Variable Model\nlatent variable modelì€ ê´€ì¸¡ê°€ëŠ¥í•œ ë°ì´í„°ë¥¼ ë³€ìˆ˜ì™€ ê´€ë ¨ì§“ëŠ” í†µê³„í•™ì  ëª¨ë¸ì´ë‹¤. ì´ë•Œì˜ ë³€ìˆ˜ë¥¼ latent variableì´ë¼ê³  í•˜ë©° ë°ì´í„°ê°€ ì–´ë–»ê²Œ ìƒì„±ë˜ëŠ”ì§€ì— ì˜í–¥ì„ ë¯¸ì¹œë‹¤. latent variable modelì—ì„œ ë°ì´í„°ê°€ ë§Œë“¤ì–´ì§€ëŠ” ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n(Data Generation Process) 1. latent variable \\(\\bf{z}\\)ê°€ \\(p(\\bf{z})\\)ì—ì„œ ë¨¼ì € samplingëœë‹¤.  2. observed data \\(\\bf{x}\\)ê°€ ê·¸ í›„ \\(p({\\bf{x}}|{\\bf{z}}) = p({\\bf{x}}|f_{\\theta}({\\bf{z}}))\\)ì—ì„œ samplingëœë‹¤. \n\n\\(\\bf{z}\\)ì˜ dimensionì€ ì¼ë°˜ì ìœ¼ë¡œ \\(\\bf{x}\\)ì˜ dimensionë³´ë‹¤ ì‘ë‹¤ê³  ê°€ì •í•œë‹¤.\nì—¬ê¸°ì„œ \\(f_\\theta(\\bf{z})\\)ëŠ” \\(\\theta\\)ë¥¼ parameterë¡œ ê°–ëŠ” vector functionìœ¼ë¡œ ì—¬ëŸ¬ê°œ(ë˜ëŠ” í•˜ë‚˜ì˜)ì˜ outputì„ ê°€ì§‘ë‹ˆë‹¤.\n\nì˜ˆë¥¼ ë“¤ì–´ í•™êµì— â€œê³ ì–‘ì´â€ë¥¼ ì£¼ì œë¡œ í•˜ì—¬ ê³¼ì œë¥¼ ì œì¶œí•´ì•¼ í•œë‹¤ê³  ê°€ì •í•´ë³´ì. latent variableì€ êµìˆ˜ë‹˜ì´ ì§€ì •í•œ ê¸€ìí¬ê¸°,ê¸€ìê°„ê²©,ê¸€ììˆ˜,ì£¼ì œ(ê³ ì–‘ì´)ë¡œ ë¹„ìœ í•  ìˆ˜ ìˆìœ¼ë©° ì œì¶œí•  ê³¼ì œëŠ” ì´ëŸ¬í•œ ì ì¬ë³€ìˆ˜ë“¤ì´ ê²°ê³¼ë¥¼ ë¯¸ì³¤ì„ ê²ƒì´ë¼ ë³¼ ìˆ˜ ìˆë‹¤. íŠ¹íˆ ì£¼ëª©í•  ì ì€ latent variable \\(\\bf{z}\\)ê°€ ê·¸ëŒ€ë¡œ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ë„ ìˆê³  ì ì ˆí•œ transform(function)ì„ ê±°ì¹œ \\(f_{\\theta}(\\bf{z})\\)ê°€ ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ë„ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ì´ëŠ” ì£¼ì œê°€ â€œê³ ì–‘ì´â€ë¡œ ì„ ì •ë˜ì—ˆì–´ë„ ëˆ„êµ°ê°€ëŠ” â€œê³ ì–‘ì´ì—ê²Œ ì¤˜ì•¼í•˜ëŠ” ìŒì‹â€ìœ¼ë¡œ ë‹¤ë¥¸ ëˆ„êµ°ê°€ëŠ” â€œê³ ì–‘ì´ì˜ í–‰ë™ë¶„ì„â€ìœ¼ë¡œ ê³¼ì œë¥¼ ì œì¶œí•˜ëŠ” ê²ƒì— ë¹„ìœ í•  ìˆ˜ ìˆë‹¤."
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#generative-model",
    "href": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#generative-model",
    "title": "Auto-Encoding Variational Bayes(ì‘ì„±ì¤‘)",
    "section": "Generative Model",
    "text": "Generative Model\nGenerative Model(ìƒì„±ëª¨ë¸)ì€ observed dataì™€ ìœ ì‚¬í•˜ë©´ì„œë„ ë‹¤ë¥¸ new samplesì„ ìƒì„±í•´ë‚´ëŠ” ê²ƒì´ ëª©ì ì´ë‹¤. ì´ë¥¼ ë‹¤ì‹œë§í•˜ë©´ Generative modelì˜ ëª©ì ì€ \\(p_{data}(x)\\)ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤. ê´€ì¸¡ëœ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” í™•ë¥ ë¶„í¬ \\(p_{data}(\\bf{x})\\)ë¥¼ ì•Œë©´ samplingì„ í†µí•´ì„œ ìƒì„±í•´ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ë‹¤."
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#variational-inference",
    "href": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#variational-inference",
    "title": "Auto-Encoding Variational Bayes(ì‘ì„±ì¤‘)",
    "section": "Variational Inference",
    "text": "Variational Inference\n\\(p(\\bf{{\\bf{z|x}}})\\)ëŠ” ì–´ë–»ê²Œ êµ¬í•  ìˆ˜ ìˆì„ê¹Œ? ë…¼ë¬¸ì—ì„œëŠ” variational inferneceë¥¼ ì‚¬ìš©í•œë‹¤.\n\\[p({\\bf{z|x}}) = \\frac{p({\\bf{x|z}})p({\\bf{z}})}{\\int_{\\bf{z}}p({\\bf{x|z}})p(\\bf{z})d{\\bf{z}}}\\]\nvariational inferenceì—ì„œ í’€ê³ ì í•˜ëŠ” ë¬¸ì œëŠ” posteriorë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. posteriorì¸ \\(p(\\bf{z|x})\\)ëŠ” ì¼ë°˜ì ìœ¼ë¡œ intractableí•˜ë‹¤. ì™œëƒí•˜ë©´ ë¶„ëª¨ì— ìˆëŠ” \\(\\bf{z}\\)ì— ëŒ€í•œ ì ë¶„ì´ ì¼ë°˜ì ìœ¼ë¡œ ë„ˆë¬´ ê³ ì°¨ì›ì´ì—¬ì„œ ê³„ì‚°ì´ ë¶ˆê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì´ë‹¤. variational inferenceëŠ” \\(p(\\bf{z|x})\\)ë¥¼ ê·¼ì‚¬ì ìœ¼ë¡œë¼ë„ êµ¬í•˜ê¸° ìœ„í•´ \\(q_{\\boldsymbol{\\phi}}(\\bf{z|x})\\)ë¥¼ ì •ì˜í•œ í›„ ì´ë¥¼ \\(p(\\bf{z|x})\\)ì— ì¶©ë¶„íˆ ê°€ê¹ê²Œ ë‹¤ê°€ê°€ë„ë¡ í•˜ëŠ” ë°©ì‹ì„ ì·¨í•œë‹¤. ì´ëŠ” ìˆ˜í•™ì ìœ¼ë¡œ ë‘ ë¶„í¬ê°„ì˜ ì°¨ì´ë¥¼ ì•Œë ¤ì£¼ëŠ” KL-divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” \\(q_{\\boldsymbol{\\phi}}(\\bf{z|x})\\)ë¥¼ êµ¬í•˜ëŠ” ë¬¸ì œì´ë‹¤. ì—¬ê¸°ì„œ \\(q_{\\boldsymbol{\\phi}}(\\bf{z|x})\\)ëŠ” ì„ì˜ì ìœ¼ë¡œ ì •í•  ìˆ˜ ìˆëŠ” í™•ë¥ ë¶„í¬ì„ì— ì£¼ëª©í•˜ì. \\(q_{\\boldsymbol{\\phi}}(\\bf{z|x})\\)ëŠ” ìš°ë¦¬ê°€ ì˜ ì•Œê³ ìˆê³  ë” ì ì€ íŒŒë¼ë¯¸í„°ë¥¼ ê°€ì§€ë©° ê³„ì‚°ì´ ì‰¬ìš´ í¸ë¦¬í•œ í•¨ìˆ˜ë¡œ ê°€ì •í•˜ì—¬ ë¬¸ì œë¥¼ ì‰½ê²Œ í’€ ìˆ˜ ìˆê²Œ í•´ì¤€ë‹¤.\n(definition of KL-divergence)\n\\[\\begin{aligned}\n\\text{D}_{KL}[q_{\\boldsymbol{\\phi}}({\\bf{z|x}})||p({\\bf{z|x}})] &:= \\int_zq_{\\boldsymbol{\\phi}}({\\bf{z|x}})\\text{log}\\frac{q_{\\boldsymbol{\\phi}}({\\bf{z|x}})}{p({\\bf{z|x}})}dz \\\\\n\\end{aligned}\\]\n(variational inference)\n\\[\\begin{aligned}\n&{\\hat{q_{\\boldsymbol{\\phi}}}} = \\underset{q_{\\boldsymbol{\\phi}}}{\\text{argmin}}\\,\\text{D}_{KL}[q_{\\boldsymbol{\\phi}}({\\bf{z|x}})||p({\\bf{z|x}})]\n\\end{aligned}\\]\nì—¬ê¸°ì„œ \\(q_{\\boldsymbol{\\phi}}\\)ëŠ” \\(\\boldsymbol{\\phi}\\)ê°€ parameterì´ë¯€ë¡œ ìœ„ì˜ KL-divergenceë¥¼ ìµœì†Œí™”í•˜ëŠ” \\(q_{\\boldsymbol{\\phi}}\\)ë¥¼ ì°¾ëŠ” ë¬¸ì œëŠ” ìµœì†Œí™”í•˜ëŠ” \\(\\boldsymbol{\\phi}\\)ë¥¼ ì°¾ëŠ”ê²ƒê³¼ ë™ì¼í•˜ë‹¤. ì¦‰,ë‹¤ìŒê³¼ ê°™ë‹¤.\n(variational inference)\n\\[\\begin{aligned}\n&{{\\boldsymbol{\\phi}}} = \\underset{{\\boldsymbol{\\phi}}}{\\text{argmin}}\\,\\text{D}_{KL}[q_{\\boldsymbol{\\phi}}({\\bf{z|x}})||p({\\bf{z|x}})]\n\\end{aligned}\\]\nKL-divergenceë¥¼ log likelyhoodì—ì„œ ì°¾ì„ ìˆ˜ ìˆë‹¤. log likelyhoodë¥¼ ì „ê°œí•˜ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n(log likelyhood expansion)\n\\[\\begin{aligned}\n\\text{log}\\,(p_{\\boldsymbol{\\theta}}({\\bf{x}}))& = \\int_z\\text{log}\\,p_{\\boldsymbol{\\theta}}({\\bf{x}})q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}}\\quad\\leftarrow \\int_z q_{\\boldsymbol{\\phi}}(\\bf{z|x})d{\\bf{z}}= 1\\\\\n&=\\int_z\\text{log}\\left(\\frac{p(\\bf{z,x})}{p(\\bf{z|x})}\\right)q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}}\\quad\\leftarrow p_{\\boldsymbol{\\theta}}({\\bf{x}}) = \\frac{p({\\bf{x,z}})}{p(\\bf{z|x})}\\\\\n&=\\int_z\\text{log}\\left(\\frac{p(\\bf{z,x})}{q_{\\boldsymbol{\\phi}}(\\bf{z|x})}\\cdot\\frac{q_{\\boldsymbol{\\phi}}({\\bf{z|x}})}{p({\\bf{z|x}})}\\right)q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}}\n\\\\\n&= \\int_z\\text{log}\\left(\\frac{p(\\bf{z,x})}{q_{\\boldsymbol{\\phi}}(\\bf{z|x})}\\right)q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}} + \\int_z\\text{log}\\left(\\frac{q_{\\boldsymbol{\\phi}}(\\bf{z,x})}{p(\\bf{z|x})}\\right)q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}}\\\\\n&= \\mathcal{L(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}})} + D_{KL}(q_{\\boldsymbol{\\phi}}({\\bf{z|x}})||p({\\bf{z|x}}))\n\n\\end{aligned}\\]\n\nELBO(\\(\\mathcal{L}\\))ëŠ” evidence lower boundì˜ ì•½ìë¡œ likelyhood(evidence,ìš©ì–´ë§Œë‹¤ë¥´ë‹¤)ì˜ í•˜í•œ(lower bound)ì´ë‹¤. ì¦‰,ëª¨ë“  xì—ì„œ evidenceëŠ” ëª¨ë“  ELBOë³´ë‹¤ í¬ë‹¤.\n\\(\\text{log}\\,p({\\bf{x}})\\) = evidence = log likelyhoodì´ë‹¤.(í—·ê°ˆë¦¬ì§€ ë§ìâ€¦)\nìì„¸í•œ ELBOì— ëŒ€í•œ ì¦ëª…ì€ Appendix ì°¸ê³ \n\nìœ— ì‹ì„ ë³´ë©´ ê²°êµ­ KL-divergenceë¥¼ maximizeí•˜ëŠ” \\({\\boldsymbol{\\phi}}\\)ë¥¼ ì°¾ëŠ” ê²ƒì€ ELBOë¥¼ minimizeí•˜ëŠ” \\({\\boldsymbol{\\phi}}\\)ë¥¼ ì°¾ëŠ” ê²ƒê³¼ ê°™ì€ ë¬¸ì œë‹¤.\n\\[\\begin{aligned}\n\\hat{\\boldsymbol{\\boldsymbol{\\phi}}} = \\underset{\\boldsymbol{\\boldsymbol{\\phi}}}{\\text{argmax}}\\,\\mathcal{L(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}})}\n\\end{aligned}\\]\nì •ë¦¬í•˜ìë©´ ëª©ì ì€ \\(p(\\bf{z|x})\\)ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ì—ˆë‹¤. ì´ëŠ” variational inferenceì—ì„œëŠ” ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” ê³„ì‚°í•˜ê¸° í¸ë¦¬í•œ í˜•íƒœì¸ \\(q_{\\boldsymbol{\\phi}}\\)ë¥¼ \\(p\\)ì™€ ë¹„ìŠ·í•˜ê²Œ ë§Œë“œëŠ” ë°©ë²•ì´ì—ˆê³  ìˆ˜í•™ì ìœ¼ë¡œ ì´ëŠ” KL-divergenceë¥¼ minimizeí•˜ëŠ” \\(q_{\\boldsymbol{\\phi}}\\) ë˜ëŠ” \\(\\boldsymbol{\\phi}\\)ë¥¼ ì°¾ëŠ” ê²ƒê³¼ ê°™ì•˜ìœ¼ë©° ê³„ì†í•´ì„œ ìˆ˜ì‹ì „ê°œí•˜ë©´ ê²°êµ­ì—ëŠ” ELBOë¥¼ maximizeí•˜ëŠ” \\(\\boldsymbol{\\boldsymbol{\\phi}}\\)ë¥¼ ì°¾ëŠ” ë¬¸ì œì™€ ê°™ì•˜ë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ë³´ë©´ì´ì œ ELBOë¥¼ maximizeí•˜ëŠ” optimizationë¬¸ì œê°€ ë˜ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ê¹Œì§€ 3ë²ˆ MLEë¡œ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤ì„ ë§‰ê¸°ìœ„í•´ ì´ìƒì ì¸ samplingí•¨ìˆ˜(posterior)ë¥¼ ì–»ëŠ” ê³¼ì •ì„ í’€ì—ˆë‹¤. ì´ì œ MLEë¥¼ í’€ê¸°ë§Œ í•˜ë©´ ëœë‹¤."
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#maximum-likleyhood-estimation",
    "href": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#maximum-likleyhood-estimation",
    "title": "Auto-Encoding Variational Bayes(ì‘ì„±ì¤‘)",
    "section": "Maximum Likleyhood Estimation",
    "text": "Maximum Likleyhood Estimation\nìœ„ì—ì„œ ì‚¬ì‹¤ì€ ì‚´ì§ MLEë¡œ \\(\\theta\\)ë¥¼ ì–¸ê¸‰í–ˆì—ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ í•˜ë ¤ëŠ” ê²ƒì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\hat{\\theta} = \\underset{{\\bf{\\theta}}}{\\text{argmax}}\\,\\text{log}\\,p_{\\boldsymbol{\\theta}}(\\bf{x})\n\\end{aligned}\\]\n\nlog likelyhoodë¥¼ maximizeí•˜ë„ë¡ ë°”ê¿ˆ.\n\nì—¬ê¸°ì‚¬ ë‹¤ì‹œ ë‚˜íƒ€ë‚˜ëŠ” ë¬¸ì œëŠ” \\(p{\\bf{({\\bf{x}})}} = \\int_z p({\\bf{z}})p({\\bf{x|z}})d{\\bf{z}}\\)ì´ë‹¤. ìœ„ì—ì„œë„ ì–¸ê¸‰í–ˆë“¯ì´ ì´ëŠ” \\(\\bf{z}\\)ì— ëŒ€í•œ ë§¤ìš° ê³ ì°¨ì› ì ë¶„ìœ¼ë¡œ explictí•˜ê²Œ êµ¬í•  ìˆ˜ ì—†ìœ¼ë©° Monte carlo methodì˜ ê²½ìš°ì—ë„ ì°¨ì›ì´ ë„ˆë¬´ ì»¤ì„œ ì¶”ì •í•˜ëŠ”ë° ë§¤ìš° ì˜¤ëœì‹œê°„ì´ ê±¸ë¦¬ë¯€ë¡œ ê±°ì˜ ë¶ˆê°€ëŠ¥ì— ê°€ê¹ë‹¤. ì—¬ê¸°ì„œ ë¬¸ì œë¥¼ ì•½ê°„ ìš°íšŒí•˜ì—¬ \\(\\text{log}\\,p(\\bf{x})\\)ì˜ lowerboundì¸ ELBOë¥¼ maximizeí•˜ëŠ” ë¬¸ì œë¡œ ë°”ê¿”ë³´ì. evidenceì˜ lowerboundì¸ ELBOë¥¼ maximizeí•˜ë©´ evidenceìì²´ë„ ì–´ëŠì •ë„ maximizeë  ê²ƒì´ë‹¤.\n\\[\\begin{aligned}\n&\\hat{\\theta} = \\underset{{\\bf{\\theta}}}{\\text{argmax}}\\,\\text{log}\\,p(\\bf{x}) \\approx \\underset{{\\bf{\\theta}}}{\\text{argmax}}\\,\\mathcal{L}({\\boldsymbol{\\theta}},\\boldsymbol{\\boldsymbol{\\phi}};{\\bf{x}}) \\\\\n\\end{aligned}\\]\nì—¬ê¸°ì„œ ì•Œ ìˆ˜ ìˆëŠ” ê²ƒì€ ë¬¸ì œê°€ í•˜ë‚˜ì˜ ELBOì— ëŒ€í•œ \\(\\phi,\\theta\\)ì˜ maximization ë¬¸ì œë¡œ ë°”ë€Œë‹¤ëŠ” ê²ƒì´ë‹¤. ì •ë¦¬í•˜ìë©´ ì´ ë¬¸ì œë§Œ í’€ê²Œë˜ë©´ ìš°ë¦¬ëŠ” **\\(q_{\\phi}({\\bf{x|z}})\\approx p({\\bf{x|z}})\\)ì¸ ì´ìƒì ì¸ ìƒ˜í”Œë§ í•¨ìˆ˜ì¸ posteriorë¥¼ ì–»ì„ ë¿ë§Œ ì•„ë‹ˆë¼ MLEë„ í’€ ìˆ˜ ìˆê²Œ ëœë‹¤.\n\\[\\begin{aligned}\n&\\boldsymbol{\\theta,\\boldsymbol{\\phi}}=\\underset{{\\boldsymbol{\\theta,\\boldsymbol{\\phi}}}}{\\text{argmax}}\\,\\mathcal{L}(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};\\bf{x})\\\\\n\\end{aligned}\\]\nê·¸ëŸ¬ë‚˜ ELBOë„ ì•„ì§ì€ intractableí•œ ì ë¶„ì´ í¬í•¨ë˜ê¸° ë•Œë¬¸ì— ì–¼í•ë³´ë©´ í’€ ìˆ˜ ì—†ëŠ” ìƒíƒœì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ìì„¸íˆ ë³´ë©´ ELBOëŠ” ìš°ë¦¬ê°€ ê°€ì •í•œ ì—¬ëŸ¬ê°œì˜ ê³„ì‚°ì´ í¸ë¦¬í•œ í•¨ìˆ˜ì´ë¯€ë¡œ optimizationì´ ê°€ëŠ¥í•œ í˜•íƒœì„ì„ ì•Œ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œ ë¶€í„°ëŠ” ì§ì ‘ ELBOë¥¼ í’€ì–´ì„œ optimizationì„ í•´ë³´ì."
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#solving-optimization-problem",
    "href": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#solving-optimization-problem",
    "title": "Auto-Encoding Variational Bayes(ì‘ì„±ì¤‘)",
    "section": "Solving Optimization Problem",
    "text": "Solving Optimization Problem\në¨¼ì € ELBOì™€ ìš°ë¦¬ê°€ ì•Œê³  ìˆëŠ” ì‚¬ì‹¤ë“¤ì„ ì •ë¦¬í•˜ì. ìœ„ì—ì„œ \\(q_{\\phi}\\)ëŠ” ì„ì˜ì ìœ¼ë¡œ ì •í•  ìˆ˜ ìˆëŠ” ê³„ì‚°í•˜ê¸° í¸ë¦¬í•œ í•¨ìˆ˜ì´ë©° ì—¬ê¸°ì„œëŠ” ì •ê·œë¶„í¬ë¡œ ê°€ì •í•´ë³´ì.\n(Goal)\n\\[\\begin{aligned}\n&\\boldsymbol{\\theta,\\boldsymbol{\\phi}}=\\underset{{\\boldsymbol{\\theta,\\boldsymbol{\\phi}}}}{\\text{argmax}}\\,\\mathcal{L}(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};\\bf{x})\\\\\n\\end{aligned}\\]\n(ELBO)\n\\[\\begin{aligned}\n\\mathcal{L(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}})}&:= \\int_z\\text{log}\\left(\\frac{p(\\bf{z,x})}{q_{\\boldsymbol{\\phi}}(\\bf{z|x})}\\right)q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}}\\\\\n\\end{aligned}\\]\n(Assumption)\n\\[\\begin{aligned}\n&p({\\bf{z}}) = \\mathcal{N}({\\bf{z}}|0,{\\bf{I}})\\\\\n&p({\\bf{x|z}}) = \\mathcal{N}({\\bf{x}}|f_{\\theta}(\\bf{z}),\\boldsymbol{\\sigma^2}{\\bf{I}})\\\\\n&q_{\\boldsymbol{\\phi}}({\\bf{z|x}}) = \\mathcal{N}({\\bf{z}};\\boldsymbol{\\mu,\\sigma^2}\\bf{I})\\\\\n\n\\end{aligned}\\]\nì—¬ê¸°ì„œ ELBOëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì „ê°œí•  ìˆ˜ ìˆë‹¤.(ìì„¸í•œ ì¦ëª…ì€ Appendix ì°¸ê³ )\n(ELBO expansion)\n\\[\\begin{aligned}\n\\mathcal{L(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}})}&= \\int_z\\text{log}\\left(\\frac{p(\\bf{z,x})}{q_{\\boldsymbol{\\phi}}(\\bf{z|x})}\\right)q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}}\\\\\n&=\\mathbb{E}_{q_{\\phi}({\\bf{z|x}})}\\left[\\text{log}\\,(p({\\bf{x}}|f_{\\theta}({\\bf{z}})))\\right] - D_{KL}(q_{\\phi}({\\bf{z|x}})||p({\\bf{z}}))\\\\\n\n\\end{aligned}\\]\nì—¬ê¸°ì„œ ì ê¹ ì „ê°œëœ ELBOë¥¼ ì‚´í´ë³´ë©´ ë§ì€ insightë¥¼ ì¤€ë‹¤. ìœ— ì‹ì˜ RHSì˜ ì²«ë²ˆì§¸ í•­ì„ \\(\\phi,\\theta\\)ì— ëŒ€í•´ maximizeí•œë‹¤ëŠ” ê²ƒì€ \\(\\theta,\\phi\\)ë¥¼ ì ì ˆíˆ í•™ìŠµí•˜ì—¬ \\(q_{\\phi}\\)ì—ì„œ samplingëœ \\(\\bf{z}\\)ë¡œë¶€í„° \\(x\\)ë¥¼ ë‹¤ì‹œ ë³µì›í•  í™•ë¥ ì„ ìµœëŒ€í™” í•œë‹¤ëŠ” ê²ƒìœ¼ë¡œ ì¼ë°˜ì ì¸ Auto-encoderì—ì„œ reconstruction errorë¥¼ ìµœì†Œí™” í•˜ëŠ”ê²ƒê³¼ ê°™ë‹¤.** ì—¬ê¸°ì„œ ë‘ë²ˆì§¸ í•­ì€ Regularizationì˜ ì—­í• ì„ í•˜ëŠ”ë° ì´ëŠ” posteriorë¥¼ ê·¼ì‚¬í•œ \\(q_{\\phi}(\\bf{z|x})\\)ê°€ \\(p(\\bf{z})\\)ì™€ ë¹„ìŠ·í•˜ë„ë¡ ìœ ì§€í•˜ê²Œ í•´ì£¼ë©° ë”°ë¼ì„œ ìš°ë¦¬ëŠ” í•™ìŠµì„ ì™„ë£Œí•œ decoderì—ì„œ ê·¸ëƒ¥ \\(q_{\\phi}\\)ì¸ encoderë¥¼ ë–¼ë²„ë¦¬ê³  ì •ê·œë¶„í¬ \\(p(\\bf{z})\\)ì—ì„œ \\(\\bf{z}\\)ë¥¼ ìƒ˜í”Œë§ í•œ í›„ Networkì˜ inputìœ¼ë¡œ ì£¼ë©´ ëœë‹¤.\n\\[\\begin{aligned}\n\\mathcal{L}(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}},{\\bf{x}}) &= \\mathbb{E}_{Z\\sim q}\\left[\\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}}) \\right] \\\\\n&= \\int_z\\left[\\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}})\\right]q({\\bf{z|x}})d{\\bf{z}}\n\\end{aligned}\\]\nì´ì „ì—ë„ ê³„ì† ë¬¸ì œì˜€ë˜ \\(d\\bf{{z}}\\)ê°€ ë˜ ë“±ì¥í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ìš°ë¦¬ëŠ” expectationì„ êµ¬í•  ìˆ˜ ì—†ë‹¤. ë”°ë¼ì„œ sampingì„ í†µí•´ expectationì„ ê·¼ì‚¬ì ìœ¼ë¡œ êµ¬í•˜ëŠ” Monte carlo methodë¥¼ ì‚¬ìš©í•œë‹¤. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\mathcal{L}(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}}) &= \\mathbb{E}_{Z\\sim q}\\left[\\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}}) \\right] \\\\\n&= \\int_z\\left[\\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}})\\right]q({\\bf{z|x}})d{\\bf{z}} \\\\\n&\\approx \\frac{1}{L}\\sum_{l=1}^L[\\text{log}\\,p({\\bf{x,z}}^{(l)}) - \\text{log}\\,q_{\\boldsymbol{{\\boldsymbol{\\phi}}}}({\\bf{z}}|{\\bf{x}}^{(l)})]\n\\end{aligned}\\]\n\\[\\text{where } {\\bf{z}}^{(l)} \\sim q_{\\boldsymbol{\\phi}}({\\bf{z}})\\]\nELBOë¥¼ ê·¼ì‚¬ì ìœ¼ë¡œ êµ¬í–ˆìœ¼ë¯€ë¡œ ìµœì í™”ë¥¼ ìœ„í•´ gradientë¥¼ êµ¬í•´ì•¼ í•œë‹¤. ë¨¼ì € \\(\\theta\\)ì— ëŒ€í•œ gradientë¥¼ êµ¬í•´ë³´ì.\n\\[\\begin{aligned}\n\\nabla_{\\theta}\\mathcal{L}(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}}) &= \\mathbb{E}_{Z\\sim q}\\left[\\nabla_\\theta\\text{log}\\,p({\\bf{x,z}})\\right]\\\\\n&\\approx \\frac{1}{L}\\sum_{l=1}^L\\nabla_\\theta\\text{log}\\,p({\\bf{x,z}}^{(l)})\n\\end{aligned}\\]\n\në’·í•­ì€ \\(\\theta\\)ì— ëŒ€í•´ parameterizeë˜ì–´ìˆì§€ ì•Šìœ¼ë¯€ë¡œ ë¯¸ë¶„ê³¼ì •ì—ì„œ ì œê±°ë¨.\n\n\\(\\theta\\)ì— ëŒ€í•œ gradientëŠ” ì „í˜€ ë¬¸ì œì—†ì´ ì˜ êµ¬í•´ì§ì„ ì•Œ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ ë¬¸ì œëŠ” \\(\\boldsymbol{\\phi}\\)ì— ëŒ€í•œ gradientë¥¼ êµ¬í•  ë•Œ ë°œìƒí•œë‹¤. \\(f({\\bf{z}}) = \\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}})\\)ë¼ í•  ë•Œ gradientëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í•´ì§„ë‹¤.(ì¦ëª…ìƒëµ)\n\\[\\begin{aligned}\n\\nabla_{\\boldsymbol{\\phi}}\\mathbb{E}_{q_{\\boldsymbol{\\phi}}({\\bf{z})}}[f({\\bf{z}})] &= \\mathbb{E}_{q_{\\boldsymbol{\\boldsymbol{\\phi}}(\\bf{z})}} \\nabla_{q_{\\boldsymbol{\\boldsymbol{\\phi}}(\\bf{z})}}\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z}}) \\\\&\\approx \\frac{1}{L}\\sum_{l=1}^L f({\\bf{z}})\\nabla_{q_{\\boldsymbol{\\boldsymbol{\\phi}}({\\bf{z}})}}\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z}}^{(l)})\n\\end{aligned}\\]\nê°‘ìê¸° ë¡œê·¸ê°€ ë“¤ì–´ê°„ í˜•íƒœë¡œ gradientê°€ êµ¬í•´ì§„ë‹¤. ì´ëŸ¬í•œ gradientì— ëŒ€í•œ ì¶”ì •ëŸ‰ì€ unbiased estimatorì´ì§€ë§Œ high varianceë¥¼ ê°€ì§„ë‹¤ê³  í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ìƒ˜í”Œë§ì„ ì•„ì£¼ë§ì´(ê±°ì˜ë¬´í•œí•˜ê²Œ)ì·¨í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë©´ ì´ ê°’ì€ ìˆ˜ë ´í•˜ì§€ ì•ŠëŠ” ê°’ì´ë¯€ë¡œ ì ë‹¹í•œ ìƒ˜í”Œë§ì„ í†µí•´ì„œ ë¬¸ì œë¥¼ í’€ì–´ì•¼ í•˜ëŠ” ìš°ë¦¬ì˜ ë°©ì‹ì—ëŠ” ë§ì§€ì•ŠëŠ”ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ìƒ˜í”Œì„ ëŒ€ì²´ì ìœ¼ë¡œ ì·¨í•˜ëŠ” ë°©ì‹ì¸ reparameterizationì„ ì‚¬ìš©í•œë‹¤. ì´ë¥¼ ì‚¬ìš©í•˜ë©´ unbiased estimatorë¥¼ ì–»ì„ë¿ë§Œ ì•„ë‹ˆë¼ low varianceë¥¼ ê°€ì§„ë‹¤. (ë” ìì„¸í•œ ë…¼ì˜ëŠ” ë§í¬ì—ì„œ í™•ì¸ ê°€ëŠ¥)\n(reparameterization trick) \\({\\bf{z}}\\)ê°€ conditional distributionì¸ \\(q_{\\boldsymbol{\\phi}}(\\bf{z|x})\\)ë¥¼ ë”°ë¥´ëŠ” continuous ë˜ëŠ” discrete random-variableì¼ë•Œ \\({\\bf{z}} = g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon},\\bf{x})\\)ì¸ gì— ëŒ€í•´ deterministicí•œ random variableë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤.(where, \\({\\bf{\\epsilon}}\\sim p(\\boldsymbol{\\epsilon})\\))\nê·¸ëŸ¬ë¯€ë¡œ ìš°ë¦¬ëŠ” reparameterization trickì„ ì‚¬ìš©í•´ì„œ ELBOë¥¼ Monte carlo methodë¥¼ í†µí•´ ë‹¤ë¥´ê²Œ ìƒê°í•  ìˆ˜ ìˆë‹¤. ì´ë ‡ê²Œ ELBOë¥¼ êµ¬í•˜ë©´ Gradient estimatorëŠ” unbiasedì¼ ë¿ë§Œ ì•„ë‹ˆë¼ low varianceë¥¼ ê°€ì§„ë‹¤.\n\\[\\begin{aligned}\n\\tilde{\\mathcal{L}}^A(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}}) &= \\mathbb{E}_{Z\\sim q}\\left[\\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}}) \\right] \\\\\n&= \\int_z\\left[\\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}})\\right]q({\\bf{z|x}})d{\\bf{z}} \\\\\n&\\approx \\frac{1}{L}\\sum_{l=1}^L[\\text{log}\\,p({\\bf{x,z}}^{(l)}) - \\text{log}\\,q_{\\boldsymbol{{\\boldsymbol{\\phi}}}}({\\bf{z}}|{\\bf{x}}^{(l)})]\n\\end{aligned}\\]\n$$ {}^{(l)} = q_{}({}^{(l)},{}),{}p()\n\nAëŠ” type Aë¥¼ ì˜ë¯¸í•¨.\n\në…¼ë¬¸ì—ëŠ” ì´ëŸ¬í•œ estimatorë§ê³ ë„ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ êµ¬í•œ ê²ƒë„ ìˆë‹¤. ì´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\tilde{\\mathcal{L}}^B(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}}) = -D_{KL}(q_{\\boldsymbol{\\phi}}({\\bf{z|x}})||p_\\theta({\\bf{z}})) + \\frac{1}{L}\\sum_{l=1}^L(\\text{log}\\,p_\\theta({\\bf{x}}|{\\bf{z}}^{(l)})\n\\end{aligned}\\]\n$$ {}^{(l)} = q_{}({}^{(l)},{}),{}p()\në‘ë²ˆì§¸ estimatorë¥¼ í†µí•´ì„œ ELBOë¥¼ maximizationí•˜ëŠ” ìµœì í™” ë¬¸ì œë¥¼ í‘¸ëŠ”ê²ƒì´ ì™œ auto-encoderì™€ ì—°ê²°ë˜ëŠ”ì§€ ì•Œ ìˆ˜ ìˆë‹¤. ELBOë¥¼ maximizeí•˜ë ¤ë©´ ì²«ë²ˆì§¸ termì„ ê°€ëŠ¥í•œ ì‘ê²Œ í•´ì•¼ í•˜ëŠ”ë° ì´ëŠ” observationì„ ë³´ê³  latent variableê°€ ë”°ë¥´ëŠ” ì˜ˆì¸¡í•œ ê°’ì´ ì–¼ë§ˆë‚˜ ì°¨ì´ê°€ ë‚˜ëŠëƒì´ë‹¤.(encoderì˜ ì„±ëŠ¥?ì¸ ê²ƒ ê°™ë‹¤.)ë‘ë²ˆì§¸ í…€ì€ decoderê°€ ì–¼ë§ˆë‚˜ latenr variableì—ì„œ input spaceë¡œ mappingì„ ì˜ í•˜ëŠëƒë¥¼ ì˜ë¯¸í•œë‹¤.(decoderì˜ ì„±ëŠ¥?)\nì—¬ê¸°ê¹Œì§€ ë‹¨ í•œê°œì˜ ê´€ì¸¡ì¹˜ì— ëŒ€í•´ì„œ ëª¨ë“  ê³¼ì •ì„ ìˆ˜í–‰í•´ë´¤ë‹¤. ì‚¬ì‹¤ì€ í•œ ê°œì˜ ê´€ì¸¡ì¹˜ê°€ ì•„ë‹ˆë¼ data setì˜ í¬ê¸°ê°€ mì¸ mini-batchì— ëŒ€í•˜ì—¬ ìœ„ì˜ ê³¼ì •ì„ ìˆ˜í–‰í•´ì¤˜ì•¼ í•˜ë¯€ë¡œ ì´ê²ƒì„ ê³ ë ¤í•œ ELBOëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\mathcal{L}(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{X}}) \\approx\n\\tilde{L}^M(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{X}})=\\frac{N}{M}\\sum_{i=1}^M\\tilde{L}(\\boldsymbol{\\theta},\\boldsymbol{\\boldsymbol{\\phi}};{\\bf{x}}^{(i)})\n\\end{aligned}\\]\n\\[\\text{where } {\\bf{z}}^{(l)} \\sim q_{\\boldsymbol{\\phi}}({\\bf{z}})\\]"
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes implement/Auto-Encoding Variational Bayes.html",
    "href": "posts/paper study/auto-encoding variational bayes implement/Auto-Encoding Variational Bayes.html",
    "title": "Untitled",
    "section": "",
    "text": "Import & Data Load\n\n# prerequisites\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms,datasets\nimport matplotlib.pyplot as plt\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch.distributions.normal import Normal\n\nbs = 100\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\ntest_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)\n\n\nsig = torch.nn.Sigmoid()\n\n\nplt.figure(figsize=(2,1))\nplt.imshow(train_dataset[0][0].permute(1,2,0).numpy(),cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x1efcc237df0>\n\n\n\n\n\n\n\n Modeling\n\nclass Encoder(nn.Module):\n    def __init__(self,x_dim,latent_dim): #latent spaceì˜ dimensionì€ output_dim // 2\n        super().__init__()\n        self.output_dim = latent_dim * 2 #hidden layer output dimension (= latent space dim * 2)\n        self.latent_dim = latent_dim     #latent variable dimension (= last hidden dim // 2)\n        \n        self.l1 = torch.nn.Linear(x_dim,x_dim // 2)\n        self.tanh = torch.nn.Tanh()\n        self.l2 = torch.nn.Linear(x_dim // 2,self.output_dim)\n        #relu\n        \n    def forward(self,x):\n        out = self.tanh(self.l1(x)) #l1 out\n        out = self.tanh(self.l2(out)) #l2 out\n        #Note : Dnn's output are mean,log variance\n        #half of last hidden layers output => mean\n        #half of last else hidden layers output => log variance        \n        mean = out[:,:self.latent_dim]    #mean\n        log_var = out[:,self.latent_dim:] #log_variance\n        return mean,log_var\n\n\nclass Z_Sampler(nn.Module):\n    def __init__(self,latent_dim):\n        super().__init__()\n        self.latent_dim = latent_dim\n    def forward(self,mean,log_var):\n        \"\"\"\n        Sampling z with reparameterization trick\n        \"\"\"\n        eps_sampler = MultivariateNormal(loc = torch.tensor([0]*(self.latent_dim)).float(),covariance_matrix = torch.eye(self.latent_dim))\n        eps_realizations = eps_sampler.sample()\n        #reparameterization trick z = mu + std * epsilon\n        #std = exp(ln(1/2 * variance))\n        z = mean + torch.exp(0.5 * log_var) * eps_realizations  \n        return z\n\n\nclass Decoder(nn.Module):\n    def __init__(self,latent_dim,out_dim):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.out_dim = out_dim \n        \n        self.l3 = torch.nn.Linear(latent_dim,latent_dim * 2)\n        self.tanh = torch.nn.Tanh()\n        self.l4 = torch.nn.Linear(latent_dim * 2,out_dim)\n        #softmax + cross entropy loss\n    def forward(self,z):\n        out = self.tanh(self.l3(z))\n        out = self.l4(out)\n        return out\n\n\nclass VAE(nn.Module):\n    def __init__(self,x_dim,latent_dim): #decoderì¶”ê°€í•´ì•¼í•¨\n        super().__init__()\n        self.x_dim = self.out_dim = x_dim\n        self.latent_dim = latent_dim\n        \n        self.encoder = Encoder(x_dim,latent_dim)\n        self.z_sampler = Z_Sampler(latent_dim)\n        self.decoder = Decoder(latent_dim,x_dim)\n    def forward(self,x):\n        mean_1,log_var_1 = self.encoder(x) # input : x // output : parameter phi of q(z|x;\\phi)\n        z = self.z_sampler(mean_1,log_var_1) # input : parameter phi // output : realization of z\n        xhat=self.decoder(z)\n        return xhat\n\n\n_tmp = train_dataset[0][0].reshape(-1,28*28)\nvae = VAE(x_dim = 28 * 28,latent_dim = 10)\nprint(vae(_tmp).shape)\nplt.imshow(sig(vae(_tmp)).reshape(28,28).data,cmap = \"gray\")\n\ntorch.Size([1, 784])\n\n\n<matplotlib.image.AxesImage at 0x1efcc45adf0>"
  },
  {
    "objectID": "posts/paper study/autoaugmentation/autoaugmentation.html",
    "href": "posts/paper study/autoaugmentation/autoaugmentation.html",
    "title": "[Paper Study] AutoAugment : Learning Augmentation Strategies from Data",
    "section": "",
    "text": "í•œ ì¤„ ìš”ì•½\n\nimage Augmentationì— ê°•í™”í•™ìŠµ \\(\\to\\) sota!\n\n\n\nIntro & Abstract\n\nData augmentationì€ image classifierì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ëŠ” ì‰¬ìš´ ë°©ë²•.\nì™œ image augmentatioì´ ì„±ëŠ¥ì„ í–¥ìƒ? \\(\\to\\) ë°ì´í„°ì˜ invarianceë“¤ì„ ì¶©ë¶„íˆ í•™ìŠµ\n\ninvariance(ë¶ˆë³€ì„±)ë€? : ì°¨ì´ê°€ ìˆê±°ë‚˜ ë³€í™˜ì´ ì ìš©ëœ í›„ì—ë„ ë¬´ì–¸ê°€ê°€ ë™ì¼í•˜ê²Œ ìœ ì§€ë˜ëŠ” ì†ì„±,ìƒíƒœë¥¼ ì˜ë¯¸í•¨\nì˜ˆë¥¼ ë“¤ì–´ ìë™ì°¨,ì‚¬ê³¼\n\nê·¸ëŸ¬ë‚˜ ë°ì´í„°ì…‹ë§ˆë‹¤ ì´ë¯¸ì§€ì˜ ë¶„í¬ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ìˆ˜ë™ì ìœ¼ë¡œ augmentationì „ëµì„ ë‹¤ë¤„ì¤˜ì•¼ í–ˆìŒ.\n\nê³¼ì¼ì´ ë§ì€ ë°ì´í„°ì— ìƒ‰ì— ëŒ€í•œ transformì„ ë§ì´ ì ìš©í•˜ë©´? \\(\\to\\) ì‚¬ê³¼ê°€ ì‚¬ê³¼ê°€ ì•„ë‹ˆê²Œ ë˜ê² ì£ ?\n\nì´ ë…¼ë¬¸ì—ì„œëŠ” ë°ì´í„°ë§ˆë‹¤ ì ì ˆí•œ augmentation policyì„ ìë™ì ìœ¼ë¡œ ì°¾ê¸°ìœ„í•´ ê°•í™”í•™ìŠµì„ ì‚¬ìš©í•¨.\nImagenetê³¼ CIFAR-10ì—ì„œ Sotaë¥¼ ë‹¬ì„±í–ˆìŒ. í›„ì— ëíŒì™• ëŠë‚Œì˜ efficient netì´ ë‚˜ì˜¤ëŠ”ë° ê·¸ ë•Œì˜ augmentationì—ë„ í™œìš©.\n\n\n\nMethod\n\n\n\nFig1\n\n\n\ní¬ê²Œ ë‘ ê°€ì§€ì˜ íŒŒíŠ¸ë¡œ êµ¬ë¶„\n\nController(RNN) : Polictê°€ ìˆëŠ” Search Spaceì—ì„œ í•˜ë‚˜ì˜ augmentation policyë¥¼ sampling.\nChild network : ì´ë¯¸ì§€ ë¶„ë¥˜ê¸° samplingëœ policyë¡œ í•™ìŠµ í›„ validation accuracy R(reward)ì„ ê³„ì‚°.\n\nê³¼ì •(ì¶©ë¶„íˆ ë°˜ë³µ) policy sampling(by controller) \\(\\to\\) fitting classifier,calculating reward \\(\\to\\) contoller update,policy sampling \\(\\to\\) fitting classifier,calculating reward \\(\\to\\) contoller update,policy sampling  \\(\\to\\) fitting classifier,calculating reward \\(\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\vdots\\) \\(\\to\\) optimal policy(converge to best augmentation strategy)\n\n\n\nSVHN datasetì— ì ìš©í•œ ì˜ˆì‹œ\ní•˜ë‚˜ì˜ PolicyëŠ” 5ê°€ì§€ì˜ subpolicyë¡œ êµ¬ì„±ë¨.\nsubpolicyëŠ” operation 2ê°œì™€ probability,magnitudeë¡œ êµ¬ì„±ë¨\n\noperation : ì´ë¯¸ì§€ ë³€í˜•í•˜ëŠ” ë°©ë²• (Rotate,Brightness,ShearX,Inver ë“±ë“±â€¦ ì´ 16ê°€ì§€ ë°©ë²•ì´ ì¡´ì¬í•¨)\nprobability : ì–¼ë§ˆë‚˜ ë§ì´ ì ìš©í• ê±°ëƒ\nmagnitude : ì–´ëŠì •ë„ ê°•ë„ë¡œ í• ê±°ëƒ\n\nìœ„ì˜ ì˜ˆì‹œì—ì„œ ë°°ì¹˜ëŠ” ì´ 15ê°œ. ê°ê°ì˜ ë°°ì¹˜ë§ˆë‹¤ ê· ì¼ë¶„í¬ë¡œ ì–´ë–¤ subpolicyê°€ í• ë‹¹ë¨.(ê·¸ë¦¼ì—ì„œëŠ” ì•„ì˜ˆ 33333ì”© í• ë‹¹ë˜ì—ˆì§€ë§Œ ì‹¤ì œëŠ” ì•„ë‹ ìˆ˜ ìˆìŒ.ì´í•´ë¥¼ ë•ê¸° ìœ„í•¨)\nNote\n\në™ì¼í•œ subpolicyê°€ ì ìš©ë˜ëŠ” ë°°ì¹˜ë“¤ì´ë¼ë„ ê° ë°°ì¹˜ë§ˆë‹¤ ë‹¤ë¥¸ operationì´ ì ìš©ë  ìˆ˜ ìˆìŒ. ì´ëŠ” probability ë˜í•œ subpolicyì— ìˆê¸° ë–„ë¬¸ì„\n\n\n\n\nImageNetì— ì ìš©í•œ ì˜ˆì‹œ vs SVHNì—ì„œ ì ìš©í•œì˜ˆì‹œ\nSVHNì—ëŠ” ê¸°í•˜í•™ì  ë³€í™˜ì´ ë§ì´ ë†’ì€ í™•ë¥ ë¡œ ì²«ë²ˆì§¸ ë³€í™˜ìœ¼ë¡œ ì ìš©ë˜ê²Œ ë˜ì–´ ìˆìŒ(Shear X,Shear Y)\n\nSVHNì€ ìˆ«ì ì´ë¯¸ì§€ dataset $\\(ìˆ«ìê°€ ë¹„í‹€ë ¤ìˆê±°ë‚˜ ì™œê³¡ë˜ì–´ ìˆëŠ” ê²½ìš°ê°€ ë§ìŒ.\\)$ ê°•ì¸í•¨ì„ policyëŠ” ê¸°í•˜í•™ì  ë³€í™˜ì„ ë§ì´ í¬í•¨.\në˜í•œ ìƒ‰ë³€í™˜ë„ ì–´ëŠì •ë„ í¬í•¨ë˜ì–´ ìˆìŒ.(ì´ëŠ” ë°ì´í„°ì—ì„œ í™•ì¸ê°€ëŠ¥,ë°ì´í„° ìì²´ì— ë°˜ì „ë˜ì–´ ìˆëŠ” ê²½ìš°ë„ ë§ìŒ)\n\nImagenetì—ëŠ” ìƒ‰ìƒ ë³€í™˜ì´ ë§ì´ ë†’ì€ í™•ë¥ ë¡œ ì²«ë²ˆì§¸ ë³€í™˜ìœ¼ë¡œ ì ìš©ë˜ê²Œ ë˜ì–´ ìˆìŒ(Shear X,Shear Y)\n\nImageNetì€ ë‹¤ì–‘í•œ ë¬¼ì²´ë“¤ì„ í¬í•¨í•˜ëŠ” dataset \\(\\to\\) ë‹¤ì–‘í•œ ìƒ‰ìƒë“¤ì´ í¬í•¨ë˜ì–´ ìˆìŒ. \\(\\to\\) ê°•ì¸í•¨ì„ ê°€ì§€ê¸° ìœ„í•´ policyëŠ” ìƒ‰ìƒ ë³€í™˜ì´ ë§ì´ í¬í•¨\n\n\n\n\nResult\nResult on many datasets\n\nResult on Imagenet dataset\n\n\n\nConclusion\n\në‹¤ì–‘í•œ ë°ì´í„°ì…‹ì— ê°•í™”í•™ìŠµì„ ì‚¬ìš©í•œ autoaugmentation ì „ëµì„ ì ìš© \\(\\to\\) Sota!!\nì¶”ê°€ì ìœ¼ë¡œ, ë°ì´ì½˜,ìºê¸€ ë“±ì—ì„œ ìœ„ì—ì„œ ë‚˜ì˜¨ augmentationì „ëµë“¤ì´ ë§ì´ ì‚¬ìš©ë¨. ì ìš©í•˜ê¸° ì‰¬ì›€\në§í¬ ë‚´êº¼ ì½”ë“œ"
  },
  {
    "objectID": "posts/paper study/batchnormalization/batchnormalization.html",
    "href": "posts/paper study/batchnormalization/batchnormalization.html",
    "title": "[Paper-study] Batch Normalization",
    "section": "",
    "text": "Deep learningì—ì„œëŠ” ê° ë ˆì´ì–´ì˜ inputì˜ ë¶„í¬ê°€ ê³„ì†í•´ì„œ ë³€í™”í•©ë‹ˆë‹¤.\nì´ëŠ” ë„¤íŠ¸ì›Œí¬ì˜ í•™ìŠµì— ì–´ë ¤ì›€ì„ ê°€ì ¸ì˜µë‹ˆë‹¤.\në…¼ë¬¸ì—ì„œëŠ” Batchë‹¨ìœ„ì˜ inputì„ normalization,shifting,scailing,í•˜ì—¬ ë¶„í¬ë¥¼ ì–´ëŠì •ë„ ì¼ì •í•˜ê²Œ ìœ ì§€ì‹œí‚¬ ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì œì•ˆí•©ë‹ˆë‹¤.\nì´ë¥¼ ë‹¹ì‹œì˜ sotaëª¨ë¸ì— ì ìš©í–ˆë”ë‹ˆ ë™ì¼í•œ ì •í™•ë„ë¥¼ 14ë°° ì ì€ training stepìœ¼ë¡œë¶€í„° ì–»ì„ ìˆ˜ ìˆì—ˆìœ¼ë©° ìƒë‹¹í•œ ê²©ì¹˜ë¥¼ ë‘ê³  ì›ë˜ëª¨ë¸ì„ ëŠ¥ê°€í–ˆìŠµë‹ˆë‹¤.\në˜í•œ ë™ì¼í•œ ë°©ë²•ì„ ì ìš©í•œ ì•™ìƒë¸” ë„¤íŠ¸ì›Œí¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ImageNet classificationì—ì„œ ê°€ì¥ ì¢‹ì€ ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.(4.9% top5 validation error,4.8% test error)"
  },
  {
    "objectID": "posts/paper study/batchnormalization/batchnormalization.html#intuition",
    "href": "posts/paper study/batchnormalization/batchnormalization.html#intuition",
    "title": "[Paper-study] Batch Normalization",
    "section": "Intuition",
    "text": "Intuition\n\nBatchNormalization - ì¶œì²˜ : JINSOL KIM\n\nì§ê´€ì ìœ¼ë¡œ internal covariate shiftë¥¼ ë§‰ê¸° ìœ„í•´ ë¶„í¬ê°€ ê³ ì •ë˜ê²Œ í•˜ë ¤ë©´ ìœ„ì™€ ê°™ì´ ê° íˆë“ ë ˆì´ì–´ì˜ outputì— normalizationì„ ì·¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ë…¼ë¬¸ì˜ ì•Œê³ ë¦¬ì¦˜ì—ì„œ ì„¤ëª…í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\nê·¸ëŸ¬ë‚˜ ì°¾ì•„ë³¸ í”íˆ Fully connected-layerì™€ activation functionì‚¬ì´ì— batchnormalization layerë¥¼ ë†“ìŠµë‹ˆë‹¤. ì´ëŠ” ë…¼ë¬¸ì˜ ì‹¤í—˜ì—ì„œ ì‚¬ìš©í•œ ë°©ë²•ì…ë‹ˆë‹¤.\nì •ë¦¬í•˜ìë©´ normalizationì„ ì ìš©í•˜ëŠ” ìœ„ì¹˜ëŠ” ë¬¸ì œë§ˆë‹¤ ë‹¤ë¥´ì§€ë§Œ í”íˆë“¤ ìœ„ì™€ ê°™ì´ Fully connected layerì™€ activation functionì‚¬ì´ì— ë†“ëŠ”ê²Œ ì¼ë°˜ì ì´ë©° ì´ëŠ” ë¹„êµì  ììœ ë¡œìš´ í¸ì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nìœ„ì™€ ê°™ì€ ë°©ë²•ìœ¼ë¡œ normalizationë§Œ ì·¨í•˜ê²Œ ëœë‹¤ë©´ ë„¤íŠ¸ì›Œí¬ì˜ í‘œí˜„ë ¥ì„ ê°ì†Œì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹œê·¸ëª¨ì´ë“œì˜ linear regimeì— ê°’ë“¤ì´ ëŒ€ë‹¤ìˆ˜ ìœ„ì¹˜í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.(ë‰´ëŸ´ë„·ì€ ì„ í˜•+ë¹„ì„ í˜• ë³€í™˜ì„ í†µí•´ì„œ ë†’ì€ í‘œí˜„ë ¥ì„ ì§€ë‹™ë‹ˆë‹¤.ë‹¨ìˆœíˆ normalizationë§Œ ì·¨í•˜ë©´ ë¹„ì„ í˜•í•¨ìˆ˜ì˜ ì—­í• ì´ ê°ì†Œí•˜ê²Œ ë©ë‹ˆë‹¤.)\n\n\n\n\nfigure3 - DNN with learnable parameter\n\n\n\në”°ë¼ì„œ normalizationëœ ê°’ì„ ì ì ˆí•˜ê²Œ shifting,scailingí•˜ë„ë¡ ê° ë‰´ëŸ°ì— ë¶™ëŠ” learnable parameter \\(\\gamma,\\beta\\)ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤.\n\n\nì •ë¦¬í•˜ìë©´ BatchNormalizationì€ Batchë‹¨ìœ„ë¡œ normalizationì„ í†µí•´ internal covariate shiftë¥¼ ë§‰ê³  ë™ì‹œì— learnable parameterë¡œ shifting,scailingí•¨ìœ¼ë¡œì„œ nonlinearityë¥¼ ìœ ì§€í•˜ì—¬ gradient vanishing(exploding),í•™ìŠµì˜ ì–´ë ¤ì›€,í‘œí˜„ë ¥ì˜ ê°ì†Œì™€ ê°™ì€ ë¬¸ì œë¥¼ í•´ê²°í–ˆë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/paper study/batchnormalization/batchnormalization.html#implementation",
    "href": "posts/paper study/batchnormalization/batchnormalization.html#implementation",
    "title": "[Paper-study] Batch Normalization",
    "section": "Implementation",
    "text": "Implementation\n\nTraining\n\n\n\nFigure4 - BN Algorithm\n\n\nnotation\n\në…¼ë¬¸ì˜ ì•Œê³ ë¦¬ì¦˜ ë¶€ë¶„ì—ì„œëŠ” Batchnormalizationì€ activation functionë°”ë¡œ ë‹¤ìŒì— ìœ„ì¹˜í•˜ëŠ” ê²ƒì„ ê¸°ì¤€ìœ¼ë¡œ ì„¤ëª…í•©ë‹ˆë‹¤.\n\\(\\mathcal{B} = \\{x_{1...m}\\}\\)ëŠ” í¬ê¸°ê°€ mì¸ batchë¥¼ ì…ë ¥í–ˆì„ë•Œ ì„ì˜ì˜ ë…¸ë“œì—ì„œ ì¶œë ¥ëœ mê°œì˜ scalarê°’ì´ë‹¤.(activation functionì„ í†µê³¼í•œ í›„ì´ë‹¤.)mê°œì˜ outputì…ë‹ˆë‹¤.\n\\(\\mu_{\\mathcal{B}}\\),\\(\\sigma^2_{\\mathcal{B}}\\)ëŠ” ê°ê° \\(\\mathcal{B}\\)ì˜ í‰ê· ,ë¶„ì‚°ì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\n\\(\\hat{x_i}\\)ëŠ” \\(\\mathcal{B}\\)ì— ì†í•˜ëŠ” ì„ì˜ì˜ ì›ì†Œ \\(x_i\\)ì— normalizationí•œ ê°’ì…ë‹ˆë‹¤.\nì—¬ê¸°ì„œ \\(\\epsilon\\)ì€ ë§¤ìš°ì‘ì€ ê°’ì„ ì˜ë¯¸í•˜ë©° ë¶„ì‚°ì´ 0ì¼ë•Œì˜ ì—°ì‚°ì´ ë¶ˆì•ˆì •í•´ì§€ëŠ” ê²ƒì„ ë§‰ìŠµë‹ˆë‹¤.\n\\(y_i\\)ëŠ” learnable parameterì¸ \\(\\gamma,\\beta\\)ì— ëŒ€í•œ ê°’ì´ë©° \\(\\text{BN}_{\\gamma,\\beta}(x_i)\\)ë¥¼ ê³„ì‚°í•œ ê²°ê³¼ì…ë‹ˆë‹¤.\n\nexplanation - ë¨¼ì € í¬í‚¤ê°€mì¸ batchì— ëŒ€í•´ì„œ ì–´ë–¤ ë…¸ë“œì—ì„œ mê°œì˜ ìŠ¤ì¹¼ë¼ê°’ì¸ \\(\\mathcal{B}\\)ê°€ ì¶œë ¥ë©ë‹ˆë‹¤. - \\(\\mathcal{B}\\)ì˜ í‰ê· ,ë¶„ì‚°ì„ ê³„ì‚°í•©ë‹ˆë‹¤. - \\(\\forall x_i \\in \\mathcal{B}\\)ì— ëŒ€í•˜ì—¬ normalizationì„ ì·¨í•˜ê³  learnable parameterì¸ \\(\\gamma,\\beta\\)ë¥¼ ê³±í•©ë‹ˆë‹¤.\ní•™ìŠµëœ \\(\\gamma\\),\\(\\beta\\)ì˜ ì˜ˆì‹œ Normalizationì—°ì‚°ì´ í•„ìš”ì—†ë‹¤ê³  í•™ìŠµí•œ ê²½ìš°,nonlinearityë¥¼ ìœ ì§€í•˜ëŠ” ê²ƒì´ ì¢‹ì€ ê²½ìš°,identityë¥¼ ìœ ì§€í•˜ëŠ”ê²Œ ì¢‹ì€ ê²½ìš°  \\[\\gamma \\approx \\sqrt{var[x]},\\beta \\approx \\mathbb{E}[x] \\rightarrow \\hat{x_i}\\approx x_i  \\] Normalizationì—°ì‚°ì´ í•„ìš”í•˜ë‹¤ê³  í•™ìŠµí•œ ê²½ìš°,linearityë¥¼ ê°€ì§€ëŠ” ê²ƒì´ ê²½ìš°,identityë¥¼ ë²„ë¦¬ëŠ”ê²Œ ì¢‹ì€ ê²½ìš°  \\[\\gamma \\approx 1,\\beta \\approx 0 \\rightarrow \\hat{x_i} \\approx \\frac{x_i-\\mu_\\mathcal{B}}{\\sqrt{\\sigma_\\mathcal{B}^2-\\epsilon}}\\]\n\n\nTest or Inference\n\ntrainingì—ì„œëŠ” minibatchë‹¨ìœ„ë¡œ í‰ê· ,ë¶„ì‚°ì„ êµ¬í•˜ì—¬ normalizationí•  ìˆ˜ ìˆì§€ë§Œ testì—ì„œëŠ” ì´ì™€ëŠ” ë‹¤ë¥´ê²Œ minibatchë‹¨ìœ„ë¡œ dataê°€ ì…ë ¥ë˜ì§€ ì•Šì„ë¿ë”ëŸ¬ ë˜í•œ ì…ë ¥ë˜ëŠ” ë°ì´í„°ê°€ í•œê°œì—¬ë„ ì˜¬ë°”ë¥´ê²Œ ì˜ˆì¸¡í•´ì•¼ ì›í•©ë‹ˆë‹¤.\në”°ë¼ì„œ ì´ë•Œì—ëŠ” trainingì—ì„œ ê°ê°ì˜ ë°°ì¹˜ë“¤ë¡œë¶€í„° ì–»ì€ í‰ê· ë“¤ê³¼ ë¶„ì‚°ë“¤ì„ ì €ì¥í•´ë†“ê³  testì—ì„œëŠ” ì´ ê°’ë“¤ë¡œ ë‹¤ì‹œ í‰ê· ì„ ì·¨í•˜ì—¬(í‰ê· ë“¤ì˜ í‰ê· ) normalizationì„ ì·¨í•©ë‹ˆë‹¤.\nì´ë•Œ ë‹¨ìˆœí•œ í‰ê· ì„ ì·¨í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì–´ëŠì •ë„ í•™ìŠµëœ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì–»ì–´ì§„ minibatchë“¤ì˜ ë°ì´í„°ë¥¼ ë” ë§ì´ ê³ ë ¤í•˜ê¸° ìœ„í•´ì„œ movingaverageë‚˜ exponentialaverageë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\nmovingaverageëŠ” í•™ìŠµë‹¨ê³„ì—ì„œ ì–»ì–´ì§„ ê°’(í‰ê· ,ë¶„ì‚°)ì˜ ì¼ë¶€ë¥¼ ì§ì ‘ ì§€ì •í•˜ì—¬ í‰ê· ì„ êµ¬í•˜ê³  exponentialaverageëŠ” ì–´ëŠì •ë„ ì•ˆì •ëœ ìƒíƒœì˜ ê°’(ë‚˜ì¤‘ê°’)ë“¤ì— ê°€ì¤‘ì¹˜ë¥¼ ë¶€ì—¬í•˜ì—¬ í‰ê· ,ë¶„ì‚°ì„ êµ¬í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤.\n\n\\[\\begin{aligned}\n&\\hat{x} = \\frac{x - \\mathbb{E}[x]}{\\text{Var}[x] + \\epsilon}\\\\\n&y = \\frac{\\gamma}{\\sqrt{\\text{var}[x] + \\epsilon}}\\cdot x + (\\beta - \\frac{\\gamma\\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}})\\\\\n&\\text{where }E[x] = E_\\mathcal{B}[\\mu_\\mathcal{B}],\\text{Var}[x] = \\frac{m}{m-1}E_\\mathcal{B}[\\sigma_\\mathcal{B}^2]\n\\end{aligned}\\]\n\n\\(\\frac{m}{m-1}\\)ì€ unbiased estimateë¥¼ ìœ„í•˜ì—¬ ê³±í•´ì§„ ê°’ì´ë©° \\(E_{\\mathcal{B}}\\)ëŠ” moving average ë˜ëŠ” exponential averageë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\ntestì—ì„œì˜ normalizationì€ ë‹¨ìˆœí•œ linear transformìœ¼ë¡œ ì·¨ê¸‰í•  ìˆ˜ ìˆëŠ”ë° ì´ëŠ” trainingì´ ëë‚œ í›„ ì‚¬ì „ì— ì´ ê°’ì„ ê³„ì‚°í•˜ì—¬ ë‹¨ìˆœíˆ ê³±í•˜ê³  ë”í•˜ëŠ” ê²ƒìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/paper study/batchnormalization/batchnormalization.html#experiments",
    "href": "posts/paper study/batchnormalization/batchnormalization.html#experiments",
    "title": "[Paper-study] Batch Normalization",
    "section": "Experiments",
    "text": "Experiments\n Figure5 - Mnist experiment\n\n(a)ëŠ” BNì„ ì‚¬ìš©í•œ ë„¤íŠ¸ì›Œí¬ì™€ ì‚¬ìš©í•˜ì§€ ì•Šì€ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¹„êµ, (b,c)ëŠ” ê° ë„¤íŠ¸ì›Œí¬ì˜ hiddenlayerì˜ sigmoidì˜ input 3ê°œë¥¼ ë¹„êµí•œ ê²ƒ\në„¤íŠ¸ì›Œí¬ëŠ” ê°ê° 100ê°œì˜ activationì„ ê°€ì§€ë©° 3ê°œì˜ hiddenlayerê°€ ì¡´ì¬\n(a)ë¥¼ ë³´ë©´BNì„ ì‚¬ìš©í•œ ë„¤íŠ¸ì›Œí¬ê°€ í›¨ì”¬ ë¹ ë¥¸ì†ë„ë¡œ ìˆ˜ë ´í•˜ê³  ìˆìŒì„ ì•Œ ìˆ˜ ìˆìŒ\n(b,c)ë¥¼ ë³´ë©´ BNì„ ì‚¬ìš©í•œ ë„¤íŠ¸ì›Œí¬ì—ì„œ ê°’ì´ í›¨ì”¬ ì•ˆì •ì ì„ì„ ì•Œ ìˆ˜ ìˆìŒ(internal covariate shiftê°€ ì ìŒ)\n\n\n\nBNì‚¬ìš©í•œ ëª¨ë¸ì´ ì¢‹ì•˜ë‹¤~"
  },
  {
    "objectID": "posts/paper study/dqn/DQN.html",
    "href": "posts/paper study/dqn/DQN.html",
    "title": "DQN",
    "section": "",
    "text": "ë‹¹ì‹œ neural networkê°€ ë°œì „í•¨ì— ë”°ë¼ì„œ RLì—ë„ ê·¸ëŒ€ë¡œ DLì„ ì ìš©í•˜ê³ ì ì‹œë„í•¨\nê·¸ëŸ¬ë‚˜ ì—¬ëŸ¬ê°€ì§€ ë¬¸ì œì ì´ ë§ì´ ì¡´ì¬\n\nThe delay between actions and resulting rewards, which can be thousands of timesteps long, seems particularly daunting when compared to the direct association between inputs and targets found in supervised learning.  => ê°•í™”í•™ìŠµì€ ë³´ìƒì„ ê¸°ë°˜ìœ¼ë¡œ í•™ìŠµì„ í•˜ë‚˜ ë³´ìƒì„ ë°›ëŠ” ì‹œì ì´ ì •í•´ì§€ì§€ ì•ŠìŒ. ì´ëŠ” ë”¥ëŸ¬ë‹ê³¼ëŠ” ë‹¤ë¦„\nAnother issue is that most deep learning algorithms assume the data samples to be independent, while in reinforcement learning one typically encounters sequences of highly correlated states. => ë”¥ëŸ¬ë‹ì€ ë³€ìˆ˜ë“¤ì´ ë…ë¦½ì„ì„ ê°€ì •í•˜ì§€ë§Œ ê°•í™”í•™ìŠµì—ì„œ í•´ê²°í•˜ê³ ì í•˜ëŠ” ë°ì´í„°(ì‹œí€€ìŠ¤)ê°€ ë†’ì€ ìƒê´€ì„±ì„ ê°€ì§„ì±„ë¡œ ì¡´ì¬í•¨\nFurthermore, in RL the data distribution changes as the algorithm learns new behaviours, which can be problematic for deep learning methods that assume a ï¬xed underlying distribution. => ë”¥ëŸ¬ë‹ì€ fixed underlying function(í™•ë¥ ë¶„í¬)ë¥¼ ê°€ì •í•˜ì§€ë§Œ ê°•í™”í•™ìŠµì˜ ê²½ìš° distributionì´ ë³€í™”í•¨.\n\në…¼ë¬¸ì—ì„œëŠ” ìœ„ì™€ ê°™ì€ ë¬¸ì œì ë“¤ì„ ê·¹ë³µí•˜ì—¬ neural networkë¥¼ RL(Q-learning)ì— ì ìš©"
  },
  {
    "objectID": "posts/paper study/dqn/DQN.html#experience-replay",
    "href": "posts/paper study/dqn/DQN.html#experience-replay",
    "title": "DQN",
    "section": " experience replay",
    "text": "experience replay\n\nstore the agentâ€™s experiences at each time-step, et = (st , at , rt , st+1 ) in a data-set D = e1 , â€¦, eN\nDuring the inner loop of the algorithm, we apply Q-learning updates, or minibatch updates, to samples of experience, e âˆ¼ D, drawn at random from the pool of stored samples.\nAfter performing experience replay, the agent selects and executes an action according to an -greedy policy."
  },
  {
    "objectID": "posts/paper study/dqn/DQN.html#algorithm",
    "href": "posts/paper study/dqn/DQN.html#algorithm",
    "title": "DQN",
    "section": " algorithm",
    "text": "algorithm"
  },
  {
    "objectID": "posts/paper study/gan/gan.html",
    "href": "posts/paper study/gan/gan.html",
    "title": "Genarative Adversarial Nets",
    "section": "",
    "text": "ì´ ê¸€ì€ Generative Adversarial Networksê³¼ ì´í›„ì— ë‚˜ì˜¨ íŠœí† ë¦¬ì–¼ì¸ NIPS 2016 Tutorial: Generative Adversarial Networksë¥¼ ì½ê³  ì •ë¦¬í•œ ë‚´ìš©ì…ë‹ˆë‹¤.\n\nIntroduction\nGANì€ ìƒì„±ëª¨ë¸ë¡œ ê´€ì¸¡ëœ ë°ì´í„°ë¥¼ ìƒì„±í•œ í™•ë¥ ë¶„í¬ë¥¼ êµ¬í•˜ì—¬ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤. GAN ì´ì „ì˜ ìƒì„±ëª¨ë¸ì€ í’€ê¸°ê°€ ì–´ë ¤ìš´ í™•ë¥ ê³„ì‚°ì´ ìˆë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤. GANì€ ì´ì™€ ë‹¬ë¦¬ Discrminatorì™€ Generatorë¥¼ ì„œë¡œ ì ëŒ€ì ìœ¼ë¡œ ê²½ìŸì‹œì¼œì„œ í•™ìŠµí•©ë‹ˆë‹¤. GeneratorëŠ” ê´€ì¸¡ ë°ì´í„°ë¥¼ ìƒì„±í•œ ë¶„í¬ì™€ ì ì  ê°€ê¹Œì›Œì§€ë©° í•™ìŠµì´ ëë‚˜ë©´ ë°ì´í„°ë¥¼ ìƒì„±í•œ í™•ë¥ ë¶„í¬ë¥¼ ê·¼ì‚¬ì ìœ¼ë¡œ ì–»ìŠµë‹ˆë‹¤.\nGANì€ ê²Œì„ì´ë¡ ì—ì„œì˜ two-player gameì—ì„œ ìœ ë˜ë˜ì—ˆìŠµë‹ˆë‹¤. ê²Œì„ì—ì„œ ê°ê°ì˜ playerëŠ” ìƒëŒ€ë°©ì˜ ì „ëµì„ ë§¤ìˆœê°„ ì¸ì§€í•˜ê³  ìˆìœ¼ë©° ê°ê° ìƒëŒ€ë°©ì˜ ì „ëµì— ëŒ€ì‘í•˜ì—¬ ë²ˆê°ˆì•„ê°€ë©° ìµœì„ ì˜ ì „ëµì„ ì·¨í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ìµœì„ ì˜ ì „ëµì„ ë²ˆê°ˆì•„ê°€ë©° ê³„ì†í•´ì„œ ì·¨í–ˆì„ë•Œ ì„œë¡œê°€ ë”ì´ìƒ ë‹¤ë¥¸ì „ëµì„ ì„¸ìš¸ í•„ìš”ê°€ ì—†ê¸°ì— ê³ ì •ëœ ì „ëµë§Œ ì·¨í•˜ëŠ” ì§€ì ì¸ ë‚´ì‹œê· í˜•(nash-equlibirum)ì— ë‹¤ë‹¤ë¦…ë‹ˆë‹¤. (ìì„¸í•œ ì„¤ëª… ì°¸ì¡°)\nì˜ˆë¥¼ ë“¤ì–´ DiscrminatorëŠ” ê²½ì°° GeneratorëŠ” ì‚¬ê¸°ê¾¼ì¸ ê²Œì„ì— ë¹„ìœ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‚¬ê¸°ê¾¼ì€ ìµœëŒ€í•œ ì§„ì§œê°™ì€ ê°€ì§œí™”íë¥¼ ìœ í†µì‹œí‚¤ë ¤ í•˜ë©° ê²½ì°°ì€ ì´ëŸ¬í•œ ê°€ì§œí™”íë¥¼ ì¡ì•„ë‚´ë ¤ í•©ë‹ˆë‹¤. ì‚¬ê¸°ê¾¼ì€ ê²½ì°°ì—ê²Œ ê±¸ë¦¬ì§€ ì•Šê¸°ìœ„í•´ ì ì  ê°€ì§œë¥¼ ì§„ì§œê°™ì´ ë§Œë“¤ë©° ê²½ì°°ì€ ë” ì •í™•íˆ ì§„ì§œì™€ ê°€ì§œë¥¼ êµ¬ë¶„í•˜ë ¤ê³  ë°œì „í•©ë‹ˆë‹¤. ë§ˆì§€ë§‰ì— ê²½ì°°ì€ ì‚¬ê¸°ê¾¼ì´ ë§Œë“¤ì–´ë‚´ëŠ” ê°€ì§œí™”íë¥¼ ì§„ì§œí™”íë¥¼ ì „í˜€ êµ¬ë¶„í•˜ì§€ ëª»í•˜ëŠ” nash-equilibriumì— ë‹¤ë‹¤ë¦…ë‹ˆë‹¤.\n\n\n\nProblem Setting\nê´€ì¸¡ëœ ë°ì´í„°ëŠ” \\(p_{data}\\)ë¼ëŠ” í™•ë¥ ë¶„í¬ì—ì„œ ìƒ˜í”Œë§ ë©ë‹ˆë‹¤. GANì€ ì—¬íƒ€ ë‹¤ë¥¸ ìƒì„±ëª¨ë¸ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ê´€ì¸¡ëœ ë°ì´í„°ë¥¼ í†µí•´ì„œ ì—­ìœ¼ë¡œ ë°ì´í„°ë¥¼ ìƒì„±í•´ë‚¸ \\(p_{data}\\)ë¥¼ ì•Œì•„ë‚´ê³ ì í•©ë‹ˆë‹¤. \\(p_{data}\\)ë§Œ ì•Œì•„ë‚¸ë‹¤ë©´ samplingì„ í†µí•´ í•™ìŠµëœ ë°ì´í„°ì™€ ìœ ì‚¬í•œ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤.\n\n\n\nMethod\nD,Gë¡œ êµ¬ì„±ëœ two-playerê°€ ì°¸ì—¬í•˜ëŠ” minimax gameì˜ value functionì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\underset{G}{\\text{min}}\\,\\underset{D}{\\text{max}}\\,V(D,G) = \\mathbb{E}_{{\\bf{x}}\\sim p_{data}}\\left[\\text{log\\,D({\\bf{x}})}\\right] + \\mathbb{E}_{{\\bf{z}}\\sim p_z(z)}\\left[\\text{log}\\,(1-D(G(\\bf{z})))\\right] \\\\\n&p_{data} : \\text{(observed) data generating Distribution}\\\\\n&p_z : \\text{prior distribution}\\\\\n&\\text{Generator }G : \\text{mapping from latent space to data(input) space}\\\\\n&\\text{Discriminator }D : \\text{probability that input came from the data rather than } p_g\\\\\n\\end{aligned}\\]\nDì— ëŒ€í•œ value functionë§Œ ë”°ë¡œë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\underset{D}{\\text{max}}\\,V(D,G) = \\mathbb{E}_{{\\bf{x}}\\sim p_{data}}\\left[\\text{log\\,D({\\bf{x}})}\\right] + \\mathbb{E}_{{\\bf{z}}\\sim p_z(z)}\\left[\\text{log}\\,(1-D(G(\\bf{z})))\\right] \\\\\n\\end{aligned}\\]\nìš°ë³€ì˜ ì²«ë²ˆì§¸ í•­ì€ Discriminatorê°€ ê´€ì¸¡ëœ real ë°ì´í„°ë¥¼ \\(x\\)ë¥¼ real ë°ì´í„°ë¼ê³  ë¶„ë¥˜í•  í™•ë¥ ì…ë‹ˆë‹¤. ë˜í•œ -ê°€ ë¶™ì€ ìš°ë³€ì˜ ë‘ë²ˆì§¸ í•­ì€ Discriminatorê°€ ìƒì„±ëœ fake ë°ì´í„° \\(G({\\bf{z}})\\)ë¥¼ fake ë°ì´í„°ë¼ê³  ë¶„ë¥˜í•  í™•ë¥ ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ì™€ ê°™ì€ objective functionì„ maximizeí•˜ëŠ” Dë¥¼ êµ¬í•˜ëŠ” ê²ƒì€ ì§„ì§œì™€ ê°€ì§œë¥¼ ì˜ ë¶„ë¥˜í•˜ë„ë¡ Discriminatorë¥¼ í•™ìŠµì‹œí‚¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\nGì— ëŒ€í•œ value functionë§Œ ë”°ë¡œë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\underset{G}{\\text{min}}\\,V(D,G) = \\mathbb{E}_{{\\bf{z}}\\sim p_z(z)}\\left[\\text{log}\\,(1-D(G(\\bf{z})))\\right] \\\\\n\\end{aligned}\\]\nìš°ë³€ì€ Discriminatorê°€ ê´€ì¸¡ëœ ê°€ì§œë°ì´í„°ì¸ \\(G({\\bf{z}})\\)ë¥¼ ì§„ì§œë¡œ ë¶„ë¥˜í•  í™•ë¥ ì…ë‹ˆë‹¤. ë”°ë¼ì„œ objective functionì„ minimizeí•˜ëŠ” Gë¥¼ êµ¬í•˜ëŠ” ê²ƒì€ Generatorê°€ Discriminatorë¥¼ ë” ì˜ ì†ì¼ ìˆ˜ ìˆë„ë¡ ì§„ì§œì™€ ê°™ì€ ë°ì´í„°ë¥¼ ìƒì„±í•˜ë„ë¡ í•™ìŠµí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\n\n\ngan-figure1\n\n\n\nê²€ì€ìƒ‰ ì ì„  : \\(p_{data}\\)\nì´ˆë¡ìƒ‰ ê³¡ì„  : \\(p_g\\)\níŒŒë‘ìƒ‰ ê³¡ì„  : \\(D(\\bf{x})\\)\nìˆ˜í‰ì„  z : latent space\nìˆ˜í‰ì„  x : data space\n\nobjective functionì„ ìµœì í™” í•˜ëŠ” ê³¼ì •ì„ ì‹œê°ì ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤.\n\n\n: \\(p_{data}\\)ì™€ \\(p_g\\)ì˜ ë¶„í¬ê°€ ì–´ëŠì •ë„ ì°¨ì´ë¥¼ ë³´ì…ë‹ˆë‹¤. ë˜í•œ \\(D(\\bf{x})\\)ë„ ì–´ëŠì •ë„ ë¶ˆì•ˆì •í•˜ê²Œ ë¶„ë¥˜ë¥¼ í•˜ëŠ” ëª¨ìŠµì…ë‹ˆë‹¤.\n\n\n: \\(D(\\bf{x})\\)ê°€ ë¨¼ì € ì˜¬ë°”ë¥´ê²Œ ë¶„ë¥˜í•  ìˆ˜ ìˆë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤. ì´ë•Œ \\(G(z)\\)ë¡œë¶€í„° ë§Œë“¤ì–´ì§„ ê°€ì§œ ë°ì´í„°ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤.\n\n\n: \\(G(z)\\)ê°€ ì ì  ë” ì§„ì§œ ë°ì´í„°ë¥¼ ìƒì„±í•˜ë„ë¡ í•™ìŠµí•©ë‹ˆë‹¤. ì´ë•Œ \\(D(\\bf{x})\\)ë¥¼ ì–¼ë§ˆë‚˜ ì˜ ì†ì´ê³  ìˆëŠ”ì§€ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤. ë˜í•œ ì´ì „ë³´ë‹¤ ì¡°ê¸ˆ ë” \\(p_g\\)ê°€ \\(p_{data}\\)ì™€ ë¹„ìŠ·í•´ì¡ŒìŠµë‹ˆë‹¤. \\(\\vdots\\)\n\n\n: Gê°€ ê±°ì˜ ì™„ë²½í•˜ê²Œ Dë¥¼ ì†ì´ë©° ì§„ì§œê°™ì€ ë°ì´í„°ë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \\(p_g \\approx p_{data}\\)\n\n\n\n\n\nImplementation\n\n\n\nalgorithm 1\n\n\n\nì‹¤ì œë¡œëŠ” ëª©ì í•¨ìˆ˜ë¥¼ D,Gì— ë²ˆê°ˆì•„ê°€ë©° numerical,iterative methodë¥¼ ì‚¬ìš©í•˜ì—¬ ìµœì í™”í•¨.\nì¶©ë¶„í•œ ì„±ëŠ¥ì„ ê°€ì ¸ì„œ ë” ì´ìƒ ê°œì„ ë˜ì§€ ì•Šì„ ë•Œ(nash equilibriumì— ë„ë‹¬í–ˆì„ ë•Œ), Gì˜ í™•ë¥ ë¶„í¬ \\(p_g \\approx p_{data}\\)\n\n(Detail) - ì£¼ì–´ì§„ Gì— ëŒ€í•´ì„œ Dë¥¼ ëê¹Œì§€ ìµœì í™” í•˜ëŠ” ê²ƒì€ ë¹„íš¨ìœ¨ì ì´ë©° ê³¼ì í•©ì„ ë°œìƒì‹œí‚´ \\(\\rightarrow\\) Discriminatorë¥¼ ì—…ë°ì´íŠ¸ íšŸìˆ˜ë¥¼ kë¡œ ì œí•œ(ë…¼ë¬¸ì—ì„œ \\(k\\) = 1) - í•™ìŠµ ì´ˆê¸°ì— Generator Gê°€ ì¢‹ì§€ ëª»í•  ê²½ìš° \\(\\text{log}(1-D(G(\\bf{z})))\\)ëŠ” ê¸°ìš¸ê¸°ê°€ ê±°ì˜ ì—†ìŒ => í•™ìŠµ ì´ˆê¸°ì—ë§Œ \\(\\text{log}(1-D(G(\\bf{z})))\\)ë¥¼ minimizeí•˜ì§€ ì•Šê³  \\(\\text{log}D(G(z))\\)ë¥¼ maximize í•˜ëŠ” ë°©ì‹ì„ ì·¨í•¨ => ë” í° gradientê°€ flow\n\n\n Proof\nGANì€ \\(p_g\\)ë¡œ ê´€ì¸¡ ë°ì´í„°ë¥¼ ìƒì„±í•˜ëŠ” í™•ë¥ ë¶„í¬ \\(p_{data}\\)ì–»ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì•„ì§ê¹Œì§€ ìœ„ì™€ ê°™ì€ GANì˜ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ ì •ë§ë¡œ \\(p_{data}\\)ë¥¼ ì–»ì„ ìˆ˜ ìˆëŠ”ì§€ëŠ” ì¦ëª…í•˜ì§€ ì•Šì•˜ê¸°ì— í™•ì‹¤í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë…¼ë¬¸ì—ì„œëŠ” ì´ì™€ ê´€ë ¨í•˜ì—¬ ì¦ëª…í•©ë‹ˆë‹¤.\n\nGlobal minimumì—ì„œ \\(p_g = p_{data}\\)\nGlobal minimumìœ¼ë¡œ ìˆ˜ë ´í•  ìˆ˜ ìˆëŠ”ê°€?(ì•„ì§ ì˜ ëª¨ë¥´ê² ë„¤ìš” ã…œã…œ)\n\n1ì„ ì¦ëª…í•˜ê¸° ìœ„í•´ì„œ value functionì„ ì¡°ê¸ˆ í’€ì–´ì“°ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&\\underset{G}{\\text{min}}\\,\\underset{D}{\\text{max}}\\,V(D,G) = \\underset{G}{\\text{min}}\\left[-\\text{log(4)} + 2\\cdot\\text{JSD}(p_{data}||p_g)\\right]\\\\\n&p_{data} : \\text{(observed) data generating Distribution}\\\\\n&p_g : \\text{(implict) generator's distribution}\\\\\n&\\text{Generator }G : \\text{mapping from latent space to data(input) space}\\\\\n&\\text{Discriminator }D : \\text{probability that input came from the data rather than } p_g\\\\\n\n\\end{aligned}\\]\nìš°ë³€ì„ ë³´ë©´ jenson-Shannon divergence(JSD)ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. JSDëŠ” ë‘ í™•ë¥ ë¶„í¬ê°„ì˜ ì°¨ì´ë¥¼ ì¸¡ì •í•˜ë©° í•­ìƒ 0ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìœ¼ë©° ë‘ í™•ë¥ ë¶„í¬ \\(p_g,p_{data}\\)ê°€ ê°™ì„ë•Œ ìµœì†Ÿê°’ 0ì„ ê°€ì§‘ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ, global optimalì—ì„œ \\(p_g = p_{data}\\)ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/paper study/MARL Selective overview/MARL Selective overview.html",
    "href": "posts/paper study/MARL Selective overview/MARL Selective overview.html",
    "title": "[Paper Study] Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms",
    "section": "",
    "text": "Abstract\n\nìµœê·¼ ê°•í™”í•™ìŠµì€ ìˆœì°¨ì ì¸ ì˜ì‚¬ê²°ì • ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ”ë°ì— ìƒë‹¹í•œ ì„±ê³µì„ ê±°ë‘ì—ˆë‹¤.\nì´ëŸ¬í•œ ì„±ê³µë“¤ì—ë„ ë¶ˆêµ¬í•˜ê³  multi agent reinforcement learning(MARL)ì— ëŒ€í•œ ì´ë¡ ì ì¸ ê¸°ì´ˆëŠ” ìƒëŒ€ì ìœ¼ë¡œ ë¶€ì¡±í•˜ë‹¤.\në”°ë¼ì„œ ì´ ë…¼ë¬¸ì—ì„œëŠ” ëª‡ ê°€ì§€ MARL ì•Œê³ ë¦¬ì¦˜ì„ ì „ì²´ì ìœ¼ë¡œ ì„¤ëª…í•˜ê³  ì´ë¡ ì ìœ¼ë¡œ ë¶„ì„í•œë‹¤.\në¿ë§Œì•„ë‹ˆë¼ ì¤‘ìš”í•˜ì§€ë§Œ ë‹¤ì†Œ ë„ì „ì ì¸ ë‚œê´€ë“¤ë„ ì œì‹œí•œë‹¤.\në…¼ë¬¸ì˜ ê¶ê·¹ì ì¸ ëª©í‘œëŠ” í˜„ì¬ì˜ MARL ë¶„ì•¼ì— ëŒ€í•œ ì˜ê²¬,í‰ê°€ì„ ì œì‹œí•˜ëŠ” ê²ƒì„ ë„˜ì–´ì„œ ìœ ìµí•œ ì—°êµ¬ë°©í–¥ì„ ì„¤ì •í•  ìˆ˜ ìˆë„ë¡ ë•ëŠ” ê²ƒì´ë‹¤.\n\n\n\nIntroduction\n\nìµœê·¼ ê°•í™”í•™ìŠµì€ ë³µì¡í•œ í•¨ìˆ˜ë¥¼ ê·¼ì‚¬í•  ìˆ˜ ìˆëŠ” ë”¥ëŸ¬ë‹ì˜ ë°œì „ê³¼ ë”ë¶ˆì–´ RLì€ ë†€ëê²Œ ì§„ë³´í–ˆë‹¤.\n\nì˜ˆë¥¼ ë“¤ë©´ playing real-time strategy games, playing car games, etc.\n\nëŒ€ë¶€ë¶„ì˜ ì„±ê³µì—ëŠ” í•˜ë‚˜ì˜ agentê°€ ì¡´ì¬í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ í•˜ë‚˜ ì´ìƒì˜ ë‹¤ìˆ˜ì˜ agentê°€ ì°¸ì—¬í•˜ë©° ì´ëŠ” MARLë¡œ ëª¨ë¸ë§ ëœë‹¤.\nMARLì€ êµ¬ì²´ì ìœ¼ë¡œ ë­˜ê¹Œ?\n\në‹¤ìˆ˜ì˜ ìë¦½í•˜ì—¬ ì›€ì§ì´ëŠ” agentë“¤ì´ ì¡´ì¬í•˜ê³ \nì´ëŸ¬í•œ agentê°€ ê³µí†µì ì¸ í™˜ê²½(environment)ì— ë†“ì—¬ìˆì„ ë•Œì˜\nìˆœì°¨ì ì¸ ì˜ì‚¬ê²°ì •ë¬¸ì œë¥¼ ë‹¤ë£¬ë‹¤.\nê°ê°ì˜ agentëŠ” í™˜ê²½,ë‹¤ë¥¸ agentë“¤ê³¼ì˜ ìƒí˜¸ì‘ìš©ì„ í†µí•´ì„œ ê·¸ë“¤ì´ ì–»ëŠ” ê°ê°ì˜ returnì„ ìµœëŒ€í™” í•˜ë ¤ê³  ë…¸ë ¥í•œë‹¤.\n\ní¬ê²Œ MARLì€ ë‹¤ìŒì˜ ì„¸ ê°€ì§€ë¡œ êµ¬ë¶„ëœë‹¤.\n\nfully cooperative \\(\\to\\) agentë“¤ì€ ê³µë™ì˜ returnì„ ìµœì í™” í•˜ê¸° ìœ„í•´ì„œ í˜‘ë ¥í•œë‹¤.\n\nex) ë¡œë´‡ì´ ë¬¼ê±´ì„ ì§€ì •ëœ ì¥ì†Œì— ìš´ë°˜í•˜ëŠ” ê²½ìš°, ì—¬ëŸ¬ëŒ€ì˜ ììœ¨ì£¼í–‰ì°¨ê°€ ëª©ì ì§€ì— ë„ì°©.\n\nfully competitive \\(\\to\\) agentë“¤ì€ í•©ì´ 0ì¸ returnì„ ì„œë¡œ ìµœì í™” í•˜ê¸° ìœ„í•´ì„œ ê²½ìŸí•œë‹¤.\n\nex) ë°”ë‘‘,ì²´ìŠ¤\n\na mix of two \\(\\to\\) agentê°€ ë³´í¸ì ì¸ returnì„ ìµœì í™” í•˜ê¸°ìœ„í•´ í˜‘ë ¥ or ê²½ìŸí•  ìˆ˜ ìˆë‹¤.(í˜‘ë ¥ë„ ê²½ìŸë„ ëª¨ë‘ ê°€ëŠ¥í•˜ë‹¤.)\n\nex) ì¶•êµ¬,ë†êµ¬\n\n\nMARLì´ ë¬´ì¡°ê±´ ì¢‹ì•„ë³´ì´ëŠ”ë°? \\(\\to\\) ì–´ë ¤ìš´ ì ì´ ë§ë‹¤.(ë³µìŠµ)\n\nê° ì—ì´ì „íŠ¸ë“¤ì€ ê°ê°ì˜ returnì„ ìµœëŒ€í™” í•˜ë ¤í•¨.\n\nê· í˜•ì ì—ì„œ ë¹„íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìœ¼ë©°(ë‚´ì‹œê· í˜•ì—ì„œ ë°œìƒí•˜ëŠ” ë¬¸ì œ)\ní†µì‹ ,í˜‘ë ¥ì´ íš¨ìœ¨ì ìœ¼ë¡œ ì´ë£¨ì–´ì§ˆ ìˆ˜ ìˆëŠ”ê°€ì— ëŒ€í•œ ì¶”ê°€ì ì¸ ê¸°ì¤€ì´ í•„ìš”í•¨.\nì ëŒ€ì  agentì— ëŒ€í•œ robustnessëŠ” ì¶©ë¶„í•œê°€?\n\nëª¨ë“  agentëŠ” ì €ë§ˆë‹¤ì˜ policyë¥¼ ê³„ì†í•´ì„œ í–¥ìƒ(ìˆ˜ì •).\n\nBë¼ëŠ” agentê°€ ì´ì „ê³¼ ë‹¤ë¥¸ actionì„ í•˜ê²Œ ë˜ë©´ environmentê°€ ë³€í•˜ê²Œ ë˜ê³ \nAë¼ëŠ” agentê°€ ì§ë©´í•˜ëŠ” environmentëŠ” non-stationaryí•´ì§€ëŠ” ê²ƒì„ ì˜ë¯¸.\n\nëª¨ë“  agentì— ëŒ€í•œ actionë“¤ì˜ ì¡°í•©,ê²°í•©ì€ agentí•œëª…ì´ ì¦ê°€í• ìˆ˜ë¡ ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€.\nê° agentëŠ” ë‹¤ë¥¸ agentê°€ ë¬´ì—‡ì„ ê´€ì¸¡í–ˆëŠ”ì§€ëŠ” ì •í™•íˆ,ëª¨ë“ ê²ƒì„ ì•Œ ìˆ˜ ì—†ìŒ.\n\në‹¤ë¥¸ agentì˜ ê´€ì¸¡ì— ëŒ€í•œ ì œí•œëœ ì •ë³´ë§Œì„ ê°€ì§€ê³  ê° agentëŠ” ê²°ì •ì„í•¨.\nìµœì ì˜ ê²°ì •ì´ ì•„ë‹Œ suboptimalí•œ ê²°ì •ì„ ê°€ì ¸ì˜´."
  },
  {
    "objectID": "posts/paper study/seq2seq/seq2seq.html",
    "href": "posts/paper study/seq2seq/seq2seq.html",
    "title": "Sequence to Sequence Learning with Neural Networks",
    "section": "",
    "text": "Deep Nueral NetworkëŠ” ë³µì¡í•œ taskì— ë†€ë¼ìš´ ì„±ëŠ¥ì„ ë³´ì—¬ì™”ì§€ë§Œ sequenceì—ì„œ sequenceë¥¼ mappingì—ì„œëŠ” taskì—ì„œëŠ” ì˜ ì‚¬ìš©ë˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\në…¼ë¬¸ì—ì„œëŠ” sequenceì˜ êµ¬ì¡°ì— ëŒ€í•œ ìµœì†Œí•œì˜ ê°€ì •ë§Œìœ¼ë¡œ í•™ìŠµí•˜ëŠ” end-to-end approachë¥¼ ì†Œê°œí•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/paper study/seq2seq/seq2seq.html#intuition",
    "href": "posts/paper study/seq2seq/seq2seq.html#intuition",
    "title": "Sequence to Sequence Learning with Neural Networks",
    "section": " Intuition",
    "text": "Intuition\n\n\n\nhttps://gaussian37.github.io/dl-concept-attention/\n\n\n(Process)\n\në¨¼ì € LSTMì— Input sequenceë¥¼ each time stepë§ˆë‹¤ ê°ê° ì…ë ¥í•©ë‹ˆë‹¤.\në§ˆì§€ë§‰ í† í°ì¸ \\(\\text{<EOS>}\\)ì´ ì…ë ¥ë˜ê³  ë‚˜ë©´ ë§ˆì§€ë§‰ hidden stateì¸ context vector(\\(\\bf{v}\\))ë¥¼ ì–»ìŠµë‹ˆë‹¤. \n\\(\\bf{v}\\)ë¥¼ ë˜ ë‹¤ë¥¸ LSTMì˜ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\nLSTMì€ each time stepë§ˆë‹¤ ì˜ˆì¸¡ yë¥¼ ì–»ì€ ë’¤ ë‹¤ìŒ stateì˜ ì…ë ¥ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤.\noutput sequenceëŠ” <EOS>ë¥¼ ë§Œë‚ ë•Œ ë” ì´ìƒ ì¶œë ¥í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n\n(context vectorë€?)\n\ncontext \\(\\bf{v}\\)ëŠ” input sequenceë³´ë‹¤ ì ì€ ì°¨ì›ì„ ê°€ì§€ë©° ëª¨ë“  inputì— ëŒ€í•´ì„œ ê³ ì •ëœ í¬ê¸°ë¥¼ ê°€ì§€ëŠ” ë²¡í„°ì´ë‹¤.\n\\(\\bf{v}\\)ëŠ” input sequenceì˜ í•µì‹¬ì ì¸ ì˜ë¯¸ë¥¼ í¬ì°©í•©ë‹ˆë‹¤.(ë¹„ìŠ·í•œ ì˜ë¯¸ë¥¼ ê°€ì§€ëŠ” ë¬¸ì¥ì€ ê±°ë¦¬ê°€ ê°€ê¹ê³  ì™„ì „ ë‹¤ë¥¸ ì˜ë¯¸ë¼ë©´ ê±°ë¦¬ê°€ ë©€ë‹¤.)\n\n(ì™œ í•˜í•„ LSTM?)\n\nLSTMëŠ” ë‹¹ì‹œì—ëŠ” ê¸´ ë¬¸ì¥ì„ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ”(long range temproal dependencies) state of the artì˜€ê¸° ë•Œë¬¸ì— LSTM ìœ„ì£¼ë¡œ ì„¤ëª…ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\në” ì„±ëŠ¥ì´ ì˜ ë‚˜ì˜¨ë‹¤ë©´ ì–¼ë§ˆë“ ì§€ ë‹¤ë¥¸ ëª¨ë¸ë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ex GRU ë“±ë“±â€¦)\n\n(Note)\n\ncontext vectorë¥¼ lstmì— ì…ë ¥í•˜ì—¬ \\(\\text{<EOS>}\\)í† í°ì„ ë§Œë‚ ë•Œê¹Œì§€ ì¶œë ¥í•˜ê¸° ë•Œë¬¸ì— inputê³¼ output sequenceì˜ ê¸¸ì´ëŠ” ë‹¤ë¦…ë‹ˆë‹¤. (ì¥ì )\nê³ ì •ëœ í¬ê¸°ì˜ context vectorë¡œ mappingë˜ê¸° ë•Œë¬¸ì— output sequenceë¥¼ ë‚´ë†“ê¸° ìœ„í•´ í•„ìš”í•œ ì •ë³´ê°€ ì‚¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ë‹¨ì )"
  },
  {
    "objectID": "posts/paper study/seq2seq/seq2seq.html#modeling",
    "href": "posts/paper study/seq2seq/seq2seq.html#modeling",
    "title": "Sequence to Sequence Learning with Neural Networks",
    "section": " Modeling",
    "text": "Modeling\n(vanila RNN)\n\\[\\begin{aligned}\n&h_t = \\text{sigm}(W^{hx}x_t + W^{hh}h_{t-1})\\\\\n&y_t = W^{yh}h_t\n\\end{aligned}\\]\n\nRNNì—ì„œ ì‚¬ìš©ë˜ëŠ” ìˆ˜ì‹ì€ ìœ„ì™€ ê°™ìŠµë‹ˆë‹¤.\nì´ë¡ ì ìœ¼ë¡œë§Œ ë³´ë©´ RNNì€ ê¸¸ì´ê°€ ê¸´ sequenceì—ì„œë„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\ní•˜ì§€ë§Œ sequenceì˜ ê¸¸ì´ê°€ ê¸¸ì–´ì§€ë©° long term dependency ê°€ì§€ëŠ” ê²½ìš° ì„±ëŠ¥ì´ ì¢‹ì§€ ëª»í•©ë‹ˆë‹¤.\në”°ë¼ì„œ vanila RNNëŒ€ì‹  LSTMì„ ì‚¬ìš©í•©ë‹ˆë‹¤.(ìˆ˜ì‹ ìƒëµ)\n\n(The goal of lstm)\n\\[\\begin{aligned}\n&p(y_1,y_2,\\dots,y_{T'}|x_1,x_2,\\dots,x_T) = \\prod_{t=1}^{T'}(y_t|v,y_1,\\dots,y_{t-1})\\\\\n&\\hat{{\\bf{y}}} = \\underset{{\\bf{y}}}{\\text{argmax}}\\,p(y_1,y_2,\\dots,y_{T'}|v,x_1,x_2,\\dots,x_T)\n\\end{aligned}\\]\n\nLSTMì˜ ëª©ì ì€ ì™¼ìª½ì˜ output \\(y_1,y_2,\\dots,y_{T'}\\)ì— ëŒ€í•œ conditional probability distributionì„ ì–»ëŠ” ê²ƒì…ë‹ˆë‹¤.\nì¦‰, each time step të§ˆë‹¤ softmaxë¥¼ í†µê³¼í•œ outputì¸ conditional distributionì„ ëª¨ë‘ ë©”ëª¨ë¦¬ì— ì €ì¥í•œ ë’¤ ì „ë¶€ ê³±í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\noutputì€ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§ˆë•Œì˜ \\(\\bf{y}\\)ë¼ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ë ‡ê²Œ sequenceë¥¼ ì¶œë ¥í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì„ exhaustive searchë¼ í•˜ë©° ë§¤ìš° ë†’ì€ ì‹œê°„ë³µì¡ë„ë¥¼ ê°€ì§„ë‹¤ëŠ” ë‹¨ì ì´ ìˆìŠµë‹ˆë‹¤.\n\nê°ê°ì˜ \\(y_1,y_2\\dots\\)ê°€ ê°€ì§ˆ ìˆ˜ ìˆëŠ” ê°’ë“¤ì´ ë„ˆë¬´ ë‹¤ì–‘í•˜ë©°(vocabì˜ í¬ê¸°) ëª¨ë“ ì¡°í•©ì— ëŒ€í•´ ê³±í•˜ì—¬ í™•ë¥ ë¶„í¬ë¥¼ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤.\ní™•ë¥ ë¶„í¬ë¥¼ ë‹¤ ê³„ì‚°í–ˆë‹¤ í•˜ë”ë¼ë„ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§€ëŠ” \\(\\bf{y}\\)ë¥¼ íƒìƒ‰í•´ì•¼ í•˜ëŠ” search spaceê°€ ë„ˆë¬´ í½ë‹ˆë‹¤.\n\në”°ë¼ì„œ ë‹¤ë¥¸ ì•Œê³ ë¦¬ì¦˜ì„ í†µí•´ì„œ conditional probability distributionì˜ maxê°’ì„ êµ¬í•´ì•¼ í•©ë‹ˆë‹¤.\n\n\n Searching Algorithm\n\n Greedy-search Algorithm\n\\[\\begin{aligned}\n\n&\\text{until } \\hat{y_t} \\text{ is not }\\text{<EOS>}\\\\\n&\\hat{y_t} = \\underset{y_t}{\\text{argmax}}\\,p(y_t|v,y_1,y_2,\\dots,y_{t-1})\\\\\n\\end{aligned}\\]\n\nGreedy Algorithmì€ Each timestep tì—ì„œ ì–»ì€ ê°ê°ì˜ condional distributionì˜ maxê°’ì„ ì°¾ëŠ” ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.\nExhaustive Algorithmê³¼ ë‹¤ë¥´ê²Œ search spaceê°€ ê·¸ë ‡ê²Œ í¬ì§€ ì•ŠìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ë” ì ì€ ì‹œê°„ë³µì¡ë„ë¥¼ ê°€ì§‘ë‹ˆë‹¤.\n\n\n\nìœ— ê·¸ë¦¼ì€ ê°ê°ì˜ timestep të§ˆë‹¤ \\(y_t\\)ë§ˆë‹¤ conditional distributionì„ ë‚˜íƒ€ë‚¸ ê·¸ë¦¼ì…ë‹ˆë‹¤.\nGreedy Algorithmì€ ê° të§ˆë‹¤ conditional distributionì„ maximizeí•˜ëŠ” í† í°ë§Œ outputsequenceë¡œ ì¶œë ¥í•©ë‹ˆë‹¤.\n\\(p(y_1,y_2,y_3,y_4) = 0.5 \\times 0.4 \\times 0.4 \\times 0.6 = 0.048\\)\n\n(Note)\n\nsearch space of Greedy algorithm < searchspace of Exhaustive Algorithm\në‹¤ë§Œ Greedy algorithmì˜ conditional distributionì˜ maxê°’ì„ ë³´ì¥í•˜ì§€ ëª»í•©ë‹ˆë‹¤.(ë‹¤ ê³„ì‚°í•˜ê³  ìµœëŒ“ê°’ì„ ë³´ëŠ”ê²Œ ì•„ë‹ˆë¼ ë”°ë¡œë”°ë¡œ maxë¥¼ ê³„ì‚°í•œê±¸ ê°€ì ¸ì˜¤ê¸° ë•Œë¬¸)\n\n\n\nìœ„ì˜ ê·¸ë¦¼ì€ t=2ì—ì„œë§Œ greedyí•˜ê²Œ ì§„í–‰í•˜ì§€ ì•Šì€ ê²½ìš°ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤.\n\\(p(y_1,y_2,y_3,y_4) = 0.054\\)ë¡œ ì˜¤íˆë ¤ ë” ë†’ì€ í™•ë¥ ì„ ê°€ì§‘ë‹ˆë‹¤.\nGreedy Algorithmì´ optimal sequenceë¥¼ ë³´ì¥í•˜ì§€ ëª»í•œë‹¤ëŠ” ì‚¬ì‹¤ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\n Beam-search Algorithm\n\nìš”ì•½í•˜ìë©´ Beam-searchëŠ” Greedy Searchë¥¼ \\(K\\)ê°œ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\n\n\nBeam-searh Algorithmì€ Greedy Algorithmì´ optimal sequenceë¥¼ ì˜ ì°¾ì§€ ëª»í•œë‹¤ëŠ” ì ì„ ë³´ì™„í•œ ì•Œê³ ë¦¬ì¦˜ì…ë‹ˆë‹¤.\nì²˜ìŒ \\(k\\)ê°œì˜ beamì„ í†µí•˜ì—¬ Greedy Searchë¥¼ ìˆ˜í–‰í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ ì…ë‹ˆë‹¤.\n\n(Beam-Algorithm Process)\n\në¨¼ì € hyparparameterì¸ Beamsize(\\(k\\))ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ìŠµë‹ˆë‹¤.\ntimestep = 1ì—ì„œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€ \\(k\\)ê°œì˜ Beamì„ ì„ íƒí•©ë‹ˆë‹¤.\nê°ê°ì˜ Beamì—ì„œ ê³„ì†í•´ì„œ conditional probabilityë¥¼ ê³„ì‚°í•˜ë©° Greedyí•˜ê²Œ íƒìƒ‰í•©ë‹ˆë‹¤.\nê°ê°ì˜ Beamì€ \\(\\text{<EOS>}\\)ë¥¼ ë§Œë‚¬ì„ë•Œ íƒìƒ‰ì„ ì¢…ë£Œí•˜ë©° candidateì— ì¶”ê°€ë©ë‹ˆë‹¤.\ncandidateì— ìˆëŠ” ëª¨ë“  Beamì— ëŒ€í•´ì„œ scoreë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\n\n(score)\n\\[\\begin{aligned}\n\\frac{1}{L^{\\alpha}}\\text{log}\\,p(y_1,\\dots,y_L|{\\bf{c}}) = \\frac{1}{L^{\\alpha}}\\sigma_{t=1}^{L}\\text{log}\\,p(y_t|y_1,\\dots,t_{t-1}|{\\bf{c}})\n\\end{aligned}\\]\n\n\\(L\\)ì€ ë¬¸ì¥ì˜ ê¸¸ì´ \\(\\alpha\\)ëŠ” ë³´í†µ 0.75ë¡œ ê¸´ ì‹œí€€ìŠ¤ì— ëŒ€í•´ì„œ íŒ¨ë„í‹°ë¥¼ ì£¼ê¸°ìœ„í•´ì„œ ì‚¬ìš©í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/paper study/seq2seq/seq2seq.html#experiments",
    "href": "posts/paper study/seq2seq/seq2seq.html#experiments",
    "title": "Sequence to Sequence Learning with Neural Networks",
    "section": " Experiments",
    "text": "Experiments\n\n Training Details\n\n4-layerê°€ ìˆëŠ” LSTM, 1000 cellìˆìœ¼ë©° 1000ì°¨ì›ì˜ ì›Œë“œ ì„ë² ë”©\në¬¸ì¥ì„ ë’¤ì§‘ì–´ì„œ inputìœ¼ë¡œ ì‚¬ìš©í–ˆìŒ.(ìì„¸í•œ ì´ìœ x)\nLSTMâ€™s parameters with the uniform distribution between -0.08 and 0.08\nSGD without momentum, ì´ˆê¸° lr = 0.7ë¡œ ê³ ì •. 5 epochsë’¤ì— lrì„ ë°˜ì ˆë¡œ ê³„ì† ë‚˜ëˆ„ì—ˆìœ¼ë©° ì´ 7.5 epochs.\nê° batchëŠ” 128 sequenceë“¤ë¡œ ì´ë£¨ì–´ì ¸ ìˆìœ¼ë©° gradientë¥¼ ê³„ì‚°í–ˆìœ¼ë©° ê³„ì‚°í•œ gradientë¥¼ ë°°ì¹˜ì‚¬ì´ì¦ˆì¸ 128ë¡œ ë‚˜ëˆ”.\ngradientì— constraintìˆìŒ\n\n\n\n Experiments Results\n\n\nSMTê¸°ë°˜ì˜ ë°©ë²•ë§Œí¼ì´ë‚˜ LSTMê¸°ë°˜ì˜ ë°©ë²•ë„ ë„ ì¢‹ì€ ê²°ê³¼ë¥¼ ëƒ„\n\n\n\n\ncontext vectorë¥¼ 2ì°¨ì›ìƒì— íˆ¬ì˜í•œ ê·¸ë¦¼\n\n\n\ncontext vectorë¥¼ PCAë¡œ 2ì°¨ì›ìƒì— íˆ¬ì˜í•œ ê·¸ë¦¼\në¹„ìŠ·í•œ ì˜ë¯¸ì˜ ë¬¸ì¥ì€ ê°€ê¹ê³  ë‹¤ë¥¸ ì˜ë¯¸ì˜ ë¬¸ì¥ì€ ê±°ë¦¬ê°€ ë©€ë”ë¼ => LSTMê¸°ë°˜ì˜ ë„¤íŠ¸ì›Œí¬ê°€ ì˜ë¯¸ë¥¼ ì˜ í¬ì°©í•¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/paper study/seq2seq/seq2seq.html#conclusion",
    "href": "posts/paper study/seq2seq/seq2seq.html#conclusion",
    "title": "Sequence to Sequence Learning with Neural Networks",
    "section": " Conclusion",
    "text": "Conclusion\n\nì…ë ¥ìˆœì„œë¥¼ ì—­ì „ì‹œí‚¤ë‹ˆ ê²°ê³¼ê°€ ì˜ ë‚˜ì™”ë‹¤.(ê·¸ëŸ¬ë‚˜ ê°œì¸ì ìœ¼ë¡œ ì´ê²Œ ì˜ë¯¸ìˆëŠ” ê²°ê³¼ì¸ì§€ëŠ” ëª¨ë¥´ê² ë‹¤.)\nLSTMê¸°ë°˜ì˜ ë°©ë²•ì´ ìƒê°ë³´ë‹¤ ì¢‹ì€ ì„±ëŠ¥ì„ ë‚´ë”ë¼.\nvocabularyê°€ ì œí•œë˜ë©° structureì— ëŒ€í•œ ì œí•œì´ ì—†ìŒì—ë„ ë¶ˆêµ¬í•˜ê³  SMT-based systemë§Œí¼ì´ë‚˜ LSTMê¸°ë°˜ì˜ ë°©ë²•ì´ ì¢‹ì€ ì„±ëŠ¥ì„ ë³´ì¸ë‹¤.\nê°„ë‹¨í•˜ê³ ,ì‰¬ìš´ ë°©ë²•ì´ SMTê¸°ë°˜ì˜ ë°©ë²•ì„ ë„˜ì—ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ,ê³„ì†í•´ì„œ ì—°êµ¬í•œë‹¤ë©´ ì´ëŸ¬í•œ ì ‘ê·¼ë²•ì´ sequence to sequence ë¬¸ì œì—ë„ ì¶©ë¶„íˆ ì ìš©ê°€ëŠ¥í•˜ë‹¤."
  },
  {
    "objectID": "posts/paper study/seq2seq with attention/seq2seq with attention.html",
    "href": "posts/paper study/seq2seq with attention/seq2seq with attention.html",
    "title": "[Paper Study] Neural Machine Translation by jointly learning to align and translate",
    "section": "",
    "text": "ë‹¹ì‹œ ì¸ê³µì‹ ê²½ë§ì„ í™œìš©í•œ ê¸°ê³„ë²ˆì—­ì—ì„œëŠ” ëŒ€ë¶€ë¶„ encoderì™€ decoderë¥¼ í¬í•¨í•œ ëª¨ë¸ì„ ì‚¬ìš©í–ˆìŠµë‹ˆë‹¤\nì´ëŸ¬í•œ ëª¨ë¸ì€ ê³ ì •ëœ ê¸¸ì´ì˜ context vectorë¥¼ ì‚¬ìš©í•˜ì—¬ ê¸¸ì´ê°€ ê¸´ ë¬¸ì¥ì—ì„œ ì„±ëŠ¥ì €í•˜ë¥¼ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤.\në”°ë¼ì„œ ë…¼ë¬¸ì—ì„œëŠ” ê°ê°ì˜ target worldì— ëŒ€í•´ ì„œë¡œë‹¤ë¥¸ context vectorë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì„œ ê¸¸ì´ê°€ ê¸´ ë¬¸ì¥ì— ëŒ€í•œ ì„±ëŠ¥ì €í•˜ë¥¼ ê°œì„ í•©ë‹ˆë‹¤.\nì´ëŸ¬í•œ ìƒˆë¡œìš´ ì ‘ê·¼ ë°©ì‹ì„ í†µí•´ ê·¸ ë‹¹ì‹œì˜ state of the artì¸ phrase-based systemê³¼ ë¹„ìŠ·í•œ ë²ˆì—­ ì„±ëŠ¥ì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/paper study/seq2seq with attention/seq2seq with attention.html#intuition",
    "href": "posts/paper study/seq2seq with attention/seq2seq with attention.html#intuition",
    "title": "[Paper Study] Neural Machine Translation by jointly learning to align and translate",
    "section": " Intuition",
    "text": "Intuition\n ì¶œì²˜ : paper-Figure1\n\nìœ„ì˜ ê·¸ë¦¼ì€ ë…¼ë¬¸ì—ì„œ ì œì‹œí•œ ëª¨ë¸ë¡œ ìœ„ëŠ” decoder ì•„ë˜ëŠ” encoderì…ë‹ˆë‹¤.\ndecoderë¥¼ ë³´ë©´ ê¸°ì¡´ì˜ seq2seq ì•„í‚¤í…ì³ì™€ ë‹¤ë¥¼ê²ƒì´ ê±°ì˜ ì—†ìŠµë‹ˆë‹¤ë§Œ ìƒˆë¡œìš´ ì •ë³´(íœ˜ì–´ì ¸ ë“¤ì–´ê°€ëŠ” í™”ì‚´í‘œ) ë“¤ì–´ê°€ë©° ì´ëŠ” ê·¸ë¦¼ì˜ ì•„ë˜ìª½ì— ìˆëŠ” encoderì—ì„œ ë§Œë“¤ì–´ì§ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nencoderëŠ” bidirectional RNNì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ì •ë°©í–¥ê³¼ ì—­ë°©í–¥ìœ¼ë¡œ ì½ì–´ë“¤ì´ë©´ì„œ input sequenceì— ëŒ€í•´ ì „ì²´ì ì´ë©´ì„œë„ íŠ¹íˆ i-th ë‹¨ì–´(í† í°)ê³¼ ì—°ê´€ëœ ì •ë³´ \\(h_t\\)ë¥¼ ë§Œë“­ë‹ˆë‹¤.\nì´ë ‡ê²Œ ë§Œë“¤ì–´ì§„ input sequenceì˜ ê° ì‹œì ì—ì„œì˜ ì •ë³´ëŠ” ê°ê°ì˜ targetì„ ë§Œë“œëŠ”ë° ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œ ì •ë³´ì¸ì§€ë¥¼ ì˜ë¯¸í•˜ëŠ” ê°’ì¸ \\(\\alpha\\)ì™€ ê³±í•˜ì—¬ ëª¨ë‘ ë”í•´ì§‘ë‹ˆë‹¤.\nì¦‰, ë”í•´ì§„ ê°’ì€ input sequenceì—ì„œ ëª¨ë“  ì •ë³´ë¥¼ targetì„ ì˜ˆì¸¡í•˜ëŠ”ë° ì–¼ë§ˆë‚˜ ì¤‘ìš”í•œì§€,ê´€ë ¨ìˆëŠ”ì§€ë¥¼ ê³ ë ¤í•´ì„œ ì¬ì¡°í•©í•œ ìƒˆë¡œìš´ ì •ë³´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ë”í•´ì§„ ê°’ì€ ìƒˆë¡œìš´ ì¶œë ¥ê°’ \\(y_t\\)ë¥¼ ë§Œë“¤ê¸° ìœ„í•œ ì •ë³´ì¸ decoderì˜ hidden state \\(s_t\\)ë¥¼ êµ¬í•˜ëŠ”ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n\n\nì–‘ë°©í–¥ vs ë‹¨ë°©í–¥ RNN\n\n\në‹¨ë°©í–¥ RNNì—ì„œ hidden state \\(h_t\\)\n\nì–´ë–»ê²Œ ìƒì„±? \\(\\rightarrow\\) input word \\(x_t\\)ì™€ ì´ì „ì— ì½ì–´ë“¤ì¸ sequenceì— ëŒ€í•œ ì •ë³´ \\(h_{t-1}\\)ë¥¼ í•©ì³ì„œ ìƒˆë¡œìš´ ì •ë³´ \nì§€ê¸ˆ ë‹¨ì–´ \\(x_t\\)ê¹Œì§€ ì…ë ¥ëœ sequenceê¹Œì§€ì˜ ëŒ€í•œ ì •ë³´ì´ì íŠ¹íˆ ë§ˆì§€ë§‰ìœ¼ë¡œ ì…ë ¥ëœ ë‹¨ì–´ë¥¼ ë§ì´ ê³ ë ¤í•œ ì •ë³´ \nì§€ê¸ˆì˜ inputì¸ \\(x_t\\)ë‹¤ìŒì— ì˜¤ëŠ” sequenceëŠ” ê³ ë ¤í•˜ì§€ ì•ŠìŒ\n\n\n\nì–‘ë°©í–¥ RNNì—ì„œì˜ hidden state \\(h_t\\)\n\nì–´ë–»ê²Œ ìƒì„±? \\(\\rightarrow\\) input sequenceë¥¼ ì •ë°©í–¥,ì—­ë°©í–¥ìœ¼ë¡œ ì„œë¡œë‹¤ë¥¸ RNNì„ í†µê³¼í•˜ì—¬ ìƒì„±\nì •ë°©í–¥ì—ì„œëŠ” ì§€ê¸ˆì˜ ë‹¨ì–´ \\(x_t\\)ê¹Œì§€ ìˆœì°¨ì ìœ¼ë¡œ ì…ë ¥ëœ sequenceì— ëŒ€í•œ ì •ë³´ì´ì íŠ¹íˆ ë§ˆì§€ë§‰ \\(x_t\\)ë¥¼ ë§ì´ ê³ ë ¤í•œ ì •ë³´ë¥¼ ë½‘ì•„ëƒ„.\në°˜ëŒ€ë¡œ ì—­ë°©í–¥ì—ì„œëŠ” ëì—ì„œë¶€í„° ì‹œì‘í•˜ì—¬ ë°˜ëŒ€ë¡œ \\(x_t\\)ê¹Œì§€ ì…ë ¥ëœ sequence ëŒ€í•œ ì •ë³´ì´ì íŠ¹íˆ ë§ˆì§€ë§‰ \\(x_t\\)ë¥¼ ë§ì´ ê³ ë ¤í•œ ì •ë³´ë¥¼ ë½‘ì•„ëƒ„.\nì •ë°©í–¥ì´ë˜ ì—­ë°©í–¥ì´ë˜ ë‹¤ìŒì— ì˜¤ëŠ” sequenceì— ëŒ€í•œ ì •ë³´ëŠ” ê³ ë ¤í•˜ì§€ ì•Šìœ¼ë‚˜ ì´ ë‘˜ì„ í•©ì³ì„œ ì „ì²´ì ì¸ ì •ë³´ + íŠ¹ì •ì‹œì  \\(x_t\\)ì— ê³ ë ¤í•œ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ.\n\n\n\n\nì§ê´€ì ì¸ ì •ë¦¬\n\n\n\n\n\n\n\n\n\n\n\n\nê¸°ì¡´ì˜ seq2seq ëª¨ë¸ì€ encoderì—ì„œ ê³ ì •ëœ ê¸¸ì´ì˜ context vectorë¡œ ë°”ê¾¼ ì •ë³´ë§Œì„ decoderì—ì„œ ì‚¬ìš©í•˜ì—¬ ê¸´ ë¬¸ì¥ì— ëŒ€í•´ì„œëŠ” ì •ë³´ì˜ ì†ì‹¤ì´ ì¼ì–´ë‚¬ìœ¼ë©° ë˜í•œ ì„±ëŠ¥ì´ ì¢‹ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.\në”°ë¼ì„œ ë…¼ë¬¸ì—ì„œëŠ” decoderì—ì„œ target(word)ì„ ë§Œë“¤ ë•Œ input sequnceì˜ ê°ê°ì˜ ìœ„ì¹˜ì—ì„œ ë‚˜ì˜¤ëŠ” ëª¨ë“  ì •ë³´ë¥¼ ì¤‘ìš”ë„ë¥¼ ë°˜ì˜í•˜ì—¬ ì¬ì¡°í•©í•œ ìƒˆë¡œìš´ ì •ë³´ë¥¼ ë§Œë“¤ê³  ì´ë¥¼ í™œìš©í•©ë‹ˆë‹¤.\nì´ë ‡ê²Œ ë§Œë“  ìƒˆë¡œìš´ ì •ë³´ëŠ” decoderê°€ output sequenceì˜ ê°ê°ì˜ targetì„ ì˜ˆì¸¡í• ë•Œ ê°€ì¤‘ì¹˜ \\(\\alpha\\)ë¥¼ í†µí•´ íŠ¹ì •ìœ„ì¹˜ ê·¼ì²˜ì˜ ë¬¸ë§¥ì— ì£¼ëª©,ì§‘ì¤‘í•œ ê°’ ì´ê¸°ë•Œë¬¸ì— ì§‘ì¤‘,ì£¼ì˜ë¥¼ ì˜ë¯¸í•˜ëŠ” attentionì´ë¼ëŠ” ìš©ì–´ë¥¼ ë”°ì™€ì„œ attention mechanismì´ë¼ê³  í•©ë‹ˆë‹¤.\nì´ë ‡ê²Œ attention mechanismì„ ì‚¬ìš©í•¨ìœ¼ë¡œì„œ encoderê°€ source sentenceì˜ ëª¨ë“ ì •ë³´ë¥¼ í•˜ë‚˜ì˜ ê³ ì •ëœ ê¸¸ì´ì˜ context vectorë¡œ ì¸ì½”ë”©í•´ì•¼ í•˜ëŠ” ë¶€ë‹´ì„ ì¤„ì—¬ì¤ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/paper study/seq2seq with attention/seq2seq with attention.html#modeling",
    "href": "posts/paper study/seq2seq with attention/seq2seq with attention.html#modeling",
    "title": "[Paper Study] Neural Machine Translation by jointly learning to align and translate",
    "section": " Modeling",
    "text": "Modeling\n\n Decoder\n\nìœ„ì™€ ê°™ì€ ìƒˆë¡œìš´ëª¨ë¸ì—ì„œ chainruleì—ì„œ ê°ê°ì˜ conditional probabilityëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n\\[\\begin{aligned}\np(y_i|y_1,\\dots,y_{i-1},{\\bf{x}}) = g(y_{i-1},s_{i-1},c_i)\n\\end{aligned}\\]\n\nê¸°ì¡´ì˜ seq2seqëª¨ë¸ì—ì„œëŠ” context vector \\(c\\)ëŠ” target \\(y_i\\)ê°€ ë°”ë€Œì–´ë„ ê³ ì •ëœ ë°”ë€Œì§€ ì•ŠëŠ” ê°’ì´ì—ˆìŠµë‹ˆë‹¤.\në…¼ë¬¸ì—ì„œ ì œì‹œëœ ëª¨ë¸ì€ ì´ì™€ëŠ” ë‹¤ë¥´ê²Œ ê°ê°ì˜ target \\(y_i\\)ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ ì„œë¡œë‹¤ë¥¸ context vector \\(c_i\\)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\nì—¬ê¸°ì„œ \\(c_i\\)ëŠ” decoderì˜ hidden stateì¸ \\(s_{i}\\)ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. ì¦‰,ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n\\[s_i = f(s_{i-1},y_{i-1},c_i)\\]\n\n\\(c_i\\)ëŠ” target \\(y_i\\)ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ input sequenceì—ì„œ ë‚˜ì˜¨ ì •ë³´ \\(h\\)ë¥¼ ì¤‘ìš”ë„ \\(a\\)ì— ë”°ë¼ ì¬ì¡°í•©í•œ ì •ë³´ì…ë‹ˆë‹¤. êµ¬ì²´ì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n\\[\\begin{aligned}\nc_i = \\sum_{j=1}^{T_x}\\alpha_{i,j}h_{j}\n\\end{aligned}\\]\n\nê°ê°ì˜ annotation ì¦‰ hidden state \\(h_j\\)ëŠ” ì „ì²´ë¬¸ì¥ì˜ ì •ë³´ë¥¼ ë‹´ê³  ìˆìœ¼ë‚˜ íŠ¹íˆ \\(j\\)ë²ˆì§¸ poistionê·¼ì²˜ì˜ ë¬¸ë§¥ì •ë³´ë¥¼ ë§ì´ ë‹´ê³  ìˆìŠµë‹ˆë‹¤.(bidirectional RNNì— ì˜í•œ ê²°ê³¼ì…ë‹ˆë‹¤.)\nì°¸ê³  - \\(i\\)ê°’ì´ ë°”ë€Œë”ë¼ë„ ì¦‰, ë˜ ë‹¤ë¥¸ targetì„ ì˜ˆì¸¡í•˜ë”ë¼ë„ ì°¸ê³ í•˜ëŠ” input sequenceì˜ ì •ë³´ì¸ annotationsëŠ” ë°”ë€Œì§€ ì•ŠìŒ(ì €ì¥í•´ë†¨ë‹¤ê°€ ê°ê°ì˜ \\(y_i\\)ê°’ì„ êµ¬í•˜ëŠ”ë° ì‚¬ìš©í•  ìˆ˜ ìˆìŒ,êµ¬í˜„ì‹œ ìœ ì˜)\nê°ê°ì˜ ê°€ì¤‘ì¹˜ \\(\\alpha\\)ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\\[\\begin{aligned}\n\\alpha_{ij} = \\frac{\\text{exp}(e_{ij})}{\\sum_{k=1}^{T_x}\\text{exp}(e_{ik})}\\\\\n\\text{where},\\,e_{ij} = a(s_{i-1},h_j)\n\\end{aligned}\\]\n\n\\(\\alpha\\)ëŠ” softmaxì˜ í•¨ìˆ«ê°’ì´ë©° \\(e\\)ë¥¼ 0ê³¼1ì‚¬ì´ì˜ ê°’ìœ¼ë¡œ scailing í–ˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì—¬ê¸°ì„œ \\(a\\)ëŠ” alignment modelë¡œ input sequenceì˜ jë²ˆì§¸ positionê³¼ output sequenceì˜ ië²ˆì§¸ positionì´ ì–¼ë§ˆë‚˜ ì¼ì¹˜í•˜ëŠ”ì§€,ì—°ê´€ë˜ì–´ìˆëŠ”ì§€,ê´€ë ¨ìˆëŠ”ì§€ ì•Œë ¤ì£¼ëŠ” ê°’ì´ë©° ì´ëŠ” feedforward nueral networkë¡œë¶€í„° ê³„ì‚°ë©ë‹ˆë‹¤.\nì´ì™€ê°™ì´ (soft)alignment(ì¼ì¹˜,ì •ë ¬)ë¥¼ ì§ì ‘ì ìœ¼ë¡œ ê³„ì‚°í•¨ìœ¼ë¡œì„œ alignmentê°€ ì ì¬ì (ë³´ì´ì§€ì•Šë˜,ìˆ¨ê²¨ì ¸ìˆì—ˆë˜)ì´ì—ˆë˜ ê¸°ì¡´ì˜ ê¸°ê³„ë²ˆì—­ ëª¨ë¸ë“¤ê³¼ëŠ” ë‹¤ë¥´ê²Œ alignmentë¥¼ ë” ì˜ í•™ìŠµí•˜ë„ë¡ gradientê°€ backpropagation ë  ìˆ˜ ìˆìœ¼ë©° ë”°ë¼ì„œ inputê³¼ output sequnceì—ì„œì˜ alignmentë¥¼ ê¸°ì¡´ëª¨ë¸ë³´ë‹¤ ë” ì˜ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\n Encoder\n\nì¸ì½”ë”ì—ì„œëŠ” ì •ë°©í–¥,ì—­ë°©í–¥ìœ¼ë¡œ input sequenceë¥¼ ëª¨ë‘ ì½ì–´ë“¤ì´ëŠ” bidirectional RNNì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\nforward RNN \\(\\overset{\\rightarrow}{f}\\)ì€ \\(x_1\\)ì—ì„œ \\(x_{T_x}\\)ê¹Œì§€ forward hidden statesì¸ \\((\\overset{\\rightarrow}{h_1},\\dots,\\overset{\\rightarrow}{h_{T_x}})\\)ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.\nbackward RNN \\(\\overset{\\leftarrow}{f}\\)ì€ \\(x_{T_x}\\)ì—ì„œ \\(x_1\\)ê¹Œì§€ bacward hidden statesì¸ \\((\\overset{\\leftarrow}{h_{T_x}},\\dots,\\overset{\\leftarrow}{h_{T_1}})\\)ì„ ê³„ì‚°í•©ë‹ˆë‹¤.\nì–‘ë°©í–¥,ì •ë°©í–¥ì˜ hidden stateë¥¼ ëª¨ë‘ ê²°í•©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ê²°í•©ëœ hidden stateê°’ì„ ê³„ì‚°í•©ë‹ˆë‹¤. \\[h_j = \\left[\\overset{\\rightarrow}{h}_j^{\\,\\,T};\\overset{\\leftarrow}{h}_j^{\\,\\,T}\\right]^T\\]\n\\(h_j\\)ëŠ” input sequence ì „ì²´ì˜ ì •ë³´ë¥¼ ëª¨ë‘ ê°–ì§€ë§Œ íŠ¹íˆ position jê·¼ì²˜ì— ì§‘ì¤‘ëœ ì •ë³´ë¥¼ ê°€ì§‘ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Probability Statistics/categori distribution.html",
    "href": "posts/Probability Statistics/categori distribution.html",
    "title": "ì¹´í…Œê³ ë¦¬ ë¶„í¬",
    "section": "",
    "text": "ì¹´í…Œê³ ë¦¬ ë¶„í¬ì— ëŒ€í•œ ì •ë¦¬\n\nì¹´í…Œê³ ë¦¬ ë¶„í¬\nì¹´í…Œê³ ë¦¬ë¶„í¬ëŠ” ì‹œí–‰ì˜ í•œë²ˆì˜ ì‹œí–‰(ë˜ëŠ” ì‹¤í—˜)ìœ¼ë¡œë¶€í„° ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ì‚¬ê±´ì´ Kê°œì¸ í™•ë¥ ë¶„í¬ë¥¼ ëª¨ë¸ë§í• ë•Œ ì“°ì´ë©° ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\begin{aligned}\n&Cat({\\bf{x};\\bf{\\mu}}) =\n\\begin{cases}\n\\mu_1\\, (\\text{if } x = (1,0,0,0,\\dots,1)) \\\\\n\\mu_2\\, (\\text{if } x = (0,1,0,0,\\dots,1)) \\\\\n\\mu_3\\, (\\text{if } x = (0,0,1,0,\\dots,1)) \\\\\n\\vdots \\\\\n\\mu_k\\, (\\text{if } x = (0,0,0,0,\\dots,1)) \\\\\n\\end{cases}\n\\\\\n&\\text{where, }x = (x_1,x_2,\\dots,x_K),\\mu = (\\mu_1,\\mu_2,\\dots,\\mu_k)\n\\end{aligned}\\]\nì¹´í…Œê³ ë¦¬ë¶„í¬ì˜ ë³€ìˆ˜\\(\\bf{X}\\)ëŠ” Kê°œì˜ ì›ì†Œë¥¼ ê°€ì§€ëŠ” ì›í•«ì¸ì½”ë”©(one-hot encoded)ëœ ë²¡í„°ì´ë©° ê°ì›ì†ŒëŠ” indicate number(ì–´ë–¤ í´ë˜ìŠ¤ì— ì†í•˜ëŠ”ì§€ ë‚˜íƒ€ë‚´ëŠ”)ì¸ 1ë˜ëŠ”0ì…ë‹ˆë‹¤. ëª¨ìˆ˜(ë²¡í„°)\\(\\mu\\)ë„ Kê°œì˜ ì›ì†Œë¥¼ ê°€ì§€ë©° ê°ê°ì˜ ì›ì†ŒëŠ” ì¹´í…Œê³ ë¦¬ í™•ë¥ ë¶„í¬ë¡œë¶€í„° ëŒ€ì‘í•˜ëŠ” ê²°ê³¼ê°’(ì›í•«ë²¡í„°)ì— ëŒ€í•œ í™•ë¥ ì…ë‹ˆë‹¤. ì¦‰,ê°ê°ì˜ ì›í•«ë²¡í„°ê°€ í‘œë³¸ì¶”ì¶œë  ê°€ëŠ¥ì„±(í™•ë¥ )ì„ ì•Œë ¤ì¤ë‹ˆë‹¤.\nìœ„ì™€ ê°™ì€ ì‚¬ì‹¤ë¡œë¶€í„° ë‹¤ìŒê³¼ ê°™ì€ 4ê°€ì§€ì˜ ì œì•½ì¡°ê±´ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\n\n\\(\\mu_i\\)ëŠ” ì›í•«ë²¡í„°ê°€ ë‚˜ì˜¬ í™•ë¥ ì…ë‹ˆë‹¤.\n\n\\[0\\leq\\mu_i\\leq1\\]\n\ní™•ë¥ ì˜ í•©ì€ 1ì…ë‹ˆë‹¤.\n\n\\[\\sum_{i=1}^{K}\\mu_i = 1\\]\n\nì›í•«ë²¡í„°ì˜ ê° ì›ì†ŒëŠ” indicate numberì¸ 1ë˜ëŠ” 0ì…ë‹ˆë‹¤.\n\n\\[\\begin{aligned}\nx_i =\n\\begin{cases}\n0\\\\\n1\n\\end{cases}\n\\end{aligned}\\]\n\nì›í•«ë²¡í„°ì˜ ëª¨ë“  ì›ì†Œì˜ í•©ì€ 1ì…ë‹ˆë‹¤. \\[\\sum_{i=1}^{K}x_i = 1\\]"
  },
  {
    "objectID": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html",
    "href": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html",
    "title": "MLE & MAP (ì‘ì„±ì¤‘)",
    "section": "",
    "text": "HYEONGMIN LEEâ€™S WEBSITE - MLE & MAPë¥¼ ê³µë¶€í•˜ê³  ì •ë¦¬í•œ ê¸€ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#problem-setting",
    "href": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#problem-setting",
    "title": "MLE & MAP (ì‘ì„±ì¤‘)",
    "section": "Problem Setting",
    "text": "Problem Setting\nì‹¤ì œ \\(N\\)ëª…ì˜ ì‚¬ëŒë“¤ë¡œë¶€í„° í‚¤(\\(x\\))ì™€ ëª¸ë¬´ê²Œ(\\(t\\))ë¥¼ ì¡°ì‚¬í•œ ë‹¤ìŒê³¼ ê°™ì€ data set \\(D\\)ë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  í•´ë³´ì.\n\\[ D = \\{(x_1,t_1),(x_2,t_2,),\\dots,(x_N,t_N)\\}\\]\ní‚¤ë¡œ ëª¸ë¬´ê²Œë¥¼ ì˜ˆì¸¡í•˜ëŠ” ëª¨í˜•ì„ ë§Œë“¤ê³ ì í•œë‹¤. ì–´ë–»ê²Œ ë§Œë“¤ ìˆ˜ ìˆì„ê¹Œ?"
  },
  {
    "objectID": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#modeling",
    "href": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#modeling",
    "title": "MLE & MAP (ì‘ì„±ì¤‘)",
    "section": " Modeling ",
    "text": "Modeling \nê°€ì¥ ë‹¨ìˆœí•œ ì ‘ê·¼ì€ í‚¤(\\(x\\))ë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ê³  ëª¸ë¬´ê²Œ(\\(t\\))ë¥¼ ì¶œë ¥í•˜ëŠ” í•¨ìˆ˜\\(y(x|\\theta)\\)ë¥¼ ë§Œë“œëŠ” ê²ƒì´ë‹¤.\n\\[t = y(x|\\theta)\\]\n\n\\(\\theta\\)ëŠ” í•¨ìˆ˜ì˜ parameterë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\n\níŒŒë¼ë¯¸í„°\\(\\theta\\)ë¥¼ í•™ìŠµí•˜ì—¬ í‚¤-ëª¸ë¬´ê²Œ ì‚¬ì´ì˜ ê´€ê³„ë¥¼ í‘œí˜„í•˜ëŠ” ê±°ì˜ ì™„ë²½í•˜ê²Œ í‘œí˜„í•˜ëŠ” í•¨ìˆ˜ë¥¼ ì–»ì—ˆë‹¤ê³  í•´ë³´ì. ê·¸ë ‡ë‹¤ë©´ í‚¤\\(x\\)ë¥¼ ì…ë ¥ìœ¼ë¡œ í•˜ì—¬ ëª¸ë¬´ê²Œì— ëŒ€í•œ ì˜ˆì¸¡ê°’ \\(t\\)í•˜ë‚˜ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ í‚¤ê°€ 175cmì¸ ì‚¬ëŒì˜ ëª¸ë¬´ê²Œê°€ í•¨ìˆ˜ë¥¼ í†µí•˜ì—¬ ì–»ì€ ì˜ˆì¸¡ê°’ì´ 72kgì´ë¼ê³  í•˜ì. ì´ ê²°ê³¼ë¥¼ ë§ë¡œí•˜ë©´ â€œí‚¤ê°€ 175cmì¸ ì‚¬ëŒì˜ ëª¸ë¬´ê²ŒëŠ” 72kgì´ì•¼.â€ë¼ê³  í•˜ëŠ” ê²ƒê³¼ ê°™ë‹¤.\nì´ë ‡ê²Œ í‚¤\\(x\\)ì— ëŒ€í•´ì„œ ëª¸ë¬´ê²Œì— ëŒ€í•œ í•˜ë‚˜ì˜ ì˜ˆì¸¡ê°’\\(y\\)ë¥¼ í•˜ë‚˜ë§Œ ëŒë ¤ì£¼ëŠ” ë°©ë²•ì€ ì˜³ì„ê¹Œ? ê²°ë¡ ì ìœ¼ë¡œ ë§í•˜ë©´ ê·¸ë ‡ì§€ ì•Šë‹¤. ì™œëƒí•˜ë©´ ì‹¤ì œ dataì˜ í˜•íƒœëŠ” í‚¤\\(x\\)ì— ëŒ€í•´ í•˜ë‚˜ì˜ ëª¸ë¬´ê²Œ\\(t\\)ë§Œì„ ê°€ì§€ì§€ ì•Šê¸°ì´ë‹¤. ì˜ˆë¥¼ ë“¤ìë©´ \\((175,62),(175,72),(175,83),(175,88)\\dots\\)ë“±ë“± ë‹¤ì–‘í•œ í‚¤-ëª¸ë¬´ê²Œ ë‹¤ì–‘í•œ ì¡°í•©ì´ ê°€ëŠ¥í•˜ì§€ë§Œ í•¨ìˆ˜ë¡œ í‘œí˜„í•  ê²½ìš° í•˜ë‚˜ì˜ ì…ë ¥ì€ í•˜ë‚˜ì˜ ì¶œë ¥ì—ë§Œ mappingë˜ë©° ì´ ë°–ì— ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ë‹¤ë¥¸ ê°’ë“¤ì— ëŒ€í•œ ì„¤ëª…ì€ ì „í˜€ í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ë ‡ë‹¤ë©´ ì–´ë– í•œ ë°©ë²•ì´ ë” ì¢‹ì„ê¹Œ? í‚¤ê°€ 175cmì¸ ì‚¬ëŒì˜ ëª¸ë¬´ê²ŒëŠ” ì•„ë§ˆë„ 72kg êº¼ì•¼ ì •ë„ì˜ ë§ì„ í•  ìˆ˜ ìˆë‹¤ë©´ ê°€ì¥ ì¢‹ì„ ê²ƒì´ë‹¤.\nì´ëŸ¬í•œ í•´ì„ì€ í‚¤ê°€ 175cmì¸ ì‚¬ëŒì— ëŒ€í•´ì„œ ëª¸ë¬´ê²Œì— ëŒ€í•œ ì˜ˆì¸¡ì´ ê±°ì˜ 72kgì´ë¼ê³  í™•ì‹ í•˜ì§€ë§Œ ì´ì™€ ë™ì‹œì— ëª¸ë¬´ê²Œì˜ ì˜ˆì¸¡ê°’ì´ ë¶ˆí™•ì‹¤í•˜ë©° í‚¤ê°€ ë™ì¼í•˜ë‹¤ í• ì§€ë¼ë„ ì–´ëŠì •ë„ëŠ” ëœë¤í•˜ë‹¤ëŠ” ê²ƒì„ ë‚´í¬í•œë‹¤.ì´ë ‡ê²Œ ë¶ˆí™•ì‹¤í•˜ë©° ëœë¤í•œ ê°€ì§€ëŠ” ê°’ì€ randomvariableë¼ê³  í•˜ë©° ë¬¸ì œì—ì„œ ëª¸ë¬´ê²Œ\\(t\\)ë¥¼ random-variableë¡œ ì·¨ê¸‰í•˜ë©´ ìœ„ì™€ ê°™ì€ í•´ì„ì´ ê°€ëŠ¥í•˜ë‹¤. ì˜ˆë¥¼ë“¤ì–´ í‚¤ê°€ \\(x\\)ì¸ ì‚¬ëŒì˜ ëª¸ë¬´ê²Œ\\(t\\)ê°€ randomvariableì´ë©° ë‹¤ìŒê³¼ ê°™ì€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ì.\n\\[\\begin{aligned}\n&t\\sim\\mathcal{N}(y(x|\\theta),\\sigma^2) \\\\\n&\\longleftrightarrow p(t|x,\\theta,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(\\frac{-(t-y(x|\\theta))^2}{2\\sigma^2})\n\\end{aligned}\\]\nìœ„ì™€ ê°™ë‹¤ë©´ \\(y(x|\\theta)\\)ì—ì‚¬ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§€ëŠ” ì •ê·œë¶„í¬ì´ë¯€ë¡œ â€œí‚¤ê°€ \\(x\\)ì¸ ì‚¬ëŒì˜ ëª¸ë¬´ê²Œ\\(t\\)ëŠ” ì•„ë§ˆë„ \\(y(x|\\theta)\\)ì¼êº¼ì•¼â€ë¼ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ë©° ë˜í•œ ë‹¤ë¥¸ ê°’ë“¤ì— ëŒ€í•œ í™•ë¥ ë“¤ë„ ì–´ëŠì •ë„ ë‹´ê³  ìˆìœ¼ë¯€ë¡œ \\(t\\)ê°€ ë¶ˆí™•ì‹¤í•˜ë©° ëœë¤í•˜ë‹¤ëŠ” ê²ƒë„ í™•ì‹¤í•˜ê²Œ í‘œí˜„í•¨ì„ ì•Œ ìˆ˜ ìˆë‹¤.\nì—¬ê¸°ì„œ \\(\\sigma^2\\)ëŠ” ìš°ë¦¬ê°€ í•œ ì˜ˆì¸¡ì— ëŒ€í•´ì„œ ì–¼ë§ˆë‚˜ ë¶ˆí™•ì‹¤ì¸ì§€ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì •ë„ì´ë‹¤. \\(\\sigma^2\\)ê°€ ì»¤ì„œ í™•ë¥ ë¶„í¬ì˜ ë³€ë™ì´ í¬ë‹¤ë©´ ëª¨ë“  ê°’ì— ëŒ€í•˜ì—¬ ì¼ì •í•œ í™•ë¥ ì„ ë¶€ì—¬í•˜ë¯€ë¡œ ì˜ˆì¸¡ê°’ \\(t\\)ì— ëŒ€í•œ í™•ì‹ ì´ ì—†ë‹¤ëŠ” ê²ƒì´ê³  ë³€ë™ì´ ì‘ë‹¤ë©´ ì–´ë–¤ íŠ¹ì • ê°’ì—ë§Œ í™•ë¥ ì„ ëª°ì•„ì„œ ë¶€ì—¬í•œ ê²ƒì´ë¯€ë¡œ ì˜ˆì¸¡ê°’\\(t\\)ì— ëŒ€í•œ í™•ì‹ ì´ í¬ë‹¤ëŠ” ê²ƒì´ë‹¤\nì˜ˆë¥¼ ë“¤ì–´ \\(x = 175,\\,y(x|\\theta) = 72,\\,t\\sim\\mathcal{N}(72,\\sigma^2)\\)ì¸ ë‘ ê°œì˜ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ë©´ ëª¨ì–‘ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n\n<matplotlib.legend.Legend at 0x19bf519daf0>\n\n\n\n\n\nì´ì™€ ê°™ì´ í‚¤\\(x=175\\)ì— ì¼ë•Œ ëª¸ë¬´ê²Œ\\(t\\)ëŠ” randomvariableì´ë©° í‰ê· ì´72 í™•ë¥ ì´ ê°€ì¥ ë†’ìœ¼ë¯€ë¡œ í‚¤ê°€ 175cmì¸ ì‚¬ëŒì˜ ëª¸ë¬´ê²ŒëŠ” ì•„ë§ˆë„ 72kg êº¼ì•¼ë¼ëŠ” ë§ì„ í•  ìˆ˜ ìˆìœ¼ë©° ë˜í•œ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë‚˜ì˜¬ í™•ë¥ ë„ ë¹ ì§ì—†ì´ í‘œí˜„ë˜ì–´ ìˆìœ¼ë¯€ë¡œ ì´ì™€ ê°™ì€ ëª¸ë¬´ê²Œ\\(t\\)ëŠ” í™•ë¥ ë³€ìˆ˜ë¡œ í‘œí˜„í•˜ëŠ” ì ì ˆí•˜ë‹¤. ë˜í•œ ë¹¨ê°„ìƒ‰ ê·¸ë˜í”„ëŠ” íŒŒë€ìƒ‰ê·¸ë˜í”„ë³´ë‹¤ \\(\\sigma^2\\)ê°€ ë” í¬ë©° \\(t=72\\)ì—ì„œ ë” ë†’ì€ í™•ë¥ ì„ ë¶€ì—¬í•˜ë¯€ë¡œ ì˜ˆì¸¡ì— ëŒ€í•œ í™•ì‹ ì´ ë” í¬ë‹¤ëŠ” ê²ƒì„ ë‚˜íƒ€ë‚¸ë‹¤.\nì§€ê¸ˆê¹Œì§€ ëª¸ë¬´ê²Œ \\(t\\)ë¥¼ randomvariableì´ë©° í™•ë¥ ë¶„í¬ \\(\\mathcal{N}(y(x|\\theta),\\sigma^2)\\)ë¥¼ ë”°ë¥¸ë‹¤ê³  í•˜ì˜€ë‹¤. ì˜ˆì‹œì—ì„œëŠ” í™•ë¥ ë¶„í¬ë¥¼ parameterë¥¼ ê°€ì •í•˜ê³  ê·¸ë˜í”„ë¥¼ ê·¸ë ¤ì„œ ë¹„êµí–ˆì§€ë§Œ ì‹¤ì œë¡œëŠ” í™•ë¥ ë¶„í¬ì˜ ì¢…ë¥˜ë„ ëª¨ë¥´ê±°ë‹ˆì™€ í™•ë¥ ë¶„í¬ì˜ parameterë„ ëª¨ë¥´ê¸°ì— ìœ„ì™€ ê°™ì€ í•´ì„ì„ í•  ìˆ˜ ì—†ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì—¬ê¸°ì„œë¶€í„°ëŠ” í™•ë¥ ë¶„í¬ë¥¼ ì¶”ì •í•˜ëŠ” ê²ƒì— ê´€í•œ ì–˜ê¸°ë¥¼ í•´ë³´ê³ ì í•œë‹¤."
  },
  {
    "objectID": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#maximum-likelyhood-estimation",
    "href": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#maximum-likelyhood-estimation",
    "title": "MLE & MAP (ì‘ì„±ì¤‘)",
    "section": "Maximum Likelyhood Estimation",
    "text": "Maximum Likelyhood Estimation\nì£¼ì–´ì§„ ë°ì´í„°ì— ë§ëŠ” í™•ë¥ ë¶„í¬ëŠ” ì–´ë–»ê²Œ ê²°ì •í•  ìˆ˜ ìˆì„ê¹Œ? í¬ê²Œ ë‘ ë‹¨ê³„ë¡œ ë‚˜ë‰œë‹¤. - í™•ë¥ ë¶„í¬ì˜ ì¢…ë¥˜ ê²°ì • - í™•ë¥ ë¶„í¬ì˜ íŒŒë¼ë¯¸í„° ì¶”ì •\ní™•ë¥ ë¶„í¬ì˜ ì¢…ë¥˜ë¥¼ ê²°ì •í•˜ëŠ” ì¼ì€ ë¬¸ì œë§ˆë‹¤ ë‹¤ë¥´ë‹¤. ìœ„ì—ì„œ ì˜ˆì‹œë¡œ ë“¤ì—ˆë˜ ëª¸ë¬´ê²Œì™€ ê°™ì€ ì—°ì†ì ì¸ í™•ë¥ ë³€ìˆ˜ì¼ ê²½ìš° ì—°ì†í™•ë¥ ë¶„í¬ ì¤‘ í•˜ë‚˜ë¥¼ ê°€ì •í•˜ê³  ì´ì‚°í™•ë¥ ë³€ìˆ˜ì¼ ê²½ìš° ì´ì‚°í™•ë¥ ë¶„í¬ ì¤‘ í•˜ë‚˜ë¥¼ ê°€ì •í•œë‹¤. ìœ„ì˜ ë¬¸ì œì—ì„œëŠ” ì •ê·œë¶„í¬ë¥¼ ëª¸ë¬´ê²Œ\\(t\\)ì˜ ë¶„í¬ë¡œ ê°€ì •í–ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŠ” ë¬¸ì œì— ë”°ë¼ ë‹¤ë¥´ë©° ë¬¸ì œì— ë§ëŠ” ì ì ˆí•œ í™•ë¥ ë¶„í¬ë¥¼ ë¨¼ì € ê°€ì •í•´ì•¼ í•œë‹¤.\ní™•ë¥ ë¶„í¬ë¥¼ ê°€ì •í–ˆë‹¤ë©´ ë‹¤ìŒì€ í™•ë¥ ë¶„í¬ì˜ parameterë¥¼ êµ¬í•˜ë©´ ëœë‹¤.(ì¶”ì •í•œë‹¤ê³  ë§í•œë‹¤.)parameterë¥¼ ì¶”ì •í•  ê²½ìš° ìš°ë¦¬ëŠ” í™•ë¥ ë¶„í¬ì— ëŒ€í•œ ì™„ì „í•œ ì‹ì„ ì–»ì„ ìˆ˜ ìˆìœ¼ë©° ë”°ë¼ì„œ í™•ë¥ ë¶„í¬ë¡œë¶€í„° ì—¬ëŸ¬ê°€ì§€ ì •ë³´ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤. ì—¬ê¸°ì„œëŠ” MLEë¥¼ í†µí•´ì„œ í™•ë¥ ë¶„í¬ì˜ íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì •í•œë‹¤.\nì•ì„  ë¬¸ì œë¥¼ í’€ê¸°ì „ì— ì•ì„œì„œ ê°„ë‹¨í•œ ì˜ˆì‹œë¡œ ì˜ˆë¥¼ MLEì— ëŒ€í•œ ê°ì„ ì¡ì•„ë³´ì.ì˜ˆë¥¼ ë“¤ì–´ ëŒ€í•œë¯¼êµ­ 20ëŒ€ ë‚¨ì„±ë“¤ ì¤‘ í•œëª…ì˜ í‚¤ë¥¼ ê³¨ë¼ì„œ ì–»ì€ data set \\(D = \\{65\\}\\)ê°€ ì£¼ì–´ì¡Œë‹¤ê³  ê°€ì • í•´ë³´ì. ë¶„í¬ì˜ ëª¨ì–‘ì€ ë¨¼ì € ì •ê·œë¶„í¬ë¡œ ê°€ì •í•œë‹¤.\n\n\n<matplotlib.legend.Legend at 0x19bf1d77f10>\n\n\n\n\n\ní‰ê· ë§Œ ë‹¤ë¥¸ ì •ê·œë¶„í¬ 3ê°œë¥¼ ê³ ë ¤í•´ë³´ì. ì´ ì¤‘ ì–´ë–¤ ë¶„í¬ì—ì„œ \\(D\\)ê°€ ë‚˜ì™”ì„ê¹Œ? ì£¼ì–´ì§„ 3ê°œì˜ ë¶„í¬ì¤‘ì—ëŠ” \\(\\mu=63\\)ì¸ íŒŒë‘ìƒ‰ì— í•´ë‹¹í•˜ëŠ” ë¶„í¬ì—ì„œ \\(D=\\{65\\}\\)ë¥¼ ë‚˜ì™”ì„ ê°€ëŠ¥ì„±ì´ ê°€ì¥ í¬ë‹¤. ì™œëƒí•˜ë©´ íŒŒë‘ìƒ‰ í™•ë¥ ë¶„í¬ì—ì„œ datasetì— ì†í•œ í•˜ë‚˜ì˜ datapointê°€ ë‚˜ì˜¬ í™•ë¥ ì´ ê°€ì¥ í¬ê¸°ë•Œë¬¸ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ 3ê°œì˜ ì •ê·œë¶„í¬ì¤‘ì—ì„œëŠ” \\(\\mu =63\\)ì¸ ê²½ìš°ê°€ \\(D\\)ì— ê°€ì¥ ì í•©í•˜ë‹¤ê³  í•  ìˆ˜ ìˆìœ¼ë©° ê·¸ ë‹¤ìŒì€ ë³´ë¼ìƒ‰ ê·¸ ë‹¤ìŒì€ ì´ˆë¡ìƒ‰ ë¶„í¬ì˜ ëª¨ìˆ˜ê°€ ì í•©í•˜ë‹¤.\nì´ë²ˆì—ëŠ” 2ê°œì˜ ê´€ì¸¡ì¹˜ê°€ ì¡´ì¬í•˜ëŠ” data set \\(D = \\{62,65\\}\\)ê°€ ì£¼ì–´ì¡Œë‹¤ê³  ê°€ì • í•´ë³´ì.\n\n\n<matplotlib.legend.Legend at 0x19bf7255e80>\n\n\n\n\n\nì–´ë–¤ í™•ë¥ ë¶„í¬ì—ì„œ \\(D=\\{62,65\\}\\)ê°€ ë‚˜ì™”ì„ê¹Œ? ë§ˆì°¬ê°€ì§€ë¡œ íŒŒë‘ìƒ‰ ë¶„í¬ê°€ ì£¼ì–´ì§„ ë¶„í¬ì¤‘ì—ì„œëŠ” ê°€ì¥ ê°€ëŠ¥ì„±ì´ ì»¤ë³´ì´ëŠ”ë° ì™œëƒí•˜ë©´ ë§ˆì°¬ê°€ì§€ë¡œ datasetì— ì†í•œ 2ê°œì˜ ê°ê°ì˜ datapointë¥¼ ì–»ì„ í™•ë¥ ì´ ê°€ì¥ ì»¤ë³´ì´ê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ìœ„ì™€ ë™ì¼í•˜ê²Œ 3ê°œì˜ ì •ê·œë¶„í¬ì¤‘ì—ì„œëŠ” \\(\\mu =63\\)ì¸ ê²½ìš°ê°€ \\(D\\)ì— ê°€ì¥ ì í•©í•˜ë‹¤ê³  í•  ìˆ˜ ìˆìœ¼ë©° ì´ë²ˆì—ëŠ” ì´ˆë¡ìƒ‰,ë³´ë¼ìƒ‰ ìˆœì„œë¡œ ëª¨ìˆ˜ê°€ ì í•©í•˜ë‹¤.\nì´ë²ˆì—ëŠ” í¬ê¸°ê°€ \\(N\\)ì¸ \\(D\\)ê°€ ì£¼ì–´ì ¸ìˆë‹¤ê³  ê°€ì •í•´ë³´ì.\n\n\n\n\n\n\nê·¸ë˜í”„ëŠ” í¬ê¸°ê°€ 30ì¸data setì´ì§€ë§Œ Nì´ë¼ê³  ë³´ì..!\n\nì´ë²ˆì—ë„ ì–´ë–¤ í™•ë¥ ë¶„í¬ì—ì„œ \\(D\\)ê°€ ë‚˜ì™”ì„ì§€ í•œë²ˆ êµ¬ë¶„í•´ë³´ì. ê·¸ëŸ¬ë‚˜ ì´ì „ì—ëŠ” \\(D\\)ì˜ í¬ê¸°ê°€ 1ë˜ëŠ” 2ì—¬ì„œ ì§ê´€ì ìœ¼ë¡œ ë³´ì˜€ì§€ë§Œ ì´ë²ˆì—ëŠ” ë°ì´í„°ê°€ ë„ˆë¬´ ë§ê¸° ë•Œë¬¸ì— ë°”ë¡œ ë³´ì´ì§€ ì•ŠëŠ”ë‹¤.\nê·¸ëŸ¬ë‚˜ ì´ì „ì— í–ˆë˜ ê³¼ì •ë“¤ ì¤‘ì—ì„œ \\(D\\)ì— ìˆëŠ” ê°ê°ì˜ datapointë¥¼ ì·¨í•  í™•ë¥ ì„ ëª¨ë‘ ê³ ë ¤í•œë’¤ì— ê·¸ ê°’ì´ ê°€ì¥ í° parameterë¥¼ ì£¼ì–´ì§„ \\(D\\)ì— ê°€ì¥ ì í•©í•œ parameter ë¡œ ê²°ì •í–ˆì—ˆëŠ”ë° ì´ ì‚¬ì‹¤ì€ ê·¸ëŒ€ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ì™œëƒí•˜ë©´ ì‚¬ì‹¤ ê°ê°ì˜ datapointë¥¼ ëª¨ë‘ ì·¨í•  í™•ë¥ ì€ ê²°í•©í™•ë¥ ì´ê¸° ë•Œë¬¸ì´ë‹¤. ë˜í•œ ê°ê°ì˜ datapointëŠ” ëª¨ë‘ ë…ë¦½ì ì´ë©° ë™ì¼í•œ ë¶„í¬ë¡œë¶€í„° ë½‘íŒ ê°’ì´ê¸° ë•Œë¬¸ì— ê²°í•©í™•ë¥ ì€ ê° datapoinë¥¼ ëª¨ë‘ ë™ì‹œì— ì·¨í•  í™•ë¥ ì´ë‹¤.\n\\[\\begin{align}\np(D|\\theta) &= p_{X_1,X_2,\\dots,X_N}(x_1,x_2,\\dots,x_N;\\theta) \\\\\n&= p_{X_1}(x_1|\\theta)\\cdot p_{X_2}(x_2|\\theta)\\dots  \\cdot p_{X_N}(x_N|\\theta) \\\\\n&= \\prod_{i=1}^{N}p_{X_i}(x_i;\\theta)\n\\end{align}\\]\n\n\\(\\theta\\)ëŠ” parameterë¥¼ ì˜ë¯¸í•œë‹¤. ì •ê·œë¶„í¬ì˜ parameterì´ë¯€ë¡œ \\(\\mu,\\sigma^2\\)ë¼ ìƒê°í•˜ë©´ ëœë‹¤.\nê²°í•©í™•ë¥ ì€ likelyhoodë¼ê³ ë„ í•œë‹¤.\n\nê·¸ëŸ¬ë¯€ë¡œ 3ê°œì˜ ì„œë¡œë‹¤ë¥¸ \\(\\theta\\)ì— ëŒ€í•œ í™•ë¥ ë¶„í¬ì— ëŒ€í•˜ì—¬ ëª¨ë‘ likelyhoodë¥¼ êµ¬í•œë’¤ ê·¸ ì¤‘ ê°€ì¥ í° ê°’ì„ parameterë¡œ í•˜ë©´ ëœë‹¤. ê·¸ëŸ°ë° ì‚¬ì‹¤ ìš°ë¦¬ì˜ ë¹„êµ ëŒ€ìƒì€ ë‹¨ 3ê°œì˜ \\(\\theta\\)ê°€ ì•„ë‹ˆë‹¤. ê°€ëŠ¥í•œ ëª¨ë“  í™•ë¥ ë¶„í¬ ì¦‰ ê°€ëŠ¥í•œ ëª¨ë“  \\(\\theta\\)ì— ëŒ€í•˜ì—¬ likelyhoodì˜ ê°€ì¥ í° ê°’ì„ êµ¬í•´ì•¼ í•œë‹¤. ê·¸ëŸ¬ë¯€ë¡œ \\(\\theta\\)ë¥¼ ë³€ìˆ˜ë¡œ ìƒê°í•˜ì—¬ likelyhoodë¥¼ ê°€ì¥í¬ê²Œ í•˜ëŠ” \\(\\theta\\)ë¥¼ ì°¾ëŠ”ë‹¤. ë”°ë¼ì„œ maximum likelyhood estimationì´ë©° ì´ë¥¼ í†µí•˜ì—¬ ë‚˜ì˜¨ parameter \\(\\theta\\)ì— ëŒ€í•œ ì¶”ì •ëŸ‰ì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MLE} &= \\underset{\\theta}{\\text{argmax}}\\,p_{X_1,X_2,\\dots,X_n}(x_1,x_2 \\dots x_n|\\theta) \\\\\n&=  \\underset{\\theta}{\\text{argmax}}\\,p_{X_1,X_2,\\dots,X_n}(x_1,x_2,\\dots,x_n|\\theta) \\\\\n&= \\underset{\\theta}{\\text{argmax}}\\,\\prod_{i=1}^{N}p_{X_i}(x_i;\\theta)\n\\end{aligned}\\]\nì •ë¦¬í•˜ìë©´ ê²°êµ­ MLEëŠ” íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ìœ¼ë¡œ likelyhood(\\(D\\)ì— ìˆëŠ” ê°ê°ì˜ datapointê°€ ëª¨ë‘ ë‚˜ì˜¬ í™•ë¥ )ë¥¼ ìµœëŒ€í™”í•˜ëŠ” íŒŒë¼ë¯¸í„°\\(\\theta\\)ë¥¼ ì¶”ì •ëŸ‰ìœ¼ë¡œ í•œë‹¤ëŠ” ê²ƒì´ë‹¤.\nì´ì œ ê¸°ì¡´ë¬¸ì œë¡œ ëŒì•„ê°€ë³´ì. ìš°ë¦¬ì—ê²Œ ì£¼ì–´ì§„ data set \\(D\\)ëŠ” ë‹¤ìŒê³¼ ê°™ì•˜ìœ¼ë©° ì •ê·œë¶„í¬ë¡œ ëª¸ë¬´ê²ŒëŠ” \\(t\\)ë¡œ ê°€ì •í–ˆì—ˆë‹¤.\n\\[\\begin{aligned}\n&D = \\{(x_1,t_1),(x_2,t_2,),\\dots,(x_N,t_N)\\} \\\\\n&t\\sim\\mathcal{N}(y(x|\\theta),\\sigma^2) \\\\\n&\\longleftrightarrow p(t|x,\\theta,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(\\frac{-(t-y(x|\\theta))^2}{2\\sigma^2})\n\\end{aligned}\\]\n\\(D\\)ì— ìˆëŠ” ê°ê°ì˜ datapointë¥¼ ëª¨ë‘ ë‚˜ì˜¬ í™•ë¥ ì€ í‚¤ê°€ \\(x_1\\)ì¼ ë•Œ ì‹¤ì œ ëª¸ë¬´ê²Œê°€ \\(t_1\\)ì´ê³ , í‚¤ê°€ \\(x_2\\)ì¼ ë•Œ ì‹¤ì œ ëª¸ë¬´ê²Œê°€ \\(t_2\\)ì´ê³ , \\(x_N\\)ì¼ ë•Œ ì‹¤ì œ ëª¸ë¬´ê²Œê°€ \\(t_N\\)ì¼ í™•ë¥ ê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\np(D|\\theta) &= p(t_1,t_2,\\dots,t_N|X,\\theta,\\sigma^2) \\\\\n&= \\prod_{n=1}^Np(t_i|x_i,\\theta,\\sigma^2) \\\\\n&= \\prod_{n=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(\\frac{-(t_i-y(x_i|\\theta))^2}{2\\sigma^2})\n\\end{aligned}\\]\nMLEëŠ” likelyhoodë¥¼ ìµœëŒ€í™” í•˜ëŠ” íŒŒë¼ë¯¸í„°ë¥¼ êµ¬í•˜ëŠ” ë°©ë²•ì´ë¯€ë¡œ \\(\\theta\\)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MLE} = \\underset{w}{\\text{argmax}}&\\prod_{n=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(\\frac{-(t_i-y(x_i|\\theta))^2}{2\\sigma^2})\n\\end{aligned}\\]\nìœ„ì˜ ì‹ìœ¼ë¡œë¶€í„° ì§ì ‘ ìµœëŒ€í™” í•˜ëŠ” \\(\\theta\\)ë¥¼ ì°¾ê¸°ëŠ” ì–´ë ¤ìš°ë¯€ë¡œ likelyhoodì— \\(-\\text{ln}\\)ë¥¼ ê³±í•´ì¤€ NLLì„ ì‚¬ìš©í•œë‹¤. \\(\\text{ln}\\)í•¨ìˆ˜ëŠ” ê³±í•´ë„ ìµœëŒ“ê°’(ë˜ëŠ” ìµœì†Ÿê°’)ì˜ ìœ„ì¹˜ê°€ ë³€í•˜ì§€ ì•Šìœ¼ë©° ê³±ì…ˆì—°ì‚°ì„ ë§ì…ˆì—°ì‚°ìœ¼ë¡œ ë°”ê¿” ê³„ì‚°ì„ ê°„ë‹¨í•˜ê²Œ í•´ì¤€ë‹¤. ë˜í•œ -ë¥¼ ê³±í•´ì¤Œìœ¼ë¡œì„œ ìµœì†Œì§€ì ì„ ì°¾ëŠ” ë¬¸ì œë¡œ ë°”ë€Œê²Œ ëœë‹¤.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MLE} &= \\underset{w}{\\text{argmin}}-\\text{ln}\\prod_{n=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(\\frac{-(t_i-y(x_i|\\theta))^2}{2\\sigma^2})\\\\\n&= \\underset{w}{\\text{argmin}}-\\sum_{n=1}^N\\text{ln}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(\\frac{-(t_i-y(x_i|\\theta))^2}{2\\sigma^2}) \\\\\n&= \\underset{w}{\\text{argmin}}-\\sum_{n=1}^{N}\\bigg[\\text{ln}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} + \\text{ln}\\,\\text{exp}(\\frac{-(t_i-y(x_i|\\theta))^2}\n{2\\sigma^2})\\bigg]\\\\\n&= \\underset{w}{\\text{argmin}}-\\sum_{n=1}^{N}\\bigg[\\text{ln}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} - \\frac{(t_i-y(x_i|\\theta))^2}{2\\sigma^2}\\bigg]\n\\end{aligned}\\]\nì—¬ê¸°ì„œ \\(\\theta\\)ì˜ ê´€ì ì—ì„œ ìƒìˆ˜(\\(\\pi,\\sigma^2\\))ë¥¼ ì œê±°í•˜ê³  ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MLE} = \\underset{w}{\\text{argmin}}\\sum_{n=1}^{N}(t_i-y(x_i|\\theta))^2\n\\end{aligned}\\]\nì–´ë””ì„ ê°€ ë§ì´ ë³¸ ì‹ì¸ë° ë°”ë¡œ L2 Lossì´ë‹¤. ì´ë ‡ê²Œ datapointê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ê³  MLEë¥¼ í†µí•´ L2 Lossë¥¼ ë‚˜ì˜´ì„ ë°í˜ìœ¼ë¡œì¨ íšŒê·€ë¬¸ì œì—ì„œ L2 Lossë¥¼ ì™œ ì‚¬ìš©í•˜ëŠ”ì§€ ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆê³  ì—­ìœ¼ë¡œ íšŒê·€ë¬¸ì œì—ì„œ L2 Lossë¥¼ ì‚¬ìš©í•œë‹¤ë©´ datapointê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ê³  MLEë¥¼ í†µí•´ì„œ íŒŒë¼ë¯¸í„°ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ê² êµ¬ë‚˜ í•˜ê³  ì•Œ ìˆ˜ ìˆë‹¤.\nì°¸ê³ ì ìœ¼ë¡œ classificationì—ì„œëŠ” datapointê°€ ë² ë¥´ëˆ„ì´ë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•œë‹¤ë©´ binary cross entropy lossë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤ê³  í•œë‹¤.\nê°„ë‹¨ì •ë¦¬ - Datasetì´ ì–´ë– í•œ í™•ë¥ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ”ì§€ ê²°ì •í•˜ê¸° ìœ„í•´ì„œëŠ” parameterë¥¼ êµ¬í•´ì•¼ í•œë‹¤. - MLEì€ Likelyhoodë¥¼ ê°€ì¥ í¬ê²Œ ë§Œë“œëŠ” parameterë¥¼ ì°¾ê³  ê·¸ ê°’ì„ parameterì— ëŒ€í•œ ì¶”ì •ëŸ‰ìœ¼ë¡œ í•œë‹¤."
  },
  {
    "objectID": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#maximum-a-posterior",
    "href": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#maximum-a-posterior",
    "title": "MLE & MAP (ì‘ì„±ì¤‘)",
    "section": "Maximum a Posterior",
    "text": "Maximum a Posterior\nMLEëŠ” likelyhoodì¸ \\(p(D|\\theta)\\)ë¥¼ maximizeí•˜ëŠ” \\(\\theta\\)ë¥¼ êµ¬í•˜ì˜€ë‹¤. MAPëŠ” ë°˜ëŒ€ë¡œ posteriorì¸ \\(p(\\theta|D)\\)ë¥¼ maximizeí•˜ëŠ” \\(\\theta\\)ë¥¼ êµ¬í•œë‹¤. í•œë§ˆë””ë¡œ ì„¤ëª…í•˜ìë©´ ì£¼ì–´ì§„ ë°ì´í„°ì…‹ \\(D\\)ì— í™•ë¥ ì´ ê°€ì¥ í° \\(\\theta\\)ë¥¼ ì„ íƒí•˜ëŠ” ë°©ì‹ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.\nMLEëŒ€ì‹  MAPë¥¼ í†µí•´ íŒŒë¼ë¯¸í„°ë¥¼ êµ¬í•¨ìœ¼ë¡œì¨ ì–»ì„ ìˆ˜ ìˆëŠ” ì¥ì ì€ ë­˜ê¹Œ? ë¨¼ì € posteriorë¥¼ bayes ruleì— ì˜í•´ì„œ ì ì–´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{p(D)} \\propto p(\\theta|D)p(\\theta)\\\\\n&\\text{where }  \\\\\n&\n\\begin{aligned}\n&p(\\theta|D) : \\text{posterior}\\\\\n&p(D|\\theta) : \\text{likelyhood}\\\\\n&p(\\theta)   : \\text{prior} \\\\\n&p(D) : \\text{normalization constant}\n&\\end{aligned}\n\\end{aligned}\\]\nì£¼ëª©í•  ì ì€ posteriorì—ëŠ” ìš°ë¦¬ê°€ ì •í•  ìˆ˜ ìˆëŠ” priorê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ë”°ë¼ì„œ priorë¥¼ í†µí•˜ì—¬ ê°€ì§€ê³  ìˆëŠ” ì‚¬ì „ì§€ì‹ì„ posteriorì— ì¶©ë¶„íˆ ë°˜ì˜í•  ìˆ˜ ìˆê³  ì‚¬ì „ì§€ì‹ì´ ì—†ë‹¤ê³  í• ì§€ë¼ë„ priorë¥¼ í†µí•´ overfittingì„ ë°©ì§€í•  ìˆ˜ ìˆë‹¤. overfittingì€ \\(|\\theta|\\)ê°€ ì»¤ì„œ ì‹ ê²½ë§ì˜ í‘œí˜„ë ¥ì´ ê³¼í•  ê²½ìš° ì¼ì–´ë‚˜ëŠ” í˜„ìƒì´ë¯€ë¡œ priorì— í‰ê· ì´ 0ì¸ ë¶„í¬ë¥¼ ê°€ì •í•´ì¤€ë‹¤ë©´ ìš°ë¦¬ëŠ” posteriorë¥¼ í†µí•´ì„œ ë¹„êµì  ì‘ì€ í¬ê¸°ë¥¼ ê°–ëŠ” \\(\\theta\\)ë¥¼ ì–»ì„ ìˆ˜ ìˆë‹¤.\nì—¬ê¸°ì„œëŠ” \\(\\theta\\)ê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ê³  overfittingì„ ë§‰ì•„ì£¼ëŠ” L2 Regularization(weight decay)ë¥¼ ìœ ë„í•´ë³´ì.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MAP} &= \\underset{\\theta}{\\text{argmax}}\\,P(\\theta|D) \\\\\n&= \\underset{\\theta}{\\text{argmax}}\\,p(\\theta|D)p(\\theta)\\\\\n&= \\underset{\\theta}{\\text{argmax}}\\,\\text{ln}\\,p(\\theta|D)p(\\theta)\\\\\n&= \\underset{\\theta}{\\text{argmax}}\\,\\text{ln}\\,p(\\theta|D) + \\text{ln}\\,p(\\theta)\\\\\n&= \\underset{\\theta}{\\text{argmin}}\\, -\\text{ln}\\,p(\\theta|D) - \\text{ln}\\,p(\\theta)\n\\end{aligned}\\]\nì—¬ê¸°ì„œ \\(-\\text{ln}p(\\theta|D)\\)ëŠ” MLEì—ì„œì˜ negative log likelyhoodì™€ ê°™ìœ¼ë©° \\(\\text{ln}p(\\theta)\\)ëŠ” ìš°ë¦¬ê°€ ì •í•˜ëŠ” priorì— ì˜í•œ ê°’ì´ë‹¤. \\(\\theta\\)ì— ëŒ€í•œ priorê°€ ì •ê·œë¶„í¬ë¥¼ ë”°ë¥¸ë‹¤ê³  ê°€ì •í•˜ê³  êµ¬í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n&\\theta \\sim \\mathcal{N}(\\theta|0,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(-\\frac{\\theta^2}{2\\sigma^2_\\theta})\\\\\n&-\\text{ln}p(\\theta) = -\\text{ln}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} + \\frac{\\theta^2}{2\\sigma^2_{\\theta}}\n\\end{aligned}\\]\nìƒìˆ˜ê°’ì„ ì œê±°í•˜ê³  ëŒ€ì…í•˜ë©´ MAPë¥¼ í†µí•´ êµ¬í•œ parameterëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MAP} &= \\underset{\\theta}{\\text{argmin}}\\sum_{i=1}^{N}(t - y(x_i|w))^2 + \\frac{w^2}{2\\sigma_{\\theta}^2}\\\\\n&= \\underset{\\theta}{\\text{argmin}}\\sum_{i=1}^{N}(t - y(x_i|w))^2 + \\alpha w^2 \\quad(\\alpha = \\frac{1}{2\\sigma^2})\n\n\\end{aligned}\\]\nì´ ë˜í•œ ì–´ë””ì„ ê°€ ë§ì´ ë³¸ ì‹ì´ë©° ë°”ë¡œ L2 Lossì— L2 Regularizationì„ ì¶”ê°€í•œ ê²ƒì´ë‹¤. ì´ì™€ ê°™ì´ parameterì˜ priorê°€ ì •ê·œë¶„í¬ë¼ê³  ê°€ì •í•˜ê³  MAPë¥¼ í†µí•´ loss functionì„ êµ¬í•œë‹¤ë©´ ìš°ë¦¬ê°€ ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” ëª¨ë¸ì˜ loss functionì—ì„œ regularization termì´ ì™œ ì €ë ‡ê²Œ ë‚˜ì˜¤ëŠ”ê°€ë¥¼ ì„¤ëª…í•  ìˆ˜ ìˆë‹¤. ë˜í•œ ì—­ìœ¼ë¡œ L2 Regularization termì´ í¬í•¨ëœ loss functionì„ ì‚¬ìš©í•œë‹¤ë©´ paraemterì— ëŒ€í•œ priorë¥¼ ì •ê·œë¶„í¬ì„ë¡œ ê°€ì •í–ˆìŒì„ ì•Œ ìˆ˜ ìˆë‹¤."
  },
  {
    "objectID": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#ë² ì´ì¦ˆ-ì •ë¦¬ì—ì„œ-ë°”ë¼ë³¸-mle",
    "href": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#ë² ì´ì¦ˆ-ì •ë¦¬ì—ì„œ-ë°”ë¼ë³¸-mle",
    "title": "MLE & MAP (ì‘ì„±ì¤‘)",
    "section": "ë² ì´ì¦ˆ ì •ë¦¬ì—ì„œ ë°”ë¼ë³¸ MLE",
    "text": "ë² ì´ì¦ˆ ì •ë¦¬ì—ì„œ ë°”ë¼ë³¸ MLE\n\\(D=(t_1,t_2,\\dots,t_n)\\)ëŠ” ìƒ˜í”Œ(ë°ì´í„°ì…‹), \\(\\theta\\)ëŠ” ìš°ë¦¬ê°€ ì•Œê³ ì‹¶ì€ í™•ë¥ ë¶„í¬ì˜ ëª¨ìˆ˜ë¼ê³  í•©ì‹œë‹¤. ë² ì´ì¦ˆì •ë¦¬ëŠ” ì¦ê±° ë˜ëŠ” ì¡°ê±´ì´ ì£¼ì–´ì§€ê¸° ì „ì˜ ì‚¬ì „í™•ë¥  \\(p(D)\\)ì™€ ì£¼ì–´ì§„ í›„ì˜ ì‚¬í›„í™•ë¥  \\(p(\\theta|D)\\)ì‚¬ì´ì˜ ê´€ê³„ë¥¼ ì•Œë ¤ì¤ë‹ˆë‹¤.\nì—¬ê¸°ì„œ ì‚¬í›„í™•ë¥ ì€ \\(\\theta\\)ì— ëŒ€í•œ í™•ë¥ ë¶„í¬ë¡œ ë°ì´í„°(ìƒ˜í”Œ)ì´ ì£¼ì–´ì§ˆë•Œ í™•ë¥ ë¶„í¬ì˜ ì„ì˜ì˜ ëª¨ìˆ˜\\(\\theta\\)ê°€ ì–¼ë§ˆë‚˜ ê°€ëŠ¥í•œì§€ ë˜ëŠ” ë¶ˆí™•ì‹¤í•œì§€ ê·¸ ì •ë„ë¥¼ ì•Œë ¤ì£¼ëŠ” í™•ë¥ ì„ í•¨ìˆ«ê°’ìœ¼ë¡œ ê°€ì§€ëŠ” í™•ë¥ í•¨ìˆ˜ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ì‚¬í›„í™•ë¥ ì„ ìµœëŒ€ë¡œ í•˜ëŠ” ëª¨ìˆ˜\\(\\theta\\)ê°€ ë°ì´í„°ì…‹ì´ ì£¼ì–´ì ¸ìˆì„ ë•Œ ê°€ì¥ í™•ë¥ ì´ ë†’ì€,ê°€ëŠ¥ì„±ì´ ë†’ì€ ëª¨ìˆ˜ì´ë¯€ë¡œ ê·¸ë•Œì˜ ê°’ì„ í™•ë¥ ë¶„í¬ì˜ ëª¨ìˆ˜ë¡œ ì¶”ì •í•˜ë©´ ë©ë‹ˆë‹¤.\në¬¸ì œëŠ” ì™¼ìª½ì˜ í™•ë¥ ë¶„í¬ëŠ” ë°”ë¡œ ì•Œê¸°ê°€ ì‰½ì§€ ì•Šë‹¤ëŠ” ì ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ë² ì´ì¦ˆì •ë¦¬ë¥¼ í†µí•˜ì—¬ ìš°ë³€ì˜ ì‹ì„ ìµœëŒ€í™” í•˜ëŠ” ê°’ì„ êµ¬í•©ë‹ˆë‹¤. ìš°ë³€ì˜ ì‹ì—ì„œ ë¶„ëª¨ëŠ” ì£¼ì–´ì§„ ë°ì´í„°ì— ì˜í•˜ì—¬ ê³ ì •ëœ ìƒìˆ˜(normalization constantë¼ê³  í•©ë‹ˆë‹¤)ì…ë‹ˆë‹¤. ê·¸ëŸ¬ë¯€ë¡œ ìµœëŒ“ê°’ì„ êµ¬í•˜ëŠ”ë° ì˜í–¥ì„ ì£¼ì§€ ì•ŠìŠµë‹ˆë‹¤. ìš°ë¦¬ëŠ” ë¶„ìì— ìˆëŠ” \\(p(D|\\theta)p(\\theta)\\)ë¥¼ ìµœëŒ€í™”í•˜ëŠ” \\(\\theta\\)ë¥¼ ì°¾ìœ¼ë©´ ë©ë‹ˆë‹¤.\nì—¬ê¸°ì„œ \\(p(D|\\theta)\\)ë¥¼ ê°€ëŠ¥ë„(likelyhood)ë¼ í•©ë‹ˆë‹¤. MLEì—ì„œëŠ” ë¶„ìì—ì„œ ê°€ëŠ¥ë„ë§Œ ìµœëŒ€ë¡œ í•˜ëŠ” \\(\\theta\\)ë¥¼ êµ¬í•©ë‹ˆë‹¤. MAPë¼ëŠ” ë‹¤ë¥¸ ë°©ë²•ì€ ë¶„ìì— ìˆëŠ” \\(p(D|\\theta)p(\\theta)\\)ë¥¼ ìµœëŒ€í™” í•˜ëŠ” \\(\\theta\\)ë¥¼ êµ¬í•œë‹¤ê³  í•©ë‹ˆë‹¤.\nê°€ëŠ¥ë„í•¨ìˆ˜ë¥¼ ìµœëŒ€ë¡œ í•˜ëŠ” ëª¨ìˆ«ê°’ì€ 175.75ì…ë‹ˆë‹¤. ë”°ë¼ì„œ MLEì— ì˜í•œ ëª¨ìˆ˜ì— ëŒ€í•œ ì¶”ì •ê°’ì€ 175.75ì…ë‹ˆë‹¤. \\[\\hat{\\theta}_{MLE} = 175.75\\]\në§í¬1(random sample vs random variable) ë§í¬2(ì¶”ì •,ì¶”ì •ëŸ‰,ì¶”ì •ê°’) ë§í¬3(mle) ë§í¬4(mle)"
  },
  {
    "objectID": "posts/Probability Statistics/notation.html",
    "href": "posts/Probability Statistics/notation.html",
    "title": "í™•ë¥ ë¶„í¬ì—ì„œ ;ì™€|ì˜ ì‚¬ìš©",
    "section": "",
    "text": "í™•ë¥ ë¶„í¬ì—ì„œ ;ì™€|ì— ê´€í•œ notation ì •ë¦¬\n\n;ì˜ ì‚¬ìš©\n; í•¨ìˆ˜ì—ì„œ íŠ¹ì • ë³€ìˆ˜ë¥¼ ê³ ì •ì‹œì¼œë†“ì„ë•Œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. ë‹¤ìŒê³¼ ê°™ì€ ì´ë³€ìˆ˜í•¨ìˆ˜ë¥¼ ìƒê°í•´ë³´ì \\[f(x,y)\\] ë…ë¦½ë³€ìˆ˜ xì™€ yë¥¼ ê°–ëŠ” í•¨ìˆ˜ì´ë©° ë‘ ë³€ìˆ˜ ëª¨ë‘ì— ëŒ€í•´ì„œ ë¯¸ë¶„ê°€ëŠ¥í•˜ë‹¤. ì—¬ê¸°ì„œ ë³€ìˆ˜ yê°€ ì–´ë–¤ ì–´ë–¤ ê°’ìœ¼ë¡œ ê³ ì •ëœê²ƒì´ ì•Œë ¤ì§€ê±°ë‚˜ ë˜ëŠ” ê³ ì •ëœìƒí™©ì„ ê°€ì •í•œ í›„ xì— ëŒ€í•´ì„œë§Œ ê´€ì‹¬ì´ ìˆë‹¤ê³  í•´ë³´ì(ë¯¸ë¶„,ê·¹í•œë“±ì„ ì•„ë§ˆë„?ì·¨í•˜ê³  ì‹¶ë‹¤..ì•„ë§ˆë„â€¦) ì´ë•ŒëŠ” ì´ë³€ìˆ˜í•¨ìˆ˜ë¡œ í‘œê¸°í•˜ëŠ”ê²ƒì´ ì•„ë‹Œ ì¼ë³€ìˆ˜í•¨ìˆ˜ë¡œ í‘œê¸°í•´ì•¼í•œë‹¤. ê·¸ë•ŒëŠ” ë‹¤ìŒê³¼ ê°™ì´ í‘œê¸°í•˜ë©´ ëœë‹¤.\n\\[\nf(x;y)\n\\]\nì´ë ‡ê²Œ í‘œê¸°í•˜ë©´ yê°’ì€ ì´ì œ ì–´ë–¤ ê°’ìœ¼ë¡œ ì´ë¯¸ ì„¤ì •(setting)ë˜ê±°ë‚˜ ê³ ì •(fixed)ë˜ì–´ìˆìŒì„ ì˜ë¯¸í•œë‹¤. ë¹„ìŠ·í•˜ê²Œ ê·¸ëƒ¥ ê³ ì •ëœ íŒŒë¼ë¯¸í„°ê°€ ì´ ë‹¤ìŒì—ë„ ì˜¬ ìˆ˜ ìˆë‹¤. ë² ë¥´ëˆ„ì´ë¶„í¬ì˜ í™•ë¥ ì§ˆëŸ‰í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[f_X(x;p) = p^x(1-p)^{1-x}\\]\nsetting,fixedëœ íŒŒë¼ë¯¸í„°ì¸ pë¥¼ ;ë‹¤ìŒì— í‘œì‹œí–ˆë‹¤.\n\n\n|ì˜ ì‚¬ìš©\n|ëŠ” given that,ifì´ë¼ëŠ” ì˜ë¯¸ì´ë©° |ì˜ ë‹¤ìŒì—ëŠ” ë³´í†µ ë¨¼ì € ê¹”ê³ ê°€ê±°ë‚˜ ì£¼ì–´ì§€ëŠ” ì „ì œ,ìƒí™©,ì¡°ê±´,ì¦ê±° ë“±ì´ ë‚˜ì˜¨ë‹¤. í™•ë¥ ,í†µê³„ ë¶„ì•¼ì—ì„œë§Œ ì“°ì´ë©° ì¡°ê±´ë¶€ í™•ë¥ ì´ë‚˜,ë² ì´ì§€ì•ˆì—ì„œ ë§ì´ ì“°ì´ëŠ” ê¸°í˜¸ë¼ê³  í•œë‹¤. ì¤‘ìš”í•œ ì ì€ íŠ¹íˆ |ë‹¤ìŒì— ì˜¤ëŠ” ìƒí™©,ì¡°ê±´,ì¦ê±°ì™€ ê°™ì€ ê²ƒë“¤ì€ ì–´ë–¤ ê´€ì ì´ëƒì— ë”°ë¼ì„œ ìƒìˆ˜ë‚˜ í™•ë¥ ë³€ìˆ˜ì˜ ê´€ì ìœ¼ë¡œ ìƒê°ë  ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. íŠ¹íˆ ë² ì´ì§€ì•ˆì—ì„œëŠ” í™•ë¥ ë³€ìˆ˜ë¡œ ìƒê°í•œë‹¤.\nì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬(ì¡°ê±´ë¶€í™•ë¥ ë°€ë„í•¨ìˆ˜,ì¡°ê±´ë¶€ í™•ë¥ í•¨ìˆ˜)ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. \\[\np_{X|Y}(x|y) = \\frac{p_{XY}(x,y)}{p_Y(y)}\\\\\np_{X|Y}(y|x) = \\frac{p_{XY}(x,y)}{p_X(x)}\n\\]  ê°€ì¥ ìœ—ì‹ì„ í•´ì„í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. - ì¢Œë³€ : í™•ë¥ ë³€ìˆ˜ Yê°€ ì£¼ì–´ì§„(given)ìƒí™©ì—ì„œ í™•ë¥ ë³€ìˆ˜Xì˜ í™•ë¥ (ë˜ëŠ” í™•ë¥ ë°€ë„)ë¥¼ ë‚˜íƒ€ë‚´ëŠ” í•¨ìˆ˜ì´ë©° ì‹œí–‰ìœ¼ë¡œë¶€í„° í™•ë¥ ë³€ìˆ˜ Xê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ê°’ xë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ëŠ”ë‹¤. ì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬ì—ì„œ Yì˜ ê°’ì€ yë¡œ ì£¼ì–´ì ¸ ìˆë‹¤ê³  ê°€ì •í•˜ë¯€ë¡œ ì…ë ¥í•˜ëŠ” ë³€ìˆ˜ê°€ ì•„ë‹ˆë‹¤. ë”°ë¼ì„œ í™•ë¥ ë³€ìˆ˜ YëŠ” ë³€ìˆ˜ê°€ ì•„ë‹ˆë¼ ëª¨ìˆ˜(parameter)ì´ë‹¤. - ìš°ë³€ : ì¡°ê±´ë¶€í™•ë¥ ë¶„í¬ ê³µì‹.\n##  ê²°ë¡  ; vs |\n\\[p_{\\theta}(x) = p(x;\\theta) = p(x|\\theta)\\]  ;ëŠ” ìˆ˜í•™ì˜ ëª¨ë“  ë¶„ì•¼ì—ì„œ ì“°ì´ì§€ë§Œ |ëŠ” í™•ë¥ í†µê³„ë¶„ì•¼ì—ì„œë§Œ ì“°ì¸ë‹¤. ;ë‹¤ìŒì—ëŠ” ì´ë¯¸ ì„¤ì •ë˜ê±°ë‚˜ ê³ ì •ëœ ê°’ì´ ë‚˜ì˜¤ë©° |ë‹¤ìŒì—ëŠ” ë¨¼ì € ì£¼ì–´ì§€ëŠ” ì¡°ê±´ì´ ë‚˜ì˜¨ë‹¤. ê·¼ë° ;ë¥¼ ë§ì´ ì“°ì§€ ì•Šê³  |ë’¤ì— ê·¸ëƒ¥ ê³ ì •ëœ ê°’ì„ ì¨ë²„ë¦¬ëŠ” ê²½ìš°ê°€ ë§ë‹¤. ì´ëŠ” ì§€ì–‘í•´ì•¼ í•  í‘œí˜„ì´ë‹¤.\nì°¸ê³ ë§í¬ ë§í¬1(;ì˜ ì‚¬ìš©ë²•) ë§í¬2(ì¡°ê±´ë¶€í™•ë¥ ë¶„í¬) ë§í¬3(í‘œê¸°ë²•ì˜ í˜¼ìš©)"
  },
  {
    "objectID": "posts/Probability Statistics/sample space and random variable.html",
    "href": "posts/Probability Statistics/sample space and random variable.html",
    "title": "í™•ë¥ ë¡  ìš©ì–´ì •ë¦¬",
    "section": "",
    "text": "Experiment and trial\nê°€ëŠ¥í•œ ê²°ê³¼ë“¤ì´ ë¯¸ë¦¬ ì •í•´ì ¸ìˆê³  ë¬´í•œíˆ ë°˜ë³µê°€ëŠ¥í•œ ê³¼ì •ì„ í™•ë¥ ì‹¤í—˜(experiment) ë˜ëŠ” ì‹œí–‰(trial)ì´ë¼ê³  í•©ë‹ˆë‹¤.\n\n\nsample space & event\nì–´ë–¤ ì„ì˜ì˜ ì‹œí–‰ì„ ê°€ì •í•´ë³¸ë‹¤ê³  í•©ì‹œë‹¤. ì‹œí–‰ìœ¼ë¡œë¶€í„° ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë“  ê²°ê³¼ë“¤ì˜ ì§‘í•©ì„ ìš°ë¦¬ëŠ” í‘œë³¸ê³µê°„(sample space)ë¼ê³  í•©ë‹ˆë‹¤. ì‚¬ê±´ì€ í‘œë³¸ê³µê°„ì´ë¼ëŠ” ì§‘í•©ì˜ ë¶€ë¶„ì§‘í•©ì…ë‹ˆë‹¤. ì—¬ëŸ¬ê°œì˜ ì›ì†Œ(ê²°ê³¼)ë“¤ì´ ëª¨ì—¬ì„œ í•˜ë‚˜ì˜ ì‚¬ê±´ì´ ë  ìˆ˜ ìˆìœ¼ë©° ë‹¨ í•˜ë‚˜ì˜ ì›ì†Œë¡œë„ ì‚¬ê±´ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì‹œí–‰ì˜ ê²°ê³¼ê°€ ì–´ë– í•œ ì‚¬ê±´(ë¶€ë¶„ì§‘í•©)ì— ì†í•˜ëŠ” ê²½ìš° ìš°ë¦¬ëŠ” â€œ~ì¸ ì‚¬ê±´ì´ ë°œìƒí–ˆë‹¤!â€ë¼ê³  í‘œí˜„í•©ë‹ˆë‹¤.\n\n\nrandom variable\ní™•ë¥ ë³€ìˆ˜ëŠ” í™•ë¥ ì‹¤í—˜ ë˜ëŠ” ì‹œí–‰ìœ¼ë¡œë¶€í„° ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ê²°ê³¼ë¥¼ ëŒ€ì‹  ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ì…ë‹ˆë‹¤. ì‹œí–‰ì„ í•˜ê¸°ì „ê¹Œì§€ëŠ” ê·¸ ê°’ì´ ì •í•´ì§€ì§€ ì•Šê³  í™•ë¥ ë¶„í¬ë§Œ ì¡´ì¬í•˜ë©° ì‹œí–‰ì„ í•˜ë©´ í™•ë¥ ë¶„í¬ì— ì˜í•´ì„œ ê²°ê³¼ê°€ ì •í•´ì§€ê³  ì‹¤ìˆ˜ê°€ ë¶€ì—¬ë©ë‹ˆë‹¤. ìˆ˜í•™ì ìœ¼ë¡œëŠ” í‘œë³¸ê³µê°„ì˜ ì›ì†Œë¥¼ ì •ì˜ì—­ìœ¼ë¡œ í•˜ì—¬ ì‹¤ìˆ˜ë¥¼ ëŒ€ì‘ì‹œí‚¤ëŠ” â€œí•¨ìˆ˜â€ì…ë‹ˆë‹¤.\n\n\nprobability distribution(function)\ní™•ë¥ ë¶„í¬(í™•ë¥ í•¨ìˆ˜)ë€ í™•ë¥ ë³€ìˆ˜ê°€ ì·¨í•  ìˆ˜ ìˆëŠ” ì‹¤ìˆ˜ê°’ì— ê°ê°ì˜ ì‹¤ìˆ˜ë¥¼ ì·¨í•  ê°€ëŠ¥ì„±ì¸ í™•ë¥  ë˜ëŠ” í™•ë¥ ë°€ë„ë¥¼ ëŒ€ì‘ì‹œí‚¤ëŠ” í•¨ìˆ˜ì…ë‹ˆë‹¤.\n\n\nì˜ˆì‹œ\në™ì „ì„ ë‘ë²ˆ ë˜ì§€ëŠ” ì‹œí–‰ì„ 3ë²ˆ ë°˜ë³µí•˜ì—¬ í¬ê¸°ê°€ 3ì¸ í‘œë³¸ì„ ì–»ì—ˆë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤. ë™ì „ì˜ ì•ë©´ì„ H(head)ë¼ í•˜ê³  ë’·ë©´ì„ T(tail)ì´ë¼ê³  í•  ë•Œ í‘œë³¸ê³µê°„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[\\Omega = \\{HH,HT,TH,TT\\}\\]\ní‘œë³¸ê³µê°„ì— ìˆëŠ” ê²°ê³¼ë“¤ ì¤‘ì—ì„œ ë™ì „ì˜ ì•ë©´ì´ 1ê°œë¼ë„ ìˆëŠ” ê²½ìš° ì‚¬ê±´A ë™ì „ì˜ ì•ë©´ì´ í•˜ë‚˜ë„ ì—†ëŠ” ê²½ìš°ë¥¼ ì‚¬ê±´Bë¼ í•©ì‹œë‹¤. ì‚¬ê±´Aì™€ BëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\\[A = \\{HH,HT,TH\\},B=\\{TT\\}\\]\n\\(X_1,X_2,X_3\\)ëŠ” ê°ê° ì²«ë²ˆì§¸ ë‘ë²ˆì§¸ ì„¸ë²ˆì§¸ ì‹œí–‰ì˜ ê²°ê³¼ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ë³€ìˆ˜ì´ë©° í™•ë¥ ë¶„í¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤ê³  í•©ì‹œë‹¤.\n\n\n\n                                                \n\n\nìœ„ì˜ í™•ë¥ ë¶„í¬ë¥¼ ë°˜ì˜í•˜ì—¬ ì‹œí–‰ì˜ ê²°ê³¼ê°€ ê²°ì •ë©ë‹ˆë‹¤. ì‹œí–‰ìœ¼ë¡œë¶€í„° ì–»ì€ í‘œë³¸ì€ \\((1,1,0)\\)ì´ë©° \\(X_1 = 1 ,X_2 =1 ,X_3 =0\\) ì…ë‹ˆë‹¤. ì‹œí–‰ì˜ ê²°ê³¼ ì–»ì€ ì‹¤ì œë¡œ ê´€ì°°ëœ í‘œë³¸ì€ \\((x_1,x_2,x_3)\\) ì´ëŸ°ì‹ìœ¼ë¡œ ê°ê°ì˜ ì›ì†Œë¥¼ ì†Œë¬¸ìì¸ ë¯¸ì§€ìˆ˜ë¡œ í‘œí˜„í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\n\ni.i.d & randomsample & realization\nìœ„ì˜ ë™ì „ë˜ì§€ê¸° ì‹¤í—˜ì—ì„œ ê°ê°ì˜ í™•ë¥ ë³€ìˆ˜ëŠ” ì´ì „ì— ë˜ì§„ ê²°ê³¼ê°€ ì´í›„ì— ë˜ì§€ëŠ” ê²°ê³¼ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê³ (ì¦‰,í™•ë¥ ë¶„í¬ì— ì˜í–¥ì„ ë¯¸ì¹˜ì§€ ì•Šê³ ) ë™ì¼í•œ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë¶„í¬ì˜€ìŠµë‹ˆë‹¤. ì´ì™€ ì—¬ëŸ¬ê°œì˜ í™•ë¥ ë³€ìˆ˜ê°€ ì„œë¡œê°„ì— ë…ë¦½ì´ë©° ë™ì¼í•œ ë¶„í¬ë¥¼ ë”°ë¥´ëŠ” í™•ë¥ ë³€ìˆ˜ë“¤ì„ Independent and identically distributed random variablesë¼ê³  í•©ë‹ˆë‹¤. ìœ„ì˜ ë¶„í¬ëŠ” ë² ë¥´ëˆ„ì´ ë¶„í¬ë¥¼ ë”°ë¥´ë¯€ë¡œ ë‹¤ìŒê³¼ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\\[X_1,X_2,X_3 \\overset{i.i.d}{\\sim} \\text{Bernoulli(p = 0.7)}\\]\nrandomsampleì€ i.i.dì¸ ì—¬ëŸ¬ê°œì˜ í™•ë¥ ë³€ìˆ˜ì˜ ëª¨ìŒì…ë‹ˆë‹¤. \\(X1,X2,X3\\)ë¥¼ ë§í•©ë‹ˆë‹¤.\nrealizationì€ ê´€ì°°ëœ ê²°ê³¼ ê°ê°ì„ ë§í•©ë‹ˆë‹¤. \\(x1\\)ë„ realization \\(x2\\)ë„ realization \\(x3\\)ë„? ëª¨ë‘ realizationì…ë‹ˆë‹¤. \\(x1,x2,x3\\)ë¥¼ ëª¨ì•„ì„œ í™•ë¥ ë³€ìˆ˜ \\(X_1,X_2,X_3\\)ì˜ realizationsì´ë¼ê³  í•©ë‹ˆë‹¤.\n\n\nì°¸ê³ ìë£Œ\nwikipedia - Experiment (probability theory) StackExchange - What is the difference between random variable and random sample? wikipedia - i.i.d ì •ë³´í†µì‹ ê¸°ìˆ ìš©ì–´í•´ì„¤"
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html",
    "title": "íŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹",
    "section": "",
    "text": "íŒŒì´ì¬ ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹\nì „ë¶ëŒ€í•™êµ ìµœê·œë¹ˆ êµìˆ˜ë‹˜ì˜ ë”¥ëŸ¬ë‹ deepcopy-shallowcopy íŠ¹ê°• ì¤‘,ë³€ìˆ˜ì™€ í• ë‹¹ë¬¸ ë¶€ë¶„ì„ ì¬êµ¬ì„±í•œ ê¸€ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#íŒŒì´ì¬ì—ì„œì˜-ë³€ìˆ˜",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#íŒŒì´ì¬ì—ì„œì˜-ë³€ìˆ˜",
    "title": "íŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹",
    "section": "íŒŒì´ì¬ì—ì„œì˜ ë³€ìˆ˜",
    "text": "íŒŒì´ì¬ì—ì„œì˜ ë³€ìˆ˜\n\níŒŒì´ì¬ì˜ ë³€ìˆ˜ëŠ” ë©”ëª¨ë¦¬ìƒì— ì €ì¥ëœ ê°ì²´ë¥¼ ì°¸ì¡°(reference)í•©ë‹ˆë‹¤.\në”°ë¼ì„œ ë³€ìˆ˜ëŠ” ìë°”ì—ì„œ ì°¸ì¡°ë³€ìˆ˜ì™€ ê°™ìŠµë‹ˆë‹¤.\në³€ìˆ˜ëŠ” ê°ì²´(object)ë¥¼ ë¶€ë¥´ëŠ” ë³„ì¹­(ë‹¤ë¥¸ì´ë¦„),ê°ì²´ì— ë¶™ì—¬ì§„ í¬ìŠ¤íŠ¸ì‡,ë˜ë‹¤ë¥¸ ë ˆì´ë¸” ì´ë¼ê³  ìƒê°í•˜ëŠ” ê²ƒì´ ë¹„ìœ ì ìœ¼ë¡œ ë§ìŠµë‹ˆë‹¤.\nì•ìœ¼ë¡œ ê°ì²´ì— ì ‘ê·¼í•˜ê¸° ìœ„í•´ì„œëŠ” aë¼ëŠ” ë³€ìˆ˜(ë³„ì¹­,ê°ì²´)ë¥¼ ì°¾ìœ¼ë©´ ë©ë‹ˆë‹¤.\në§ˆì¹˜ Cì–¸ì–´ì˜ í¬ì¸í„°ì™€ ìœ ì‚¬í•˜ê²Œ ë™ì‘í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#íŒŒì´ì¬ì—ì„œì˜-í• ë‹¹ë¬¸",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#íŒŒì´ì¬ì—ì„œì˜-í• ë‹¹ë¬¸",
    "title": "íŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹",
    "section": "íŒŒì´ì¬ì—ì„œì˜ í• ë‹¹ë¬¸(=)",
    "text": "íŒŒì´ì¬ì—ì„œì˜ í• ë‹¹ë¬¸(=)\n\n=ì€ íŒŒì´ì¬(ë¿ë§Œì•„ë‹ˆë¼ ëŒ€ë¶€ë¶„ì˜ í”„ë¡œê·¸ë˜ë°ì–¸ì–´)ì—ì„œ í• ë‹¹ë¬¸ì…ë‹ˆë‹¤.\ní• ë‹¹ë¬¸ì„ ì‹¤í–‰í•˜ë©´ = ì˜¤ë¥¸ìª½ì— ìˆëŠ” ê°ì²´ë¥¼ ë¨¼ì € ë©”ëª¨ë¦¬ ìƒì— ìƒì„±í•˜ê±°ë‚˜ ê°€ì ¸ì˜µë‹ˆë‹¤.\n=ì˜ ì™¼ìª½ì—ëŠ” ë³€ìˆ˜ê°€ ì¡´ì¬í•˜ë©° íŒŒì´ì¬ì€ ë³€ìˆ˜ì— ìƒì„±ëœ ê°ì²´ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.\në³€ìˆ˜ëŠ” ê°ì²´ì— ë¶™ì—¬ì§€ëŠ” ë³„ì¹­,ë ˆì´ë¸”ë¡œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë ‡ê²Œ ê°ì²´ì— ë³€ìˆ˜ê°€ í• ë‹¹ë˜ê³ ë‚˜ë©´ í• ë‹¹ëœ ë³€ìˆ˜ì™€ ê°ì²´ì— ëŒ€í•´ì„œ â€œë³€ìˆ˜ê°€ ê°ì²´ì— ë°”ì¸ë”©(ë¬¶ì´ë‹¤)ë˜ì–´ìˆë‹¤â€ê³  í‘œí˜„í•©ë‹ˆë‹¤.\n\n\na = [1,2,3]\nprint(id(a))\n\n1819154486912\n\n\nìœ„ ì½”ë“œì—ì„œ ê°ì²´[1,2,3]ì— ë³€ìˆ˜ aê°€ ë°”ì¸ë”© ë˜ì—ˆìŠµë‹ˆë‹¤.ë‚´ë¶€ë™ì‘ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 1. ë©”ëª¨ë¦¬ ì£¼ì†Œ(1819161042880)ì— ë¦¬ìŠ¤íŠ¸ ê°ì²´[1,2,3]ì„ ìƒì„±í•©ë‹ˆë‹¤ 2. ìƒì„±ëœ ë¦¬ìŠ¤íŠ¸ê°ì²´ë¥¼ ë³€ìˆ˜ aì— í• ë‹¹í•©ë‹ˆë‹¤. aëŠ” ê°ì²´[1,2,3]ì— ë¶™ì—¬ì§€ëŠ” ë³„ì¹­,ë ˆì´ë¸”ì´ë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nì—ì¼ë¦¬ì–´ì‹±\nì—ì¼ë¦¬ì–´ì‹±ì€ í•˜ë‚˜ì˜ ê°ì²´ë¥¼ ì—¬ëŸ¬ê°œì˜ ë³€ìˆ˜ê°€ ì°¸ì¡°í•˜ê²Œ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. í•˜ë‚˜ì˜ ê°ì²´ì— ì—¬ëŸ¬ê°œì˜ ë³„ì¹­,ë³„ëª…,ë ˆì´ë¸”ì„ ë¶™ì´ëŠ” ê²ƒì´ë¼ê³ ë„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\nb = a\nprint(id(a));print(id(b))\nprint(a);print(b)\n\n1819154486912\n1819154486912\n[1, 2, 3]\n[1, 2, 3]"
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#idvalue",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#idvalue",
    "title": "íŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹",
    "section": "id,value",
    "text": "id,value\nidëŠ” ê°ì²´ê°€ ê°€ì§€ëŠ” ë©”ëª¨ë¦¬ìƒì˜ ê³ ìœ í•œ ì£¼ì†Œì…ë‹ˆë‹¤. ì„œë¡œë‹¤ë¥¸ ê°ì²´ëŠ” ë‹¤ë¥¸ ê°’ì„ ê°€ì§ˆìˆ˜ë„ ê°™ì€ê°’ì„ ê°€ì§ˆìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n\na=[1,2,3]\nb=a\na.append(4)\nc=[1,2,3,4]\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\n\na:[1, 2, 3, 4] b:[1, 2, 3, 4] c:[1, 2, 3, 4]\nid(a):1819154849280,id(b):1819154849280,id(c):1819155097408\n\n\në³€ìˆ˜a,b,cëŠ” ëª¨ë‘ ê°™ì€ value(ê°’)ì„ ê°€ì§‘ë‹ˆë‹¤.aì™€bëŠ” ê°™ì€ ê°ì²´ì— ë°”ì¸ë”©ë˜ì–´ìˆì§€ë§Œ cëŠ” ë˜ë‹¤ë¥¸ ê°ì²´ì— ë°”ì¸ë”©ë˜ì–´ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#is-vs",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#is-vs",
    "title": "íŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹",
    "section": "is vs ==",
    "text": "is vs ==\n- isëŠ” ê°ì²´ë¹„êµì—°ì‚°ìë¡œ ë‘ ë³€ìˆ˜ê°€ ë™ì¼í•œ ê°ì²´ë¥¼ ì°¸ì¡°í•˜ëŠ”ì§€ ì•„ë‹ˆë©´ ë‹¤ë¥¸ê°ì²´ë¥¼ ì°¸ì¡°í•˜ëŠ”ì§€ í™•ì¸í•œ í›„ True or Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. íŒŒì´ì¬ì€ ë‚´ë¶€ì ìœ¼ë¡œ ë™ì¼í•œ ê°ì²´ì¸ì§€ ì•„ë‹Œì§€ë¥¼ íŒë‹¨í• ë•Œì—ëŠ” ë©”ëª¨ë¦¬ì£¼ì†Œë¥¼ í™•ì¸í•œë‹¤ê³  í•©ë‹ˆë‹¤. - ==ëŠ” ê°’ë¹„êµì—°ì‚°ìë¡œ ë‘ ë³€ìˆ˜ê°€ ì°¸ì¡°í•˜ëŠ” ê°ì²´ì˜ ê°’ì´ ê°™ì€ì§€ ì•„ë‹ˆë©´ ê°’ì´ ë‹¤ë¥¸ì§€ë¥¼ í™•ì¸í•œ í›„ True or Falseë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤. - ì°¸ì¡°í•˜ë‹¤ëŠ” ë­”ê°€ ì˜ ì™€ë‹¿ëŠ”ë° ì°¸ì¡°`ë¼ëŠ” ìš©ì–´ëŠ” ì˜ ì™€ë‹¿ì§€ê°€ ì•ŠìŠµë‹ˆë‹¤â€¦ë³€ìˆ˜ê°€ ì°¸ì¡°í•˜ëŠ”(ë˜ëŠ” ê°€ë¦¬í‚¤ëŠ”,ì§€ì¹­í•˜ëŠ”)ê°ì²´(object) ê·¸ ìì²´ì…ë‹ˆë‹¤.\n\ncode1\n\na=[1,2,3] #1\nprint(\"appendí•˜ê¸° ì „ id(a):\",id(a))\nb=a #2 ì—ì¼ë¦¬ì–´ì‹±,ë™ì¼í•œ ê°ì²´ë¥¼ ê°€ë¦¬í‚¤ë„ë¡ í•¨.\na.append(4) #3\nc=[1,2,3,4] #4\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\nprint(\"aì˜ ì°¸ì¡°(reference)ì™€ bì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”??\",a is b)\nprint(\"aì˜ ì°¸ì¡°ì™€ bì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”?\",a == b)\nprint(\"aì˜ ì°¸ì¡°(reference)ì™€ cì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”??\",a is c)\nprint(\"aì˜ ì°¸ì¡°ì™€ cì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”?\",a == c)\n\nappendí•˜ê¸° ì „ id(a): 1819155097408\na:[1, 2, 3, 4] b:[1, 2, 3, 4] c:[1, 2, 3, 4]\nid(a):1819155097408,id(b):1819155097408,id(c):1819155103296\naì˜ ì°¸ì¡°(reference)ì™€ bì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”?? True\naì˜ ì°¸ì¡°ì™€ bì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”? True\naì˜ ì°¸ì¡°(reference)ì™€ cì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”?? False\naì˜ ì°¸ì¡°ì™€ cì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”? True\n\n\nì½”ë“œì„¤ëª… 1. ë³€ìˆ˜aì— [1,2,3]ì„ í• ë‹¹í•©ë‹ˆë‹¤.aëŠ” [1,2,3]ì„ ì°¸ì¡°í•©ë‹ˆë‹¤. 2. ì—ì¼ë¦¬ì–´ì‹±ìœ¼ë¡œ ë³€ìˆ˜bë„ [1,2,3]ì„ ì°¸ì¡°í•©ë‹ˆë‹¤. 3. ë³€ìˆ˜aê°€ ì°¸ì¡°í•˜ëŠ” ë¦¬ìŠ¤íŠ¸ê°ì²´[1,2,3]ì— 4ë¥¼ ì¶”ê°€í•©ë‹ˆë‹¤. 4. ë³€ìˆ˜cì— [1,2,3,4]ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.\n\n\ncode2 - ì‚´ì§ ì‹¬í™”\n\na=[1,2,3] #1\nprint(\"ì¬í• ë‹¹ í•˜ê¸° ì „ id(a):\",id(a))\nb=a #2ì—ì¼ë¦¬ì–´ì‹±,ë™ì¼í•œ ê°ì²´ë¥¼ ê°€ë¦¬í‚¤ë„ë¡ í•¨\na=[1,2,3]+[4] #3ì¬í• ë‹¹\nc=[1,2,3,4] #4\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\nprint(\"aì˜ ì°¸ì¡°(reference)ì™€ bì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”??\",a is b)\nprint(\"aì˜ ì°¸ì¡°ì™€ bì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”?\",a == b)\nprint(\"aì˜ ì°¸ì¡°(reference)ì™€ cì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”??\",a is c)\nprint(\"aì˜ ì°¸ì¡°ì™€ cì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”?\",a == c)\n\nì¬í• ë‹¹ í•˜ê¸° ì „ id(a): 1819155102592\na:[1, 2, 3, 4] b:[1, 2, 3] c:[1, 2, 3, 4]\nid(a):1819184303168,id(b):1819155102592,id(c):1819184334272\naì˜ ì°¸ì¡°(reference)ì™€ bì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”?? False\naì˜ ì°¸ì¡°ì™€ bì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”? False\naì˜ ì°¸ì¡°(reference)ì™€ cì˜ ì°¸ì¡°ëŠ” ë™ì¼í•œ ê°ì²´ì¸ê°€ìš”?? False\naì˜ ì°¸ì¡°ì™€ cì˜ ì°¸ì¡°ëŠ” ê°’(value)ì´ ê°™ë‚˜ìš”? True\n\n\nì½”ë“œì„¤ëª… 1. ë³€ìˆ˜aì— [1,2,3]ì„ í• ë‹¹í•©ë‹ˆë‹¤.aëŠ” [1,2,3]ì„ ì°¸ì¡°í•©ë‹ˆë‹¤. 2. ì—ì¼ë¦¬ì–´ì‹±ìœ¼ë¡œ ë³€ìˆ˜bë„ [1,2,3]ì„ ì°¸ì¡°í•©ë‹ˆë‹¤. 3. ë³€ìˆ˜aì— ë¦¬ìŠ¤íŠ¸ê°ì²´[1,2,3,4]ë¥¼ ì¬í• ë‹¹í•©ë‹ˆë‹¤. 4. ë³€ìˆ˜cì— [1,2,3,4]ë¥¼ í• ë‹¹í•©ë‹ˆë‹¤.\na,b,c ê°ê°ì˜ ê°’ì€ code1ê³¼ code2ì—ì„œ ëª¨ë‘ ê°™ìŠµë‹ˆë‹¤. ì°¨ì´ì ì€ code1ì—ì„œëŠ” aê°€ ì°¸ì¡°í•˜ëŠ” ë¦¬ìŠ¤íŠ¸[1,2,3]ì— 4ë¥¼ ì¶”ê°€í•˜ê³  code2ì—ì„œëŠ” aì— ë¦¬ìŠ¤íŠ¸[1,2,3,4]ë¥¼ ì¬í• ë‹¹í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.ì¤‘ìš”í•œ ì°¨ì´ì ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n- code1ì— appendì „ í›„ì˜ aê°€ ì°¸ì¡°í•˜ëŠ” ê°ì²´ëŠ” ì£¼ì†ŒëŠ” ë³€í•˜ì§€ ì•Šì€ ê²ƒìœ¼ë¡œ ë³´ì•„ ë™ì¼í•œ ê°ì²´ì— ì›ì†Œë§Œ ì¶”ê°€ë˜ì—ˆìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. - ë°˜ë©´ code2ì—ì„œ í• ë‹¹ë¬¸ ì „ í›„ì˜ aê°€ ì°¸ì¡°í•˜ëŠ” ê°ì²´ì˜ ì£¼ì†Œê°€ ë³€í•©ë‹ˆë‹¤. - ì´ì „ì— ì—†ì—ˆë˜ 1)ê°ì²´ê°€ ë©”ëª¨ë¦¬ì— ìƒì„±ë˜ê³  2)ë³€ìˆ˜aëŠ” ì´ì „ì˜ [1,2,3]ì„ ë” ì´ìƒ ì°¸ì¡°í•˜ì§€ ì•Šê³  ìƒì„±ëœ ê°ì²´[1,2,3,4]ë¥¼ ì°¸ì¡°**í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#ì¸í„°ë‹",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#ì¸í„°ë‹",
    "title": "íŒŒì´ì¬ - ë³€ìˆ˜,í• ë‹¹ë¬¸,ì¸í„°ë‹",
    "section": "ì¸í„°ë‹",
    "text": "ì¸í„°ë‹\nì¸í„°ë‹ì´ë€ ì´ë¯¸ ìƒì„±ëœ ê°ì²´ë¥¼ ì¬ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ê°ì²´ì˜ ë¹ ë¥¸ ì¬ì‚¬ìš©ì„ ê°€ëŠ¥í•˜ê²Œ í•˜ë©° ë©”ëª¨ë¦¬ë¥¼ ì ˆì•½í•œë‹¤ê³  í•©ë‹ˆë‹¤. ë‚´ë¶€ì ìœ¼ë¡œëŠ” ë‹¤ìŒê³¼ ê°™ì´ êµ¬í˜„ë©ë‹ˆë‹¤. 1. ì„ì˜ì˜ í• ë‹¹ë¬¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. 2. = ì˜¤ë¥¸ìª½ì— ìˆëŠ” ê°ì²´ê°€ Intern ì»¬ë ‰ì…˜ì— ë“±ë¡ë˜ì–´ ìˆëŠ”ì§€ ì•„ë‹Œì§€ í™•ì¸í•©ë‹ˆë‹¤. 3. ë“±ë¡ë˜ì–´ ìˆëŠ” ê°ì²´ì˜ ê²½ìš° ê·¸ ê°ì²´ë¥¼ ê·¸ëŒ€ë¡œ ì°¸ì¡°í•©ë‹ˆë‹¤. ë“±ë¡ë˜ì§€ ì•Šì€ ê²½ìš° ë©”ëª¨ë¦¬ì— ê°ì²´ë¥¼ ìƒì„±í•˜ë©° ë³€ìˆ˜ëŠ” ìƒì„±ëœ ê°ì²´ì˜ ì£¼ì†Œë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤\nìì£¼ ì‚¬ìš©í•˜ëŠ” ê°ì²´ì˜ ê²½ìš° ì§ì ‘ Intern ì»¬ë ‰ì…˜ì— ë“±ë¡í•  ìˆ˜ ìˆê³  ë¹ ë¥´ê²Œ ì¬ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. -5~256ì‚¬ì´ì˜ ì •ìˆ˜ì´ê±°ë‚˜ 20ì ë¯¸ë§Œì˜ ë¬¸ìì—´ì€ í• ë‹¹ë¬¸ì„ ì‹¤í–‰í•˜ë©´ ìë™ìœ¼ë¡œ Inter ì»¬ë ‰ì…˜ì— ë“±ë¡ë©ë‹ˆë‹¤. ë”°ë¼ì„œ í•´ë‹¹í•˜ëŠ” ì •ìˆ˜ë‚˜ ë¬¸ìì—´ì„ ë˜ë‹¤ë¥¸ í• ë‹¹ë¬¸ì— ì‹¤í–‰í•˜ë©´ ë³€ìˆ˜ëŠ” ê°™ì€ ê°ì²´ë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤.\nì˜ˆì œ1-ì¸í„°ë‹ X\n\na=1+2021\nb=2023-1\nc=2022\nprint(id(a),id(b),id(c))\nprint(a,b,c)\n\n1819155040400 1819155039600 1819155040688\n2022 2022 2022\n\n\na,b,cëŠ” ì„œë¡œë‹¤ë¥¸ ê°ì²´ë¥¼ ì°¸ì¡°í•˜ë©° ê°ì²´ë“¤ì€ ëª¨ë‘ ê°™ì€ ê°’ì„ ê°€ì§‘ë‹ˆë‹¤.\nì˜ˆì œ2-ì¸í„°ë‹ O\n\na=1+2 \nb=4-1\nc=3\nprint(id(a),id(b),id(c))\nprint(a,b,c)\n\n1819075897712 1819075897712 1819075897712\n3 3 3\n\n\na,b,cëŠ” ëª¨ë‘ê°™ì€ ê°ì²´ë¥¼ ì°¸ì¡°í•©ë‹ˆë‹¤. ë‚´ë¶€ì ì¸ ë™ì‘ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. 1. a=1+2 í• ë‹¹ë¬¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. 2. í• ë‹¹ë˜ëŠ” ê°ì²´ê°€ -5~256ì‚¬ì´ì˜ ì •ìˆ˜ì´ë¯€ë¡œ ìë™ìœ¼ë¡œ Intern ì»¬ë ‰ì…˜ì— ë“±ë¡ë©ë‹ˆë‹¤. 3. bì™€cì—ë„ ì •ìˆ˜ 3ì„ í• ë‹¹í•©ë‹ˆë‹¤. 3ì€ Intern ì»¬ë ‰ì…˜ ë“±ë¡ë˜ì–´ìˆëŠ” ê°ì²´ì´ë©° ë©”ëª¨ë¦¬ìƒì— ìƒì„±ë˜ì–´ìˆëŠ” ê°ì²´ì´ë¯€ë¡œ ìƒˆë¡œìš´ 3ê°ì²´ê°€ ìƒì„±ë˜ì§€ b,cëŠ” ì´ë¯¸ ìƒì„±ëœ 3ê°ì²´ë¥¼ ê°€ë¦¬í‚µë‹ˆë‹¤.\nì°¸ê³ ë§í¬1 : https://guebin.github.io/DL2022/posts/Appendix/2022-12-14-A1.html ì°¸ê³ ë§í¬2 : http://pythonstudy.xyz/python/article/512-%ED%8C%8C%EC%9D%B4%EC%8D%AC-Object-Interning"
  },
  {
    "objectID": "posts/python/python class,instance,self,namespace.html",
    "href": "posts/python/python class,instance,self,namespace.html",
    "title": "HIHO",
    "section": "",
    "text": "íŒŒì´ì¬ì—ì„œ ì–´ë–¤ ë¶€ë¶„(ìŠ¤ì½”í”„)ì—ì„œ ì½”ë“œ(í• ë‹¹ë¬¸)ê°€ ì‘ì„±ë˜ëŠëƒì— ë”°ë¼ ê°ì²´ë¥¼ ê°€ë¦¬í‚¤ëŠ” ì‹ë³„ì(ë³€ìˆ˜ëª…)ì™€ ê°ì²´ëŠ” ë‹¤ë¥¸ ê³µê°„ì— ì†í•˜ê²Œ ëœë‹¤.\nì´ì™€ê°™ì€ ì‹ë³„ìì™€ ê°ì²´ê°€ ì†í•˜ëŠ” ì„œë¡œë‹¤ë¥¸ ê³µê°„ì„ Namespace(ì´ë¦„ê³µê°„)ì´ë¼ê³  í•œë‹¤.\nëª¨ë“ˆì—ì„œ ì½”ë“œê°€ ì‘ì„±ë˜ë©´ ëª¨ë“ˆì´ ê°€ì§€ëŠ” ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì¸ ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì‹ë³„ìì™€ ê°ì²´ê°€ ìƒì„±ë˜ë©° ë§¤í•‘ë˜ëŠ” ê´€ê³„ë¥¼ ê°€ì§€ê²Œ ëœë‹¤.\ní•¨ìˆ˜ì•ˆì—ì„œ ì½”ë“œê°€ ì‘ì„±ë˜ë©´ í•¨ìˆ˜ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì¸ ì§€ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œ ì‹ë³„ìì™€ ê°ì²´ê°€ ìƒì„±ë˜ë©° ì„œë¡œ ë§¤í•‘ë˜ëŠ” ê´€ê³„ë¥¼ ê°€ì§€ë©° í´ë˜ìŠ¤,ì¸ìŠ¤í„´ìŠ¤ì—ì„œë„ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¼ëŠ” ê°œë…ì´ ì¡´ì¬í•œë‹¤.\nì´ ë§¤í•‘ê´€ê³„ëŠ” íŒŒì´ì¬ì—ì„œ ë”•ì…”ë„ˆë¦¬ë¡œ êµ¬í˜„ëœë‹¤.\n\n\n\n\n\në„¤ì„ìŠ¤í˜ì´ìŠ¤ëŠ” ë‹¨ìˆœíˆ ì‹ë³„ìì™€ ê°ì²´ê°€ ì†Œì†ë˜ëŠ” ê³µê°„ì´ë¼ëŠ” ì˜ë¯¸ë¥¼ ë„˜ì–´ì„œì„œ ë‚˜ì•„ê°€ ì½”ë“œì˜ ì‹¤í–‰ì´ ì´ë£¨ì–´ì§€ëŠ” ê³µê°„ì„ ì˜ë¯¸í•˜ê¸°ë„ í•œë‹¤.\n\n\n\nê°ì²´ì— ëŒ€í•´ ë­”ê°€ ì‹¤í–‰ -> ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ì„œ ê°ì²´ë¥¼ ì¡°íšŒ -> ê°ì²´ë¥¼ ê°€ì ¸ì˜´(ì—†ê±°ë‚˜ ë‹¤ë¥¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì—ë„ ì—†ìœ¼ë©´ ì—ëŸ¬) ì§€ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì—†ìœ¼ë©´ -> ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì¡°íšŒ,ì—¬ê¸°ì—ë„ ì—†ìœ¼ë©´ -> ë¹ŒíŠ¸ì¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì¡°íšŒ,ì—¬ê¸°ì—ë„ ì—†ìœ¼ë©´ ì˜¤ë¥˜\nìŠ¤ì½”í”„ : ì½”ë“œìƒì˜ ì–´ë–¤ ì˜ì—­ì„ ë§í•œë‹¤. í•˜ë‚˜ ë˜ëŠ” ì—¬ëŸ¬ê°œì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ì—°ê²°ë˜ì–´ ìˆë‹¤.\nì°¸ê³ ë§í¬ https://post.naver.com/viewer/postView.nhn?volumeNo=27785662&memberNo=21815&navigationType=push https://schoolofweb.net/blog/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-oop-part-3-%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%B3%80%EC%88%98class-variable/ https://wikidocs.net/1743 https://hcnoh.github.io/2021-04-17-dkvmn"
  },
  {
    "objectID": "posts/python/python class,instance,self,namespace.html#class",
    "href": "posts/python/python class,instance,self,namespace.html#class",
    "title": "HIHO",
    "section": "CLASS",
    "text": "CLASS\n\nclass Resnet(): #í´ë˜ìŠ¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ëŠ” ê²½ìš° defëŒ€ì‹  classë¥¼ ì…ë ¥í•œë‹¤.\n  temp = 25\n  #print(locals())\n# ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ cëŠ” Resnetì´ë¼ëŠ” í´ë˜ìŠ¤ í•¨ìˆ˜ì˜ í˜¸ì¶œì„ í†µí•´ ìƒì„±ëœ ê°ì²´ì˜ ì‹ë³„ìê°€ ëœë‹¤.\n# í´ë˜ìŠ¤ í•¨ìˆ˜ëŠ” ê·¸ ë°˜í™˜ê°’ì´ ì‚¬ìš©ìê°€ ì •ì˜í•œ ê°ì²´ê°€ ë˜ë©° ì´ ê°ì²´ë¥¼ ë³€ìˆ˜cê°€ ê°€ë¦¬í‚¤ê²Œ ëœë‹¤.\n\n\nResnet.__dict__ #í´ë˜ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸\n#í´ë˜ìŠ¤ì— print(locals())ë¡œë„ í™•ì¸í•  ìˆ˜ ìˆìŒ.\n\nmappingproxy({'__module__': '__main__',\n              'temp': 25,\n              '__dict__': <attribute '__dict__' of 'Resnet' objects>,\n              '__weakref__': <attribute '__weakref__' of 'Resnet' objects>,\n              '__doc__': None})\n\n\n\nglobals() #ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— í´ë˜ìŠ¤ Resnetì´ ì¶”ê°€ëœ ê²ƒë„ í™•ì¸\n\n{'__name__': '__main__',\n '__doc__': 'Automatically created module for IPython interactive environment',\n '__package__': None,\n '__loader__': None,\n '__spec__': None,\n '__builtin__': <module 'builtins' (built-in)>,\n '__builtins__': <module 'builtins' (built-in)>,\n '_ih': ['',\n  'a=10;b=20',\n  '#ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ë³´ì—¬ì£¼ëŠ” ëª…ë ¹ì–´\\n#ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” ì‹ë³„ìì™€ ê°ì²´ë“¤ì„ ë³¼ ìˆ˜ ìˆë‹¤.\\nglobals()',\n  '#ì§€ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ë³´ì—¬ì£¼ëŠ” ëª…ë ¹ì–´\\n#ì§€ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ëŠ” ì–´ë–¤ ìŠ¤ì½”í”„ì—ì„œì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ë³´ì—¬ì¤€ë‹¤.\\n#ëª¨ë“ˆì˜ ì§€ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ëŠ” ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ê°™ìœ¼ë¯€ë¡œ ë‘ ê²°ê³¼ëŠ” ì„œë¡œ ê°™ë‹¤.\\nlocals()',\n  'def printhi():\\n  text = \"hi\"\\n  text2 = \"hello\"\\n  print(\"hi\")\\n  print(__name__)\\n  print(locals())\\nprinthi()',\n  'globals()',\n  'x = 1\\ndef no_local():\\n    print(locals())\\n    print(x)\\nno_local()',\n  'class Resnet(): #í´ë˜ìŠ¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ëŠ” ê²½ìš° defëŒ€ì‹  classë¥¼ ì…ë ¥í•œë‹¤.\\n  temp = 25\\n  #print(locals())\\n# ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ cëŠ” Resnetì´ë¼ëŠ” í´ë˜ìŠ¤ í•¨ìˆ˜ì˜ í˜¸ì¶œì„ í†µí•´ ìƒì„±ëœ ê°ì²´ì˜ ì‹ë³„ìê°€ ëœë‹¤.\\n# í´ë˜ìŠ¤ í•¨ìˆ˜ëŠ” ê·¸ ë°˜í™˜ê°’ì´ ì‚¬ìš©ìê°€ ì •ì˜í•œ ê°ì²´ê°€ ë˜ë©° ì´ ê°ì²´ë¥¼ ë³€ìˆ˜cê°€ ê°€ë¦¬í‚¤ê²Œ ëœë‹¤.',\n  'Resnet.__dict__ #í´ë˜ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸\\n#í´ë˜ìŠ¤ì— print(locals())ë¡œë„ í™•ì¸í•  ìˆ˜ ìˆìŒ.',\n  'globals() #ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— í´ë˜ìŠ¤ Resnetì´ ì¶”ê°€ëœ ê²ƒë„ í™•ì¸'],\n '_oh': {2: {...},\n  3: {...},\n  5: {...},\n  8: mappingproxy({'__module__': '__main__',\n                'temp': 25,\n                '__dict__': <attribute '__dict__' of 'Resnet' objects>,\n                '__weakref__': <attribute '__weakref__' of 'Resnet' objects>,\n                '__doc__': None})},\n '_dh': ['/content'],\n 'In': ['',\n  'a=10;b=20',\n  '#ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ë³´ì—¬ì£¼ëŠ” ëª…ë ¹ì–´\\n#ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” ì‹ë³„ìì™€ ê°ì²´ë“¤ì„ ë³¼ ìˆ˜ ìˆë‹¤.\\nglobals()',\n  '#ì§€ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ë³´ì—¬ì£¼ëŠ” ëª…ë ¹ì–´\\n#ì§€ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ëŠ” ì–´ë–¤ ìŠ¤ì½”í”„ì—ì„œì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ë³´ì—¬ì¤€ë‹¤.\\n#ëª¨ë“ˆì˜ ì§€ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ëŠ” ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ê°™ìœ¼ë¯€ë¡œ ë‘ ê²°ê³¼ëŠ” ì„œë¡œ ê°™ë‹¤.\\nlocals()',\n  'def printhi():\\n  text = \"hi\"\\n  text2 = \"hello\"\\n  print(\"hi\")\\n  print(__name__)\\n  print(locals())\\nprinthi()',\n  'globals()',\n  'x = 1\\ndef no_local():\\n    print(locals())\\n    print(x)\\nno_local()',\n  'class Resnet(): #í´ë˜ìŠ¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ëŠ” ê²½ìš° defëŒ€ì‹  classë¥¼ ì…ë ¥í•œë‹¤.\\n  temp = 25\\n  #print(locals())\\n# ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ cëŠ” Resnetì´ë¼ëŠ” í´ë˜ìŠ¤ í•¨ìˆ˜ì˜ í˜¸ì¶œì„ í†µí•´ ìƒì„±ëœ ê°ì²´ì˜ ì‹ë³„ìê°€ ëœë‹¤.\\n# í´ë˜ìŠ¤ í•¨ìˆ˜ëŠ” ê·¸ ë°˜í™˜ê°’ì´ ì‚¬ìš©ìê°€ ì •ì˜í•œ ê°ì²´ê°€ ë˜ë©° ì´ ê°ì²´ë¥¼ ë³€ìˆ˜cê°€ ê°€ë¦¬í‚¤ê²Œ ëœë‹¤.',\n  'Resnet.__dict__ #í´ë˜ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸\\n#í´ë˜ìŠ¤ì— print(locals())ë¡œë„ í™•ì¸í•  ìˆ˜ ìˆìŒ.',\n  'globals() #ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— í´ë˜ìŠ¤ Resnetì´ ì¶”ê°€ëœ ê²ƒë„ í™•ì¸'],\n 'Out': {2: {...},\n  3: {...},\n  5: {...},\n  8: mappingproxy({'__module__': '__main__',\n                'temp': 25,\n                '__dict__': <attribute '__dict__' of 'Resnet' objects>,\n                '__weakref__': <attribute '__weakref__' of 'Resnet' objects>,\n                '__doc__': None})},\n 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7fe22c68e490>>,\n 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7fe229652590>,\n 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7fe229652590>,\n '_': mappingproxy({'__module__': '__main__',\n               'temp': 25,\n               '__dict__': <attribute '__dict__' of 'Resnet' objects>,\n               '__weakref__': <attribute '__weakref__' of 'Resnet' objects>,\n               '__doc__': None}),\n '__': {...},\n '___': {...},\n '_i': 'Resnet.__dict__ #í´ë˜ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸\\n#í´ë˜ìŠ¤ì— print(locals())ë¡œë„ í™•ì¸í•  ìˆ˜ ìˆìŒ.',\n '_ii': 'class Resnet(): #í´ë˜ìŠ¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ëŠ” ê²½ìš° defëŒ€ì‹  classë¥¼ ì…ë ¥í•œë‹¤.\\n  temp = 25\\n  #print(locals())\\n# ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ cëŠ” Resnetì´ë¼ëŠ” í´ë˜ìŠ¤ í•¨ìˆ˜ì˜ í˜¸ì¶œì„ í†µí•´ ìƒì„±ëœ ê°ì²´ì˜ ì‹ë³„ìê°€ ëœë‹¤.\\n# í´ë˜ìŠ¤ í•¨ìˆ˜ëŠ” ê·¸ ë°˜í™˜ê°’ì´ ì‚¬ìš©ìê°€ ì •ì˜í•œ ê°ì²´ê°€ ë˜ë©° ì´ ê°ì²´ë¥¼ ë³€ìˆ˜cê°€ ê°€ë¦¬í‚¤ê²Œ ëœë‹¤.',\n '_iii': 'x = 1\\ndef no_local():\\n    print(locals())\\n    print(x)\\nno_local()',\n '_i1': 'a=10;b=20',\n 'a': 10,\n 'b': 20,\n '_i2': '#ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ë³´ì—¬ì£¼ëŠ” ëª…ë ¹ì–´\\n#ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì¡´ì¬í•˜ëŠ” ì‹ë³„ìì™€ ê°ì²´ë“¤ì„ ë³¼ ìˆ˜ ìˆë‹¤.\\nglobals()',\n '_2': {...},\n '_i3': '#ì§€ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ë³´ì—¬ì£¼ëŠ” ëª…ë ¹ì–´\\n#ì§€ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ëŠ” ì–´ë–¤ ìŠ¤ì½”í”„ì—ì„œì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ë³´ì—¬ì¤€ë‹¤.\\n#ëª¨ë“ˆì˜ ì§€ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ëŠ” ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì™€ ê°™ìœ¼ë¯€ë¡œ ë‘ ê²°ê³¼ëŠ” ì„œë¡œ ê°™ë‹¤.\\nlocals()',\n '_3': {...},\n '_i4': 'def printhi():\\n  text = \"hi\"\\n  text2 = \"hello\"\\n  print(\"hi\")\\n  print(__name__)\\n  print(locals())\\nprinthi()',\n 'printhi': <function __main__.printhi()>,\n '_i5': 'globals()',\n '_5': {...},\n '_i6': 'x = 1\\ndef no_local():\\n    print(locals())\\n    print(x)\\nno_local()',\n 'x': 1,\n 'no_local': <function __main__.no_local()>,\n '_i7': 'class Resnet(): #í´ë˜ìŠ¤ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ëŠ” ê²½ìš° defëŒ€ì‹  classë¥¼ ì…ë ¥í•œë‹¤.\\n  temp = 25\\n  #print(locals())\\n# ì´ ì½”ë“œë¥¼ ì‹¤í–‰í•˜ë©´ cëŠ” Resnetì´ë¼ëŠ” í´ë˜ìŠ¤ í•¨ìˆ˜ì˜ í˜¸ì¶œì„ í†µí•´ ìƒì„±ëœ ê°ì²´ì˜ ì‹ë³„ìê°€ ëœë‹¤.\\n# í´ë˜ìŠ¤ í•¨ìˆ˜ëŠ” ê·¸ ë°˜í™˜ê°’ì´ ì‚¬ìš©ìê°€ ì •ì˜í•œ ê°ì²´ê°€ ë˜ë©° ì´ ê°ì²´ë¥¼ ë³€ìˆ˜cê°€ ê°€ë¦¬í‚¤ê²Œ ëœë‹¤.',\n 'Resnet': __main__.Resnet,\n '_i8': 'Resnet.__dict__ #í´ë˜ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸\\n#í´ë˜ìŠ¤ì— print(locals())ë¡œë„ í™•ì¸í•  ìˆ˜ ìˆìŒ.',\n '_8': mappingproxy({'__module__': '__main__',\n               'temp': 25,\n               '__dict__': <attribute '__dict__' of 'Resnet' objects>,\n               '__weakref__': <attribute '__weakref__' of 'Resnet' objects>,\n               '__doc__': None}),\n '_i9': 'globals() #ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— í´ë˜ìŠ¤ Resnetì´ ì¶”ê°€ëœ ê²ƒë„ í™•ì¸'}\n\n\n\ndefë¡œ ì •ì˜í•˜ëŠ” í•¨ìˆ˜ê°ì²´ì™€ëŠ” ë‹¬ë¦¬ classë¡œ ì •ì˜í•œ í´ë˜ìŠ¤ í•¨ìˆ˜ ê°ì²´ì™€ ì´ í´ë˜ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì†í•œ ê°ì²´ë“¤ì„ â€œ.â€ì´ë¼ëŠ” ê¸°í˜¸ë¥¼ í†µí•´ ì ‘ê·¼í•  ìˆ˜ ìˆìŒ.\nìœ„ì™€ ê°™ì´ í´ë˜ìŠ¤ì•ˆì—ì„œ 25ë¼ëŠ” ê°ì²´ì™€ ë°”ì¸ë”©ëœ tempë¥¼ í´ë˜ìŠ¤ë³€ìˆ˜ë¼í•˜ë©° ì‹ë³„ìì™€ ê°ì²´ëŠ” ë³„ë„ì˜ í´ë˜ìŠ¤(ë¼ëŠ” ë˜í•˜ë‚˜ì˜ ë‹¤ë¥¸ í•¨ìˆ˜)ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì†í•˜ê²Œ ëœë‹¤."
  },
  {
    "objectID": "posts/python/python class,instance,self,namespace.html#instance",
    "href": "posts/python/python class,instance,self,namespace.html#instance",
    "title": "HIHO",
    "section": "instance",
    "text": "instance\n\ní´ë˜ìŠ¤ë¼ëŠ” í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ë©´ ê°ì²´ë¥¼ ë°˜í™˜í•¨.\nì´ ë°˜í™˜ëœ ê°ì²´ë¥¼ ì¸ìŠ¤í„´ìŠ¤ë¼ í•˜ë©° ì¸ìŠ¤í„´ìŠ¤ëŠ” í´ë˜ìŠ¤ê°€ ë³„ë„ë¡œ í´ë˜ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ê°€ì§„ê²ƒì²˜ëŸ¼ ì¸ìŠ¤í„´ìŠ¤ë„ ë³„ë„ë¡œ ì¸ìŠ¤í„´ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ê°€ì§.\n\n\nr1 = Resnet() #í´ë˜ìŠ¤ í˜¸ì¶œí•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±\nr2 = Resnet()\nprint(id(r1),id(r2))\n#ë©”ëª¨ë¦¬ìƒì— ë‹¤ë¥¸ ì£¼ì†Œì—ì„œ ì €ì¥ë¨ì„ í™•ì¸\n\n140609160422160 140609160422416\n\n\n\ndir() #ì „ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— r1,r2ê°€ ì¶”ê°€ì ìœ¼ë¡œ ìˆìŒì„ í™•ì¸\n\n['In',\n 'Out',\n 'Resnet',\n '_',\n '_2',\n '_3',\n '_5',\n '_8',\n '_9',\n '__',\n '___',\n '__builtin__',\n '__builtins__',\n '__doc__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n '_dh',\n '_i',\n '_i1',\n '_i10',\n '_i11',\n '_i2',\n '_i3',\n '_i4',\n '_i5',\n '_i6',\n '_i7',\n '_i8',\n '_i9',\n '_ih',\n '_ii',\n '_iii',\n '_oh',\n 'a',\n 'b',\n 'exit',\n 'get_ipython',\n 'no_local',\n 'printhi',\n 'quit',\n 'r1',\n 'r2',\n 'x']\n\n\n\nr1.__dict__ #ì¸ìŠ¤í„´ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸\n#ì•„ë¬´ê²ƒë„ ì—†ìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ\n\n{}\n\n\n\nr2.__dict__\n\n{}\n\n\ní´ë˜ìŠ¤ í•¨ìˆ˜ë¥¼ í˜¸ì¶œí•˜ì—¬ ì¸ìŠ¤í„´ìŠ¤(ê°ì²´)ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ìƒˆë¡œìš´ ì‹ë³„ìì™€ ê°ì²´ê°€ ì†í•˜ê²Œ í•˜ë ¤ë©´ init(self)ì™€ self.ë³€ìˆ˜ëª… ì„ ì‘ì„±í•˜ë©´ ë¨.\n\nclass Resnet():\n  temp = 25\n  def __init__(self):\n    self.v1 = [1,2,3,4]\n    self.s1 = \"hi\"\n  #print(locals())\nr1 = Resnet()\nr2 = Resnet()\n\n\nr1.__dict__ #ê°ì²´(ì¸ìŠ¤í„´ìŠ¤)ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ìƒˆë¡œìš´ ì‹ë³„ìì™€ ë³€ìˆ˜ë“¤ì˜ ë§¤í•‘ê´€ê³„ê°€ ìˆìŒì„ í™•ì¸í•  ìˆ˜ ìˆìŒ.\n\n{'v1': [1, 2, 3, 4], 's1': 'hi'}\n\n\n\nr2.__dict__\n\n{'v1': [1, 2, 3, 4], 's1': 'hi'}\n\n\n\n#í´ë˜ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— \".\"ì´ë¼ëŠ” ê¸°í˜¸ë¡œ ì ‘ê·¼í•  ìˆ˜ ìˆìŒ\nprint(Resnet.temp)\nprint(id(Resnet.temp))\n\n#ì¸ìŠ¤í„´ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— \".\"ì´ë¼ëŠ” ê¸°í˜¸ë¡œ ì ‘ê·¼ê°€ëŠ¥\nc = Resnet()\nprint(c.temp)\nprint(id(c.temp))\n\n#ë©”ëª¨ë¦¬ìƒì—ì„œ ê°™ì€ ì£¼ì†Œì— ì €ì¥ë˜ì–´ ìˆìŒë„ í™•ì¸\n\n25\n11127456\n25\n11127456"
  },
  {
    "objectID": "posts/python/python class,instance,self,namespace.html#lcgb-rule",
    "href": "posts/python/python class,instance,self,namespace.html#lcgb-rule",
    "title": "HIHO",
    "section": "LCGB RULE",
    "text": "LCGB RULE\n\nìƒˆë¡œ ìƒì„±ëœ ê°ì²´ì˜ ë‚´ë¶€ ì§€ì—­ë„¤ì„ìŠ¤í˜ì´ìŠ¤ ì¦‰ ì¸ìŠ¤í„´ìŠ¤ì˜ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì—†ëŠ” ê°ì²´ë¥¼ ê°€ì ¸ì˜¤ë¼ ëª…ë ¹=> í´ë˜ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ì¡°íšŒí•˜ê³  ê·¸ë˜ë„ ì—†ìœ¼ë©´ ìƒìœ„ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ì¡°íšŒí•˜ê²Œ ëœë‹¤.\n\n\nclass Resnet_2():\n  z=2\n  def __init__(self):\n    temp = 2\n    temp = \"hi\"\n    #print(locals())\n\n\nresnet = Resnet_2()\nprint(resnet.__dict__,resnet.z)\n#resnetì´ë¼ëŠ” ì¸ìŠ¤í„´ìŠ¤(ê°ì²´)ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— zë¼ëŠ” ë³€ìˆ˜ê°€ ì—†ìŒ\n#ê·¼ë° zë¼ëŠ” ë³€ìˆ˜ê°€ ê°€ë¦¬í‚¤ëŠ” ê°ì²´ë¥¼ ê°€ì ¸ì˜¤ë¼ê³  ëª…ë ¹ -> í´ë˜ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ì¡°íšŒí•˜ê³  \n#ê·¸ë˜ë„ ì—†ìœ¼ë©´ ë” ìƒìœ„ì˜ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ì¡°íšŒ\n\n{} 2\n\n\n\nclass Resnet_2():\n  z=2\n  def __init__(self):\n    temp = 2\n    temp = \"hi\"\n    self.z = 32\nresnet = Resnet_2()\nprint(resnet.__dict__,resnet.z)\n#ì´ ê²½ìš°ëŠ” ì¸ìŠ¤í„´ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì‹ë³„ìì™€ ê°ì²´ê°€ ìˆìœ¼ë¯€ë¡œ í´ë˜ìŠ¤ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ì¡°íšŒí•˜ì§€ ì•ŠìŒ.\n\n{'z': 32} 32\n\n\n#selfì— ëŒ€í•œ ê³ ì°° ê²°ë¡  1. selfëŠ” íƒœëª½? ê°™ì€ ëŠë‚Œì´ë‹¤. ì¦‰,í´ë˜ìŠ¤ì— ëŒ€í•´ì„œ ì½”ë“œë¥¼ ì‘ì„±í• ë•Œì—ëŠ” ë§Œë“¤ì§€ ì•Šì•˜ì§€ë§Œ â€¦ ë¯¸ë˜ì— í´ë˜ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ ê°ì²´ë¥¼ ë§Œë“¤ë©´ ì–´ë–¤ ê°ì²´ê°€ ë©”ëª¨ë¦¬ìƒì˜ ì–´ë–¤ ê³µê°„ì— ìˆì„í…ë° ê·¸ ê°ì²´ë¥¼ ê°€ë¦¬í‚¤ë©° ê°ì²´ì™€ ë°”ì¸ë”©ë˜ëŠ” ë³€ìˆ˜ë¼ê³  ìƒê°í•˜ë©´ ëœë‹¤. ì˜ˆë¥¼ë“¤ì–´ì„œ, cookie = Cookie()ì´ë ‡ê²Œ í•˜ë©´ cookieë¼ëŠ” ë³€ìˆ˜ëŠ” Cookieê°ì²´ì™€ ë°”ì¸ë”©ëœë‹¤. selfë„ ì´ì™€ê°™ì´ Cookieê°ì²´ì™€ ë°”ì¸ë”©ë˜ëŠ”ë° ì´ì œ í´ë˜ìŠ¤ë¥¼ ì½”ë“œë¡œ ì‘ì„±í• ë•Œì—ëŠ” ë¯¸ë¦¬ë§Œë“¤ì–´ì§ˆ ê°ì²´ì— ì§€ì¹­ì„ í•´ì£¼ê¸° ìœ„í•´ì„œ ì´ëŸ° ì´ë¦„ì´ ë¶™ì—ˆë‹¤ê³  ìƒê°í•˜ë©´ ë  ê²ƒ ê°™ë‹¤. 2. â€œ.â€ì„ ì‚¬ìš©í•˜ì—¬ ê°ì²´ì™€ ë³€ìˆ˜ì‚¬ì´ì— .ì„ ë¶™ì´ë©´ ì¦‰,ê°ì²´.ë³€ìˆ˜ í•˜ë©´ ê°ì²´ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì•ˆì— ìˆëŠ” ë˜ë‹¤ë¥¸ ê°ì²´ë“¤ì— ì ‘ê·¼í•  ìˆ˜ ìˆë‹¤..(ì •í™•íˆëŠ” ê°ì²´(ì¸ìŠ¤í„´ìŠ¤)ì™€ë°”ì¸ë”©ëœë³€ìˆ˜.ê°ì²´(ì¸ìŠ¤í„´ìŠ¤)ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì†í•œ ë³€ìˆ˜ëª…(ì‹ë³„ì)ë¼ê³  í•  ìˆ˜ ìˆë‹¤.)í´ë˜ìŠ¤ë¥¼ ì‘ì„±í• ë•Œ ë©”ì†Œë“œì•ˆì— ì´ selfë¥¼ ì ëŠ” ì´ìœ ëŠ” í´ë˜ìŠ¤ë¡œë¶€í„° ë§Œë“¤ì–´ì§„ ì¸ìŠ¤í„´ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì•ˆì— ìˆëŠ” ê°ì²´ì— ëŒ€í•´ì„œ ì ‘ê·¼í•˜ì—¬ ë­”ê°€ë¥¼ ë°”ê¾¸ê±°ë‚˜ ì¡°ì‘í•˜ê¸° ìœ„í•´ì„œì´ë‹¤. 3. ì¸ìŠ¤í„´ìŠ¤.ë©”ì„œë“œ()ë¥¼ í•˜ë©´ í•¨ìˆ˜ì— ì•„ë¬´ì¸ìë¥¼ ë„£ì–´ì£¼ì§€ì•Šì•˜ì§€ë§Œ íŒŒì´ì¬ìŠ¤ìŠ¤ë¡œ ê°ì²´(ì¸ìŠ¤í„´ìŠ¤)ê·¸ ìì²´ë¥¼ ì²«ë²ˆì§¸ ì¸ìë¡œ ì „ë‹¬í•´ì¤€ë‹¤.ì´ëŠ” í´ë˜ìŠ¤.ë©”ì†Œë“œ(ì¸ìŠ¤í„´ìŠ¤)ë¼ëŠ” ì½”ë“œì™€ ì™„ì „íˆ ë™ì¼í•˜ë‹¤. 4. ë§Œì•½ì— ì¸ìŠ¤í„´ìŠ¤(ê°ì²´)ì˜ ë©”ì†Œë“œë¥¼ ì‹¤í–‰ì‹œí‚¤ì§€ ì•Šê³  í´ë˜ìŠ¤ì˜ ìì²´ì—ì„œ ë©”ì†Œë“œë¥¼ ì‹¤í–‰ì‹œí‚¤ëŠ” ê²½ìš°,selfëŠ” í•„ìš”ê°€ ì—†ë‹¤.\ní•µì‹¬ìš”ì•½ 1. selfëŠ” ê°ì²´ ë˜ëŠ” ì¸ìŠ¤í„´ìŠ¤ ê·¸ ìì²´ë¥¼ ê°€ë¦¬í‚¤ëŠ” ë³€ìˆ˜. 2. ê°ì²´(ì¸ìŠ¤í„´ìŠ¤)ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ì†í•œ ë˜ë‹¤ë¥¸ ê°ì²´ë“¤ì€ ê°ì²´(ì¸ìŠ¤í„´ìŠ¤)ì™€ ë°”ì¸ë”©ëœ ë³€ìˆ˜.ê°ì²´ì™€ ë°”ì¸ë”©ëœë³€ìˆ˜ë¼ëŠ” ë°©ì‹ìœ¼ë¡œ ì ‘ê·¼ ê°€ëŠ¥.ê°„ë‹¨íˆ ì¸ìŠ¤í„´ìŠ¤.ë©¤ë²„ë³€ìˆ˜ë¡œ ì ‘ê·¼ê°€ëŠ¥ 3. 1,2ì— ì˜í•˜ì—¬, selfë¼ëŠ” ê°ì²´ì™€ ë°”ì¸ë”©ëœ ë³€ìˆ˜ë¥¼ ë©”ì†Œë“œì˜ ì¸ìë¡œ ë°›ìœ¼ë©´ self.ë©¤ë²„ë³€ìˆ˜ ì´ëŸ°ì‹ìœ¼ë¡œ ì¸ìŠ¤í„´ìŠ¤ì˜ ë³€ìˆ˜ì— ì ‘ê·¼ ê°€ëŠ¥í•¨.\nì¸ìŠ¤í„´ìŠ¤ì˜ ê°€ìš©ë²”ìœ„ : https://andamiro25.tistory.com/38\n\nclass cookie():\n  given = \"cookie\" #í´ë˜ìŠ¤ë³€ìˆ˜ cookieì„ ì–¸ \n  maked_num = 0 #í´ë˜ìŠ¤ë³€ìˆ˜ ì„ ì–¸,ëª¨ë“  ì¸ìŠ¤í„´ìŠ¤ì— ëŒ€í•´ì„œ ê³µí†µëœ ì—°ì‚°ì´ ë˜ì–´ì•¼ í•  ë•Œ í• ë‹¹.\n  def __init__(self,flavor,size):\n    self.flavor = flavor\n    self.size = size\n    print(\"ë©”ëª¨ë¦¬ ì£¼ì†Œ :\",id(self))\n  def sizeup(self): #selfë¥¼ ì™œì“¸ê¹Œ? => selfëŠ” ê°ì²´(ì¸ìŠ¤í„´ìŠ¤)ê·¸ ìì²´ì´ë¯€ë¡œ self.~~ë¡œ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ìˆëŠ” ê°ì²´ì— ì ‘ê·¼ ê°€ëŠ¥í•¨.ì¸ìŠ¤í„´ìŠ¤ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ì— ìˆëŠ” ê°ì²´ì— ë­”ê°€ë¥¼ ì¶”ê°€í•˜ê±°ë‚˜ ì¡°ì‘í•˜ê±°ë‚˜ í•´ì•¼í•˜ëŠ” ë©”ì„œë“œëŠ” selfë¥¼ ì‚¬ìš©í•¨\n    self.size +=5\ncookie1 = cookie(\"banana\",25)\ncookie2 = cookie(\"melon\",10)\nprint(\"ì¿ í‚¤1\",id(cookie1))\nprint(\"ì¿ í‚¤2\",id(cookie2))\n#selfë¼ëŠ” ê°ì²´ì˜ ë©”ëª¨ë¦¬ì£¼ì†Œë¥¼ ê°€ë¦¬ì¼œë³´ë‹ˆ ë³€ìˆ˜cookie1,cookie2ê°€ ê°€ë¦¬í‚¤ëŠ” ê°ì²´ì˜ ë©”ëª¨ë¦¬ì£¼ì†Œì™€ ê°™ë‹¤\n#ì¦‰,selfëŠ” ê°ì²´(ì¸ìŠ¤í„´ìŠ¤) ê·¸ ìì²´ë¥¼ ê°€ë¦¬í‚¨ë‹¤!\n\në©”ëª¨ë¦¬ ì£¼ì†Œ : 140609086555024\në©”ëª¨ë¦¬ ì£¼ì†Œ : 140609086554896\nì¿ í‚¤1 140609086555024\nì¿ í‚¤2 140609086554896\n\n\n\nprint(cookie1.maked_num,cookie1.flavor,cookie1.size)\nprint(cookie2.maked_num,cookie2.flavor,cookie2.size)\n\n0 banana 25\n0 melon 10\n\n\n\ncookie1.__dict__,cookie2.__dict__ #cookie1,cookie2ë¼ëŠ” ê°ì²´(ì¸ìŠ¤í„´ìŠ¤)ì˜ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ í™•ì¸\n#ì„œë¡œë‹¤ë¥¸ ë…ë¦½ì ì¸ ë„¤ì„ìŠ¤í˜ì´ìŠ¤ë¥¼ ê°€ì§€ëŠ” ê²ƒì„ í™•ì¸\n\n({'flavor': 'banana', 'size': 25}, {'flavor': 'melon', 'size': 10})\n\n\n\ncookie.sizeup(cookie1);print(cookie1.size)\n#==cookie1.sizeup()\n\n30\n\n\n\nclass cookie_inform():\n  #í´ë˜ìŠ¤ ë³€ìˆ˜ ì„ ì–¸\n  making_time = \"1345\"\n  price =2000\n  maked_num = 0\n  def making_up(maked_num):\n    maked_num+=1\ncookie_inform.making_up(cookie_inform.maked_num)\ncookie_inform.maked_num\n\n0\n\n\n\ncookie1.__dict__\n\n{'flavor': 'banana', 'size': 25, 'd': 'hi'}"
  },
  {
    "objectID": "posts/python data structure and algorithm/CP1/CP1-1 Programming and Execution Environment.html",
    "href": "posts/python data structure and algorithm/CP1/CP1-1 Programming and Execution Environment.html",
    "title": "[Algorithm & Datastructure]1-1.Programming and Execution Environment",
    "section": "",
    "text": "Programming and DS&A\n\ní”„ë¡œê·¸ë ˆë°,Data structure(DS), Algorithm(A)ì€ ì„œë¡œë‹¤ë¥¸ ê³„ì¸µì— ìœ„ì¹˜í•˜ì§€ë§Œ ìƒë‹¹í•œ ì—°ê´€ì„±ì„ ê°€ì§„ë‹¤.\nê±´ì¶•ê°€ëŠ” ê±´ë¬¼ì„ ì§“ê¸° ìœ„í•´ êµ¬ì¡°,ë™ì‘ì„ ê°ì•ˆí•œ ì„¤ê³„ë„ë¥¼ ë§Œë“¤ê³  ë„êµ¬,ì¥ë¹„ë¥¼ ì‚¬ìš©í•˜ì—¬ êµ¬í˜„,í˜„ì‹¤í™” í•œë‹¤.\nê°œë°œìì˜ ì…ì¥ì—ì„œ êµ¬í˜„ì€ í”„ë¡œê·¸ë˜ë°ì´ë©° ì„¤ê³„ëŠ” UMLì´ë¼ëŠ” í‘œì¤€ì ì¸ í˜•ì‹ì„ ì‚¬ìš©í•˜ì§€ë§Œ ê·¸ ì•ˆì—ì„œ ìë£Œêµ¬ì¡°ì™€ ì•Œê³ ë¦¬ì¦˜ì´ íŠ¹íˆ ì¤‘ìš”í•˜ë‹¤.\nì„¤ê³„,êµ¬í˜„ ë‘˜ ë‹¤ ì¤‘ìš”í•˜ë‹¤.\n\n\n\nPython\n\nì¸í„°í”„ë¦¬í„° ì–¸ì–´\nê°ì²´ì§€í–¥\nDynamic type of variables\n\níŒŒì´ì¬ì€ Dynamic Type Languageë¡œ ëŸ°íƒ€ì„ì—ì„œ ë³€ìˆ˜ì˜ typeì´ ê²°ì •ëœë‹¤. ì‰½ê²Œ ë§í•˜ë©´ ë³€ìˆ˜ì˜ typeì„ ë¯¸ë¦¬ ì •í•´ì£¼ì§€ ì•Šê³  ì‹¤í–‰ë ë•Œë§ˆë‹¤ ê·¸ë•Œê·¸ë•Œ íƒ€ì…ì´ dynamicí•˜ê²Œ ë³€í•œë‹¤.\nìë°”ëŠ” Static Type Languageë¡œ ì»´íŒŒì¼íƒ€ì„ì—ì„œ ë³€ìˆ˜ì˜ typeì´ ê²°ì •ëœë‹¤. ì‰½ê²Œ ë³€ìˆ˜ì˜ íƒ€ì…ì„ ë¯¸ë¦¬ ì§€ì •í•´ì¤˜ì•¼ í•œë‹¤.\nì¸í„°í”„ë¦¬í„° ì–¸ì–´ì˜ íŠ¹ì§• 1\n\në¹ ë¥¸ ê°œë°œ ì†ë„,ëŠë¦° ì‹¤í–‰ ì†ë„(ì¸í„°í”„ë¦¬í„°ì–¸ì–´ì˜ íŠ¹ì§• 2)\në…íŠ¹í•œ ì½”ë“œ êµ¬ì¡°ë¥¼ ì§€ë‹Œë‹¤.(ì¢‹ì€ ë°©í–¥ìœ¼ë¡œ)\në°ì´í„° ë¶„ì„ì— íŠ¹í™”ë˜ì–´ ìˆë‹¤."
  },
  {
    "objectID": "posts/python data structure and algorithm/CP1/CP1-2 Procedure-oriented program.html",
    "href": "posts/python data structure and algorithm/CP1/CP1-2 Procedure-oriented program.html",
    "title": "[Algorithm & Datastructure]1-2.Procedure-oriented program vs Object-oriented program",
    "section": "",
    "text": "Procedure-oriented program\n\nì ˆì°¨ì§€í–¥í”„ë¡œê·¸ë¨ì´ë¼ í•˜ë©° í•¨ìˆ˜ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë§ì€ ë¶€ë¶„ì´ êµ¬í˜„ëœë‹¤.\n\në©”ì¸í•¨ìˆ˜ê°€ ì¡´ì¬í•œë‹¤.\n\ní¬ê²Œ ë‘ ê°€ì§€ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì§„ë‹¤.\n\nDefinition part(ì •ì˜,ì„ ì–¸)\nExecution part(ì‹¤í–‰)\n\n\n\n# Definition part(ì •ì˜í•´ë´)\ndef main(): \n    print(\"Hello, world\")\n    print(\"This program computes the average of two number\")\n\n    n1 = int(input(\"Input first number\"))\n    n2 = int(input(\"Second number\"))\n    average = (n1+n2) / 2\n    print(\"The average of the scores is :\",average)\n\n# Execution part(ì‹¤í–‰í•´ë´)\nmain()\n\nHello, world\nThis program computes the average of two number\nThe average of the scores is : 3.0\n\n\n\ndefì´ë¼ëŠ” keywordë¥¼ í•¨ìˆ˜ë¥¼ ì •ì˜í• ë•Œ ì‚¬ìš©í•œë‹¤.\ní•¨ìˆ˜ë‚´ë¶€ì˜ blockì€ :ë¡œ ì‹œì‘í•˜ë©° indentation(ë“¤ì—¬ì“°ê¸°)ì„ ì‚¬ìš©í•˜ì—¬ ë‹¤ë¥¸ blockê³¼ êµ¬ë¶„í•œë‹¤.\n\nprint(â€œhello, worldâ€) ~ print(â€œThe average of â€¦.ê¹Œì§€ê°€ mainì— ì†í•˜ëŠ” í•˜ë‚˜ì˜ ë¸”ë¡ì´ëœë‹¤.\nmain()ë¶€í„°ëŠ” ë‹¤ë¥¸ ë¸”ë¡ì´ë‹¤.\në”°ë¼ì„œ ìœ„ì˜ ì½”ë“œì—ì„œëŠ” ì´2ê°œì˜ ë¸”ë¡ì´ ì¡´ì¬í•œë‹¤.\n\ndefinitionì€ ì´ëŸ°ê²Œ ìˆë‹¤~ë¼ê³  ì»´í“¨í„°ì—ê²Œ ì•Œë ¤ì£¼ëŠ” ì—­í• ì´ë‹¤.\nexecutionì„ ìœ„í•´ main()ì„ ì…ë ¥í•´ì¤˜ì•¼ í•œë‹¤. ì…ë ¥í•´ì£¼ë©´ í•¨ìˆ˜ê°€ ì‹¤í–‰ë˜ë©´ì„œ í”„ë¡œê·¸ë¨ì´ ëŒì•„ê°„ë‹¤.\n\n\n\nObject-oriented program\n\nê°ì²´ì§€í–¥í”„ë¡œê·¸ë¨ì´ë¼ í•˜ë©° í•¨ìˆ˜ì™¸ì—ë„ í´ë˜ìŠ¤,ì¸ìŠ¤í„´ìŠ¤(ê°ì²´)ê¸°ë°˜ìœ¼ë¡œ ë§ì€ ë¶€ë¶„ì´ êµ¬í˜„ëœë‹¤.\n\në©”ì¸í•¨ìˆ˜ê°€ ì¡´ì¬í•œë‹¤.\n\ní¬ê²Œ ë‘ ê°€ì§€ ë¶€ë¶„ìœ¼ë¡œ ë‚˜ëˆ„ì–´ ì§„ë‹¤.\n\nDefinition part(ì •ì˜,ì„ ì–¸)\nExecution part(ì‹¤í–‰)\n\n\n\nclass Helloworld:\n    def __init__(self):\n        print(\"Hello World! Just one more time\")\n    def __del__(self):\n        print(\"Good bye!\")\n    def performAverage(self,val1,val2):\n        average = (val1 + val2) / 2.0\n        print(\"The average of the score is :\",average)\ndef main():\n    world = Helloworld()\n    score1 = int(input(\"Input first score\"))\n    score2 = int(input(\"Input second score\"))\n    world.performAverage(score1,score2)\nmain()\n\nHello World! Just one more time\nThe average of the score is : 6.0\nGood bye!\n\n\n\nê°ì²´ì§€í–¥ì—ì„œëŠ” ê¸°ë³¸ì ìœ¼ë¡œ í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•œë‹¤.\nclassë¼ëŠ” keywordëŠ” í´ë˜ìŠ¤ë¥¼ ì •ì˜í•˜ê¸° ìœ„í•´ì„œ ì‚¬ìš©í•œë‹¤.\ní•¨ìˆ˜ì™€ ë§ˆì°¬ê°€ì§€ë¡œ í´ë˜ìŠ¤ë‚´ë¶€ì˜ ë¸”ë¡ì€ :ë¡œ ì‹œì‘í•˜ë©° ë“¤ì—¬ì“°ê¸°ë¥¼ í†µí•´ì„œ í´ë˜ìŠ¤ì˜ ë©¤ë²„ë³€ìˆ˜(attribute),ë©¤ë²„í•¨ìˆ˜(method)ë¥¼ ì •ì˜í•œë‹¤.(í•¨ìˆ˜ì—ì„œëŠ” ì‹¤í–‰í•  êµ¬ë¬¸ì„ blockì— ì ì—ˆë‹¤.)\nHellworld()ë¥¼ í˜¸ì¶œí•˜ì—¬ instaneë¥¼ ìƒì„±í•˜ê³  worldì— ì €ì¥í•œë‹¤.\n\nHelloworld()ëŠ” í´ë˜ìŠ¤ë¥¼ í˜¸ì¶œí•˜ì—¬ instanceë¥¼ ìƒì„±í•˜ëŠ” ê²ƒì´ë‹¤.ì´ëŠ” ë¹µí‹€ì„ ì‚¬ìš©í•˜ì—¬ ì‹¤ì œë¡œ ë¹µì„ ë§Œë“¤ì–´ë‚´ëŠ” ê²ƒì´ë¼ í•  ìˆ˜ ìˆë‹¤.(ì‹¤ì²´ê°€ ìˆëŠ” ë­”ê°€ë¥¼ ë§Œë“ ë‹¤.)\ninstanceëŠ” ë©”ëª¨ë¦¬ìƒì˜ ì–´ë–¤ ìœ„ì¹˜ì— ì €ì¥ëœë‹¤. worldëŠ” instanceë¥¼ ì €ì¥í•˜ê³ ,ê°€ë¦¬í‚¤ê³  ìˆëŠ” variableì´ë‹¤.(í¬ì¸í„° ëŠë‚Œì´ë‹¤.)\n\në§ˆì°¬ê°€ì§€ë¡œ main()ì„ í†µí•´ì„œ ì‹¤í–‰í•˜ëŠ” Execution partê°€ ì¡´ì¬í•œë‹¤.\nì´ì™¸ í´ë˜ìŠ¤ì— ëŒ€í•œ self,__init__,__del__ì— ëŒ€í•œ ì„¤ëª…ì€ ì°¨í›„ ê°•ì˜ì—ì„œ ë“¤ì–´ë³´ì~\n\n\n\nì •ë¦¬\n\nProcedure-oriented programì€ functionìœ„ì£¼ë¡œ êµ¬í˜„ë˜ë©° Object-oriented programì€ ì´ì™€ ë”ë¶ˆì–´ class,instanceì™€ í•¨ê»˜ ì‚¬ìš©í•œë‹¤.\nclassëŠ” instanceë¥¼ ì‹¤ì²´í™” í•  ìˆ˜ ìˆëŠ” í…œí”Œë¦¿ì´ë‹¤. ì¦‰, ê°ì²´ë¥¼ ìƒì„±í•œë‹¤.\n\ní‘œí˜„ì´ í—·ê°ˆë¦¬ëŠ”ë° classëŠ” instanceì™€ ì§ìœ¼ë¡œ ê°ì²´ëŠ” ë‹¨ë…ìœ¼ë¡œ ì˜¬ ë•Œê°€ ë§ë‹¤.\nex) classì˜ instanceë¥¼ ìƒì„±í•œë‹¤. ê°ì²´ë¥¼ ìƒì„±í•œë‹¤.\nê·¼ë° ê·¸ëƒ¥ ë§‰ í˜¼ìš©í•´ì„œ ì“´ë‹¤.\nex) ì¸ìŠ¤í„´ìŠ¤ê°€ ìƒì„±ë˜ì—ˆë‹¤.\n\nObject oriented programì€ ê°ì²´ë¥¼ ë¨¼ì € ìƒì„±í•˜ê³  ì´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ êµ¬í˜„í•œë‹¤."
  },
  {
    "objectID": "posts/python data structure and algorithm/CP1/CP1-3 Naming, Styling and Comments.html",
    "href": "posts/python data structure and algorithm/CP1/CP1-3 Naming, Styling and Comments.html",
    "title": "[Algorithm & Datastructure]1-2. Naming, Styling and Comments",
    "section": "",
    "text": "Naming and Styling\n\nNaming: ì˜ë¯¸ë¥¼ í™•ì‹¤í•˜ê²Œ ì „ë‹¬í•´ì•¼ í•¨\n\ncamel casingì„ ì‚¬ìš©í•¨.\n\nclass name : í´ë˜ìŠ¤ë¡œ í‘œí˜„ë˜ëŠ” ê°œë…ì— ëŒ€í•œ ëª…ì‚¬ë¡œ\n\nê° ë‹¨ì–´ì˜ ì²« ê¸€ìëŠ” ëŒ€ë¬¸ìë¡œ\ne.g class MyFirstClass\n\nvariable name : ì €ì¥ë˜ëŠ” ê°œë…ì— ëŒ€í•œ ëª…ì‚¬ë¡œ\n\nì†Œë¬¸ìë¡œ ì‹œì‘\ne.g numberOfStudents = 100\nintCountì™€ ê°™ì€ ì´ë¦„ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ(íŒŒì´ì¬ì€ ë™ì  ë³€ìˆ˜ ìë£Œí˜•ì„ ì§€ì›í•˜ê¸° ë•Œë¬¸)\n\nmethod name : ë©”ì†Œë“œê°€ í•˜ëŠ” í–‰ë™ì— ëŒ€í•œ ë™ì‚¬ë¡œ\n\nì†Œë¬¸ìë¡œ ì‹œì‘\ne.g def performAverage(self,val1,val2)\n\n\n\nIndentation(ë“¤ì—¬ì“°ê¸°)\n\n4ì¹¸ì˜ ê³µë°±ì„ ê° levelì— ì‚¬ìš©\n\n\n\n\nComments\n\nì½”ë“œì— ëŒ€í•œ ì„¤ëª…ì„ ë‹¬ê¸°\nì—¬ëŸ¬ì¤„ì€ ``` or â€œâ€œâ€œì„ ì‚¬ìš©\n\n'''\ncreated on 2023-04-24\n\nauthor : hoyeon\n'''\n\n\"\"\"\ncreated on 2023-04-24\n\nauthor : hoyeon\n\"\"\"\ní•œ ì¤„ì€ #ì„ ì‚¬ìš©\n# printë¬¸ ì¶œë ¥í•˜ê¸°\nprint(\"hello\")"
  },
  {
    "objectID": "posts/python data structure and algorithm/CP2/CP2.html",
    "href": "posts/python data structure and algorithm/CP2/CP2.html",
    "title": "Untitled",
    "section": "",
    "text": "ì¼ë°˜ì ìœ¼ë¡œ abstraction(ì¶”ìƒí™”)ë¼ëŠ” ìš©ì–´ëŠ” ëŒ€ìƒì˜ ëŠë‚Œ,í˜•íƒœì™€ ê°™ì€ ë‚´ì¬ë˜ì–´ ìˆëŠ” ëª¨ìŠµì— ëŒ€í•œ ê°„ë‹¨í•œ í‘œí˜„.\nAbstract Data type(ADT)ì´ë€?\n\nData structureê°€ ì–´ë–»ê²Œ ëŒì•„ê°€ëŠ”ì§€ì— ëŒ€í•´ ê°„ë‹¨íˆ,ì¶”ìƒì ìœ¼ë¡œ í‘œí˜„í•œ ê²ƒ.\nADTëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²ƒë“¤ì„ ê¸°ìˆ í•¨\n\nì–´ë–¤ ë°ì´í„°ê°€ ì €ì¥ë˜ëŠ”ê°€?(ì–´ë–¤ ë°ì´í„°ê°€ ìˆì–´?)\nì €ì¥ëœ ë°ì´í„°ëŠ” ì–´ë–¤ ì—°ì‚°ì´ ê°€ëŠ¥í•œ ê²ƒì¸ê°€?(ë°ì´í„°ë¡œ ë­˜ ê³„ì‚°í•  ìˆ˜ ìˆì–´?)\nì—°ì‚°ì—ì„œ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ì—ëŸ¬ì¡°ê±´ì€ ì–´ë–»ê²Œ ë˜ëŠ”ê°€?(ê³„ì‚°ì—ì„œ ì–´ë–¤ ì˜¤ë¥˜ê°€ ë‚˜ì˜¬ ìˆ˜ ìˆì–´?)\n\nStock trading system\n\nbuy,sellì— ëŒ€í•œ orderê°€ ì €ì¥ëœë‹¤.\nì§€ì›ë˜ëŠ” ì—°ì‚°ë“¤\n\norder buy(stock,shares,price)\norder sell(storck,shares,price)\nvoid cancel(order)\n\nError conditions:\n\nì¡´ì¬í•˜ì§€ ì•ŠëŠ” stockì— ëŒ€í•œ buy,sell\nì¡´ì¬í•˜ì§€ ì•ŠëŠ” orderë¥¼ ì·¨ì†Œ"
  },
  {
    "objectID": "posts/python data structure and algorithm/CP2/CP2.html#search",
    "href": "posts/python data structure and algorithm/CP2/CP2.html#search",
    "title": "Untitled",
    "section": "Search",
    "text": "Search\nArray xì—ì„œ dì™€ cë¥¼ ì°¾ëŠ”ë‹¤ê³  í•´ë³´ì.\n\nëŒ€ëµì ì¸ ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ì„ ê²ƒì´ë‹¤.\n\nì²˜ìŒë¶€í„° ëì— ìˆëŠ” ëª¨ë“  ìš”ì†Œë¥¼ í•˜ë‚˜í•˜ë‚˜ì”© ì ‘ê·¼í•œë‹¤.\nd or cë¥¼ ë§Œë‚˜ë©´ ì¢…ë£Œí•œë‹¤.\n\nd or cê°€ í¬í•¨ë˜ì–´ìˆì§€ ì•Šë‹¤ë©´? \\(\\to\\) N(ë¦¬ìŠ¤íŠ¸ì˜ ê¸¸ì´)ë²ˆì˜ retrievalsê°€ í•„ìš”\nd or cê°€ í¬í•¨ë˜ì–´ ìˆë‹¤ë©´? \\(\\to\\) ìµœëŒ€ Në²ˆì˜ retrievalsê°€ í•„ìš”\n\n\n\nretrievalì€ ì—°ì‚°(operation)ìœ¼ë¡œ ì¼ë‹¨ ì´í•´í•˜ì.(ìì„¸í•˜ê²Œ ë­”ì§€ëŠ” ëª¨ë¥´ê² ë‹¤.)\nëŒ€ëµ ëª‡ ë²ˆì •ë„ì˜ ì—°ì‚°ì´ ì¼ì–´ë‚˜ëŠ”ì§€ë¥¼ ì˜ë¯¸í•œë‹¤.\n\n\n# c\nx = ['a','b','d','e','f']\n\nfor itr in range(0,len(x)):\n    if \"c\" == x[itr]:\n        print(True)\n        break\n    else:\n        pass\n\n\n# d\nx = ['a','b','d','e','f']\n\nfor itr in range(0,len(x)):\n    if \"d\" == x[itr]:\n        print(True)\n        break\n    else:\n        pass\n\nTrue"
  },
  {
    "objectID": "posts/python data structure and algorithm/CP2/CP2.html#insert",
    "href": "posts/python data structure and algorithm/CP2/CP2.html#insert",
    "title": "Untitled",
    "section": "Insert",
    "text": "Insert\nArray xì˜ ì›ì†Œì¸ bì™€ dì‚¬ì´ì— cë¥¼ ë„£ìœ¼ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼í• ê¹Œ?\n\nëŒ€ëµì ì¸ ê³¼ì •ì€?\n\nìƒˆë¡œìš´ ë¦¬ìŠ¤íŠ¸ yë¥¼ ë§Œë“ ë‹¤.\n\nê¸°ì¡´ë¦¬ìŠ¤íŠ¸ë³´ë‹¤ ê¸¸ì´ê°€ 1ë§Œí¼ ë” ê¸¸ë‹¤\n\nx[0:a-1]ê¹Œì§€ y[0:a-1]ì— ìˆëŠ” referenceë¥¼ ë³µì‚¬í•œë‹¤. (retrieval cnt : a)\n\naëŠ” bì™€ dì‚¬ì´ì˜ indexë¥¼ ì˜ë¯¸í•œë‹¤.\n0:a-1ê¹Œì§€ yê°€ xì˜ referenceë¥¼ ë³µì‚¬í–ˆìœ¼ë¯€ë¡œ ê°™ì€ objectë¥¼ ê°€ë¦¬í‚¤ê³  ìˆë‹¤.\në¦¬ìŠ¤íŠ¸ëŠ” ê°ì²´ì— ëŒ€í•œ referenceë¥¼ ê°€ë¦¬í‚¤ê³  ìˆëŠ” êµ¬ì¡°ì„ì„ ìœ ì˜í•˜ì.\n\ncì˜ ë ˆí¼ëŸ°ìŠ¤ë¥¼ y[a]ì— ë„£ëŠ”ë‹¤. (retrieval cnt : 1)\në‚˜ë¨¸ì§€ ì›ì†Œ d,e,fì— ëŒ€í•œ ë˜í¼ëŸ°ìŠ¤ë¥¼ ë³µì‚¬í•œë‹¤. ì¦‰ x[a:]ë¥¼ y[a+1:]ì— ë³µì‚¬í•œë‹¤.\në§ˆì§€ë§‰ì— xì˜ referenceë¥¼ yì˜ referenceë¡œ ë°”ê¾¼ë‹¤.(ìš°ë¦¬ëŠ” xì— ë„£ëŠ” ê²ƒì„ ì›í–ˆë‹¤.)\n\n\nì´ në²ˆì˜ retrievalsê°€ í•„ìš”í•˜ë‹¤."
  },
  {
    "objectID": "posts/RL/A simple bandit algorithm.html",
    "href": "posts/RL/A simple bandit algorithm.html",
    "title": "[ê°•í™”í•™ìŠµ] Simple Bandit Algorithm Implementation",
    "section": "",
    "text": "Import\n\nimport numpy as np\nfrom scipy.stats import bernoulli,norm\nimport random\n\n\n\n Implementation\n\ndef argmax(q_func):\n    q_max = max(q_func.values())\n    max_actions = []\n    for action in q_func.keys():\n        if q_func[action] == q_max:\n            max_actions.append(action)\n    argmax_q = random.sample(max_actions,1)\n    return argmax_q[0]\n\n\ndef bandit(q_star):\n    #ê° banditì— í•´ë‹¹í•˜ëŠ” ì •ê·œë¶„í¬ì—ì„œ ìƒ˜í”Œí•˜ë‚˜ ê°€ì ¸ì˜¤ê¸°\n    sampled_reward = norm.rvs(loc=q_star,scale=1,size=1)\n    return sampled_reward[0]\n\n\ndef simple_bandit(num_actions, #actionì˜ ê°¯ìˆ˜\n                  epsilon,     #greedyí•˜ê²Œ ì•ˆ ì›€ì§ì¼ í™•ë¥ \n                  q_stars, #ê° action(kê°œì˜ bandit)ì— ëŒ€í•œ rewardì˜ (ì •ê·œ)ë¶„í¬ì˜ mean\n                  terminate_cond\n                  ):\n    if num_actions != len(q_stars):\n        print(\"actionì˜ ê°¯ìˆ˜ì™€ bandit_meanì˜ ê°¯ìˆ˜ëŠ” ê°™ì•„ì•¼ í•¨\")\n        return \n    Q = {}\n    n = {}\n\n    #Initialize,for a=1 to k\n    for i in range(1,num_actions+1):\n        Q[i] = 0\n        n[i] = 0\n    \n    #set probability,reward distribution(bandit)\n    max_prob = 1-epsilon\n\n    #epsilon-greedy action\n    while True:\n\n        #epsilon-greedy action\n        #1.greedy action? or random action?\n        greedy_or_random = bernoulli.rvs(p=max_prob,size=1)\n        #2.select action\n        if greedy_or_random == 1: \n            action = argmax(Q)\n        else:\n            action = random.sample(Q.keys(),1)[0]\n\n        #sampling reward from gaussian(mean = q_star,variance = 1)\n        q_star = q_stars[action-1]\n        reward = bandit(q_star) #sampling\n\n        #incremental Q-update\n        n[action] +=1\n        Q[action] = Q[action] + (1/n[action]) * (reward - Q[action])\n\n        #Terminating\n        if np.max(q_stars - np.array(list(Q.values()))) <= terminate_cond:\n            break\n    return n,Q\n\n\nnum_actions=10\nq_stars = norm.rvs(loc=0,scale=1,size=num_actions)\nnum_action,Q_estimated = simple_bandit(num_actions=num_actions,epsilon=0.1,q_stars=q_stars,terminate_cond=0.01)\nfor i in range(1,num_actions+1):\n    print(\"action :\",i)    \n    print(\"taken_num\",num_action[i])\n    print(\"q_star :\",q_stars[i-1])\n    print(\"Q_estimated\",Q_estimated[i]) \n    print(\"=============================================\")\n\nC:\\Users\\22668\\AppData\\Local\\Temp\\ipykernel_5296\\2459308436.py:30: DeprecationWarning: Sampling from a set deprecated\nsince Python 3.9 and will be removed in a subsequent version.\n  action = random.sample(Q.keys(),1)[0]\n\n\naction : 1\ntaken_num 7710\nq_star : 1.0622402967553395\nQ_estimated 1.0780006731009761\n=============================================\naction : 2\ntaken_num 7750\nq_star : -0.04393405775378968\nQ_estimated -0.0464016778867334\n=============================================\naction : 3\ntaken_num 7688\nq_star : -0.2977471187595547\nQ_estimated -0.28941229886239545\n=============================================\naction : 4\ntaken_num 7766\nq_star : -0.6942140347822109\nQ_estimated -0.6957038264269383\n=============================================\naction : 5\ntaken_num 702767\nq_star : 1.114864011478195\nQ_estimated 1.1163009140188438\n=============================================\naction : 6\ntaken_num 7695\nq_star : -2.0791842661153535\nQ_estimated -2.0891168451782134\n=============================================\naction : 7\ntaken_num 7704\nq_star : -1.204190113360393\nQ_estimated -1.2086679112146015\n=============================================\naction : 8\ntaken_num 7754\nq_star : 0.9093721477182354\nQ_estimated 0.908951189305117\n=============================================\naction : 9\ntaken_num 7658\nq_star : 0.0790709385366291\nQ_estimated 0.08704136534792037\n=============================================\naction : 10\ntaken_num 7736\nq_star : -0.06424396205536466\nQ_estimated -0.05355515687875377\n============================================="
  },
  {
    "objectID": "posts/RL/Bellman Equation(ë²ˆì™¸).html",
    "href": "posts/RL/Bellman Equation(ë²ˆì™¸).html",
    "title": "Bellman Equation",
    "section": "",
    "text": "\\[\\begin{aligned}\nv_{\\pi}(s) &= \\mathbb{E}[G_t|S_t = s]\\\\\n&= \\sum_{g_t}p(g_t|s)g_t \\\\\n&= \\sum_{g_t}\\sum_{a}p(g_t,a|s)g_t\\\\\n&= \\sum_{g_t}\\sum_{a}p(g_t|s,a)p(a|s)g_t\\\\\n&= \\sum_{a}p(a|s)\\sum_{g_t}p(g_t|s,a)g_t \\\\\n&= \\sum_{a}Pr(A_t=a|S_t=s)\\mathbb{E}[G_t|S_t=s,A_t=a] \\\\\n&= \\sum_{a}\\pi(a|s)q_{\\pi}(s,a)\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/RL/Bellman Equation(ë²ˆì™¸).html#optimal-state-value-function",
    "href": "posts/RL/Bellman Equation(ë²ˆì™¸).html#optimal-state-value-function",
    "title": "Bellman Equation",
    "section": "Optimal state value function",
    "text": "Optimal state value function\n\\[\\begin{aligned}\nv_{*}(s) = \\underset{\\pi}{\\text{max}}\\,v_{\\pi}(s) , \\forall s \\in S\n\\end{aligned}\\]\nê°ê°ì˜ ëª¨ë“  stateì—ì„œ ëª¨ë“  policyë¥¼ ê³ ë ¤í–ˆì„ë•Œ state-value functionì˜ maxê°’ì„ í•¨ìˆ«ê°’ìœ¼ë¡œ ê°€ì§€ëŠ” í•¨ìˆ˜ì´ë‹¤. ì´ë•Œ í•¨ìˆ«ê°’(maxê°’)ì€ optimal policyì— ì˜í•œ ê°’ì´ë‹¤."
  },
  {
    "objectID": "posts/RL/Bellman Equation(ë²ˆì™¸).html#optimal-state-value-function-1",
    "href": "posts/RL/Bellman Equation(ë²ˆì™¸).html#optimal-state-value-function-1",
    "title": "Bellman Equation",
    "section": "Optimal state value function",
    "text": "Optimal state value function\n\\[\\begin{aligned}\nq_{*}(s,a) = \\underset{\\pi}{\\text{max}}\\,q_{\\pi}(s,a),\\, \\forall s \\in S,\\forall a \\in A(s)\n\\end{aligned}\\]\nê°ê°ì˜ ëª¨ë“  state-action pairì—ì„œ ëª¨ë“  policyë¥¼ ê³ ë ¤í–ˆì„ë•Œ action-value functionì˜ maxê°’ì„ í•¨ìˆ«ê°’ìœ¼ë¡œ ê°€ì§€ëŠ” í•¨ìˆ˜. ì´ë•Œ í•¨ìˆ«ê°’(maxê°’)ì€ ë§ˆì°¬ê°€ì§€ë¡œ optimal policyì— ì˜í•œ ê°’ì´ë‹¤."
  },
  {
    "objectID": "posts/RL/Bellman Equation(ë²ˆì™¸).html#optimal-policy",
    "href": "posts/RL/Bellman Equation(ë²ˆì™¸).html#optimal-policy",
    "title": "Bellman Equation",
    "section": "Optimal policy",
    "text": "Optimal policy\n\nIf \\(v_{\\pi}(s) \\geq v_{\\pi'}(s)\\) for all \\(s \\in S\\) then we say \\(\\pi\\) is better than or equal to \\(\\pi'\\).\nThere is always at least one policy that is better than or equal to all othere policies => ë‹¤ë¥¸ ëª¨ë“  ì •ì±…ë“¤ê³¼ ë¹„êµí–ˆì„ë•Œ ëª¨ë“  stateì—ì„œ value ê°™ê±°ë‚˜ ë” ë‚˜ì€ policyëŠ” ìµœì†Œ í•œ ê°œ ì´ìƒ ì¡´ì¬í•œë‹¤. ì´ëŸ¬í•œ policyë“¤ì„ ëª¨ë‘ optimal policyë¼ê³  í•˜ë©° \\(\\pi_*\\) ë¡œ í‘œê¸°í•œë‹¤.\noptimal policyëŠ” ì—¬ëŸ¬ê°œì¼ ìˆ˜ ìˆë‹¤.\noptimal policyë“¤ì€ ëª¨ë‘ ë™ì¼í•œ optimal state value functionê³¼ optimal action value functionì„ ê³µìœ í•œë‹¤.\n\n\\[\\begin{aligned}\np(a|s) =\n\\begin{cases}\n1,\\quad a = \\text{argmax}_{a\\in A(s)}q_*(s,a) \\\\\n0,\\quad\\text{else}\n\\end{cases}\n\\end{aligned}\\]\noptimal policy \\(\\pi_*\\)ëŠ” state-value functionì´ ëª¨ë“  sì—ì„œ ë‹¤ë¥¸ ëª¨ë“  policyë“¤ ë³´ë‹¤ ë†’ê±°ë‚˜ ê°™ì€ ê°’ì„ ê°€ì§€ëŠ” í•¨ìˆ˜ì´ë‹¤. ì¦‰,ë‹¤ìŒê³¼ ê°™ë‹¤.\nstate-value functionì€ action-value functionìœ¼ë¡œ í‘œí˜„í•˜ëŠ” Bellman equationì— ì˜í•´ ì•„ë˜ì²˜ëŸ¼ í‘œí˜„í•  ìˆ˜ ìˆì—ˆë‹¤.\n\\[\\begin{aligned}\nv_{\\pi}(s) &= \\sum_{a}\\pi(a|s)q_{\\pi}(s,a)\\\\\n&= \\sum_{a}p(a|s)q_{\\pi}(s,a)\n\\end{aligned}\\]\n\\(v_{\\pi}(s)\\)ë¥¼ maximizeí•˜ëŠ” policyëŠ” ì–´ë–»ê²Œ êµ¬í•´ì•¼ í• ê¹Œ? ë¨¼ì € optimal action value functionì„ êµ¬í–ˆë‹¤ê³  ê°€ì •í•´ë³´ì.(ì‹¤ì œë¡œ ë‚˜ì¤‘ì— ì´ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì´ ë‚˜ì˜¨ë‹¤.)\noptimal action value functionì„ êµ¬í–ˆë‹¤ëŠ” ê²ƒì€ ë­˜ê¹Œ? optimal action value functionì€ agentê°€ ì–´ë–¤ state,action pairë¥¼ ì„ íƒí–ˆì„ë•Œ ì—¬ëŸ¬ê°€ì§€ policyë¥¼ ë‹¤ ê³ ë ¤í•´ë´ì„œ return(ë¯¸ë˜ì— ë°›ì„ rewardì˜ ì´í•©)ì˜ ìµœëŒ“ê°’ì„ ëŒë ¤ì¤€ë‹¤. ì¦‰, agentê°€ stateì—ì„œ actionì„ ì„ íƒí–ˆì„ ë•Œ ê°€ì¥ ë§ì´ ë°›ì„ ìˆ˜ ìˆëŠ” returnì´ë‹¤.\nì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ì. ì„ì˜ì˜ state \\(s\\)ì—ì„œ ê°€ëŠ¥í•œ actionì˜ ì§‘í•©ì„ \\(A(s)= \\{a_1,a_2,a_3,a_4\\}\\)ì´ê³  optimal action value functionì˜ ê°’ì´ ë‹¤ìŒê³¼ ê°™ë‹¤ê³  í•˜ì.\n\\[\\begin{aligned}\n&q_{*}(s,a_1) = 4 \\\\\n&q_{*}(s,a_2) = -2 \\\\\n&q_{*}(s,a_3) = 2 \\\\\n&q_{*}(s,a_4) = 9 \\\\\n\\end{aligned}\\]\na1ì´ë¼ëŠ” í–‰ë™ì„ í•˜ë©´ ê°€ì¥ ë§ì´ returnì„ ë°›ì•„ë´¤ì 4ì´ê³  a2í–‰ë™ì„ í•˜ë©´ ê°€ì¥ ë§ì´ ë°›ì•„ë´¤ì returnì´ -2ë¡œ ë” ì‘ì„ê²ƒì´ë‹¤. a4ë¥¼ ì„ íƒí–ˆì„ë•Œì—ëŠ” ê°€ì¥ë§ì´ ë°›ìœ¼ë©´ returnì´ 9ì´ë¯€ë¡œ a4ë¥¼ s4ì—ì„œì˜ actionìœ¼ë¡œ ì·¨í•˜ë©´ ê°€ì¥ í•©ë¦¬ì ì¼ ê²ƒì´ë‹¤.\n\\(v_{\\pi}(s)\\)ë¥¼ maximizeí•˜ëŠ” policyëŠ” ì–´ë–»ê²Œ êµ¬í•´ì•¼ í• ê¹Œ? ë¨¼ì € state value functionìì²´ê°€ action value functionë¶€í„° maximizeê°€ ë˜ì–´ ìˆì–´ì•¼ ë¨¼ì € optimal action value functionì„ êµ¬í–ˆë‹¤ê³  ê°€ì •í•´ë³´ì.(ì‹¤ì œë¡œ ë‚˜ì¤‘ì— ì´ë¥¼ ì¶”ì •í•˜ëŠ” ë°©ë²•ì´ ë‚˜ì˜¨ë‹¤.)\nëª¨ë“  \\(\\pi\\)ë¥¼ ê³ ë ¤í–ˆì„ ë•Œ ìœ„ì˜ state-value functionì„ maximizeí•˜ëŠ” policyê°€ optimal policyì˜€ë‹¤. ë‹¤ì‹œì“°ë©´ ì•„ë˜ì™€ ê°™ë‹¤.\n\\[\\pi_* = \\underset{\\pi}{\\text{argmax}}\\,v_{\\pi}(s)\\,,\\forall s\\]"
  },
  {
    "objectID": "posts/RL/Chatper1 - Introduction/Introduction.html",
    "href": "posts/RL/Chatper1 - Introduction/Introduction.html",
    "title": "[ê°•í™”í•™ìŠµ] Introduction",
    "section": "",
    "text": "ìš°ë¦¬ëŠ” í™˜ê²½ê³¼ ìƒí˜¸ì‘ìš©(interacting)í•˜ë©´ì„œ ë°°ìš´ë‹¤ëŠ” ê²ƒì€ ìš°ë¦¬ê°€ í•™ìŠµì˜ ë³¸ì§ˆì— ëŒ€í•´ ìƒê°í• ë•Œ ê°€ì¥ ë¨¼ì € ë– ì˜¬ë¦´ ìˆ˜ ìˆëŠ” ìƒê°ì´ë‹¤.\nì´ ì±…ì—ì„œëŠ” ìƒí˜¸ì‘ìš©ìœ¼ë¡œë¶€í„° ë°°ìš°ê¸° ìœ„í•œ computational approachë¥¼ ë°°ìš´ë‹¤\nìš°ë¦¬ê°€ ë°°ìš¸ ì ‘ê·¼ë°©ì‹ ì¦‰, reinforcement learningì€ ê¸°ê³„í•™ìŠµì˜ ë‹¤ë¥¸ ì ‘ê·¼ë²•ë“¤ë³´ë‹¤ í›¨ì”¬ ëª©í‘œì§€í–¥ì  í•™ìŠµì— ì¤‘ì ì„ ë‘”ë‹¤."
  },
  {
    "objectID": "posts/RL/Chatper1 - Introduction/Introduction.html#ê°•í™”í•™ìŠµì´ë€",
    "href": "posts/RL/Chatper1 - Introduction/Introduction.html#ê°•í™”í•™ìŠµì´ë€",
    "title": "[ê°•í™”í•™ìŠµ] Introduction",
    "section": " ê°•í™”í•™ìŠµì´ë€?",
    "text": "ê°•í™”í•™ìŠµì´ë€?\n\nê°•í™”í•™ìŠµì€ ì–´ë–¤ situationì—ì„œ ì–´ë–»ê²Œ actionì„ ëŒ€ì‘ì‹œí‚¬ì§€ë¥¼ ë°°ìš°ëŠ” í•™ìŠµì´ë‹¤.\nlearnerì—ê²Œ ì–´ë–¤ í–‰ë™ì„ ì·¨í•´ì•¼í• ì§€ ì•Œë ¤ì£¼ì§€ ì•Šìœ¼ë©° ë”°ë¼ì„œ learnerëŠ” ì–´ë–¤ actionì´ ê°€ì¥ ë§ì€ rewardë¥¼ ì£¼ëŠ”ì§€ ì—¬ëŸ¬ë²ˆ ì‹œë„í•˜ë©´ì„œ ë°œê²¬í•´ì•¼ í•©ë‹ˆë‹¤.\nëŒ€ë¶€ë¶„ì˜ ë¬¸ì œì—ì„œ actionì€ ì¦‰ê°ì ìœ¼ë¡œ ë°›ëŠ” reward ë¿ë§Œ ì•„ë‹ˆë¼ ë‹¤ìŒ situationê³¼ ë’¤ë”°ë¥´ëŠ” ëª¨ë“  í›„ì† rewardì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ì™€ ê°™ì€ ê°•í™”í•™ìŠµì˜ ë‘ ê°€ì§€ íŠ¹ì„± trial and error search ê·¸ë¦¬ê³  delayed rewardëŠ” ê°•í™”í•™ìŠµì˜ ê°€ì¥ ì¤‘ìš”í•˜ë©´ì„œ ì°¨ë³„ì ì¸ ë‘ ê°€ì§€ íŠ¹ì§•ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/RL/Chatper1 - Introduction/Introduction.html#ê°•í™”í•™ìŠµ-vs-ì§€ë„í•™ìŠµ",
    "href": "posts/RL/Chatper1 - Introduction/Introduction.html#ê°•í™”í•™ìŠµ-vs-ì§€ë„í•™ìŠµ",
    "title": "[ê°•í™”í•™ìŠµ] Introduction",
    "section": " ê°•í™”í•™ìŠµ vs ì§€ë„í•™ìŠµ",
    "text": "ê°•í™”í•™ìŠµ vs ì§€ë„í•™ìŠµ\n\nì§€ë„í•™ìŠµì€ labelingì´ ë˜ì–´ ìˆëŠ” example(ë˜ëŠ” observation)ë˜ëŠ” training setìœ¼ë¡œë¶€í„° í•™ìŠµí•©ë‹ˆë‹¤.\nê° exampleì€ situationê³¼ ê·¸ë•Œì˜ situationì— ëŒ€í•´ ì‹œìŠ¤í…œì´ ì·¨í•´ì•¼ í•˜ëŠ” ì˜¬ë°”ë¥¸ í–‰ë™(label)ì— ëŒ€í•œ labelì…ë‹ˆë‹¤.\nì§€ë„í•™ìŠµì˜ ëª©ì ì€ training setì— ì—†ì—ˆë˜ ìƒí™©ì—ì„œ ì˜¬ë°”ë¥´ê²Œ ë™ì‘í•˜ë„ë¡ extrapolate(ì¶”ë¡ )í•˜ê±°ë‚˜ generalization(ì¼ë°˜í™”)í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\nìƒí˜¸ì‘ìš©í•˜ëŠ” ë¬¸ì œì—ì„œ agentì—ê²Œ ë°”ë¼ê±°ë‚˜,í•´ì•¼í•˜ëŠ” í–‰ë™ì„ ì •í™•íˆ ëŒ€í‘œí•˜ëŠ” ì˜ˆì‹œë¥¼ ì–»ëŠ” ë‹¤ëŠ” ê²ƒì€ ë¹„í˜„ì‹¤ì ì…ë‹ˆë‹¤. (ì‰½ê²Œ ìƒê°í•´ë³´ìë©´ ë§í•˜ìë©´ ì •í™•íˆ ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–¤ í–‰ë™ì„ í•˜ë¼ê³  ì •í™•í•˜ê²Œ ì•Œë ¤ì£¼ëŠ” ê²ƒì€ ë¶ˆê°€ëŠ¥í•˜ë‹¤. ì•„ë§ˆë„ ìƒí™©ì´ ìˆ˜ì—†ì´ ë§ì„ ë¿ë”ëŸ¬ ì–´ë–¤ í–‰ë™ì´ ìµœì ì˜ ì„ íƒì¸ì§€ë„ ëª¨ë¥´ê¸° ë•Œë¬¸ì¸ ê²ƒ ê°™ë‹¤.)\në¯¸ì§€ì˜ ì˜ì—­(í•™ìŠµì´ ê°€ì¥ ë„ì›€ì´ ë  ê²ƒìœ¼ë¡œ ì˜ˆìƒë˜ëŠ”)ì—ì„œ ì—ì´ì „íŠ¸ëŠ” ìì‹ ì˜ ê²½í—˜ìœ¼ë¡œ ë¶€í„° ìŠ¤ìŠ¤ë¡œ ë°°ìš¸ ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/RL/Chatper1 - Introduction/Introduction.html#ê°•í™”í•™ìŠµ-vs-ë¹„ì§€ë„í•™ìŠµ",
    "href": "posts/RL/Chatper1 - Introduction/Introduction.html#ê°•í™”í•™ìŠµ-vs-ë¹„ì§€ë„í•™ìŠµ",
    "title": "[ê°•í™”í•™ìŠµ] Introduction",
    "section": " ê°•í™”í•™ìŠµ vs ë¹„ì§€ë„í•™ìŠµ",
    "text": "ê°•í™”í•™ìŠµ vs ë¹„ì§€ë„í•™ìŠµ\n\në¹„ì§€ë„í•™ìŠµì€ unlabeled dataì— ëŒ€í•œ hidden structureë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©ì ì…ë‹ˆë‹¤.\nê°•í™”í•™ìŠµì´ correct behaviorì— ëŒ€í•œ labeled dataë¥¼ ê°€ì§€ê³  ìˆì§€ ì•Šê¸° ë•Œë¬¸ì— ëˆ„êµ°ê°€ëŠ” ë¹„ì§€ë„ í•™ìŠµìœ¼ë¡œ ì°©ê°í•˜ê¸°ë„ í•˜ì§€ë§Œ ê°•í™”í•™ìŠµì˜ ëª©ì ì€ reward signalì„ ìµœëŒ€í™” í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë©° ë¹„ì§€ë„í•™ìŠµ ì²˜ëŸ¼ hidden structureë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©ì ì´ ì•„ë‹™ë‹ˆë‹¤.\në¬¼ë¡  hidden structureë¥¼ ì°¾ëŠ” ê²ƒì´ ê°•í™”í•™ìŠµì— ìœ ìš©í•  ìˆ˜ëŠ” ìˆì§€ë§Œ reward signalì„ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ ëª©ì ì´ë¼ëŠ” ê°•í™”í•™ìŠµì˜ ë¬¸ì œë¥¼ ì„¤ëª…í•˜ì§€ ëª»í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/RL/Chatper1 - Introduction/Introduction.html#exploration-vs-exploitation",
    "href": "posts/RL/Chatper1 - Introduction/Introduction.html#exploration-vs-exploitation",
    "title": "[ê°•í™”í•™ìŠµ] Introduction",
    "section": " exploration vs exploitation",
    "text": "exploration vs exploitation\n\nê°•í™”í•™ìŠµì—ë§Œ ì¡´ì¬í•˜ëŠ” í•œê°€ì§€ ë¬¸ì œëŠ” explorationê³¼ exploitationì‚¬ì´ì˜ trade-offì…ë‹ˆë‹¤.\nexploration,exploitationì€ ì˜ì‚¬ê²°ì •ë¬¸ì œì—ì„œ ê°€ëŠ¥í•œ ë‘ê°€ì§€ í–‰ë™ë°©ì‹ìœ¼ë¡œ ì„œë¡œê°„ì— ì¥ë‹¨ì ì„ ê°€ì§‘ë‹ˆë‹¤.\nexploitation\n\në” ë§ì€ rewardë¥¼ ì–»ê¸° ìœ„í•´ ì§€ê¸ˆê¹Œì§€ì˜ ë°ì´í„°ë¥¼ í†µí•´ optimalí•˜ë‹¤ê³  ì—¬ê²¨ì§€ëŠ” decisionì„ ì„ íƒí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.(ì•Œë ¤ì§„ ê²ƒì„ ê³„ì† ì—°êµ¬í•˜ëŠ” ê²ƒ)\në¦¬ì›Œë“œë¥¼ ìµœëŒ€í™”í•˜ê¸°ìœ„í•´ ì•Œë ¤ì§„ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ í–‰ë™í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\nexploration\n\nì§€ê¸ˆê¹Œì§€ ë°ì´í„°ë¥¼ í†µí•´ optimalí•˜ë‹¤ê³  ì—¬ê²¨ì§€ëŠ” decisionì„ ì„ íƒí•˜ì§€ ì•ŠëŠ” ê²ƒ ì…ë‹ˆë‹¤.\nenvironmentì— ëŒ€í•´ì„œ ë” ë§ì€ ì •ë³´ë¥¼ ì°¾ê¸°ìœ„í•´ ëœë¤í•˜ê²Œ í–‰ë™í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\nì´ëŠ” ê´€ì¸¡ëœ ë°ì´í„°ê°€ best optionì„ ê²°ì •í•˜ê¸°ì— ì¶©ë¶„í•˜ì§€ ì•Šë‹¤ëŠ” ì‚¬ì‹¤ì„ ê°€ì •í•©ë‹ˆë‹¤.(ì•Œë ¤ì§„ ê²ƒì„ ê±°ë¶€í•˜ê³  ìƒˆë¡­ê²Œ íƒí—˜í•˜ëŠ” ê²ƒ)."
  },
  {
    "objectID": "posts/RL/RL 1.html",
    "href": "posts/RL/RL 1.html",
    "title": "[ê°•í™”í•™ìŠµ] 1 - ê°•í™”í•™ìŠµ ìš©ì–´ì •ë¦¬",
    "section": "",
    "text": "ê°•í™”í•™ìŠµ ë…í•™í•˜ë©´ì„œ ìµìˆ™í•˜ì§€ ì•Šê±°ë‚˜ ëª¨ë¥´ëŠ” ìš©ì–´ë“¤ì„ ì •ë¦¬í•´ë†“ì€ í˜ì´ì§€ì…ë‹ˆë‹¤.ê°œì„ í•  ë¶€ë¶„ì´ ìˆë‹¤ë©´ ëŒ“ê¸€ë¡œ ì•Œë ¤ì£¼ì„¸ìš”..!\n\n ìš©ì–´ì •ë¦¬\n\nagent : ê°•í™”í•™ìŠµì—ì„œ í•™ìŠµí•˜ëŠ” ëŒ€ìƒ(object)ë¥¼ ë§í•œë‹¤.\naction : agentê°€ í•˜ëŠ” ì‹¤ì œ í–‰ë™ì„ ë§í•œë‹¤. actionì„ ì·¨í•˜ë©´ stateê°€ ë³€í™”í•œë‹¤.\nstate : agentê°€ ì¸ì‹í•˜ê³  ìˆëŠ” ìì‹ ì˜ ìƒíƒœì´ë©° actionì„ ì·¨í•˜ê¸° ìœ„í•œ êµ¬ì²´ì  ì •ë³´ì´ë‹¤.ì–´ë– í•œ actionì„ ì·¨í•˜ë©´ (environmentì— ì˜í•´)ë³€í™”í•œë‹¤.\nobservation : ì‹¤ì œë¬¸ì œì—ì„œ agentëŠ” ëª¨ë“  stateë¥¼ ì•Œê¸°ëŠ” ë¶ˆê°€ëŠ¥í•˜ë‹¤. agentëŠ” stateì¤‘ ì¼ë¶€ì •ë³´ë§Œì„ ë°›ëŠ”ë° ì´ê²ƒì„ observationì´ë¼ê³  í•œë‹¤.\nreward : agentê°€ actionì„ ì·¨í–ˆì„ë•Œ ë°›ëŠ” ë³´ìˆ˜,ë³´ìƒ,ê°’ì´ë‹¤. ì–´ë– í•œ actionì„ ìœ ë„í•˜ê¸° ìœ„í•´ì„œëŠ” + ë³´ìƒì„ actionì„ ë°©ì§€í•˜ê¸° ìœ„í•´ì„œëŠ” -ë³´ìƒì„ ì¤€ë‹¤.\nEnvironment  â€“ 1)agentê°€ ë†“ì—¬ìˆëŠ” world(ì„¸ê³„)ì´ë‹¤. â€“ 2)agentê°€ ì–´ë–¤ actionì„ ì·¨í•˜ë©´ stateê°€ ë³€í™”í•˜ëŠ”ë° ì´ ë•Œ ìƒˆë¡œìš´ stateë¥¼ returní•´ì£¼ëŠ” ì¡´ì¬ë¥¼ ë§í•œë‹¤. â€“ 3)agentê°€ ì–´ë–¤ í–‰ë™ì„ í–ˆì„ë•Œ ì–´ë–¤ ë³´ìƒì„ ì¤˜ì•¼í•˜ëŠ”ì§€ì— ëŒ€í•œ settingì´ë‹¤. â€“ ë­”ê°€ ì§ê´€ì ìœ¼ë¡œ ì´í•´ê°€ ê°€ì§€ ì•ŠëŠ”ë‹¤.ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ìë©´ ë§ì€ ì•ˆë˜ì§€ë§Œ ì•„ì£¼ë”ìš´ ë‚˜ê°€ê¸°ë©´ í•˜ë©´ ë•€ë‚˜ëŠ” ì•„í”„ë¦¬ì¹´ ì‚¬ë§‰ì—ì„œ 1ì‹œê°„ ë‹¬ë¦¬ëŠ” ê²ƒê³¼ ì‹œì›í•œ í—¬ìŠ¤ì¥ì—ì„œ 1ì‹œê°„ ë‹¬ë¦¬ëŠ” ê²ƒì„ ë¹„êµí•´ë³´ì. ì‚¬ë§‰ì—ì„œ ë‹¬ë¦¬ëŠ”ê²Œ ì°¨ì›ì´ ë‹¤ë¥¸ í˜ë“¦ê³¼ ì²´ì¤‘ê°ëŸ‰ íš¨ê³¼ë¥¼ ì¤„ ê²ƒì´ë‹¤. ì´ëŠ” ê°™ì€ actionì„ ì·¨í•´ë„ ì„œë¡œë‹¤ë¥¸ envirionmentê°€ ë‹¤ë¥¸stateë¥¼ return(ì œê³µ)í•´ì£¼ê¸°ì— ê·¸ë ‡ë‹¤. ê°ê°ì˜ í™˜ê²½ì€ ë˜í•œ ê°ëŸ‰ì— ëŒ€í•˜ì—¬ ì„œë¡œë‹¤ë¥¸ settingì— ì˜í•´ rewardë¥¼ ì¤„ê²ƒì´ë‹¤.\npolicy  â€“ agentê°€ ì–´ë–»ê²Œ í–‰ë™ì„ í•´ì•¼í• ì§€ ì•Œë ¤ì£¼ëŠ” ë°©í–¥,ê°€ì´ë“œ,ì§€í‘œì˜ ëŠë‚Œì´ë‹¤. ì •ì±…ë¼ëŠ” ì˜ë¯¸ê°€ ë„ˆë¬´ ì™€ë‹¿ì§€ ì•Šì•„ì„œ ë„¤ì´ë²„ì— ê²€ìƒ‰í•´ë³´ë‹ˆ ê°œì¸ì˜ ì•ìœ¼ë¡œ ë‚˜ì•„ê°ˆ ë…¸ì„ ì´ë‚˜ ì·¨í•´ì•¼ í•  ë°©ì¹¨ë¼ê³  í•œë‹¤. (ì§€í‘œì˜ ê·¸ëƒ¥ ì–´ë ¤ìš´ ë§) â€“ policyëŠ” í¬ê²Œ deterministic policyì™€ stochastic policyë¡œ ë‚˜ë‰œë‹¤ê³  í•œë‹¤. ë‚´ê°€ ì´í•´í•˜ëŠ”ëŒ€ë¡œ ì‰½ê²Œ ë§í•´ë³´ìë©´ ìƒí™©ì— ë”°ë¼ì„œ ê°€ëŠ¥í•œ í–‰ë™ì´ ë‹¨ í•œê°€ì§€ë¼ë©´ deterministic(ì •í•´ì ¸ ìˆëŠ”,ê²°ì •ë¡ ì ì¸) policyê°€ ìˆëŠ” ê²ƒì´ê³  í•˜ê³  ê°€ëŠ¥í•œ í–‰ë™ì´ ì—¬ëŸ¬ê°€ì§€ë¼ë©´ stochastic(ì•ˆì •í•´ì ¸ ìˆëŠ”,í™•ë¥ ë¡ ì ì¸) policyê°€ ìˆëŠ” ê²ƒì´ë‹¤.\nreturn  agentëŠ” ë‹¹ì¥ì— ë°›ëŠ” rewardë¿ë§Œ ì•„ë‹ˆë¼ ë¯¸ë˜ì— ë°›ëŠ” rewardë„ ê³ ë ¤í•´ì•¼ í•œë‹¤. returnì€ í˜„ì¬ì˜ stateë¡œë¶€í„° ë¯¸ë˜ì˜ action,stateë¥¼ ì „ë¶€ ê³ ë ¤í–ˆì„ë•Œ ë‚´ê°€ ë¯¸ë˜ì— ë°›ê²Œ ë  discounted rewardì˜ ì´í•©,ëˆ„ì í•©ì´ë‹¤. ì—¬ê¸°ì„œ discountedê°€ ë¶™ëŠ” ì´ìœ ëŠ” ë¯¸ë˜ì— ë°›ê²Œë˜ëŠ” rewardì— ëŒ€í•´ì„œëŠ” ì–´ëŠì •ë„ discountê°€ ë“¤ì–´ê°€ê¸° ë•Œë¬¸ì´ë‹¤. discountì˜ ì–‘ì€ discount factor(\\(\\gamma\\))ì— ì˜í•´ ê²°ì •ë˜ëŠ”ë° ì´ê°’ì„ 1ì— ê°€ê¹ê²Œ í• ìˆ˜ë¡ ë¯¸ë˜ì— ë°›ëŠ” action ëŒ€í•œ rewardì˜ discountê°€ ì»¤ì§€ê³  0ì— ê°€ê¹ê²Œ í• ìˆ˜ë¡ ë¯¸ë˜ì˜ actionì— ëŒ€í•œ rewardì˜ discountê°€ ì‘ì•„ì§„ë‹¤. ê°•í™”í•™ìŠµì˜ ìµœì¢… ëª©ì ì€ í‰ê· ì ìœ¼ë¡œ ë°›ëŠ” returnì´ ê°€ì¥ ì»¤ì§€ë„ë¡(returnë„ ëœë¤ì´ê¸° ë•Œë¬¸ì— í‰ê· ì ìœ¼ë¡œ ì»¤ì ¸ì•¼ í•œë‹¤.)policyë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.\nexploration í˜„ì¬ agentê°€ ê°€ì§€ê³  ìˆëŠ” ì‚¬ì „ì§€ì‹,ê²½í—˜ì„ ë²„ë¦¬ê³  í‹€ì„ê¹¨ì–´ ìƒˆë¡œìš´ ë°©ì‹ìœ¼ë¡œ actioní•˜ì—¬ ë³´ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ëœë¤í•œ í™•ë¥ (\\(\\epsilon\\))ë¡œ policyë¥¼ ê±°ìŠ¬ëŸ¬ì„œ ë‹¤ë¥¸ ëª¨ë“  ì•¡ì…˜ì¤‘ì— ëœë¤í•œ í•˜ë‚˜ì˜ actionì„ ì·¨í•˜ê²Œ ëœë‹¤. locacl optimaì—ì„œ ë²—ì–´ë‚˜ ìµœì ì˜ policyë¥¼ íƒìƒ‰í•  ë” ë§ì€ ê¸°íšŒë¥¼ ì œê³µí•œë‹¤.\nexploitation í˜„ì¬ agentê°€ ê°€ì§€ê³  ìˆëŠ” ì‚¬ì „ì§€ì‹,ê²½í—˜ì„ ìµœëŒ€í•œ í™œìš©í•˜ì—¬ actionì„ ì·¨í•˜ëŠ” ê²ƒì„ ë§í•œë‹¤.\nexploration exploitation trade off ë„ˆë¬´ exploitationë§Œ í•  ê²½ìš° ì‚¬ì „ì§€ì‹ë§Œì„ ê°€ì§€ê³  actioní•˜ì—¬ ë” ì¢‹ì€ policyë¥¼ ì°¾ì§€ ëª»í•˜ê³  ë„ˆë¬´ exploration ìœ„ì£¼ì¸ ê²½ìš° ì‚¬ì „ì§€ì‹ì€ í™œìš©í•˜ì§€ ì•Šê²Œë˜ê³  í•™ìŠµì´ ë”ë””ê²Œ ì§„í–‰ëœë‹¤.\n\në¹„ìœ ì ìœ¼ë¡œ ìƒê°í•´ë³´ì.ë§Œì•½ì— ë‚´ê°€ ì—„ì²­ ì‚´ì´ì° ìƒíƒœ(state)ì—ì„œ ë‚˜ ìì‹ ì„ ê°•í™”í•™ìŠµ?í•œë‹¤ê³  í•˜ì. 2ë‹¬ë™ì•ˆ ì‚´ì„ ë¹¼ëŠ” ê²ƒì„ ëª©ì ìœ¼ë¡œ í•˜ê³  BMIìˆ˜ì¹˜ê°€ ì¢‹ì•„ì§€ë©´ rewardë¥¼ ë°›ëŠ”ë‹¤ê³  í•˜ì.ë‚˜(agent)ëŠ” ì—¬ëŸ¬ê°€ì§€ ìš´ë™(action)ì„ í•´ë³¼ ìˆ˜ ìˆë‹¤. ì²˜ìŒì—ëŠ” ì¡°ê¸ˆ í˜ë“œë‹ˆê¹Œ ì‰¬ìš´ ìš´ë™(ê±·ê¸°?ìˆ¨ì‰¬ê¸°..)ì •ë„ í•˜ì§€ë§Œ ì´ë ‡ê²Œ í•˜ë©´ ë‚˜ëŠ” ì ˆëŒ€ë¡œ ëª¸ë¬´ê²Œê°€ ë¹ ì§€ì§€ ì•Šê±°ë‚˜ ì˜¤íˆë ¤ ë” ì°ŒëŠ” ìƒí™©(state)ì´ ë°œìƒí•  ìˆ˜ ìˆë‹¤.(rewardê°€ 0ì´ê±°ë‚˜ ì˜¤íˆë ¤ -reward)\nì´ë ‡ê²Œ ë  ê²½ìš° í•´ì•¼í•  ìš´ë™ì˜ ë°©í–¥(policy)ì— ëŒ€í•´ ë‹¤ì‹œ ìƒê°í•´ë³¸ë‹¤. ìƒê°í•´ë´¤ë”ë‹ˆ ê±·ê¸°,ë‹¬ë¦¬ê¸°,ê·¼ë ¥ìš´ë™,êµ¶ê¸° ê°™ì€ ë” ì¢‹ì€ ë°©ë²•,ìš´ë™ë“¤ì´ ìˆì—ˆê³  ì´ë²ˆì—ëŠ” ê·¸ëŸ¬í•œ ë°©ë²•ë“¤ì„ ì‹¤ì²œí•´ë³¸ë‹¤. ì´ëŸ¬í•œ ê²½ìš°ë“¤ ëª¨ë‘ BMIìˆ˜ì¹˜ê°€ ê°œì„ ë˜ì–´ ë” ì¢‹ì•„ì¡ŒëŠ”ë° ê·¸ëŸ°ë° êµ¶ê¸°ì˜ ê²½ìš° ë‹¹ì¥ì— ê°€ì¥ ë§ì´ BMIìˆ˜ì¹˜ê°€ ê°œì„ ë˜ì–´ ì•ìœ¼ë¡œë„ ì´ ë°©ë²•ë§Œ ì„ íƒí•œë‹¤ê³  í•´ë³´ì. BMIìˆ˜ì¹˜ëŠ” ê·¼ìœ¡ëŸ‰ë„ ê³ ë ¤í•˜ê¸° ë•Œë¬¸ì— ì¥ê¸°ì ìœ¼ë¡œ ë³¼ë•Œ êµ¶ê¸°ë§Œ í•œë‹¤ë©´ ëª¸ë¬´ê²Œê°€ ë¹ ì§€ê¸´ í•˜ëŠ”ë° ê²°êµ­ì—ëŠ” ê·¼ìœ¡ë§Œ ë‹¤ ë¹ ì ¸ì„œ ê²°êµ­ì—ëŠ” ëª¸ì´ ì•ˆì¢‹ì•„ì§ˆ ê²ƒì´ë‹¤.(rewardì˜ í•©ì¸ returnì„ ê³ ë ¤í•˜ëŠ” ì´ìœ !) ê·¸ëŸ¬ë¯€ë¡œ êµ¶ëŠ” ë°©ë²•ì€ ì–´ëŠì •ë„ ìì œí•˜ê³  ê±·ê¸°,ë›°ê¸°,ê·¼ë ¥ìš´ë™í•˜ê¸° ìœ„ì£¼ë¡œ ë‚´ê°€ ìš´ë™í•´ì•¼í•  ë°©í–¥ì´ ê²°ì •ëœë‹¤.\n\n\nexploration exploitation tradeoff\nì‚´ì„ ë¹¼ê³  ìˆëŠ”ë° ì–´ëŠë‚  ê°‘ìê¸° ì¹œêµ¬ê°€ ë‚´ê°€ ì •í•œ ìš´ë™ì‹œê°„ ë§ê³  ê°™ì´ ìš´ë™í•˜ìê³  í–ˆë‹¤. ê·¼ë° ë‚˜ëŠ” ì—¬íƒœê¹Œì§€ ë°¤ì— ì‚°ì±…í•˜ëŠ”ê²Œ ì¢‹ì•˜ê¸°ì— (ë°¤ì— ì‚°ì±…í•˜ê¸°ê°€) ê±°ì ˆí•œë‹¤ê³  í•˜ë©´ ì´ê±´ exploitation ìœ„ì£¼ë¡œ í•™ìŠµì„ í•˜ëŠ” ê²ƒì´ë‹¤. ì´ì™€ëŠ” ë‹¤ë¥´ê²Œ ë‚®ì— ì¹œêµ¬ì˜ ì œì•ˆì„ ë°›ì•„ë“¤ì—¬ ê°™ì´ í—¬ìŠ¤ì¥ìœ¼ë¡œ ìš´ë™í•˜ëŸ¬ ê°€ëŠ”ê±°ë©´ ì´ëŠ” explorationìœ„ì£¼ë¡œ í•™ìŠµì„ í•´ë´¤ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ì¤‘ìš”í•œ ì ì€ ì´ ë‘˜ ì‚¬ì´ì—ëŠ” tradeoffê°€ ìˆë‹¤ëŠ” ê²ƒì´ë‹¤. ë„ˆë¬´ exploitationë§Œ í•˜ë©´ ë” ì¢‹ì€ ë°©ë²•(ì¹œêµ¬ê°€ ì˜ ë§ìœ¼ë©´ ìµœì¢…ì ìœ¼ë¡œëŠ” ë” ìš´ë™ì„ ë§ì´í•˜ê²Œ ë˜ì–´ BMIê°€ ë‚®ì•„ì§ˆ ê²ƒì´ë‹¤.)ì„ ë†“ì¹˜ê²Œ ë˜ê³  ë„ˆë¬´ explorationë§Œ í•  ê²½ìš°ëŠ”(ì—¬ëŸ¬ëª…ì˜ ì¹œêµ¬ì—ê²Œ ì œì•ˆì´ ì™”ë‹¤ë©´ ê·¸ ì¹œêµ¬ì¤‘ì—ëŠ” ìš´ë™ì€ ë³„ë¡œì•ˆí•˜ëŠ”ë° ëë‚˜ê³  ê°™ì´ì¹˜í‚¨ë¨¹ëŠ” ì¢‹ì€?ì¹œêµ¬ë„ ìˆë‹¤.)í•™ìŠµì„ ë”ë””ê²Œ í•œë‹¤. ì´ëŸ¬í•œ tradeoffê°€ ìˆì–´ì„œ ê²°êµ­ì— 2ë‹¬ë™ì•ˆ ìµœëŒ€í•œ ì‚´ì„ ë§ì´ ë¹¼ë ¤ëŠ” ê¶ê·¹ì ì¸ ëª©ì ì„ ì´ë£¨ë ¤ë©´ ì´ ë‘˜ ì‚¬ì´ì˜ tradeoffë¥¼ ì˜ ì¡°ì ˆí•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•œ ìš”ì†Œì´ë‹¤.\në¡¤ë¡œ ë¹„ìœ í•˜ëŠ” ê²ƒë„ ê°€ëŠ¥í•˜ë‹¤. ìµœì¢…ì ìœ¼ë¡œ ìƒˆë¡­ê²Œ ì‹œì‘ë˜ëŠ” 2023ì‹œì¦Œì— ì±Œë¦°ì €ë¥¼ ê°€ëŠ”ê²Œ ëª©í‘œë¼ê³  í•˜ì. í•˜ë˜ì±”ë§Œ ê³„ì†í•˜ëŠ” ê²ƒì´ exploitationì´ê³  ë‹¤ë¥¸ì±”ë„ í•´ë³´ëŠ”ê²Œ exploitationì´ë‹¤. í•˜ë˜ì±”ë§Œ í•˜ë©´ ë‚˜ëŠ” ëª¨ë¥´ì§€ë§Œ ë‚´ê°€ ì¬ëŠ¥ì„ ê°€ì§„ ì±”í”¼ì–¸ì„ ì˜ì›íˆ ëª¨ë¥¼ìˆ˜ë„ ìˆê³  ì¬ëŠ¥ì„ ê°€ì§„ ì±”í”¼ì–¸ì„ ì°¾ê³ ì ìƒˆë¡œìš´ ì±”í”¼ì–¸ë§Œ ê³„ì†í•˜ë‹¤ë³´ë©´ ê²½í—˜ì¹˜ê°€ ì˜ ìŒ“ì´ì§€ ì•Šê³  í•™ìŠµì´ ë”ë””ê²Œ ëœë‹¤. ì—¬ê¸°ì„œë„ ë§ˆì°¬ê°€ì§€ë¡œ 2023 - ì±Œë¦°ì €ë¼ëŠ” ëª©í‘œë¥¼ ì´ë£¨ê¸° ìœ„í•´ì„œëŠ” ë‘˜ ì‚¬ì´ì˜ tradeoffë¥¼ ì ì ˆíˆ ì¡°ì ˆí•˜ëŠ”ê²Œ ì¤‘ìš”í•˜ë‹¤.\n\n\n ì°¸ê³ ìë£Œ\në‚˜ë¬´ìœ„í‚¤ - ê°•í™”í•™ìŠµìš©ì–´ì •ë¦¬\nFundamental of Reinforcement Learning - ì´ì›…ì›ë‹˜ ê¹ƒë¶\nMLI\në‚´ê°€ ë³´ë ¤ëŠ” ê¸°ìˆ  ë¸”ë¡œê·¸"
  },
  {
    "objectID": "posts/RL/RL 2 - MDP.html",
    "href": "posts/RL/RL 2 - MDP.html",
    "title": "[ê°•í™”í•™ìŠµ] 2-1 Markov Decision process and property",
    "section": "",
    "text": "Markov Decision Process\nìê¸°ì „ì— ìŠ¤íƒ€ë¥¼ í–ˆë”ë‹ˆ ìƒëŒ€ë°©ì—ê²Œ í•µì„ ë§ì•˜ë‹¤ê³  í•˜ìâ€¦\n\në‚˜ëŠ” ì™œ í•µì„ ë§ì•˜ì„ê¹Œ? ê²Œì„ì•ˆì—ì„œ ì£¼ì–´ì§€ëŠ” ì—¬ëŸ¬ê°€ì§€ stateì—ì„œ actionì„ ì·¨í–ˆì„ ë•Œ ê°€ëŠ¥í•œ ì—¬ëŸ¬ê°€ì§€ ê²°ê³¼ ì¤‘ í•˜ë‚˜ê°€ ë‚˜ì—ê²Œ ëŒì•„ì˜¨ ê²ƒì´ë‹¤.\nì¡°ê±´ë¶€ í™•ë¥ ê³¼ ìš©ì–´ë¥¼ ë¹Œë¦¬ë©´ actionë“¤ê³¼ stateë“¤ì— ëŒ€í•œ ì¡°ê±´ì´ ì£¼ì–´ì ¸ ìˆì„ë•Œ stateì— ëŒ€í•œ ì¡°ê±´ë¶€í™•ë¥ ë¶„í¬(conditional distribution)ì—ì„œ í•˜ë‚˜ì˜ eventê°€ sampleë¡œ ë½‘íŒ ê²ƒì´ë‹¤.(ì—¬ê¸°ì„œëŠ” í•µì„ ë§ëŠ” ì‚¬ê±´ì´ ë½‘íŒê²ƒì´ë‹¤.)\n\\[p_{S_t|S_0,A_0\\dots,S_{t-1}A_{t-1}}(s_t|s_0,a_0,\\dots,s_{t-1},a_{t-1})\\]\nì´ì™€ ë¹„ìŠ·í•˜ê²Œ ë‚´ê°€ í•µì„ ë§ê¸°ì „ì— ì·¨í•  ìˆ˜ ìˆëŠ” íŒë‹¨ë„ ì—¬ëŸ¬ê°€ì§€ë©° ë˜í•œ ì·¨í•˜ëŠ” íŒë‹¨ì˜ ê·¼ê±°ë„ ë§ˆì°¬ê°€ì§€ë¡œ ì´ì „ì˜ ìƒí™©ê³¼ ë‚´ê°€ ì·¨í•œ íŒë‹¨ì— ì˜í•´ ê²°ì •ë˜ì—ˆì„ ê²ƒì´ë‹¤. actionê³¼ stateë“¤ì´ ì£¼ì–´ì¡Œì„ë•Œ ì·¨í•  ìˆ˜ ìˆëŠ” actionì— ëŒ€í•œ ì¡°ê±´ë¶€ í™•ë¥ ë¶„í¬ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[p_{A_t|S_0,A_0,\\dots,S_{t-1},A_{t-1}}(a_t|s_0,a_0,\\dots,s_t)\\]\nìœ„ì˜ ë‚´ìš©ì€ ì¼ë°˜ì ìœ¼ë¡œ ìš°ë¦¬ê°€ ìƒê°í•˜ëŠ” ì§ê´€ê³¼ ì¼ì¹˜í•˜ëŠ” ê²½ìš°ì´ë‹¤. í•˜ì§€ë§Œ (ì§€ê¸ˆê¹Œì§€ ì—°êµ¬ëœ)ê°•í™”í•™ìŠµì˜ ê²½ìš° ì´ì™€ëŠ” ë‹¤ë¥´ê²Œ Markov Decision Processë¥¼ ê°€ì •í•œë‹¤. ê·¸ë ‡ë‹¤ë©´ Markov decision processë€ ë­”ê°€? ìœ„í‚¤í”¼ë””ì•„ì˜ ì •ì˜ì— ì˜í•˜ë©´ â€œê° ì‚¬ê±´ì— ëŒ€í•œ í™•ë¥ ì´ ì‚¬ê±´ìœ¼ë¡œë¶€í„° ì–»ì€ ìƒíƒœì—ë§Œ ì˜ì¡´í•˜ëŠ” ì¼ë ¨ì˜ ê°€ëŠ¥í•œ ì´ë²¤íŠ¸ë¥¼ ì„¤ëª…í•˜ëŠ” í™•ë¥ ì  ëª¨ë¸â€ì´ë¼ê³  ì í˜€ìˆë‹¤. ê°•í™”í•™ìŠµì´ MDPë¥¼ ë”°ë¥´ë¯€ë¡œ ë‹¤ì‹œ ë§í•˜ìë©´ í˜„ì¬ ë‚´ê°€ ë†“ì¸ ìƒí™©(ì˜ í™•ë¥ ë¶„í¬)ì´ë‚˜ í˜„ì¬ ë‚´ê°€ í•˜ëŠ” ì•¡ì…˜(ì˜ í™•ë¥ ë¶„í¬)ì€ ë°”ë¡œ ì´ì „ì˜ ìƒíƒœë‚˜ í–‰ë™ì˜ ì˜í–¥ë§Œì„ ë°›ëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤. MDPê°€ ë”°ë¥´ëŠ” ì´ëŸ¬í•œ íŠ¹ì„±ì„ Markov propertyë¼ê³  í•œë‹¤.\n\\[\\begin{aligned}\n&\\text{Markov property}\\\\\n&\\forall{t,}\\,\\,p_{\\small{S_t|S_0,A_0\\dots,S_{t-1}A_{t-1}}}(s_t|s_0,a_0,\\dots,s_{t-1},a_{t-1}) = p_{\\small{S_t|S_{t-1}},A_{t-1}}(s_t|s_{t-1},a_{t-1})\\\\\n&\\forall{t,}\\,\\,p_{\\small{A_t|S_0,A_0,\\dots,S_{t-1},A_{t-1}}}(a_t|s_0,a_0,\\dots,s_t)=p_{\\small{A_t|S_{t}}}(a_t|s_t)\n\\end{aligned}\\]\nì²«ë²ˆì§¸ í™•ë¥ ì€ ìƒíƒœ\\(s_{t-1}\\)ì—ì„œ (\\(a_{t-1}\\)ì„ ì·¨í•˜ì—¬) \\(s_{t}\\)ë¡œ transition(ë³€í• ë•Œ)ì— ëŒ€í•œ í™•ë¥ ë¶„í¬ì´ë¯€ë¡œ transition probabilityë¼ê³  ë¶€ë¥¸ë‹¤. ë‘ë²ˆì§¸ í™•ë¥ ì€ policy(ì •ì±…,ì§€í‘œ)ë¡œ ì–´ë–¤ ìƒí™©ì—ì„œ ì–´ë–¤ ì•¡ì…˜ì„ ì·¨í• ì§€ì— ëŒ€í•œ ê¸°ì¤€ì´ ë˜ëŠ” í™•ë¥ ë¶„í¬ì´ë‹¤.(í›„ì— optimal policyì— ëŒ€í•´ ìì„¸íˆ ë‹¤ë£¬ë‹¤.)\n\n\ntransition probability\n\\[\\begin{aligned}\n&\\text{definition of transition probability}\\\\\n&p_{\\small{S_t|S_{t-1}},A_{t-1}}(s_t|s_{t-1},a_{t-1})\\\\\n\\end{aligned}\\]\n\nìœ„ì™€ ê°™ì€ ê·¸ë¦¼ì„ ì‚´í´ë³´ì.ì™¼ìª½ë¡œë´‡ì˜ ê²½ìš° Deterministic Grid World(=Envirion ment)ì— ë†“ì—¬ìˆê³  ì•ìœ¼ë¡œ ê°€ëŠ” actionì„ ì·¨í•  ê²½ìš° ë°˜ë“œì‹œ ì•ìœ¼ë¡œ ê°€ë¯€ë¡œ stateê°€ ê²°ì •ì (deterministic)ì´ë¼ê³  í•  ìˆ˜ ìˆë‹¤.ë°˜ë©´ì— ì˜¤ë¥¸ìª½ë¡œë´‡ì˜ ê²½ìš° Stochastic Grid Worldì— ë†“ì—¬ìˆë‹¤.ì´ëŸ¬í•œ ê²½ìš°ì—ëŠ” actionì„ ì·¨í•´ë„ 3ê°€ì§€ ìƒí™©ì— ì·¨í•´ì§ˆ ìˆ˜ ìˆìœ¼ë©° ì´ ê²½ìš° stateëŠ” Stochasticí•˜ë‹¤ê³ (ë°”ëŒì˜ ì˜í–¥ì´ë‚˜,ë¡œë´‡ì´ ì˜¤ì‘ë™í•˜ê±°ë‚˜)í•  ìˆ˜ ìˆë‹¤.ë‹¤ì‹œë§í•˜ë©´ stateëŠ” í™•ë¥ ë¶„í¬ì— ë”°ë¼ ì„ì˜ì (randomly)ì´ë‹¤.\n\n\nPolicy\nì •ì±…\\(\\pi(a_t|s_t)\\)ëŠ” ì–´ë–¤ ìƒíƒœê°€ ì£¼ì–´ì§ˆë•Œ ì–´ë–¤ í–‰ë™ì„ ì·¨í•  ê²ƒì¸ì§€ ëª…ì‹œí•œ (ì¡°ê±´ë¶€)í™•ë¥ ë¶„í¬ë¥¼ ë§í•œë‹¤.\n\\[\\begin{aligned}\n&\\text{definition of poilcy} \\\\\n&{\\pi}({a_t,s_t}) \\overset{\\Delta}{=}p_{A_t|S_{t}}(a_t|s_t)\n\\end{aligned}\\]\nì•„ë˜ì™€ ê°™ì€ ê·¸ë¦¼ì„ ë³´ì ì´ˆê¸° stateëŠ” íŒŒë‘ìƒ‰ ìœ„ì¹˜ì´ë©° agentëŠ” ì™¼ìª½ìœ„ë‚˜ ì˜¤ë¥¸ìª½ì•„ë˜ì˜ ì¢…ë£Œì§€ì ê¹Œì§€ ê°€ì•¼í•œë‹¤.\n\n\nì™¼ìª½ìœ„ì˜ ê·¸ë¦¼ì—ì„œ ëª¨ë“  policyëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\n\\forall{t},\\pi(a_t,s_t) = p_{\\small{A_t|S_t}}(a_t|s_t) =\n\n\\begin{cases}\np_{\\small{A_t|S_t}}(\\text{right}|s_t) = \\frac{1}{4}\\\\\np_{\\small{A_t|S_t}}(\\text{left}|s_t) = \\frac{1}{4}\\\\\np_{\\small{A_t|S_t}}(\\text{up}|s_t) = \\frac{1}{4}\\\\\np_{\\small{A_t|S_t}}(\\text{down}|s_t) = \\frac{1}{4}\\\\\n\n\\end{cases}\n\n\\end{aligned}\\]\nê°ê°ì˜ stateì—ì„œ actionì€ ìœ„ì™€ ê°™ì€ policyë¥¼ ë”°ë¥´ë¯€ë¡œ ì•„ë˜ì™€ ê°™ì€ ê²½ë¡œê°€ ì˜ˆì œí•´ë¡œ ë‚˜ì˜¬ ìˆ˜ ìˆë‹¤.\nì™¼ìª½ì—ì„œ ë‘ë²ˆì§¸ ìˆëŠ” ê·¸ë¦¼ì˜ ëª¨ë“  policyëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\begin{aligned}\np_{\\small{A_t|S_t}}(a_t|s_t)=\n\\begin{cases}\n\\frac{1}{4} \\text{  if } a_t = \\text{down} \\\\\n0\\text{ otherwise}\n\\end{cases}\n\\end{aligned}\\]\nìœ„ì™€ ê°™ì€ policyë¥¼ ë”°ë¥´ë¯€ë¡œ ëª¨ë“  stateì— ëŒ€í•´ì„œ ë‚¨ìª½ë°©í–¥ìœ¼ë¡œë§Œ ë‚˜ì˜¨ë‹¤.\nì˜¤ë¥¸ìª½ì—ì„œ ë‘ë²ˆì§¸ ê·¸ë¦¼ì˜ policyëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nê°€ì¥ ì˜¤ë¥¸ìª½ ê·¸ë¦¼ì˜ policyëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\nìƒê°í•´ë³´ë©´ ê°€ì¥ì˜¤ë¥¸ìª½ ìœ„ ì‚¬ê°í˜•ê°™ì€ policyê°€ ì •í•´ì§€ë©´ ê°€ì¥ ë¹ ë¥´ê²Œ ëª©í‘œì— ë„ë‹¬í•  ìˆ˜ ìˆë‹¤. ì´ëŠ” ìµœê³ ì˜ rewardë¥¼ ë°›ë„ë¡ í•™ìŠµëœ ê²°ê³¼ì´ë‹¤.\n\n\nì •ë¦¬\n\nê°•í™”í•™ìŠµì€ MDPë¥¼ ê°€ì •í•œë‹¤. ì´ëŠ” ì´ì „ stateë‚˜ ì•¡ì…˜ì— ì˜í•´ì„œë§Œ í™•ë¥ ë¶„í¬ê°€ ì˜í–¥ì„ ë°›ëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤.\npolicyëŠ” ì„ì˜ì˜ stateì— ì·¨í•œ actionì˜ í™•ë¥ ë¶„í¬í•¨ìˆ˜ë¡œ ì–´ë–¤ actionì„ í• ì§€ëŠ” ì´ê²ƒì— ì˜í•´ ê²°ì •ëœë‹¤.\n\n\n\nì°¸ê³ ìë£Œ\nìœ„í‚¤í”¼ë””ì•„ - markov chain(=markov process) í˜íœí•˜ì„-[ê°•í™”í•™ìŠµ] 2-1ê°•. Markov Decision Process (MDP) ì‰¬ìš´ ì„¤ëª… Fundamental of Reinforcement Learning wordbe"
  },
  {
    "objectID": "posts/RL/RL2-2 value function.html",
    "href": "posts/RL/RL2-2 value function.html",
    "title": "[ê°•í™”í•™ìŠµ] 2-2 Reward & Return & State Value f & Action Value f",
    "section": "",
    "text": "ë¦¬ì›Œë“œì˜ ì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤.\n\\[\\mathcal{R}_{t+1} \\overset{\\Delta}{=} \\mathbb{E}({R}_{t+1}|S_t=s,A_t=a)\\]\nì´ëŠ” \\(s_t\\)ì—ì„œ \\(a_t\\)ë¥¼ í–ˆì„ë•Œ t+1ì—ì„œ ì–»ëŠ” ê°’ì„ ë‚˜íƒ€ë‚´ëŠ” í™•ë¥ ë³€ìˆ˜\\(R_{t+1}\\)ì˜ ê¸°ëŒ“ê°’ì´ë‹¤.ì´ë‹¤.\nì•ŒíŒŒê³ ë¥¼ ì˜ˆë¥¼ ë“¤ì–´ì„œ ìƒê°í•´ë³´ì. ì•ŒíŒŒê³ ê°€ ë°”ë‘‘íŒì—(\\(s_t\\))ì— ê²€ì€ëŒì„ ë†“ìœ¼ë©´(\\(a_{t}\\)) ìƒëŒ€í•˜ëŠ” ì‚¬ëŒ(ë˜ëŠ”ê¸°ê³„)ë„ ì–´ë–¤ ìœ„ì¹˜ì— í°ëŒì„ ë†“ì„ê²ƒì´ë‹¤. ì´ í°ëŒì˜ ìœ„ì¹˜ëŠ” randomì´ê¸° ë•Œë¬¸ì— ë”°ë¼ì„œ ì•ŒíŒŒê³ ê°€ í™•ë¥ ë³€ìˆ˜ \\(R_{t+1}\\)ì´ ì¡´ì¬í•˜ë©° ê·¸ê²ƒì˜ í‰ê· ê°’ì„ ë¦¬ì›Œë“œ\\(\\mathcal{R}_{t+1}\\)ë¡œ ì •ì˜í•œë‹¤.\nìƒê°í•´ë³´ë©´ ë¦¬ì›Œë“œëŠ” ë­”ê°€ \\(s_t\\)ì—ì„œ \\(a_t\\)ë¥¼ í•´ì„œ ë³€í•˜ëŠ” ìƒí™© \\(s_{t+1}\\)ì— ë¶€ì—¬ë˜ëŠ”ê²Œ ë§ì„ ê²ƒ ê°™ë‹¤.ì°¾ì•„ë³´ë‹ˆ ìœ„í‚¤í”¼ë””ì•„ì—ëŠ” rewardë¥¼ \\(R_a(s,s')\\)ë¡œ ì“´ë‹¤. ë³€í•˜ëŠ” ìƒí™©ì— ë”°ë¼ ë¶€ì—¬ë˜ëŠ”ê²ƒë„ ë§ê³  ì–´ë–¤ ì•¡ì…˜ì„ ì·¨í•˜ë©´ ê·¸ê²ƒì— ìƒì‘í•œë‹¤ê³  ë´ë„ ë¬´ë°©í•  ê²ƒ ê°™ë‹¤.(ê°œì¸ì ì¸ ì˜ê²¬ì…ë‹ˆë‹¤.)\nê·¸ëƒ¥ \\(R_{t+1}\\)ì„ ë¦¬ì›Œë“œë¼ í•˜ëŠ” ê²½ìš°ë„ ë§ì€ ê²ƒ ê°™ë‹¤\n\n\n\n\n\n\nNote\n\n\n\nìœ„ì—ì„œ ì¸ì‹í•˜ëŠ” ìƒí™©ì´ë¼ê³  ì¼ëŠ”ë° ì‚¬ì‹¤ stateëŠ” agentê°€ ì¸ì‹í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ì‚¬ì‹¤ì€ environmentì—ê°€ ë°˜í™˜(return)í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. agentëŠ” stateì¤‘ ì¼ë¶€ë¥¼ ë°›ëŠ”ë° ì´ê²ƒì„ observationì´ë¼ê³  í•©ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì‹¤ì œ ë…¼ë¬¸ì—ì„œëŠ” ë”±íˆ stateì™€ observationì„ êµ¬ë³„í•˜ì§€ì•Šê³  ì“°ëŠ” ê²½ìš°ê°€ ë§ë‹¤ê³  í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/RL/RL2-2 value function.html#state-value-function",
    "href": "posts/RL/RL2-2 value function.html#state-value-function",
    "title": "[ê°•í™”í•™ìŠµ] 2-2 Reward & Return & State Value f & Action Value f",
    "section": "state value function",
    "text": "state value function\n\nThe value function of a state \\(s\\) under a policy \\(\\pi\\) is the expected return when starting \\(s\\) and following \\(\\pi\\) thereafter. \\(\\leftrightarrow\\) policy \\(\\pi\\)ë¥¼ ë”°ë¥¼ë•Œ state \\(s\\)ì˜ value functionì€ state \\(s\\)ì—ì„œ ì‹œì‘í•˜ì—¬ ê·¸ í›„ \\(\\pi\\)ë¥¼ ë”°ë¥¼ë•Œ returnì˜ ê¸°ëŒ“ê°’ì´ë‹¤.\nê°ê°ì˜ ìƒíƒœê°€ ì–¼ë§ˆë‚˜ ì¢‹ì€ì§€ ê·¸ ê°€ì¹˜ë¥¼ expected returnìœ¼ë¡œ ê³„ì‚°í•œ í•¨ìˆ˜ì´ë¯€ë¡œ state value functionì´ë¼ëŠ” ì´ë¦„ì´ ë¶™ì—ˆë‹¤.\në‹¨,\\(s\\)ì—ì„œ ì‹œì‘í•˜ë‹¤ëŠ” ì¡°ê±´í•˜ì—ì„œì˜ ê¸°ëŒ“ê°’ì´ë¯€ë¡œ \\(s\\)ê°€ givenì¸ conditional expectationì„ êµ¬í•˜ë©´ ëœë‹¤.\n\n\\[v_{\\pi}\\overset{\\Delta}{=} \\mathbb{E}_{\\pi}[G_t|S_t=s] = \\mathbb{E_{\\pi}}\\left[{\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}|S_t=s}\\right],\\text{for all s } \\in S\\]"
  },
  {
    "objectID": "posts/RL/RL2-2 value function.html#action-value-function",
    "href": "posts/RL/RL2-2 value function.html#action-value-function",
    "title": "[ê°•í™”í•™ìŠµ] 2-2 Reward & Return & State Value f & Action Value f",
    "section": "action value function",
    "text": "action value function\n\nSimilary, we define the value of taking action \\(a\\) in state \\(s\\) under a policy \\(\\pi\\) as the expected return starting from s,taking action a,and thereafter following policy \\(\\pi\\) \\(\\leftrightarrow\\) ìœ ì‚¬í•˜ê²Œ policy \\(\\pi\\)ë¥¼ ë”°ë¥´ê³  state \\(s\\)ì—ì„œ action \\(a\\)ë¥¼ ì·¨í•˜ëŠ” ê²ƒì˜ ê°€ì¹˜ëŠ” state \\(s\\)ì—ì„œ ì‹œì‘í•˜ì—¬ action \\(a\\)ë¥¼ ì·¨í•˜ê³  ê·¸ í›„ \\(\\pi\\)ë¥¼ ë”°ë¥¼ë•Œì˜ ê¸°ëŒ“ê°’ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\nê°ê°ì˜ stateì—ì„œ actionì„ ì·¨í–ˆì„ë•Œ ê·¸ ê°€ì¹˜ë¥¼ expected returnìœ¼ë¡œ ì¸¡ì •í•˜ë¯€ë¡œ action value functionì´ë¼ëŠ” ì´ë¦„ì´ ë¶™ì—ˆë‹¤.\nì—¬ê¸°ì„œëŠ” state sì—ì„œ action aë¥¼ ì·¨í•œê²ƒì— ëŒ€í•œ ê¸°ëŒ“ê°’ì„ ê³„ì‚°í•˜ë¯€ë¡œ given s,aì¼ë•Œì˜ conditional expectation of returnì„ êµ¬í•˜ë©´ ëœë‹¤.\n\n\\[q_{\\pi}(s,a) \\overset{\\Delta}{=} \\mathbb{E}[G_t|S_t = s,A_t = a] = \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}|S_t=s,A_t=a\\right]\\]"
  },
  {
    "objectID": "posts/RL/RL2-2 value function.html#recursive-relationships",
    "href": "posts/RL/RL2-2 value function.html#recursive-relationships",
    "title": "[ê°•í™”í•™ìŠµ] 2-2 Reward & Return & State Value f & Action Value f",
    "section": "recursive relationships",
    "text": "recursive relationships\n\nvë¥¼ next vë¡œ í‘œí˜„í•˜ê¸°\n\nFor any policy \\(\\pi\\) and any state \\(s\\), the following consistency condition holds between the value of \\(s\\) and the value of its possible succesor states \\(\\leftrightarrow\\) ìƒíƒœ \\(s\\)ì˜ state value functionì€ ë‹¤ìŒ ìƒíƒœ \\(s'\\)ì— ëŒ€í•œ state value functionì´ í¬í•¨ëœ ì‹ìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.ì¦‰,í˜„ì¬ìƒíƒœì˜ ê°€ì¹˜ëŠ” ë‹¤ìŒìƒíƒœì˜ ê°€ì¹˜ì™€ ì—°ê´€ê³¼ ê´€ë ¨ì´ ìˆë‹¤.\n\n\\[\\begin{align}\nv_{\\pi}(s) &\\overset{\\Delta}{=} \\mathbb{E}_{\\pi}[G_t|S_t=s] \\\\\n&=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_t=s] \\\\\n&=\\sum_a \\pi(a|s)\\sum_{s'}\\sum_{r}p(s',r|s,a)\\left[r + \\gamma E_{\\pi}[G_{t+1}|S_{t+1}=s'\\right]]\\\\\n&=\\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r + \\gamma  v_{\\pi}(s')\\right] , \\text{for all } s \\in S\\\\\n\\end{align}\\]\n\ní˜¹ì‹œ 2ì—ì„œ 3ë²ˆì‹ìœ¼ë¡œ ë„˜ì–´ê°€ëŠ”ê²Œ ì´í•´ê°€ ì˜ ì•ˆëœë‹¤ë©´ ë§í¬ë¥¼ ì°¸ê³ . í•µì‹¬ì€ law of total expectationì„ ì´í•´í•˜ëŠ” ê²ƒì´ë‹¤.\nì´ ì‹ì´ ê¸°ì–µí•˜ê¸°ê°€ ìƒë‹¹íˆ ì–´ë µë‹¤. ë”°ë¼ì„œ \\(s \\rightarrow s'\\)ì„ ë‚˜íƒ€ë‚¸ \\(v_{\\pi}\\)ì˜ backup diagramì„ í†µí•´ì„œ ìƒê°í•´ë³¼ ìˆ˜ ìˆë‹¤.\n\n\n\n\nbackup diagram of \\(v_{\\pi}\\)\n\n\n\nê°ê°ì˜ í°ìƒ‰ì›ì€ stateë¥¼ ê²€ì€ìƒ‰ì›ì€ state,action pairë¥¼ ë‚˜íƒ€ë‚¸ë‹¤.\nìƒíƒœsì—ì„œ ì‹œì‘í•˜ë©° policy \\(\\pi\\)ì— ì˜í•˜ì—¬ actionì„ ì·¨í•œë‹¤.\nactionì„ ì·¨í•˜ë©´ ë¦¬ì›Œë“œ\\(r\\)ì„ ì–»ê³  \\(s \\rightarrow s'\\)ìœ¼ë¡œ ìƒíƒœê°€ ë°”ë€Œë©° ì´ëŠ” transition \\(p(s',r|s,a)\\)ì— ì˜í•´ ê²°ì •ëœë‹¤.\n\nThe Bellman equation (3.14) averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way. \\(\\leftrightarrow\\)ì¦‰, ì´ˆê¸°ìƒíƒœ \\(s\\)ì—ì„œ ì‹œì‘í•˜ì—¬ ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” ëª¨ë“  \\(r,v_{\\pi'}\\)ì˜ ê²½ìš°ì— ëŒ€í•´ ê°ê°ì´ ë‚˜ì˜¬ í™•ë¥ ì„ ê³±í•˜ì—¬ averaging(expectation)ì„ êµ¬í•˜ë©´ \\(v_{\\pi}(s)\\)ë¼ëŠ” ê²ƒì´ë‹¤.\n\\[\\begin{align}\nv_{\\pi}(s) &= \\sum_{a,s',r}\\pi(a|s)p(s',r|s,a)\\left[r + \\gamma  v_{\\pi}(s')\\right] , \\\\\n&= \\sum_{a}\\pi(a|s) \\sum_{s',r}(s',r|s,a)\\left[r+\\gamma v_{\\pi}(s')\\right] \\,\\text{for all } s \\in S\\\\\n\\end{align}\\]\nê°œì¸ì ìœ¼ë¡œ ì¢€ ë” ìì„¸íˆ ê¸°ì–µí•˜ë ¤ê³  ì •ë¦¬í•´ë´¤ë‹¤.diagramì€ ìˆ˜ì‹ìì²´ëŠ” ì•„ë‹ˆê¸° ë•Œë¬¸ì— ê·¸ë¦¼ì„ ë³´ê³  ë‚˜ë¦„ëŒ€ë¡œ ê¸°ì–µí•  ìˆ˜ ìˆëŠ” ë°©ë²•ì„ ì°¾ìœ¼ë©´ ëœë‹¤ê³  ìƒê°í•œë‹¤. ë”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì´ ì •ë¦¬í•´ë´¤ì§€ë§Œ í—·ê°ˆë¦¬ë©´ passí•´ë„ ë¬´ë°©í•˜ë‹¤.\n\n\\(s \\rightarrow s'\\)ëŠ” ì—¬ëŸ¬ê°€ì§€ ê²½ìš°ê°€ ì¡´ì¬í•˜ë©° ê·¸ëŸ¬ë¯€ë¡œ ìƒíƒœ \\(s\\)ì˜ ê°€ì¹˜ \\(v_{\\pi}(s)\\)ëŠ” ë‹¤ìŒìƒíƒœ \\(s'\\)ì˜ ê°€ì¹˜ \\(v_{\\pi}(s')\\)ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤.\nê·¸ëŸ°ë° \\(s'\\)ì€ \\(r\\)ì´ í•­ìƒ ê°™ì´ ë”°ë¼ì˜¤ë¯€ë¡œ \\(v_{\\pi}(s)\\)ëŠ” \\(r+v_{\\pi}(s')\\)ì— ì˜í–¥ì„ ë°›ëŠ”ë‹¤.\nìƒíƒœsì—ì„œ aë¥¼ ì·¨í•˜ë©° \\(r\\)ê³¼ \\(s'\\)ì´ ë‚˜ì˜¬ í™•ë¥ ì€ policyì™€ transitionì˜ ê³±ì´ë‹¤. ì¦‰ \\(\\pi(a|s)p(s',r|s,a)\\)ì´ë‹¤.\nëª¨ë“  ê²½ìš°ì— ëŒ€í•´ ê³ ë ¤í•´ì•¼ í•˜ë¯€ë¡œ ëª¨ë“  \\(a,r,s'\\)ì— ê³±í•´ì¤€ë‹¤.\n\n\n\nQë¥¼ next Që¡œ í‘œí˜„í•˜ê¸°\n\në§ˆì°¬ê°€ì§€ë¡œ \\(s,a\\)ì—ì„œì˜ action value functionë„ next \\(s',a'\\)ì—ì„œì˜ action value functionìœ¼ë¡œ í‘œí˜„í•  ìˆ˜ ìˆë‹¤.\n\n\\[\\begin{align}\nq_{\\pi}(s,a) &\\overset{\\Delta}{=} \\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a] \\\\\n&=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_t=s,A_t=a] \\\\\n&=\\sum_{s',r}p(s',r|s,a)\\big[r+\\sum_{a'}p(s',r|s,a)\\pi(a'|s')q_{\\pi}(a',s')]\n\\end{align}\\]\n\nbackup diagramì€ ë‹¤ìŒê³¼ ê°™ë‹¤.\n\n\n\n\nbackup diagram of \\(q_{\\pi}\\)\n\n\n\nbackup diagramì„ ë³´ê³  ë‚˜ì˜¬ ìˆ˜ ìˆëŠ” \\(r,q_{\\pi}(s',a')\\)ì˜ ëª¨ë“  ê²½ìš°ì— ëŒ€í•˜ì—¬ ê³„ì‚°í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤.(state value functionì˜ \\(r,s'\\)ì€ ê°™ì´ ë”°ë¼ì˜¤ë¯€ë¡œ ê°™ì´ ê³„ì‚°í•´ì¤¬ì§€ë§Œ action value functionì€ \\(r,a\\)ëŠ” ë”°ë¡œì´ë¯€ë¡œ ë”°ë¡œ ê³„ì‚°í•˜ì—¬ ë”í•´ì¤€ë‹¤.)\n\n\\[\\begin{aligned}\nq_{\\pi}(s,a)&=\\sum_{s',r}p(s',r|s,a)r + \\sum_{s',r,a'}p(s',r|s,a)\\pi(a'|s')q_{\\pi}(a',s')\\\\\n&=\\sum_{s',r}p(s',r|s,a)\\big[r+\\sum_{a'}p(s',r|s,a)\\pi(a'|s')q_{\\pi}(a',s')]\\,\\text{for all } s \\in S,a \\in A(s)\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/RL/RL2-2 value function.html#optimal-policy",
    "href": "posts/RL/RL2-2 value function.html#optimal-policy",
    "title": "[ê°•í™”í•™ìŠµ] 2-2 Reward & Return & State Value f & Action Value f",
    "section": "optimal policy",
    "text": "optimal policy\noptimal policyëŠ” value functionì¸ \\(V(s_t)\\)ë¥¼ ê°€ì¥ í¬ê²Œ í•˜ëŠ” policyë“¤(\\(p(a_t|s_t),p(a_{t+1}|s_{t+1}),\\dots,p(a_{\\infty}|s_{\\infty})\\))ì´ë‹¤.ë‚˜ì¤‘ì— ë” ìì„¸íˆ ê³µë¶€í•´ì•¼í•  ê²ƒ ê°™ë‹¤.\naction value functionì€ state,actionì˜ í•¨ìˆ˜ì˜ ì…ë ¥ìœ¼ë¡œ ì£¼ì–´ì§€ëŠ” ì–´ë–¤ stateì—ì„œ (ë§ˆì°¬ê°€ì§€ë¡œ ì£¼ì–´ì§€ëŠ”)actionì„ ì·¨í–ˆì„ë•Œ íŠ¹ì • policyê°€ ì¢‹ì€ì§€ ë‚˜ìœì§€(ê°€ì¹˜)ë¥¼ í‰ê°€í•œë‹¤. í‰ê°€ëŠ” actionì„ ì·¨í•œë’¤ì˜ ë‹¤ìŒ stateë¶€í„° ë§ˆì§€ë§‰ ì‹œì ê¹Œì§€ Agentê°€ ê°€ëŠ¥í•œ ëª¨ë“  actionê³¼ ë†“ì—¬ì§ˆ ìˆ˜ ìˆëŠ” ëª¨ë“  stateë¥¼ ê³ ë ¤í•˜ì—¬ ê¸°ëŒ€ë˜ëŠ” ë³´ìˆ˜ì˜ ì´í•©ì„ ê³„ì‚°í•˜ëŠ”ë°©ì‹ì´ë‹¤. ì´ê²ƒë„ ë§ˆì°¬ê°€ì§€ë¡œ ê²°êµ­ì€ \\(G_t\\)ì— ëŒ€í•œ ì¡°ê±´ë¶€ í•¨ìˆ˜ì´ë‹¤.\n\\[\\begin{aligned}\nQ^{\\pi}(s_t,a_t) \\underset{=}{\\Delta} \\mathbb{E}[G_t|A_t=a_t,S_t=s_t,\\pi] = \\int_{s_{t+1}:a_{\\infty}}G_tp(s_{t+1},\\dots,a_{\\infty})ds_{t+1}:da_{\\infty}\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\end{aligned}\\]\n\\[\\begin{aligned}\nv_{\\pi}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": "",
    "text": "ì•ì„  í¬ìŠ¤íŒ…ì—ì„œ ë¥´ë²¡ë©”ì ¸ë¥¼ ë„ì…í•˜ì—¬ ë¬´ë¦¬ìˆ˜ ì§‘í•©ì˜ ê¸¸ì´ëŠ” \\(2\\pi\\) ìœ ë¦¬ìˆ˜ ì§‘í•©ì˜ ê¸¸ì´ëŠ” 0ì´ì˜€ì—ˆìŠµë‹ˆë‹¤.\nê·¸ëŸ¬ë‚˜ ì™œ ì´ë ‡ê²Œ ë˜ëŠ”ì§€ì— ëŒ€í•œ ì§ê´€ì€ ì „í˜€ ì–»ì„ ìˆ˜ ì—†ê³  ë°›ì•„ë“¤ì—¬ì•¼ í–ˆìŠµë‹ˆë‹¤.\nì´ë²ˆí¬ìŠ¤íŒ…ì—ì„œëŠ” ì´ëŠ” ì§‘í•©ê°„ì˜ ì „ì‚¬,ë‹¨ì‚¬,ì „ë‹¨ì‚¬ í•¨ìˆ˜ê°€ ì¡´ì¬ìœ ë¬´ë¥¼ í†µí•´ì„œ ë¬´í•œì§‘í•©ë“¤ê°„ì˜ cardinality(ìœ í•œì§‘í•©ì—ì„œì˜ í¬ê¸° ê°œë…)ë¥¼ ë¹„êµí•˜ê³ ì í•©ë‹ˆë‹¤.\nì´ë¥¼ í†µí•´ ìœ ë¦¬ìˆ˜ì§‘í•©ì˜ ê¸¸ì´ê°€ ë¬´ë¦¬ìˆ˜ì§‘í•©ì˜ ê¸¸ì´ë³´ë‹¤ ì™œ ì‘ì€ì§€ ì§ê´€ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ê·¸ë ‡ë‹¤ê³  ëª¨ìˆœì´ ì™„ì „íˆ ì‚¬ë¼ì§€ì§€ëŠ” ì•ŠìŠµë‹ˆë‹¤. ë¥´ë²¡ë©”ì ¸ë¥¼ ë„ì…í•´ë„ ì—¬ì „íˆ í™•ë¥ ì„ ëª¨ìˆœì—†ì´ ì •ì˜í•˜ê¸°ì—ëŠ” ë¶€ì¡±í•©ë‹ˆë‹¤.)"
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#injective-function",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#injective-function",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": " Injective function",
    "text": "Injective function\n\n\n\n\n\n\ndefinition of injective function\n\n\n\n\\[\\begin{aligned}\n&\\text{The function is injective, or one-to-one}\\\\\n&\\Longleftrightarrow \\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\n\\end{aligned}\\]\n\n\n\ndomainì— ì†í•˜ëŠ” ì„œë¡œë‹¤ë¥¸ ëª¨ë“  ë‘ ì›ì†Œê°€ codomainì— ì†í•˜ëŠ” ì„œë¡œë‹¤ë¥¸ ë‘ ì›ì†Œì— mappingë  ë•Œ Injection functionì…ë‹ˆë‹¤.\nì•”ê¸°(ì…ë ¥ì´ ë‹¤ë¥´ë©´ ì¶œë ¥ì´ ë‹¤ë¥´ë‹¤)\nëŠë‚Œ(í™”ì‚´í‘œê°€ ì«™ í´ì§€ëŠ” ëŠë‚Œì´ë‹¤.)"
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#surjective-function",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#surjective-function",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": " surjective function",
    "text": "surjective function\n\n\n\n\n\n\ndefinition of surjective function\n\n\n\n\\[\\begin{aligned}\n&\\text{The function is surjective, or onto} \\\\\n&\\Longleftrightarrow \\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\n\\end{aligned}\\]\n\n\n\ncodomainì— ì†í•˜ëŠ” ëª¨ë“  ì›ì†Œê°€ domainì— ì†í•˜ëŠ” ì›ì†Œì— ì ì–´ë„ í•˜ë‚˜ì˜ mappingì„ ë°›ì„ ë•Œ surjective functionì…ë‹ˆë‹¤.\nì•”ê¸°1:domianì˜ image(ìƒ)ì¸ ì¹˜ì—­ì´ ê³µì—­ê³¼ ê°™ìŠµë‹ˆë‹¤.\nì•”ê¸°2:ê³µì—­ì˜ ëª¨ë“  ì›ì†Œê°€ ì—­ìƒ(inverseimage)ì´ ì¡´ì¬í•©ë‹ˆë‹¤.\nëŠë‚Œ : í™”ì‚´í‘œê°€ ì«™ ëª¨ì´ëŠ” ëŠë‚Œ"
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#bijective-function",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#bijective-function",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": " bijective function",
    "text": "bijective function\n\n\n\n\n\n\ndefinition of bijective function\n\n\n\n\\[\\begin{aligned}\n&\\text{The function is bijective, or one-to-one and onto}\\\\\n&\\Longleftrightarrow \\forall y \\in Y,\\exists !x \\in X \\text{ such that } y = f(x) \\\\\n&\\text{where } \\exists!x \\text{ means \"there exists exactly one x\".}\n\\end{aligned}\\]\n\n\n\ncodomainì— ì†í•˜ëŠ” ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ domainì— ì†í•˜ëŠ” ì •í™•íˆ í•˜ë‚˜ì˜ ì›ì†Œì—ë§Œ mappingë˜ëŠ” ê²½ìš° bijective functionì´ë¼ê³  í•œë‹¤.\ninjectiveì´ë©° ë™ì‹œì— surjectiveì¸ functionì´ë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#ìš©ì–´-ì •ë¦¬í—·ê°ˆë¦¼",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#ìš©ì–´-ì •ë¦¬í—·ê°ˆë¦¼",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": " ìš©ì–´ ì •ë¦¬(í—·ê°ˆë¦¼)",
    "text": "ìš©ì–´ ì •ë¦¬(í—·ê°ˆë¦¼)\n\ninjective = ë‹¨ì‚¬ = one-to-one, injective function = injection = ë‹¨ì‚¬í•¨ìˆ˜\nsurjective = ì „ì‚¬ = onto, surjective function = surjection = ì „ì‚¬í•¨ìˆ˜\nbijecitve = ì „ë‹¨ì‚¬ = one-to-one and onto , bijective function = bijection = ì „ë‹¨ì‚¬í•¨ìˆ˜"
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#í•¨ìˆ˜ì˜-ì¢…ë¥˜ì™€-cardinalityì™€ì˜-ê´€ê³„",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#í•¨ìˆ˜ì˜-ì¢…ë¥˜ì™€-cardinalityì™€ì˜-ê´€ê³„",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": " í•¨ìˆ˜ì˜ ì¢…ë¥˜ì™€ Cardinalityì™€ì˜ ê´€ê³„",
    "text": "í•¨ìˆ˜ì˜ ì¢…ë¥˜ì™€ Cardinalityì™€ì˜ ê´€ê³„\n\n\n\n\n\n\nproperty1 : ì „ì‚¬í•¨ìˆ˜ì´ë©´ ë‹¨ì‚¬í•¨ìˆ˜ì´ë©´ ì „ë‹¨ì‚¬í•¨ìˆ˜ì´ë‹¤\n\n\n\n\nì–´ë–¤ í•¨ìˆ˜ê°€ ì „ì‚¬í•¨ìˆ˜ & ë‹¨ì‚¬í•¨ìˆ˜ \\(\\Longleftrightarrow\\) ì „ë‹¨ì‚¬í•¨ìˆ˜\n\n\n\n\n\n\n\n\n\nproperty2 : í•¨ìˆ˜ì˜ ì¢…ë¥˜ì™€ ë‘ ì§‘í•©ê°„ì˜ í¬ê¸°ë¹„êµ\n\n\n\në‘ ì§‘í•©ì‚¬ì´ì— ì ì ˆí•œ í•¨ìˆ˜ë¥¼ ì •ì˜í•˜ì—¬ ì§‘í•©ê°„ì˜ í¬ê¸°ë¹„êµê°€ ê°€ëŠ¥í•˜ë‹¤.\n\në‹¨ì‚¬í•¨ìˆ˜ì™€ cardinality\n\nì§‘í•© \\(X\\)ì—ì„œ ì§‘í•© \\(Y\\)ë¡œ ê°€ëŠ” ë‹¨ì‚¬í•¨ìˆ˜ê°€ ì¡´ì¬í•œë‹¤. \\(\\Longleftrightarrow |X| \\leq |Y|\\)\nì§‘í•© \\(X\\)ì—ì„œ ì§‘í•© \\(Y\\)ë¡œ ê°€ëŠ” ë‹¨ì‚¬í•¨ìˆ˜ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. \\(\\Longleftrightarrow |X| > |Y|\\)\nì£¼ì˜! ë‘ ëª…ì œê°€ â€œì´â€ì˜ ê´€ê³„ì— ìˆì–´ì„œ ìëª…í•˜ê²Œ ì°¸ì´ë˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ ëŒ€ì‘ê´€ê³„ë¥¼ ë´¤ì„ë•Œ ì°¸ì´ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n\n\n\nì „ì‚¬í•¨ìˆ˜ì™€ cardinality\n\nì§‘í•© \\(X\\)ì—ì„œ ì§‘í•© \\(Y\\)ë¡œ ê°€ëŠ” ì „ì‚¬í•¨ìˆ˜ê°€ ì¡´ì¬í•œë‹¤. \\(\\Longleftrightarrow |X| \\geq |Y|\\)\nì§‘í•© \\(X\\)ì—ì„œ ì§‘í•© \\(Y\\)ë¡œ ê°€ëŠ” ì „ì‚¬í•¨ìˆ˜ê°€ ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. \\(\\Longleftrightarrow |X| < |Y|\\)\n\n\n\nì „ë‹¨ì‚¬í•¨ìˆ˜ì™€ cardinality\n\nì§‘í•© \\(X\\)ì—ì„œ ì§‘í•© \\(Y\\)ë¡œ ê°€ëŠ” ì „ë‹¨ì‚¬í•¨ìˆ˜ê°€ ì¡´ì¬í•œë‹¤. \\(\\Longleftrightarrow |X| = |Y|\\)\nì£¼ì˜! ì „ë‹¨ì‚¬ í•¨ìˆ˜ëŠ” ì¡´ì¬í•˜ì§€ ì•Šì•„ë„ ë‘ ì§‘í•©ì˜ í¬ê¸°ëŠ” ê°™ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ìœ„ì˜ ê·¸ë¦¼ì˜ ë‹¨ì‚¬ë„ ì „ì‚¬ë„ ì•„ë‹Œ ê²½ìš°ì—ì„œ Yì˜ aë¥¼ ë¹¼ê³  ìƒê°í•´ë´…ì‹œë‹¤.)\n\n\n\n\n\n\n\n\n\n\nproperty3 : ë¶€ë¶„ì§‘í•©ê³¼ í¬ê¸°ë¹„êµ\n\n\n\n\\(X \\subset Y \\Rightarrow |X| \\leq |Y|\\)  \\(X \\supset Y \\Leftrightarrow |X| \\geq |Y|\\)  \\(X = Y \\Rightarrow |X| = |Y|\\)\n\n\n\n\n\n\n\n\në™ì¹˜ëª…ì œ ì •ë¦¬\n\n\n\n\n\\(X \\subset Y \\Longleftrightarrow |X| \\leq |y|\\) \n\\(X \\supset Y \\Longleftrightarrow |X| \\geq |y|\\) \n\\(X \\subset = \\Longleftrightarrow |X| = |y|\\)\n\n\n\n\nìœ„ì˜ ì‚¬ì‹¤ë“¤ì€ ìœ í•œì§‘í•©ì— ëŒ€í•´ì„œ ìƒê°í•´ë³´ë©´ ìëª…í•œ ì‚¬ì‹¤ë“¤ì…ë‹ˆë‹¤.\nìš°ë¦¬ëŠ” ì´ë¥¼ ë¬´í•œì§‘í•©ì— í™•ì¥í•˜ì—¬ ì ìš©í•¨ìœ¼ë¡œì„œ ë¬´í•œì§‘í•©ê°„ì˜ cardinalityë¥¼ ë¹„êµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#ì •ë¦¬",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#ì •ë¦¬",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": " ì •ë¦¬",
    "text": "ì •ë¦¬\n\nìœ í•œì§‘í•©ì—ì„œ ë‘ ì§‘í•©ê°„ì˜ í¬ê¸°ì˜ ë¹„êµëŠ” ë‘ ì§‘í•©ê°„ì— ì¡´ì¬í•  ìˆ˜ ìˆëŠ” í•¨ìˆ˜ê°€ ì–´ë–¤ í•¨ìˆ˜ì¸ì§€ë¥¼ í†µí•´ì„œ ê°€ëŠ¥í•©ë‹ˆë‹¤.\nì´ë¥¼ ë¬´í•œì§‘í•©ì—ì„œ ì ìš©í•˜ì—¬ ë‘ ì§‘í•©ê°„ì˜ í¬ê¸°ë¹„êµë¥¼ í•´ë³´ê³ ì í•©ë‹ˆë‹¤.(ë‹¤ìŒ í¬ìŠ¤íŒ…!)"
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html",
    "title": "[Stochestic Process] 3.ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸° ë¹„êµ",
    "section": "",
    "text": "ì´ì „í¬ìŠ¤íŒ…ì—ì„œ ìœ í•œì§‘í•©ê°„ì˜ í¬ê¸°ë¹„êµëŠ” ë‘ ì§‘í•©ê°„ì˜ ì–´ë–¤ í•¨ìˆ˜ê°€ ì¡´ì¬í•˜ëŠ”ì§€ë¥¼ í†µí•´ì„œ ê°€ëŠ¥í•˜ë‹¤ê³  ì •ë¦¬í–ˆìŠµë‹ˆë‹¤.\nì—¬ê¸°ì„œëŠ” ì´ë¥¼ ë¬´í•œì§‘í•©ì—ì„œ í™•ì¥í•˜ì—¬ ì‹¤ì œë¡œ ì—¬ëŸ¬ê°€ì§€ ì„œë¡œë‹¤ë¥¸ ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸°(ì—„ë°€íˆ,cardinalityì…ë‹ˆë‹¤. í¬ìŠ¤íŒ…ì—ì„œëŠ” í¬ê¸°ì™€ cardinalityë¥¼ í˜¼ìš©í•©ë‹ˆë‹¤.)ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\nì¦‰, ë‘ ë¬´í•œì§‘í•©ê°„ì˜ ì „ì‚¬,ë‹¨ì‚¬,ì „ë‹¨ì‚¬ í•¨ìˆ˜ê°€ ì¡´ì¬ì—¬ë¶€ë¥¼ í™•ì¸í•©ë‹ˆë‹¤.\nê²°ê³¼ì ìœ¼ë¡œ ì´ì „ì˜ í¬ìŠ¤íŒ…ì—ì„œ ë¬´ë¦¬ìˆ˜ ì§‘í•©ì˜ ê¸¸ì´ëŠ” \\(2\\pi\\) ìœ ë¦¬ìˆ˜ ì§‘í•©ì˜ ê¸¸ì´ëŠ” 0ì´ì˜€ëŠ”ë° ì´ì— ëŒ€í•œ ì§ê´€ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìì—°ìˆ˜ì§‘í•©ì˜-vs-í™€ìˆ˜ì§‘í•©ì§ìˆ˜ì§‘í•©",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìì—°ìˆ˜ì§‘í•©ì˜-vs-í™€ìˆ˜ì§‘í•©ì§ìˆ˜ì§‘í•©",
    "title": "[Stochestic Process] 3.ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸° ë¹„êµ",
    "section": " ìì—°ìˆ˜ì§‘í•©ì˜ vs í™€ìˆ˜ì§‘í•©,ì§ìˆ˜ì§‘í•©",
    "text": "ìì—°ìˆ˜ì§‘í•©ì˜ vs í™€ìˆ˜ì§‘í•©,ì§ìˆ˜ì§‘í•©\n\ní™€ìˆ˜ì§‘í•© vs ì§ìˆ˜ì§‘í•©ì˜ ê²½ìš°\n\në¨¼ì € í™€ìˆ˜ì§‘í•© vs ì§ìˆ˜ì§‘í•©ì˜ í¬ê¸°ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\nì¦‰,\\(X = \\{1,3,5,\\dots\\},Y = \\{2,4,6,\\dots\\}\\)ì˜ í¬ê¸°ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\në‘ ë¬´í•œì§‘í•©ì‚¬ì´ì— í¬ê¸°ë¥¼ ë¹„êµí•˜ë ¤ë©´ ì–´ë–¤í•¨ìˆ˜ê°€ ì •ì˜ë  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•œ í›„ í•¨ìˆ˜ì˜ ì¢…ë¥˜ì™€ Cardinalityì™€ì˜ ê´€ê³„ì„ í†µí•´ì„œ ê²°ë¡ ì„ ë‚´ë¦¬ë©´ ë©ë‹ˆë‹¤.\n\n\n\\(f\\)ì •ì˜í•˜ê¸°\n\\[\\begin{aligned}\nf(1) &= 2 \\\\\nf(3) &= 4 \\\\\nf(5) &= 6 \\\\\n&\\vdots\n\\end{aligned}\\]\në‹¨ì‚¬í•¨ìˆ˜ì¸ê°€?(ì •ì˜ë¥¼ í†µí•´ í™•ì¸)\n\ndefinition : \\(\\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\\)\nì •ì˜ì—­ì— ì†í•œ ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ ê³µì—­ì— ì†í•˜ëŠ” ì„œë¡œë‹¤ë¥¸ ì›ì†Œì— mappingë˜ë¯€ë¡œ ë‹¨ì‚¬í•¨ìˆ˜ì´ë‹¤.\n\nì „ì‚¬í•¨ìˆ˜ì¸ê°€?(ì •ì˜ë¥¼ í†µí•´ í™•ì¸)\n\ndefinition : \\(\\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\\)\nê³µì—­ì— ì†í•œ ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ ì ì–´ë„ í•˜ë‚˜ì˜ ì •ì˜ì—­ì— ì†í•˜ëŠ” ì›ì†Œì— mappingë˜ë¯€ë¡œ ì „ì‚¬í•¨ìˆ˜ì´ë‹¤.\n\n\n\n\n\n\n\n\ní™€ìˆ˜ì§‘í•© vs ì§ìˆ˜ì§‘í•©\n\n\n\n\në¬´í•œì§‘í•©ì¸ í™€ìˆ˜ì§‘í•©ê³¼ ì§ìˆ˜ì§‘í•©ì‚¬ì´ì—ëŠ” ì „ë‹¨ì‚¬í•¨ìˆ˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\në”°ë¼ì„œ í™€ìˆ˜ì§‘í•©ê³¼ ì§ìˆ˜ì§‘í•©ì˜ í¬ê¸°ëŠ” ê°™ìŠµë‹ˆë‹¤.\nì¦‰,\\(|X| = |Y|\\)\n\n\n\n\n\nìì—°ìˆ˜(ì–‘ì˜ì •ìˆ˜)ì§‘í•© vs í™€ìˆ˜,ì§ìˆ˜ì§‘í•©ì˜ ê²½ìš°\n\n\\(f\\)ì •ì˜í•˜ê¸°\n\\[\\begin{aligned}\nf(1) &= 2 \\\\\nf(2) &= 4 \\\\\nf(3) &= 6 \\\\\n&\\vdots\n\\end{aligned}\\]\në‹¨ì‚¬í•¨ìˆ˜ì¸ê°€?\n\ndefinition : \\(\\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\\)\nì •ì˜ì—­ì— ì†í•œ ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ ê³µì—­ì— ì†í•˜ëŠ” ì„œë¡œë‹¤ë¥¸ ì›ì†Œì— mappingë˜ë¯€ë¡œ ë‹¨ì‚¬í•¨ìˆ˜ì´ë‹¤.\n\nì „ì‚¬í•¨ìˆ˜ì¸ê°€?\n\ndefinition : \\(\\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\\)\nê³µì—­ì— ì†í•œ ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ ì ì–´ë„ í•˜ë‚˜ì˜ ì •ì˜ì—­ì— ì†í•˜ëŠ” ì›ì†Œì— mappingë˜ë¯€ë¡œ ì „ì‚¬í•¨ìˆ˜ì´ë‹¤.\n\n\n\n\n\n\n\n\nìì—°ìˆ˜ì§‘í•© vs í™€ìˆ˜,ì§ìˆ˜ì§‘í•©\n\n\n\n\nìì—°ìˆ˜ì§‘í•©ê³¼ ì§ìˆ˜ì§‘í•©ì‚¬ì´ì— ì „ë‹¨ì‚¬í•¨ìˆ˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\në”°ë¼ì„œ ìì—°ìˆ˜ì§‘í•©ì˜ í¬ê¸°ì™€ ì§ìˆ˜ì§‘í•©ì˜ í¬ê¸°ëŠ” ê°™ìŠµë‹ˆë‹¤.\nì´ì „ì— ì§ìˆ˜ì§‘í•©ì˜ í¬ê¸°ì™€ í™€ìˆ˜ì§‘í•©ì˜ í¬ê¸°ëŠ” ê°™ì•˜ë‹¤ê³  ê²°ë¡ ë‚´ë ¸ìœ¼ë¯€ë¡œ ì„¸ ì§‘í•©ì˜ í¬ê¸°ëŠ” ê°™ìŠµë‹ˆë‹¤.\nì¦‰, \\(|\\mathbb{N}| = |Y| = \\aleph_0\\)(ì—¬ê¸°ì„œ yëŠ” ì§ìˆ˜ì§‘í•© or í™€ìˆ˜ì§‘í•©ì˜ í¬ê¸°ë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.)\n\n\n\n\nê²°ë¡  Q-ê¶ê¸ˆí•œê±° : ì •ìˆ˜ì§‘í•©ì— ëŒ€í•´ì„œëŠ” ì–´ë–»ê²Œ cardinalityê°€ ê°™ìŒì„ ìœ ë„í•  ìˆ˜ ìˆëŠ”ê°€?\n\në˜í•œ ì´ë¥¼ í™•ì¥í•˜ì—¬,ì •ìˆ˜ì§‘í•©(ì–‘ì˜ì •ìˆ˜ + ìŒì˜ì •ìˆ˜ + 0)ë„ ìì—°ìˆ˜ì§‘í•©ê³¼ cardinalityê°€ ê°™ìŒì„ ë³´ì¼ ìˆ˜ ìˆë‹¤.(???)\nì¦‰, \\(|\\mathbb{N}| = |\\mathbb{N}| = \\aleph_0\\)"
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìì—°ìˆ˜ì§‘í•©-vs-ìì—°ìˆ˜ì§‘í•©-cup-0",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìì—°ìˆ˜ì§‘í•©-vs-ìì—°ìˆ˜ì§‘í•©-cup-0",
    "title": "[Stochestic Process] 3.ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸° ë¹„êµ",
    "section": "ìì—°ìˆ˜ì§‘í•© vs ìì—°ìˆ˜ì§‘í•© \\(\\cup \\{0\\}\\)",
    "text": "ìì—°ìˆ˜ì§‘í•© vs ìì—°ìˆ˜ì§‘í•© \\(\\cup \\{0\\}\\)\n\nìì—°ìˆ˜ì§‘í•©ê³¼ ìì—°ìˆ˜ì§‘í•© \\(\\cup \\{0\\}\\)ì˜ í¬ê¸°ë¥¼ ë¹„êµí•´ë³´ê² ìŠµë‹ˆë‹¤.\nì¦‰, \\(\\mathbb{N} = \\{1,2,3,\\dots\\}, \\mathbb{N}\\cup \\{0\\} = \\{0,1,2,3,\\dots\\}\\)ì˜ í¬ê¸°ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\n\n\n\\(f\\)ì •ì˜í•˜ê¸°\n\\[\\begin{aligned}\nf(1) &= 0 \\\\\nf(2) &= 1 \\\\\nf(3) &= 2 \\\\\n&\\vdots\n\\end{aligned}\\]\në‹¨ì‚¬í•¨ìˆ˜ì¸ê°€?(ì •ì˜ë¥¼ í†µí•´ í™•ì¸)\n\ndefinition : \\(\\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\\)\nì •ì˜ì—­ì— ì†í•œ ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ ê³µì—­ì— ì†í•˜ëŠ” ì„œë¡œë‹¤ë¥¸ ì›ì†Œì— mappingë˜ë¯€ë¡œ ë‹¨ì‚¬í•¨ìˆ˜ì…ë‹ˆë‹¤.\n\nì „ì‚¬í•¨ìˆ˜ì¸ê°€?(ì •ì˜ë¥¼ í†µí•´ í™•ì¸)\n\ndefinition : \\(\\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\\)\nê³µì—­ì— ì†í•œ ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ ì ì–´ë„ í•˜ë‚˜ì˜ ì •ì˜ì—­ì— ì†í•˜ëŠ” ì›ì†Œì— mappingë˜ë¯€ë¡œ ì „ì‚¬í•¨ìˆ˜ì…ë‹ˆë‹¤.\n\n\n\n\n\n\n\n\nìì—°ìˆ˜ì§‘í•© vs ìì—°ìˆ˜ì§‘í•© \\(\\cup \\{0\\}\\)\n\n\n\n\nìì—°ìˆ˜ì§‘í•©ê³¼ ìì—°ìˆ˜ì§‘í•© \\(\\cup \\{0\\}\\)ì‚¬ì´ì— ì „ë‹¨ì‚¬í•¨ìˆ˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\në”°ë¼ì„œ ë‘ ì§‘í•©ì˜ í¬ê¸°ëŠ” ê°™ìŠµë‹ˆë‹¤.\nì¦‰, |\\(\\mathbb{N}| = |\\mathbb{N} \\cup \\{0\\}|\\)ì…ë‹ˆë‹¤.\nì¡°ê¸ˆ ê³¼ê°í•˜ê²Œ \\(\\aleph_0 + 1 = \\aleph_0\\)ìœ¼ë¡œ ê¸°ì–µí•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.(ì—„ë°€í•˜ì§„ ì•ŠìŠµë‹ˆë‹¤.)"
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìì—°ìˆ˜ì§‘í•©-vs-ìì—°ìˆ˜ì§‘í•©-cup-ìŒì˜ì •ìˆ˜ì§‘í•©",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìì—°ìˆ˜ì§‘í•©-vs-ìì—°ìˆ˜ì§‘í•©-cup-ìŒì˜ì •ìˆ˜ì§‘í•©",
    "title": "[Stochestic Process] 3.ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸° ë¹„êµ",
    "section": "ìì—°ìˆ˜ì§‘í•© vs ìì—°ìˆ˜ì§‘í•© \\(\\cup\\) ìŒì˜ì •ìˆ˜ì§‘í•©",
    "text": "ìì—°ìˆ˜ì§‘í•© vs ìì—°ìˆ˜ì§‘í•© \\(\\cup\\) ìŒì˜ì •ìˆ˜ì§‘í•©\n\nìì—°ìˆ˜ì§‘í•©ê³¼ ì–‘ì˜ì •ìˆ˜ì§‘í•©\\(\\cup\\)ìŒì˜ì •ìˆ˜ì§‘í•©ì„ ë¹„êµí•©ë‹ˆë‹¤.\nì¦‰, \\(\\mathbb{N} = \\{1,2,3,\\dots\\}, \\mathbb{N}\\cup \\mathbb{N}^- = \\{-1,1,-2,2,\\dots\\}\\)ì˜ í¬ê¸°ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\n\n\n\\(f\\)ì •ì˜í•˜ê¸°\n\\[\\begin{aligned}\nf(1) &= -1 \\\\\nf(2) &= 1 \\\\\nf(3) &= -2 \\\\\nf(4) &= 2 \\\\\n&\\vdots\n\\end{aligned}\\]\n\n\në‘ ì§‘í•©ê°„ì˜ mappingì„ -1,1,-2,2 ì´ëŸ°ì‹ìœ¼ë¡œ mappingí•˜ëŠ”ê²Œ í¬ì¸íŠ¸ì…ë‹ˆë‹¤.(ì „ ëª»í–ˆìŠµë‹ˆë‹¤â€¦)\n\n\në‹¨ì‚¬í•¨ìˆ˜ì¸ê°€?(ì •ì˜ë¥¼ í†µí•´ í™•ì¸)\n\ndefinition : \\(\\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\\)\nì •ì˜ì—­ì— ì†í•œ ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ ê³µì—­ì— ì†í•˜ëŠ” ì„œë¡œë‹¤ë¥¸ ì›ì†Œì— mappingë˜ë¯€ë¡œ ë‹¨ì‚¬í•¨ìˆ˜ì…ë‹ˆë‹¤.\n\nì „ì‚¬í•¨ìˆ˜ì¸ê°€?(ì •ì˜ë¥¼ í†µí•´ í™•ì¸)\n\ndefinition : \\(\\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\\)\nê³µì—­ì— ì†í•œ ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ ì ì–´ë„ í•˜ë‚˜ì˜ ì •ì˜ì—­ì— ì†í•˜ëŠ” ì›ì†Œì— mappingë˜ë¯€ë¡œ ì „ì‚¬í•¨ìˆ˜ì…ë‹ˆë‹¤.\n\n\n\n\n\n\n\n\nìì—°ìˆ˜ì§‘í•© vs ìì—°ìˆ˜ì§‘í•© \\(\\cup\\) ìŒì˜ì •ìˆ˜ì§‘í•©\n\n\n\n\nìì—°ìˆ˜ì§‘í•©ê³¼ ìì—°ìˆ˜ì§‘í•© \\(\\cup\\) ìŒì˜ì •ìˆ˜ì§‘í•©ì‚¬ì´ì— ì „ë‹¨ì‚¬í•¨ìˆ˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\në”°ë¼ì„œ ë‘ ì§‘í•©ì˜ í¬ê¸°ëŠ” ê°™ìŠµë‹ˆë‹¤.\nì¦‰, |\\(\\mathbb{N}| = |\\mathbb{N} \\cup \\mathbb{N}^-| = \\aleph_0\\)ì…ë‹ˆë‹¤.\nì´ê²ƒë„ ì¡°ê¸ˆ ê³¼ê°í•˜ê²Œ \\(\\aleph_0 \\times 2 = \\aleph_0\\)ë¡œ ê¸°ì–µí•˜ë©´ ì¢‹ìŠµë‹ˆë‹¤.\nìì—°ìˆ˜ì§‘í•©í¬ê¸°\\(\\aleph_0\\) = ìì—°ìˆ˜ì§‘í•©í¬ê¸°\\(\\aleph_0\\) + ìŒì˜ì •ìˆ˜ì§‘í•©í¬ê¸°\\(\\aleph_0\\)ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.(ìŒì˜ì •ìˆ˜ì§‘í•©ì˜ í¬ê¸°ëŠ” ì¦ëª…ìƒëµ,í•´ë³´ë©´ ìì—°ìˆ˜ì§‘í•©ê³¼ ê°™ìŠµë‹ˆë‹¤.)"
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìì—°ìˆ˜ì§‘í•©-vs-ì •ìˆ˜ì§‘í•©",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìì—°ìˆ˜ì§‘í•©-vs-ì •ìˆ˜ì§‘í•©",
    "title": "[Stochestic Process] 3.ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸° ë¹„êµ",
    "section": "ìì—°ìˆ˜ì§‘í•© vs ì •ìˆ˜ì§‘í•©",
    "text": "ìì—°ìˆ˜ì§‘í•© vs ì •ìˆ˜ì§‘í•©\n\nìì—°ìˆ˜ì§‘í•©ê³¼ ì–‘ì˜ì •ìˆ˜ì§‘í•©\\(\\cup\\)ìŒì˜ì •ìˆ˜ì§‘í•©ì„ ë¹„êµí•©ë‹ˆë‹¤.\nì¦‰, \\(\\mathbb{N} = \\{1,2,3,\\dots\\}, \\mathbb{Z} = \\{0,-1,1,-2,2,\\dots\\}\\)ì˜ í¬ê¸°ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\n\n\n\\(f\\)ì •ì˜í•˜ê¸°\n\\[\\begin{aligned}\nf(1) &= 0 \\\\\nf(2) &= -1 \\\\\nf(3) &= -1 \\\\\nf(4) &= -2 \\\\\nf(5) &= -2 \\\\\n&\\vdots\n\\end{aligned}\\]\n\n\nì´ì „ ì±•í„°ì™€ ë¹„ìŠ·í•˜ê²Œ ë‘ ì§‘í•©ê°„ì˜ mappingì„ 0,-1,1,-2,2 ì´ëŸ°ì‹ìœ¼ë¡œ êµì°¨í•˜ë©´ì„œ mappingí•©ë‹ˆë‹¤.\n\n\në‹¨ì‚¬í•¨ìˆ˜ì¸ê°€?(ì •ì˜ë¥¼ í†µí•´ í™•ì¸)\n\ndefinition : \\(\\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\\)\nì •ì˜ì—­ì— ì†í•œ ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ ê³µì—­ì— ì†í•˜ëŠ” ì„œë¡œë‹¤ë¥¸ ì›ì†Œì— mappingë˜ë¯€ë¡œ ë‹¨ì‚¬í•¨ìˆ˜ì…ë‹ˆë‹¤.\n\nì „ì‚¬í•¨ìˆ˜ì¸ê°€?(ì •ì˜ë¥¼ í†µí•´ í™•ì¸)\n\ndefinition : \\(\\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\\)\nê³µì—­ì— ì†í•œ ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ ì ì–´ë„ í•˜ë‚˜ì˜ ì •ì˜ì—­ì— ì†í•˜ëŠ” ì›ì†Œì— mappingë˜ë¯€ë¡œ ì „ì‚¬í•¨ìˆ˜ì…ë‹ˆë‹¤.\n\n\n\n\n\n\n\n\nìì—°ìˆ˜ì§‘í•© vs ì •ìˆ˜ì§‘í•©\n\n\n\n\nìì—°ìˆ˜ì§‘í•©ê³¼ ì •ìˆ˜ì§‘í•©ì‚¬ì´ì— ì „ë‹¨ì‚¬í•¨ìˆ˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤.\në”°ë¼ì„œ ë‘ ì§‘í•©ì˜ í¬ê¸°ëŠ” ê°™ìŠµë‹ˆë‹¤.\nì¦‰, |\\(\\mathbb{N}| = |\\mathbb{Z}| = \\aleph_0\\)ì…ë‹ˆë‹¤.\nì´ê²ƒì— ìƒê°í–ˆë˜ ê³¼ê°í•œ ë…¼ë¦¬ë“¤ì„ í•©ì³ì„œ êµ¬í•´ë³¼ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.(\\(\\aleph_0 \\times 2 = \\aleph_0\\),\\(\\aleph_0 + 1 = \\aleph_0\\))"
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìì—°ìˆ˜ì§‘í•©-vs-ìœ ë¦¬ìˆ˜ì§‘í•©",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìì—°ìˆ˜ì§‘í•©-vs-ìœ ë¦¬ìˆ˜ì§‘í•©",
    "title": "[Stochestic Process] 3.ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸° ë¹„êµ",
    "section": "ìì—°ìˆ˜ì§‘í•© vs ìœ ë¦¬ìˆ˜ì§‘í•©",
    "text": "ìì—°ìˆ˜ì§‘í•© vs ìœ ë¦¬ìˆ˜ì§‘í•©\n\nìì—°ìˆ˜ì§‘í•©ê³¼ ìœ ë¦¬ìˆ˜ì§‘í•©ì˜ í¬ê¸°ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\nìœ ë¦¬ìˆ˜ ì´ì „ì˜ ë¹„êµì²˜ëŸ¼ ë­”ê°€ í•œë²ˆì— ë‘ ì§‘í•©ì‚¬ì´ë¥¼ ë¹„êµí•˜ê¸°ê°€ ì–´ë µìŠµë‹ˆë‹¤.\në”°ë¼ì„œ ë¨¼ì € ì–‘ì˜ ìœ ë¦¬ìˆ˜ì§‘í•© (\\(\\mathbb{Q}^+\\)ë¡œ ì •ì˜í•˜ê² ìŠµë‹ˆë‹¤.)ì„ í¬í•¨í•˜ëŠ” ì§‘í•© \\(Y\\)ì„ ê°€ì •í•˜ê³  ì´ë¥¼ ë¨¼ì € ìì—°ìˆ˜ ì§‘í•©ê³¼ ë¹„êµí•©ë‹ˆë‹¤.\nì¦‰,\\(\\mathbb{N} = \\{1,2,\\dots\\},Y = \\{1,\\frac{2}{1}\\,\\frac{1}{2},\\frac{1}{3},\\frac{2}{2},\\frac{3}{1}\\dots\\} \\supset \\mathbb{Q}^+\\)ì…ë‹ˆë‹¤.\nê·¸ë ‡ë‹¤ë©´, ì§‘í•©\\(Y\\)ì˜ ëª¨ë“  ì›ì†ŒëŠ” ë‹¤ìŒê³¼ ê°™ì€ ê²©ìì˜ ê°€ë¡œì¶•ì„ ë¶„ëª¨ë¡œ ì„¸ë¡œì¶•ì„ ë¶„ìë¡œ ìƒê°í• ë•Œ ê²©ììœ„ì— ëª¨ë‘ ë†“ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\n\\(f\\)ì •ì˜í•˜ê¸°\n\\[\\begin{aligned}\nf(1) &= 1 \\\\\nf(2) &= \\frac{2}{1} \\\\\nf(3) &= \\frac{1}{2} \\\\\nf(4) &= \\frac{1}{3} \\\\\nf(5) &= \\frac{2}{2} \\\\\nf(6) &= \\frac{3}{1} \\\\\n&\\vdots\n\\end{aligned}\\]\n\n\nìì—°ìˆ˜1ë¶€í„° ì°¨ë¡€ëŒ€ë¡œ ê²©ììœ„ì˜ í™”ì‚´í‘œì˜ ìˆœì„œë¥¼ ë”°ë¼ì„œ ì§‘í•©\\(Y\\)ì˜ ì›ì†Œì— ë§¤í•‘í•©ë‹ˆë‹¤.\nëŒ€ê°ì„  ì–´ë–¤ ë°©í–¥ì´ë˜ ì¼ë‹¨ ë°©í–¥ì´ ì •í•´ì§€ë©´ ì •í•´ì§„ ë°©í–¥ì˜ ì›ì†ŒëŠ” ëª¨ë‘ ìŠ¤ì¹˜ë©´ì„œ ì§€ë‚˜ê°‘ë‹ˆë‹¤.\n\n\në‹¨ì‚¬í•¨ìˆ˜ì¸ê°€?\n\ndefinition : \\(\\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\\)\nì •ì˜ì—­ì— ì†í•œ ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ ê³µì—­ì— ì†í•˜ëŠ” ì„œë¡œë‹¤ë¥¸ ì›ì†Œì— mappingë˜ë¯€ë¡œ ë‹¨ì‚¬í•¨ìˆ˜ì´ë‹¤.\n\nì „ì‚¬í•¨ìˆ˜ì¸ê°€?\n\ndefinition : \\(\\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\\)\nê³µì—­(ì–‘ì˜ ìœ ë¦¬ìˆ˜ì§‘í•©)ì— ì†í•œ ì„ì˜ì˜,ëª¨ë“  ì›ì†Œê°€ ì ì–´ë„ í•˜ë‚˜ì˜ ì •ì˜ì—­ì— ì†í•˜ëŠ” ì›ì†Œì— mappingë˜ë¯€ë¡œ ì „ì‚¬í•¨ìˆ˜ì´ë‹¤.\n\nìœ ë„\n\n\nìš°ë¦¬ëŠ” \\(\\mathbb{Q}^+ \\subset Y\\) ì´ë¯€ë¡œ \\(|\\mathbb{Q}^+| \\leq |Y|\\)ì™€ \\(\\mathbb{N} \\subset \\mathbb{Q}^+\\)ì´ë¯€ë¡œ \\(|\\mathbb{N}| = \\aleph_0 \\leq |\\mathbb{Q}^+|\\)ì„ ì´ë¯¸ ì•Œê³ ìˆìŠµë‹ˆë‹¤.(ì–‘ì˜ ìœ ë¦¬ìˆ˜ì§‘í•©ì€ ìì—°ìˆ˜ë¥¼ í¬í•¨í•˜ë©°, ëª…ì œëŠ” í•¨ìˆ˜ì˜ ì¢…ë¥˜ì™€ cardinalityì™€ì˜ ê´€ê³„ì—ì„œ ì¦ëª…ëœ ì‚¬ì‹¤ì…ë‹ˆë‹¤.)\në˜í•œ 3ë²ˆê¹Œì§€ì˜ ìœ ë„ê³¼ì •ì— ì˜í•˜ì—¬ ìƒê¸°ì‹ì— ì˜í•˜ì—¬ \\(|\\mathbb{N}| = |Y| = \\aleph_0\\) ì…ë‹ˆë‹¤.\nê·¸ë ‡ë‹¤ë©´ \\(|\\mathbb{N}| \\leq|\\mathbb{Q}^+| \\leq |Y|\\) ì´ê³  \\(|\\mathbb{N}| = |Y| = \\aleph_0\\)ì´ë¯€ë¡œ \\(|\\mathbb{Q}|^+ = \\aleph_0\\)ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\në˜í•œ ìŒì˜ ìœ ë¦¬ìˆ˜ì§‘í•© \\(|\\mathbb{Q}|^-\\)ë„ ìì—°ìˆ˜ì§‘í•©ì˜ í¬ê¸°ì™€ ê°™ìŒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ì¦ëª…ìƒëµ,ìœ„ì™€ ê°™ì´ ê²©ìì— ëŒ€ê³  í•˜ë©´ ë©ë‹ˆë‹¤.)\nìœ ë¦¬ìˆ˜ì§‘í•©ì€ ì–‘ì˜ ìœ ë¦¬ìˆ˜ì§‘í•© + ìŒì˜ ìœ ë¦¬ìˆ˜ì§‘í•© + \\(\\{0\\}\\)ì…ë‹ˆë‹¤.\nì—¬ê¸°ì„œ ê³¼ê°í•˜ê²Œ ì‚¬ìš©í•œ ì•ì„  ì •ë¦¬ë“¤ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. (\\(\\aleph_0 \\times 2 = \\aleph_0\\),\\(\\aleph_0 + 1 = \\aleph_0\\))\në”°ë¼ì„œ \\(|\\mathbb{Q}| = |\\mathbb{Q}^+| + |\\mathbb{Q}^-| + |{0}| = \\aleph_0 \\times 2 + 1 = \\aleph_0 + 1 = \\aleph_0\\) ì…ë‹ˆë‹¤.\n\n\n\n\n\n\n\nìì—°ìˆ˜ì§‘í•© vs ìœ ë¦¬ìˆ˜ì§‘í•©\n\n\n\n\nìœ ë¦¬ìˆ˜ì§‘í•©ë³´ë‹¤ ë” í° ì§‘í•©ì„ ìƒì •í•˜ê³  ë¶€ë¶„ì§‘í•©ê³¼ ê´€ë ¨ëœ ëª…ì œë¥¼ ì‚¬ìš©í•˜ì—¬ ìœ ë¦¬ìˆ˜ì§‘í•©ì˜ í¬ê¸°ë¥¼ ìœ ë„í–ˆìŠµë‹ˆë‹¤.\nê²°ê³¼ì ìœ¼ë¡œ, ìœ ë¦¬ìˆ˜ì§‘í•©ê³¼ ìì—°ìˆ˜ì§‘í•©ì˜ í¬ê¸°ëŠ” ê°™ìŠµë‹ˆë‹¤.\nì¦‰, \\(|\\mathbb{Q}| = |\\mathbb{N}| = \\aleph_0\\)ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ì§ê´€ì ì¸-ì •ë¦¬",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ì§ê´€ì ì¸-ì •ë¦¬",
    "title": "[Stochestic Process] 3.ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸° ë¹„êµ",
    "section": "ì§ê´€ì ì¸ ì •ë¦¬",
    "text": "ì§ê´€ì ì¸ ì •ë¦¬\n\nìœ„ì—ì„œ ë‚˜ì™”ë˜ (ì—„ë°€í•˜ì§„ ì•Šì€..)ì‚¬ì‹¤ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.\n\\(\\aleph_0 + 1 = \\aleph_0\\,\\,,|\\mathbb{N}| = |\\mathbb{N}| \\cup \\{0\\}\\)\n\\(\\aleph_0 \\times 2 = \\aleph_0\\,\\,,\\)ìì—°ìˆ˜ì§‘í•©ê³¼ ìŒì˜ì •ìˆ˜ì§‘í•©ì„ ë”í•œ ì§‘í•©ì˜ ì¹´ë””ë„ë¦¬í‹°ëŠ” ìì—°ìˆ˜ì§‘í•©ì˜ ì¹´ë””ë„ë¦¬í‹°ì™€ ê°™ìŠµë‹ˆë‹¤.\n\\(\\aleph_0\\times \\aleph_0 = \\aleph_0 ^2 = \\aleph_0\\,\\,,\\)ìì—°ìˆ˜ì§‘í•©ê³¼ ìì—°ìˆ˜ì§‘í•©ì˜ ê³±ì§‘í•©ìœ¼ë¡œ ìœ ë¦¬ìˆ˜ì§‘í•©(ë³´ë‹¤ í°)ì„ ë§Œë“¤ ìˆ˜ ìˆëŠ”ë° ì´ ì§‘í•©ë„ ìì—°ìˆ˜ì§‘í•©ì˜ ì¹´ë””ë„ë¦¬í‹°ì™€ ê°™ìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ê·€ë¥˜ë²•",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ê·€ë¥˜ë²•",
    "title": "[Stochestic Process] 3.ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸° ë¹„êµ",
    "section": "ê·€ë¥˜ë²•",
    "text": "ê·€ë¥˜ë²•\n\nê·€ë¥˜ë²•ì€ ëª…ì œì˜ ì°¸ or ê±°ì§“ì„ ì¦ëª…í•˜ëŠ” í•˜ë‚˜ì˜ ë°©ë²•ì…ë‹ˆë‹¤.\nëŒ€ëµ ì´ëŸ° ëŠë‚Œìœ¼ë¡œ ì¦ëª…ì„ ì§„í–‰í•©ë‹ˆë‹¤.\n\nì–´ë–¤ ëª…ì œê°€ ì°¸ì„ì„ ì¦ëª…í•˜ê³ ì í•œë‹¤ê³  ê°€ì •í•´ë´…ì‹œë‹¤. ex)â€œAëŠ” ì¹˜í‚¨ë¨¹ì„ë•Œ ë¬¼ì€ ì ˆëŒ€ ì•ˆë¨¹ëŠ”ë‹¤.â€ë¼ëŠ” ëª…ì œê°€ ì°¸ì„ì„ ì¦ëª…í•´ë´…ì‹œë‹¤.\nê·¸ë ‡ë‹¤ë©´ ì´ì— ì •í™•íˆ ë°˜ëŒ€ê°€ ë˜ëŠ” ëª…ì œë¥¼ ê°€ì •í•˜ê³  ì‹ì„ ì „ê°œí–ˆì„ë•Œ ì–´ë–¤ ì´ìƒí•¨,ë§ì§€ì•ŠìŒ(ëª¨ìˆœ)ì„ ì°¾ìŠµë‹ˆë‹¤. ex)ì—¬ê¸°ì„œëŠ” â€œAê°€ ë¬¼ì„ ë¨¹ì„ë•Œë„ ìˆë‹¤.â€ë¼ëŠ” ëª…ì œê°€ ì°¸ì´ë¼ê³  ê°€ì •í•˜ê³  í•˜ê³  ëª¨ìˆœì„ ì°¾ìŠµë‹ˆë‹¤.\në§Œì•½ ëª¨ìˆœì„ ì°¾ëŠ”ë‹¤ë©´ ë°˜ëŒ€ê°€ ë˜ëŠ” ëª…ì œëŠ” ì´ìƒí•¨ì´ ìˆëŠ” ê²ƒì´ë¯€ë¡œ ì°¸ì¸ëª…ì œê°€ ì•„ë‹™ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ì— ë°˜ëŒ€ë˜ëŠ” ëª…ì œëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤. ex)ì¹˜í‚¨ì„ ê°™ì´ ë¨¹ì–´ë³¸ ê²°ê³¼ Aê°€ ë¬¼ì„ ì „í˜€ ì•ˆ ë¨¹ì—ˆìŠµë‹ˆë‹¤. ê·¸ëŸ¬ë©´ â€œAê°€ ë¬¼ì„ ë¨¹ì„ë•Œë„ ìˆë‹¤.â€ë¼ëŠ” ëª…ì œëŠ” ê±°ì§“ì´ë¯€ë¡œ â€œAëŠ” ì¹˜í‚¨ë¨¹ì„ë•Œ ë¬¼ì€ ì ˆëŒ€ ì•ˆë¨¹ëŠ”ë‹¤.â€ë¼ëŠ” ëª…ì œê°€ ì°¸ì…ë‹ˆë‹¤.\n\nëŠë‚Œ : í‹€ë¦¬ê±°ë‚˜ ì •í™•í•˜ê±°ë‚˜,ë§ê±°ë‚˜ ì•„ë‹ˆê±°ë‚˜ ë­ë˜ì§€ í•˜ë‚˜ê°€ ì•„ë‹ˆë¼ê³  ë‚˜ì˜¤ë©´ ë‹¤ë¥¸ê±´ ë¬´ì¡°ê±´ ë§ëŠ” ë§ì¸ ì´ë¶„ë²•ëŠë‚Œ?"
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìœ ë¦¬ìˆ˜ì§‘í•©-vs-ì‹¤ìˆ˜ì§‘í•©",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìœ ë¦¬ìˆ˜ì§‘í•©-vs-ì‹¤ìˆ˜ì§‘í•©",
    "title": "[Stochestic Process] 3.ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸° ë¹„êµ",
    "section": "ìœ ë¦¬ìˆ˜ì§‘í•© vs ì‹¤ìˆ˜ì§‘í•©",
    "text": "ìœ ë¦¬ìˆ˜ì§‘í•© vs ì‹¤ìˆ˜ì§‘í•©\n\në¬¸ì œ ì‰½ê²Œë§Œë“¤ê¸°\n\nìš°ë¦¬ì˜ ëª©í‘œëŠ” ìœ ë¦¬ìˆ˜ì§‘í•© vs ë¬´ë¦¬ìˆ˜ì§‘í•©ì˜ í¬ê¸°ë¥¼ ë¹„êµí•˜ì—¬ ê¸¸ì´ê°€ ë‹¤ë¦„ì— ëŒ€í•œ ì§ê´€ì„ ì–»ëŠ” ê²ƒì´ì˜€ìŠµë‹ˆë‹¤.\nê·¸ëŸ¬ê¸° ìœ„í•´ì„œ ë¨¼ì € ìœ ë¦¬ìˆ˜ì§‘í•©ê³¼ ì‹¤ìˆ˜ì§‘í•©ì˜ í¬ê¸°ë¥¼ ë¹„êµí•©ë‹ˆë‹¤.\nì¦ëª…í•˜ê³ ì ëª…ì œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n\\[|\\mathbb{Q}| < |\\mathbb{R}|\\]\n\nê·¸ëŸ°ë° ì—¬ê¸°ì„œ ë¬¸ì œë¥¼ ì‚´ì§ ë°”ê¿”ì„œ \\(|\\mathbb{Q}| = |\\mathbb{N}|\\)ì„ ì´ìš©í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì€ ëª…ì œë¥¼ ì¦ëª…í•˜ëŠ” ë¬¸ì œë¡œ ë°”ê¿”ì„œ ìƒê°í•©ë‹ˆë‹¤.(ì•„ë§ˆ ë°”ë¡œ í’€ê¸° ì–´ë ¤ì›Œì„œ ê·¸ëŸ° ê²ƒ ê°™ìŠµë‹ˆë‹¤.)\n\n\\[ |\\mathbb{N}| < |\\mathbb{R}|\\]\n\nì—¬ê¸°ì„œ ë˜ í•œë²ˆ ë¬¸ì œë¥¼ ë°”ê¿”ì„œ ìƒê°í•©ë‹ˆë‹¤.\n\\([0,1] \\subset \\mathbb{R}\\)ì´ë¯€ë¡œ \\(|[0,1]| \\leq |\\mathbb{R}|\\)ì…ë‹ˆë‹¤.\në§Œì•½ ìì—°ìˆ˜ì§‘í•©ì˜ í¬ê¸°ê°€ [0,1]ì§‘í•©ì˜ í¬ê¸°ë³´ë‹¤ ì‘ë‹¤ëŠ” ëª…ì œê°€ ì°¸ì´ë¼ê³  ì¦ëª…ëœë‹¤ë©´ [0,1]ì§‘í•©ì€ ë°˜ë“œì‹œ ì‹¤ìˆ˜ì§‘í•©ì˜ í¬ê¸°ë³´ë‹¤ ì‘ê±°ë‚˜ ê°™ìœ¼ë¯€ë¡œ ìœ„ì˜ ëª…ì œë„ ë°˜ë“œì‹œ ì°¸ì…ë‹ˆë‹¤.(ì‘ì€ ì§‘í•©ë³´ë‹¤ ì‘ì€ë° ê·¸ë³´ë‹¤ í°(ì ì–´ë„ ê°™ì€)ì§‘í•©ë³´ë‹¤ í´ ìˆ˜ ê°€ ì—†ì–´ìš”,ì¶©ë¶„ì¡°ê±´)\nì¦‰ ì´ ë¬¸ì œëŠ” ë‹¤ìŒì˜ ëª…ì œë¥¼ ì¦ëª…í•˜ëŠ” ë¬¸ì œë¡œ ë°”ë€Œê²Œ ë©ë‹ˆë‹¤.\n\n\\[ |\\mathbb{N}| < |[0,1]|\\]\n\n\nê·€ë¥˜ë²• ì‚¬ìš©í•˜ê¸°\n\nìœ„ì˜ ëª…ì œê°€ ì°¸ì„ì„ ì¦ëª…í•˜ê¸° ìœ„í•´ì„œ ê·€ë¥˜ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\në¨¼ì € ì •í™•íˆ ë°˜ëŒ€ê°€ ë˜ëŠ” ëª…ì œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n\\[|\\mathbb{N}| \\geq |[0,1]|\\]\n\nì´ ëª…ì œê°€ ì°¸ì´ë¼ëŠ” ê²ƒì€ \\(f : \\mathbb{N} \\rightarrow [0,1]\\)ì¸ ì „ì‚¬í•¨ìˆ˜ê°€ ì¡´ì¬í•œë‹¤ëŠ” ê²ƒê³¼ ë™ì¹˜ì…ë‹ˆë‹¤.\në”°ë¼ì„œ ì•„ë˜ì™€ ê°™ì€ ì „ì‚¬í•¨ìˆ˜ê°€ ì¡´ì¬í•œë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\n\n\\[\\begin{aligned}\n&f(1) = 0.7302 \\\\\n&f(2) = 0.4823 \\\\\n&f(3) = 0.2325 \\\\\n&\\quad\\quad\\quad\\vdots\n\\end{aligned}\\]\n\nì „ì‚¬í•¨ìˆ˜ë¼ ê°€ì •í–ˆìœ¼ë¯€ë¡œ ì •ì˜ì— ì˜í•´ \\([f(1),f(2),f(3),\\dots] = [0,1]\\)ì´ ë©ë‹ˆë‹¤.(ì „ì‚¬í•¨ìˆ˜ëŠ” ì¹˜ì—­ê³¼ ê³µì—­ì´ ê°™ì€ í•¨ìˆ˜ì…ë‹ˆë‹¤. ê³µì—­ì˜ ëª¨ë“  ì›ì†ŒëŠ” ì •ì˜ì—­ì˜ ì›ì†Œì— ëŒ€ì‘ë©ë‹ˆë‹¤.)\nì´ì œ \\(0.x_1x_2x_3\\dots\\)ë¼ëŠ” ìˆ«ìë¥¼ ê°€ì •í•´ë´…ì‹œë‹¤.\n\\(x_i\\)ëŠ” ì†Œìˆ˜ì  ì•„ë˜ \\(i\\)ë²ˆì§¸ ìˆ«ìë¥¼ ì˜ë¯¸í•˜ë©° ì´ëŠ” \\(f(i)\\)ì˜ ì†Œìˆ˜ì  ì²«ë²ˆì§¸ ìˆ«ìì™€ëŠ” ë‹¤ë¥¸ ìˆ«ìì…ë‹ˆë‹¤.\nìœ„ì˜ ì˜ˆì‹œì—ì„œëŠ” \\(x_1 \\not=7,x_2\\not=4,x_3\\not=2\\)ì…ë‹ˆë‹¤.\nì†Œìˆ˜ì  í•œìë¦¬ì”© ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì‚¬ì‹¤ì´ ë‚˜íƒ€ë‚©ë‹ˆë‹¤. \\[0.x_1 \\not\\in [f(1)],0.x_1x_2 \\not\\in [f(1),f(2)],0.x_1x_2x_3 \\not\\in [f(1),f(2),f(3)]\\]\nì´ë¥¼ ê³„ì†í•´ì„œ ë¬´í•œíˆ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.(ì§‘í•©ê¸°í˜¸ ì˜¤ë¥¸ìª½ì€ ì „ì‚¬í•¨ìˆ˜ ì •ì˜ì— ì˜í•´ ê³µì—­ = ì¹˜ì—­ì´ë¯€ë¡œ ê°™ìŠµë‹ˆë‹¤.) \\[0.x_1x_2x_3x_4\\dots \\not\\in [f(1),f(2),f(3),\\dots] = [0,1]\\]\në¶„ëª…íˆ \\(0.x_1x_2x_3x_4\\dots\\)ëŠ” \\([0,1]\\)ì— ì†í•˜ëŠ” ìˆ«ìì¸ë° ì „ê°œê°€ ëœ ìœ„ì˜ ì‹ì— ì˜í•˜ì—¬ \\([0,1]\\)ì— ì†í•˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ëª¨ìˆœì´ ë°œìƒí•©ë‹ˆë‹¤.\në”°ë¼ì„œ ìœ„ì˜ ëª…ì œ \\(|\\mathbb{N}| \\geq |[0,1]|\\) ê±°ì§“ì´ë©° \\(|\\mathbb{N}| < |[0,1]|\\)ê°€ ì°¸ì¸ ëª…ì œì…ë‹ˆë‹¤.\nì´ëŠ” ê¶ê·¹ì ìœ¼ë¡œ ìš°ë¦¬ê°€ ê¶ê¸ˆí–ˆë˜ ëª…ì œ\\(|\\mathbb{Q}| < |\\mathbb{R}|\\)ê°€ ì°¸ì„ì„ ì¦ëª…í•˜ê¸° ìœ„í•œ ì¶©ë¶„ì¡°ê±´ì´ì˜€ìŠµë‹ˆë‹¤.\në”°ë¼ì„œ \\(|\\mathbb{Q}| < |\\mathbb{R}|\\)ëŠ” ì°¸ì…ë‹ˆë‹¤.\n\n\n\n\n\n\n\nìœ ë¦¬ìˆ˜ì§‘í•© vs ì‹¤ìˆ˜ì§‘í•©\n\n\n\n\në¬¸ì œë¥¼ ì¡°ê¸ˆ ì‰½ê²Œ ë°”ê¾¸ê³  ê·€ë¥˜ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë°˜ëŒ€ë˜ëŠ” ëª…ì œê°€ ê±°ì§“ì„ì„ ë°í˜”ìŠµë‹ˆë‹¤.\nê²°ê³¼ì ìœ¼ë¡œ \\(|\\mathbb{Q}| < |\\mathbb{R}|\\)ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìœ ë¦¬ìˆ˜ì§‘í•©-vs-ë¬´ë¦¬ìˆ˜ì§‘í•©",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#ìœ ë¦¬ìˆ˜ì§‘í•©-vs-ë¬´ë¦¬ìˆ˜ì§‘í•©",
    "title": "[Stochestic Process] 3.ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸° ë¹„êµ",
    "section": "ìœ ë¦¬ìˆ˜ì§‘í•© vs ë¬´ë¦¬ìˆ˜ì§‘í•©",
    "text": "ìœ ë¦¬ìˆ˜ì§‘í•© vs ë¬´ë¦¬ìˆ˜ì§‘í•©\n\nì—¬ê¸°ì„œëŠ” ê·€ë¥˜ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë¬´ë¦¬ìˆ˜ì§‘í•©ì˜ ì¹´ë””ë„ë¦¬í‹°ê°€ ë¬´ë¦¬ìˆ˜ì§‘í•©ì˜ ì¹´ë””ë„ë¦¬í‹°ì™€ ë‹¤ë¦„ì„ ì¦ëª…í•©ë‹ˆë‹¤.\nì¦‰ ë‹¤ìŒì„ ì¦ëª…í•˜ê³ ì í•˜ëŠ” ëª…ì œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. \\[|\\mathbb{Q}| \\not= |\\mathbb{Q}^c|\\]\në¨¼ì € ì´ì™€ ë°˜ëŒ€ë˜ëŠ” ëª…ì œë¥¼ ê°€ì •í•©ë‹ˆë‹¤. \\[|\\mathbb{Q}| = |\\mathbb{Q}^c| = \\aleph_0\\]\nìœ—ì‹ì—ì„œ ìœ ë¦¬ìˆ˜ì§‘í•©ì˜ í¬ê¸°ëŠ” \\(\\aleph_0\\)ë¡œ ì´ë¯¸ ì¦ëª…ëœ ì‚¬ì‹¤ì…ë‹ˆë‹¤.\në”°ë¼ì„œ ë°˜ëŒ€ë˜ëŠ” ëª…ì œì—ì„œ ìœ ë¦¬ìˆ˜ì§‘í•©ê³¼ ë¬´ë¦¬ìˆ˜ì§‘í•©ì˜ í¬ê¸°ëŠ” \\(\\aleph_0\\)ë¡œ ê°™ìŠµë‹ˆë‹¤.\nìŒì˜ì •ìˆ˜ ì§‘í•©ì˜ í¬ê¸° \\(|\\mathbb{N}^-| = \\aleph_0\\)ì´ë¯€ë¡œ ë°˜ëŒ€ë˜ëŠ” ëª…ì œì™€ í•¨ê»˜ ë‹¤ìŒì˜ ëª…ì œë¥¼ ìœ ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\\(|\\mathbb{Q}| = \\aleph_0\\)ì´ë¯€ë¡œ \\(f : \\mathbb{Q} \\rightarrow \\mathbb{N}\\)ì¸ ì „ë‹¨ì‚¬í•¨ìˆ˜ \\(f\\)ê°€ ì¡´ì¬í•œë‹¤.\n\\(|\\mathbb{Q^c}| = \\aleph_0\\)ì´ë¯€ë¡œ \\(g : \\mathbb{Q^c} \\rightarrow \\mathbb{N^-}\\)ì¸ ì „ë‹¨ì‚¬í•¨ìˆ˜ \\(g\\)ê°€ ì¡´ì¬í•œë‹¤.\n\në”°ë¼ì„œ \\(h : \\mathbb{Q}\\cup\\mathbb{Q^c} \\rightarrow \\mathbb{N} \\cup \\mathbb{N}^-\\)ì¸ ì „ë‹¨ì‚¬í•¨ìˆ˜ê°€ \\(h\\)ê°€ ì¡´ì¬í•¨ì„ ì•Œ ìˆ˜ ìˆìœ¼ë©° \\(|\\mathbb{Q}\\cup\\mathbb{Q^c}| = |\\mathbb{N} \\cup \\mathbb{N}^-| = \\aleph_0\\)ì„ì„ ìœ ë„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ì •ìˆ˜ì§‘í•©ì—ì„œ 0ì„ ëº€ ì§‘í•©ì¸ \\(\\mathbb{N} \\cup \\mathbb{N}^-\\)ë„ ìì—°ìˆ˜ì§‘í•©ê³¼ í¬ê¸°ê°€ ê°™ìŒì„ ìœ„í•´ì„œ ì¦ëª…í–ˆì—ˆìŠµë‹ˆë‹¤.)\nê·¸ëŸ¬ë‚˜ ì—¬ê¸°ì„œ \\(|\\mathbb{Q} \\cup \\mathbb{Q^c}|\\)ëŠ” ìš°ë¦¬ëŠ” ì‹¤ìˆ˜ì§‘í•© \\(\\mathbb{R}\\)ì´ë©° \\(|\\mathbb{Q}| < |\\mathbb{R}|\\)ì´ë¯¸ ì°¸ì¸ ëª…ì œì„ì„ ì´ì „ì— ì¦ëª…í–ˆì—ˆìŠµë‹ˆë‹¤.\në”°ë¼ì„œ ëª¨ìˆœì´ ìƒê²¼ìœ¼ë¯€ë¡œ \\(|\\mathbb{Q}| = |\\mathbb{Q}^c|\\)ëŠ” ê±°ì§“ì¸ ëª…ì œì´ë©° ë”°ë¼ì„œ ì¦ëª…í•˜ê³ ì í–ˆë˜ ëª…ì œ\\(|\\mathbb{Q}| \\not= |\\mathbb{Q^c}|\\)ëŠ” ì°¸ì„ì´ ë°í˜€ì¡ŒìŠµë‹ˆë‹¤.\n\n\n\n\n\n\n\nìœ ë¦¬ìˆ˜ì§‘í•© vs ë¬´ë¦¬ìˆ˜ì§‘í•©\n\n\n\n\nê·€ë¥˜ë²•ì„ ì‚¬ìš©í•˜ì—¬ ë°˜ëŒ€ë˜ëŠ” ëª…ì œê°€ ê±°ì§“ì„ì„ ì¦ëª…í–ˆìŠµë‹ˆë‹¤.\nê²°ê³¼ì ìœ¼ë¡œ \\(|\\mathbb{Q}| \\not= |\\mathbb{Q^c}|\\)ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#í•´ê²°í•´ì•¼-í• -ê²ƒ",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#í•´ê²°í•´ì•¼-í• -ê²ƒ",
    "title": "[Stochestic Process] 3.ë¬´í•œì§‘í•©ê°„ì˜ í¬ê¸° ë¹„êµ",
    "section": "í•´ê²°í•´ì•¼ í•  ê²ƒ",
    "text": "í•´ê²°í•´ì•¼ í•  ê²ƒ\n\n\\(|\\mathbb{Q}| < |\\mathbb{Q^c}|\\) ì¦ëª…í•˜ëŠ” ë°©ë²•?"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": "",
    "text": "ì§€ë‚œ í¬ìŠ¤íŒ…ì—ì„œ ìœ ë¦¬ìˆ˜ì§‘í•©ê³¼ ë¬´ë¦¬ìˆ˜ì§‘í•©ì˜ ê¸¸ì´ê°€ ë‹¤ë¦„ì„ ì¦ëª…í–ˆê³  ë”°ë¼ì„œ ë¥´ë²¡ë©”ì ¸ë¥¼ í†µí•œ ë‘ ì§‘í•©ì˜ ê¸¸ì´ê°€ ì™œ ë‹¤ë¥¸ì§€ ì–´ëŠì •ë„ì˜ ì§ê´€ì„ ì–»ì„ ìˆ˜ ìˆì—ˆìŠµë‹ˆë‹¤.\nì´ë¡œì„œ â€œí‘œë³¸ê³µê°„ì˜ ëª¨ë“  ë¶€ë¶„ì§‘í•©ì— ëŒ€í•´ì„œ ëª¨ìˆœì—†ì´ í™•ë¥ ì„ ì •ì˜í•  ìˆ˜ ìˆëŠ”ê±´ê°€?â€ ì‹¶ì—ˆì§€ë§Œ ì—¬ì „íˆ ëª¨ìˆœì´ ë°œìƒí•˜ê²Œ í•˜ëŠ” ì§‘í•©(ë¹„íƒˆë¦¬ì§‘í•©)ì´ ì¡´ì¬í•¨ì´ ë°í˜€ì¡ŒìŠµë‹ˆë‹¤.\nê·¸ë ‡ë‹¤ë©´ ë‹¤ë¥¸ measureë¥¼ ë„ì…í•˜ê±°ë‚˜ ì•„ì˜ˆ ëª¨ë“  ë¶€ë¶„ì§‘í•©ì— ëŒ€í•´ì„œ í™•ë¥ ì„ ëª¨ìˆœì—†ì´ ì •ì˜í•˜ëŠ” ê²ƒì„ í¬ê¸°í•˜ëŠ” ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ë‹¤ë¥¸ measureë¥¼ ë„ì…í•´ë„ ì—¬ì „íˆ ëª¨ìˆœì´ ë°œìƒí•¨ì´ ì´ë¯¸ ë°í˜€ì ¸ ìˆìŠµë‹ˆë‹¤.\në”°ë¼ì„œ í‘œë³¸ê³µê°„ì˜ ëª¨ë“  ë¶€ë¶„ì§‘í•©ì´ ì•„ë‹ˆë¼ ê°€ëŠ¥í•œ ëª¨ë“  ë¶€ë¶„ì§‘í•©ë“¤ ì¤‘ ì¼ë¶€ì´ì í™•ë¥ ì„ ëª¨ìˆœì—†ì´ ì •ì˜í•  ìˆ˜ ìˆëŠ” ì§‘í•©ë“¤ì˜ ëª¨ìŒì¸ sigma fieldë¥¼ ì•½ì†í•˜ê³  ì´ì— ëŒ€í•˜ì—¬ í™•ë¥ ì„ ì •ì˜í•©ë‹ˆë‹¤.\nì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” í™•ë¥ ì„ ëª¨ìˆœì—†ì´ ì •ì˜í•  ìˆ˜ ìˆëŠ” ì§‘í•©ë“¤ì˜ ëª¨ìŒì¸ simga fieldì—ëŠ” ì–´ë– í•œ ì¡°ê±´ì´ ë“¤ì–´ê°ˆì§€ì— ëŒ€í•´ ëŒ€ëµì ìœ¼ë¡œ íŒŒì•…í•˜ê³  ë˜í•œ sigma fieldë¥¼ ì •ì˜í•˜ê¸°ê±° ì–¼ë§ˆë‚˜ ì–´ë ¤ìš´ì§€ë¥¼ ì •ë¦¬í•©ë‹ˆë‹¤.\nì´ì–´ì§€ëŠ” í¬ìŠ¤íŒ…ì—ì„œëŠ” sigma fieldë¥¼ ë³´ë‹¤ ì—„ê²©í•˜ê²Œ + ìµœëŒ€í•œ ê°„ë‹¨íˆ ì •ì˜í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ1-omega-ht-mathcalf-emptysethtomega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ1-omega-ht-mathcalf-emptysethtomega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " ì˜ˆì‹œ1 \\(\\Omega = \\{H,T\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\{H\\},\\{T\\},\\Omega\\}\\)",
    "text": "ì˜ˆì‹œ1 \\(\\Omega = \\{H,T\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\{H\\},\\{T\\},\\Omega\\}\\)\n\ncollection \\(\\mathcal{F}\\) ì¦‰, í™•ë¥ ì„ ëª¨ìˆœì—†ì´ ì •ì˜í•  ìˆ˜ ìˆëŠ” ì§‘í•©ë“¤ì˜ ëª¨ìŒì¸ sigma fieldë¥¼ ìœ„ì™€ ê°™ì´ ì •í•˜ëŠ” ê²ƒì€ í•©ë¦¬ì ì¼ê¹Œìš”?\nì—¬ê¸°ì„œ ëª¨ìˆœì´ ì—†ë‹¤ëŠ” í™•ë¥ ì— ëŒ€í•´ ì•Œê³ ìˆë˜ ì‚¬ì‹¤ë“¤ê³¼ ê³µë¦¬ë¥¼ ë²—ì–´ë‚˜ì§€ ì•ŠìŒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.(ê°„ë‹¨íˆ)\ní•˜ë‚˜ì˜ ì˜ˆì‹œë¥¼ ë“¤ì–´ë³´ìë©´ í™•ë¥ ì„ \\(P(\\emptyset) = 0,P(\\{H\\}) = \\frac{1}{3},P(\\{T\\}) = \\frac{2}{3},P(\\Omega) = 1\\)ì²˜ëŸ¼ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ë ‡ê²Œ í™•ë¥ ì„ ì •ì˜í•œë‹¤ë©´ ì–´ë– í•œ ëª¨ìˆœë„ ë³´ì´ì§€ ì•ŠìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ2-omega-ht-mathcalf-emptysethomega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ2-omega-ht-mathcalf-emptysethomega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " ì˜ˆì‹œ2 \\(\\Omega = \\{H,T\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\{H\\},\\Omega\\}\\)",
    "text": "ì˜ˆì‹œ2 \\(\\Omega = \\{H,T\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\{H\\},\\Omega\\}\\)\n\nsigma fieldë¥¼ ìœ„ì™€ ê°™ì´ ì •í•˜ëŠ” ê²ƒì€ í•©ë¦¬ì ì¼ê¹Œìš”?\në¨¼ì € sigma fieldë¥¼ ë‹¤ì‹œ ìƒê°í•´ë´…ì‹œë‹¤.\nsigma fieldëŠ” ëª¨ìˆœì—†ì´ í™•ë¥ ì„ ì •ì˜í•  ìˆ˜ ìˆëŠ” ì§‘í•©ë“¤ì˜ ë¬¶ìŒì´ì˜€ìŠµë‹ˆë‹¤.\nì—¬ê¸°ì„œ ì •ì˜ì™€ ì–´ê¸‹ë‚¨ì´ ë°œìƒí•©ë‹ˆë‹¤.(í•©ë¦¬ì ì´ì§€ ëª»í•´ìš”)ë™ì „ë˜ì§€ê¸°ë¥¼ ì˜ˆì‹œë¡œ ë“¤ì–´ë´…ì‚¬ë””\n\nì•ë©´(H)ê°€ ë‚˜ì˜¬ í™•ë¥  = \\(P(\\{H\\}) = \\frac{1}{2}\\)ë¼ê³  í•©ì‹œë‹¤.\nê·¸ë ‡ë‹¤ë©´ ë™ì „ì˜ ë’·ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì€ ìì—°ìŠ¤ëŸ½ê²Œ \\(P(\\{H\\}^c) = P(\\{T\\}) = 1- \\frac{1}{2} = \\frac{1}{2}\\)ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì¦‰ ì–´ë–¤ ì›ì†Œì˜ ì—¬ì‚¬ê±´ì„ í¬í•¨ì‹œì¼œë„ ëª¨ìˆœì—†ì´ í™•ë¥ ì´ ì •ì˜ê°€ ë˜ì§€ë§Œ ìœ„ì—ì„œ \\(\\mathcal{F}\\)ëŠ” ì—¬ì‚¬ê±´ì„ í¬í•¨ì‹œí‚¤ì§€ ì•Šê³  ìˆìŠµë‹ˆë‹¤.\në§ë¡œ ì„¤ëª…í•˜ìë©´ \\(\\mathcal{F}\\)ëŠ” â€œë™ì „ì˜ ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ë§Œ ì•Œ ìˆ˜ ìˆì–´.â€ì…ë‹ˆë‹¤. ì´ëŠ” í•©ë¦¬ì ì´ì§€ ëª»í•˜ë©° â€œë™ì „ì˜ ì•ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ì„ ì•Œë©´ ë’·ë©´ì´ ë‚˜ì˜¬ í™•ë¥ ë„ ì•Œ ìˆ˜ ìˆì–´â€ê°€ í•©ë¦¬ì ì¸ ì£¼ì¥ì…ë‹ˆë‹¤.\n\nìœ„ì˜ ì˜ˆì‹œë¡œë¶€í„° sigma fieldë¥¼ í•©ë¦¬ì ìœ¼ë¡œ ì •ì˜í•˜ê¸° ìœ„í•œ ì¡°ê±´ ì¤‘ í•˜ë‚˜ëŠ” â€œí‘œë³¸ê³µê°„ì— ì†í•˜ëŠ” ëª¨ë“  ì‚¬ê±´ì— ëŒ€í•´ì„œ ê·¸ ì‚¬ê±´ì´ sigma fieldì— í¬í•¨ë˜ë©´ ì—¬ì‚¬ê±´ë„ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.â€ë¥¼ ë„ì¶œí•´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì¦‰, ë‹¤ìŒì˜ ëª…ì œëŠ” ì°¸ì…ë‹ˆë‹¤.\n\n\n\n\n\n\n\nì¡°ê±´1\n\n\n\n\n\\(\\forall A \\subset \\Omega : A \\in \\mathcal{F} \\Rightarrow A^c \\in \\mathcal{F}\\)"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ3-omega-ht-mathcalf-htomega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ3-omega-ht-mathcalf-htomega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " ì˜ˆì‹œ3 \\(\\Omega = \\{H,T\\}\\), \\(\\mathcal{F} = \\{\\{H\\},\\{T\\},\\Omega\\}\\)",
    "text": "ì˜ˆì‹œ3 \\(\\Omega = \\{H,T\\}\\), \\(\\mathcal{F} = \\{\\{H\\},\\{T\\},\\Omega\\}\\)\n\nsigma fieldë¥¼ ìœ„ì™€ ê°™ì´ ì •í•˜ëŠ” ê²ƒì€ í•©ë¦¬ì ì¼ ê¹Œìš”?\nì¡°ê±´1ì— ì˜í•˜ë©´ sigma fieldì— ì†í•œ ì„ì˜ì˜ ì›ì†Œì— ëŒ€í•œ ì—¬ì‚¬ê±´ë„ ë°˜ë“œì‹œ sigma fieldì— í¬í•¨ë˜ì–´ì•¼ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\nê·¸ëŸ¬ë‚˜ \\(\\Omega^c = \\emptyset \\not\\in \\mathcal{F}\\)ì´ë¯€ë¡œ ìœ„ì™€ ê°™ì€ sigma fieldëŠ” í•©ë¦¬ì ì´ì§€ ëª»í•©ë‹ˆë‹¤.\n\n\n\n\n\n\n\nì¡°ê±´2\n\n\n\n\n\\(\\Omega,\\emptyset \\in \\mathcal{F}\\)"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ5-omega-123456-mathcalf-emptyset12dots12dots123456omega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ5-omega-123456-mathcalf-emptyset12dots12dots123456omega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " ì˜ˆì‹œ5\\(\\Omega = \\{1,2,3,4,5,6\\}\\)\\(\\mathcal{F} = \\{\\emptyset,\\{1\\},\\{2\\},\\dots,\\{1,2\\},\\dots,\\{1,2,3,4,5,6\\},\\Omega\\}\\)",
    "text": "ì˜ˆì‹œ5\\(\\Omega = \\{1,2,3,4,5,6\\}\\)\\(\\mathcal{F} = \\{\\emptyset,\\{1\\},\\{2\\},\\dots,\\{1,2\\},\\dots,\\{1,2,3,4,5,6\\},\\Omega\\}\\)\n\nì˜ˆì‹œ5ëŠ” sigma fieldê°€ sample spaceì—ì„œ ëª¨ë“  ë¶€ë¶„ì§‘í•©ë“¤ì˜ ëª¨ìŒì¸ ë©±ì§‘í•©(powerset,\\(2^\\Omega\\)ë¡œ í‘œê¸°)ì…ë‹ˆë‹¤.\nìœ„ì™€ ê°™ì€ sigma fieldëŠ” í•©ë¦¬ì ì¸ë° ìœ„ì˜ ì˜ˆì‹œë¥¼ ì£¼ì‚¬ìœ„ë¥¼ ë˜ì§€ëŠ” ì‹¤í—˜ì´ë¼ê³  ìƒê°í•˜ë©´ ëª¨ë“  ê²½ìš°ì— ëŒ€í•´ í™•ë¥ ì„ ëª¨ìˆœì—†ì´ ì •ì˜í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\nì£¼ëª©í•  ì ì€ ì´ ê²½ìš°ì—ëŠ” powerset ì¦‰, í‘œë³¸ê³µê°„ì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  ë¶€ë¶„ì§‘í•©ë“¤ì˜ ëª¨ìŒì„ sigma fieldë¡œ ì •í–ˆì§€ë§Œ ì´ëŠ” í•­ìƒ í•©ë¦¬ì ì´ì§„ ì•Šìœ¼ë©° sample spaceë§ˆë‹¤ ë‹¤ë¥´ë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\nì£¼ëª©í•  ì ì€ ì´ ê²½ìš°ì—ëŠ” powerset ì¦‰, í‘œë³¸ê³µê°„ì—ì„œ ê°€ëŠ¥í•œ ëª¨ë“  ë¶€ë¶„ì§‘í•©ë“¤ì˜ ëª¨ìŒì„ sigma fieldë¡œ ì •í•´ë„ í•©ë¦¬ì ì´ë¼ëŠ” ì‚¬ì‹¤ì…ë‹ˆë‹¤.\nìš°ë¦¬ëŠ” sample spaceì˜ ëª¨ë“  ë¶€ë¶„ì§‘í•©ë“¤ì˜ ëª¨ì„(powerset)ì— ëŒ€í•´ì„œ í™•ë¥ ì„ ëª¨ìˆœì—†ì´ ì •ì˜í•˜ëŠ” ê²ƒì´ ë¶ˆê°€ëŠ¥ í•œ ê²½ìš°ê°€ ìˆê¸°ì— í•©ë¦¬ì ì¸ sigma fieldì˜ ì¡°ê±´ì„ ì°¾ëŠ” ì¤‘ì´ì—ˆìŠµë‹ˆë‹¤.\nìœ„ì˜ ì˜ˆì‹œëŠ” sample spaceì˜ ëª¨ë“  subsetì— ëŒ€í•´ì„œ í™•ë¥ ì„ ì •ì˜í•˜ëŠ”ê²Œ ê°€ëŠ¥í•œ ê²½ìš°ë„ ìˆë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤.(í•­ìƒ ì¼ë¶€ê°€ ì•„ë‹™ë‹ˆë‹¤.)"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ6-omega-123456-mathcalf-emptyset612345omega-2omega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ6-omega-123456-mathcalf-emptyset612345omega-2omega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " ì˜ˆì‹œ6 \\(\\Omega = \\{1,2,3,4,5,6\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\{6\\},\\{1,2,3,4,5\\},\\Omega\\} = 2^{\\Omega}\\)",
    "text": "ì˜ˆì‹œ6 \\(\\Omega = \\{1,2,3,4,5,6\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\{6\\},\\{1,2,3,4,5\\},\\Omega\\} = 2^{\\Omega}\\)\n\nì˜ˆì‹œ6ì€ sample spaceê°€ ì˜ˆì‹œ5ì™€ ê±°ì˜ ë™ì¼í•˜ì§€ë§Œ sigma field \\(\\mathcal{F}\\)ë§Œ ë‹¤ë¥¸ ê²½ìš°ì…ë‹ˆë‹¤.\nìœ„ì™€ ê°™ì€ sigama fieldë„ í•©ë¦¬ì ì…ë‹ˆë‹¤.\në‹¤ë§Œ ì˜ˆì‹œ5ì—ì„œëŠ” í‘œë³¸ê³µê°„ì˜ ëª¨ë“  ë¶€ë¶„ì§‘í•©ì— ëŒ€í•´ì„œ ê´€ì‹¬ì´ ìˆì—ˆë‹¤ë©´ ì´ ê²½ìš°ëŠ” 6ì´ê±°ë‚˜ 6ì´ì•„ë‹Œ ê²½ìš°ì˜ í™•ë¥ ë§Œ ê´€ì‹¬ì´ ìˆê¸° ë•Œë¬¸ì— sigma fieldë¥¼ ë” ì‘ê²Œ ì •ì˜í–ˆë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ë ‡ê²Œ sigma fieldë¥¼ ì •ì˜í•˜ëŠ” ë˜ë‹¤ë¥¸ ì˜ˆì‹œë“¤ì€ ì•„ë˜ì™€ ê°™ì€ ê²ƒë“¤ì´ ìˆìŠµë‹ˆë‹¤.\n\në¡¤ì—ì„œ ë¯¸ë“œ vs ë‹¤ë¥¸ë¼ì¸ ê³ ë¥´ëŠ” ê²½ìš°(íƒ‘orì›ë”œâ€¦)ì— ëŒ€í•œ í™•ë¥  (ì €ëŠ” ì´ê²Œ ì œì¼ ì™€ë‹¿ë„¤ìš” ã…ã…)\nì¹´í˜ì—ì„œ ì•„ë©”ë¦¬ì¹´ë…¸ë¥¼ ê³ ë¥´ê±°ë‚˜ ê³ ë¥´ì§€ ì•ŠëŠ” ê²½ìš°(ì¹´í˜ë¼ë–¼,ì¹´í‘¸ì¹˜ë…¸ ë“±ë“±ë“±..)ì— ëŒ€í•œ í™•ë¥ \n\nìœ„ì™€ ê°™ì€ ì˜ˆì‹œë¡œë¶€í„° í•˜ë‚˜ì˜ sample spaceì— ëŒ€í•´ì„œ sigma fieldëŠ” ìœ ì¼í•˜ì§€ ì•Šìœ¼ë©° ë‹¤ì–‘í•˜ê²Œ ì¡´ì¬í•¨ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n\n\n\n\n\n\n\nì•Œ ìˆ˜ ìˆëŠ” ì„±ì§ˆ\n\n\n\n\ní•˜ë‚˜ì˜ í‘œë³¸ê³µê°„ì— ëŒ€í•˜ì—¬ ì‹œê·¸ë§ˆ í•„ë“œëŠ” ìœ ì¼í•˜ì§€ ì•Šë‹¤.\nê´€ì‹¬ìˆëŠ” ì‚¬ê±´ì´ ë‹¤ë¥¸ ê²½ìš° ì‹œê·¸ë§ˆ í•„ë“œë¥¼ ë‹¤ë¥´ê²Œ ì •ì˜í•  ìˆ˜ ìˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ7-omega-123456-mathcalf-emptysetomega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ7-omega-123456-mathcalf-emptysetomega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " ì˜ˆì‹œ7 \\(\\Omega = \\{1,2,3,4,5,6\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\Omega\\}\\)",
    "text": "ì˜ˆì‹œ7 \\(\\Omega = \\{1,2,3,4,5,6\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\Omega\\}\\)\n\nìœ„ì™€ ê°™ì€ sigma fieldë„ í•©ë¦¬ì ì´ê¸´ í•©ë‹ˆë‹¤.\ní•˜ì§€ë§Œ \\(P(\\Omega) = 1,P(\\emptyset) = 0\\)ì¸ ê²ƒì€ ìëª…í•©ë‹ˆë‹¤.(ì“¸ëª¨ê°€ ì—†ìŠµë‹ˆë‹¤.)\nì´ëŸ¬í•œ sigma fieldë¥¼ trivial sigma fieldë¼ê³  í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ8-omega-1234mathcalfemptyset-1-2-234-134-omega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#ì˜ˆì‹œ8-omega-1234mathcalfemptyset-1-2-234-134-omega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " ì˜ˆì‹œ8 \\(\\Omega = \\{1,2,3,4\\},\\mathcal{F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega \\}\\)",
    "text": "ì˜ˆì‹œ8 \\(\\Omega = \\{1,2,3,4\\},\\mathcal{F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega \\}\\)\n\n í’€ì´1\n\nìœ„ì™€ ê°™ì€ sigma fieldëŠ” í•©ë¦¬ì ì´ì§€ ëª»í•©ë‹ˆë‹¤.\nì˜ˆì‹œë¥¼ ë“¤ì–´ë´…ì‹œë‹¤.\n\nì£¼ì‚¬ìœ„ë¥¼ ë˜ì ¸ì„œ 1ì´ ë‚˜ì˜¬ í™•ë¥ ê³¼ 2ê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ì•Œê³ ìˆìŠµë‹ˆë‹¤.\nê·¸ë ‡ë‹¤ë©´ 3ë˜ëŠ”4ê°€ ë‚˜ì˜¤ëŠ” ì‚¬ê±´ \\(\\{1,2\\}\\)ì— ëŒ€í•œ í™•ë¥ ì€ ê°ê°ì˜ í™•ë¥ ì„ ë”í•¨ìœ¼ë¡œì„œ ìëª…í•˜ê²Œ êµ¬í•  ìˆ˜ ìˆìœ¼ë©° ì´ë¥¼ sigma fieldì— í¬í•¨ì‹œì¼œë„ í™•ë¥ ì— ëª¨ìˆœì´ ì—†ìŠµë‹ˆë‹¤.\nì¦‰, í‘œë³¸ê³µê°„ì— ì†í•˜ëŠ” êµì§‘í•©ì´ ì—†ëŠ” ì„ì˜ì˜ ë‘ ì‚¬ê±´ì´ sigma fieldì— ì†í•  ë•Œ ì„ì˜ì˜ ë‘ ì‚¬ê±´ì— ëŒ€í•œ í•©ì§‘í•©ì´ sigma fieldí¬í•¨ë˜ì–´ë„ ëª¨ìˆœì´ ì—†ì§€ë§Œ ìœ„ì—ì„œ \\(\\mathcal{F}\\)ëŠ” í•©ì§‘í•©ì„ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\në§ë¡œ ì„¤ëª…í•˜ìë©´ ìœ„ì˜ \\(\\mathcal{F}\\)ëŠ” â€œ3ì´ ë‚˜ì˜¬ í™•ë¥ ì€ ì•Œê³  4ê°€ ë‚˜ì˜¬ í™•ë¥ ì€ ì•„ëŠ”ë° 3ë˜ëŠ”4ê°€ ë‚˜ì˜¬ í™•ë¥ ì€ ëª°ë¼â€ë¼ê³  ë§í•˜ê³  ìˆìŠµë‹ˆë‹¤.\nì´ëŠ” í•©ë¦¬ì ì´ì§€ ëª»í•˜ë©° â€œ3ì´ ë‚˜ì˜¬ í™•ë¥ ì€ ì•Œê³  4ê°€ ë‚˜ì˜¬ í™•ë¥ ì€ ì•„ë‹ˆê¹Œ 3ë˜ëŠ”4ê°€ ë‚˜ì˜¬ í™•ë¥ ë„ ì•Œì•„â€ê°€ í•©ë¦¬ì ì…ë‹ˆë‹¤.\n\nìœ„ì˜ ì˜ˆì‹œë¡œë¶€í„° sigma fieldë¥¼ í•©ë¦¬ì ìœ¼ë¡œ ì •ì˜í•˜ê¸° ìœ„í•œ ì¡°ê±´ ì¤‘ í•˜ë‚˜ëŠ” â€œí‘œë³¸ê³µê°„ì— ì†í•˜ëŠ” ì„ì˜ì˜ ë‘ ì‚¬ê±´ì´ êµì§‘í•©ì´ ì¡´ì¬í•˜ì§€ ì•Šì„ ë•Œ ë‘ ì‚¬ê±´ì´ ëª¨ë‘ sigma fieldì— ì†í•œë‹¤ë©´ ë‘ ì‚¬ê±´ì˜ í•©ì§‘í•©ë„ í¬í•¨ë˜ì–´ì•¼ í•œë‹¤.â€ë¥¼ ë„ì¶œí•´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì¦‰, ë‹¤ìŒì˜ ëª…ì œëŠ” ì°¸ì…ë‹ˆë‹¤.\n\n\n\n\n\n\n\nì¡°ê±´3\n\n\n\n\\[\\forall A,B \\subset \\Omega \\text{ such that } A \\cap B = \\emptyset : A,B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}\\]\n\n\n\nì´ì™€ ê°™ì€ ë…¼ë¦¬ë¡œ disjointí•œ ë‘ ì‚¬ê±´ì˜ í•©ì§‘í•©ì„ sigma fieldì— í¬í•¨ì‹œí‚¤ê³  ë˜í•œ ì—¬ì§‘í•©ë„ ê³ ë ¤í–ˆì„ë•Œ ìˆ˜ì •ëœ ì‹œê·¸ë§ˆí•„ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤\n\n\\[\\Omega = \\{1,2,3,4\\},\\mathcal{F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega,\\{1,2\\},\\{3,4\\}\\}\\]\n\n\n í’€ì´2\n\nì˜ˆì‹œë¥¼ ë“¤ì–´ë´…ì‹œë‹¤.\n\nì£¼ì‚¬ìœ„ë¥¼ ë˜ì ¸ì„œ 1ì´ê±°ë‚˜ 3ì´ê±°ë‚˜ 4ê°€ ë‚˜ì˜¬ í™•ë¥ ê³¼ 1ì´ ë‚˜ì˜¬ í™•ë¥ ì„ ì•Œê³  ìˆìŠµë‹ˆë‹¤.\nê·¸ë ‡ë‹¤ë©´ \\(\\{3,4\\}\\)ì´ ë‚˜ì˜¬ í™•ë¥ ì€ \\(P(\\{1,3,4\\}) - P(\\{1\\})\\)ë¡œ ìëª…í•˜ê²Œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì¦‰, í‘œë³¸ê³µê°„ì— ì†í•˜ë©° í¬í•¨ê´€ê³„ì— ìˆëŠ” ì„ì˜ì˜ ë‘ ì‚¬ê±´ì´ sigma fieldì— ì†í•  ë•Œ ë‘ ì‚¬ê±´ì— ëŒ€í•œ ì°¨ì§‘í•©ë„ sigma fieldì— í¬í•¨ì‹œì¼œë„ í™•ë¥ ì„ ì •ì˜í•  ë•Œ ëª¨ìˆœì´ ì—†ì§€ë§Œ ìœ„ì—ì„œ \\(\\mathcal{F}\\)ëŠ” ì°¨ì§‘í•©ì„ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\në§ë¡œ ì„¤ëª…í•˜ìë©´ ìœ„ì˜ \\(\\mathcal{F}\\)ëŠ” â€œ1ì•„ë‹ˆë©´ 3ì•„ë‹ˆë©´ 4ê°€ ë‚˜ì˜¬ í™•ë¥ ì€ ì•Œê³  1ì´ ë‚˜ì˜¬ í™•ë¥ ì€ ì•„ëŠ”ë° 3ë˜ëŠ”4ê°€ ë‚˜ì˜¬ í™•ë¥ ì€ ëª¨ë¥¸ë‹¤â€ë¼ê³  ë§í•˜ê³  ìˆëŠ” ê²ƒì…ë‹ˆë‹¤.\nì´ëŠ” í•©ë¦¬ì ì´ì§€ ëª»í•˜ë©° â€œ1ì•„ë‹ˆë©´ 3ì•„ë‹ˆë©´ 4ê°€ ë‚˜ì˜¬ í™•ë¥ ì„ ì•Œê³  1ì´ ë‚˜ì˜¬ í™•ë¥ ë„ ì•Œê¸° ë•Œë¬¸ì— 3ë˜ëŠ”4ê°€ ë‚˜ì˜¬ í™•ë¥ ë„ ì•Œì•„â€ê°€ í•©ë¦¬ì ì…ë‹ˆë‹¤.\n\n\n\n\n\n\n\n\nì¡°ê±´4\n\n\n\n\\[\\forall A,B \\subset \\Omega \\text{ such that } A \\subset B: A,B \\in \\mathcal{F} \\Rightarrow B - A \\in \\mathcal{F}\\]\n\n\n\nì´ì™€ ê°™ì€ ë…¼ë¦¬ë¡œ í¬í•¨ê´€ê³„ì— ìˆëŠ” ë‘ ì‚¬ê±´ì˜ ì°¨ì§‘í•©ì„ sigma fieldì— í¬í•¨ì‹œí‚¤ê³  ë˜í•œ ì—¬ì§‘í•©ë„ ê³ ë ¤í–ˆì„ë•Œ ìˆ˜ì •ëœ ì‹œê·¸ë§ˆí•„ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n\\[\\Omega = \\{1,2,3,4\\},\\mathcal{F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega,\\{3,4\\},\\{1,2\\}\\}\\]"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field2.html",
    "href": "posts/stochastic process/SP-sigmafield/sigma field2.html",
    "title": "[Stochestic Process] 5.sigma field(2)",
    "section": "",
    "text": "ì§€ë‚œ í¬ìŠ¤íŒ…ì—ì„œëŠ” í™•ë¥ ì„ ëª¨ìˆœì—†ì´ ì •ì˜í•  ìˆ˜ ìˆëŠ” ì§‘í•©ë“¤ì˜ ëª¨ìŒì¸ simga fieldì—ëŠ” ì–´ë– í•œ ì¡°ê±´ì´ ë“¤ì–´ê°ˆì§€ì— ëŒ€í•´ ëŒ€ëµì ìœ¼ë¡œ íŒŒì•…í•˜ê³  ë˜í•œ sigma fieldë¥¼ ì •ì˜í•˜ê¸°ê±° ì–¼ë§ˆë‚˜ ì–´ë ¤ìš´ì§€ë¥¼ ì •ë¦¬í–ˆì—ˆìŠµë‹ˆë‹¤.\nì´ë²ˆ í¬ìŠ¤íŒ…ì—ì„œëŠ” sigma fieldì— ë³´ë‹¤ ì—„ê²©í•˜ê²Œ ì •ì˜í•˜ê¸° ìœ„í•œ ì¶”ê°€ì ì¸ ì¡°ê±´ë“¤ì„ ì •ë¦¬í•©ë‹ˆë‹¤.\në˜í•œ ì¡°ê±´ë“¤ì„ ì—¬ëŸ¬ê°€ì§€ ì¡°ê±´ë“¤ì„ ìµœëŒ€í•œ ê°„ë‹¨íˆí•˜ì—¬ sigma fieldë¥¼ ì •ì˜í•©ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field2.html#ìš©ì–´ì •ë¦¬",
    "href": "posts/stochastic process/SP-sigmafield/sigma field2.html#ìš©ì–´ì •ë¦¬",
    "title": "[Stochestic Process] 5.sigma field(2)",
    "section": " ìš©ì–´ì •ë¦¬",
    "text": "ìš©ì–´ì •ë¦¬\n\nì¸¡ì •í•  ìˆ˜ ìˆëŠ” ì§‘í•© = ì´ ìˆ˜ ìˆëŠ” ì§‘í•© = measurable set\nì¸¡ì •í•  ìˆ˜ ìˆëŠ” ì§‘í•©ë“¤ì˜ ëª¨ì„ = measureì—ì„œ sigma field"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field2.html#measure",
    "href": "posts/stochastic process/SP-sigmafield/sigma field2.html#measure",
    "title": "[Stochestic Process] 5.sigma field(2)",
    "section": " measure",
    "text": "measure\n\nì§€ê¸ˆê¹Œì§€ ìš°ë¦¬ëŠ” sigma fieldë¥¼ í™•ë¥ ì„ ëª¨ìˆœì—†ì´ ì •ì˜í•  ìˆ˜ ìˆëŠ” sample spaceì˜ ë¶€ë¶„ì§‘í•©ì˜ ëª¨ìŒì´ë¼ê³  ìƒê°í•˜ë©° í™•ë¥ ì— ëŒ€í•´ì„œë§Œ ìƒê°í•´ì™”ìŠµë‹ˆë‹¤.\ní•˜ì§€ë§Œ ì‚¬ì‹¤ sigma fieldëŠ” í™•ë¥ ì„ í¬í•¨í•˜ëŠ” ì¢€ ë” ë„“ê³  ì¼ë°˜ì ì¸ ê°œë…ì¸ measureì™€ í•¨ê»˜ ì‚¬ìš©í•©ë‹ˆë‹¤.\nmeasureëŠ” ê¸°í•˜í•™ì ì¸ ì¸¡ì •(ê¸¸ì´,ë©´ì ,ë¶€í”¼)ê³¼ ë”ë¶ˆì–´ ì§ˆëŸ‰,í™•ë¥ ê³¼ ê°™ì€ ë‹¤ë¥¸ í”í•œ ê°œë…ì„ ì¼ë°˜í™” ë˜ëŠ” í˜•ì‹í™”í•œ ê²ƒì…ë‹ˆë‹¤.\në¶„ëª…íˆ ì´ëŸ¬í•œ ê°œë…ì€ ì„œë¡œë‹¤ë¥¸ êµ¬ë³„ë˜ëŠ” ê°œë…ì´ì§€ë§Œ ìœ ì‚¬í•œ ì ì´ ë§ê³  í•˜ë‚˜ì˜ ìˆ˜í•™ì ì¸ ë§¥ë½ìœ¼ë¡œ ìì£¼ ì·¨ê¸‰í• ë•Œê°€ ë§ë‹¤ê³  í•©ë‹ˆë‹¤.\në”°ë¼ì„œ ì´ëŸ¬í•œ ê°œë…ë“¤ì„ ì¡°ê¸ˆ ë” ì¼ë°˜í™”í•œ ê°œë…ì¸ measureë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\nì •ì˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n\n\n\n\n\n\n\nDefinition\n\n\n\n\nmeasureëŠ” ì–´ë–¤ ì§‘í•© \\(X\\)ê°€ ìˆì„ ë•Œ, \\(X\\)ì— ëŒ€í•œ sigma fieldê°€ ì •ì˜ì—­ì´ê³  ê³µì—­ì€ \\(\\mathbb{R}\\)ì¸ ì§‘í•©ì— ëŒ€í•œ í•¨ìˆ˜ì´ë©° ë‹¤ìŒê³¼ ê°™ì€ propertyë¥¼ ê°€ì§‘ë‹ˆë‹¤.\n\nmeasureì˜ ê°’ì€ í•­ìƒ 0ë³´ë‹¤ í¬ê±°ë‚˜ ê°™ìŠµë‹ˆë‹¤.\nê³µì§‘í•©ì˜ ê²½ìš° measureëŠ” 0ì…ë‹ˆë‹¤.\nêµì§‘í•©ì´ ì—†ëŠ” disjoint setsë“¤ì— ëŒ€í•˜ì—¬ countable unionì˜ measureëŠ” ê°ê°ì˜ ì§‘í•©ì—ì„œ êµ¬í•œ measureì˜ countable sumê³¼ ê°™ìŠµë‹ˆë‹¤.\n\n\n\n\n\nmeasureì—ì„œ ìƒê°í•´ë´¤ì„ë•Œ sigma fieldëŠ” ëª¨ìˆœì—†ì´ ì´ ìˆ˜ ìˆëŠ” ì „ì²´ì§‘í•©ì˜ ë¶€ë¶„ì§‘í•©(ì´ ìˆ˜ ìˆëŠ” ì§‘í•©)ì˜ ëª¨ìŒì…ë‹ˆë‹¤.(collection of measurable sets)\n\n\n\n\nmeasure\n\n\n\nmeasureëŠ” í•¨ìˆ˜ë¡œ ë³¸ë‹¤ë©´ monotone functionìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì˜ˆë¥¼ ë“¤ì–´ \\(A \\subseteq B\\)ë¼ë©´ \\(\\mu(A)\\leq\\mu(B)\\)ì„ì´ ì¼ë°˜ì ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field2.html#êµ¬í•´ë†¨ë˜-ì¡°ê±´ë“¤",
    "href": "posts/stochastic process/SP-sigmafield/sigma field2.html#êµ¬í•´ë†¨ë˜-ì¡°ê±´ë“¤",
    "title": "[Stochestic Process] 5.sigma field(2)",
    "section": " êµ¬í•´ë†¨ë˜ ì¡°ê±´ë“¤",
    "text": "êµ¬í•´ë†¨ë˜ ì¡°ê±´ë“¤\n\nsigma fieldëŠ” í™•ë¥ ì„ ëª¨ìˆœì—†ì´ ì •ì˜í•  ìˆ˜ ìˆëŠ” í‘œë³¸ê³µê°„ì˜ ë¶€ë¶„ì§‘í•©ë“¤ì˜ ëª¨ìŒì´ì—ˆìŠµë‹ˆë‹¤.\nì§€ê¸ˆê¹Œì§€ êµ¬í•´ë³¸ sigma fieldê°€ ë§Œì¡±í•˜ëŠ” ì¡°ê±´ì€ ëŒ€ëµì ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì•˜ìŠµë‹ˆë‹¤.\n\n\n\n\n\n\n\nsigma fieldê°€ ê°–ì¶°ì•¼í•  ì¡°ê±´ë“¤\n\n\n\n\n\\(\\Omega,\\emptyset \\in \\mathcal{F}\\)\n\\(\\forall A \\subset \\Omega : A \\in \\mathcal{F} \\Rightarrow A^c \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega : A,B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega : A,B \\in \\mathcal{F} \\Rightarrow A - B \\in \\mathcal{F}\\)"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field2.html#ì¶”ê°€ì ì¸-ì¡°ê±´ë“¤-êµ¬í•˜ê¸°",
    "href": "posts/stochastic process/SP-sigmafield/sigma field2.html#ì¶”ê°€ì ì¸-ì¡°ê±´ë“¤-êµ¬í•˜ê¸°",
    "title": "[Stochestic Process] 5.sigma field(2)",
    "section": " ì¶”ê°€ì ì¸ ì¡°ê±´ë“¤ êµ¬í•˜ê¸°",
    "text": "ì¶”ê°€ì ì¸ ì¡°ê±´ë“¤ êµ¬í•˜ê¸°\n(ì—„ë°€í•˜ì§€ ì•Šì€ ì„¤ëª…ì…ë‹ˆë‹¤.)\n\ní™•ë¥ ì€ ìœ„ì™€ ê°™ì€ measureì— í•œ ì¢…ë¥˜ì…ë‹ˆë‹¤.\nì—¬ê¸°ì„œ measure ì¤‘ í•˜ë‚˜ì¸ ê¸¸ì´ì— ëŒ€í•˜ì—¬ ë‹¤ìŒì„ ìƒê°í•´ë´…ì‹œë‹¤.\n\n[1,3]ê³¼ [2,4]ì˜ ê¸¸ì´ê°€ ì–´ëŠì •ë„ì¸ì§€ ì´ ìˆ˜ ìˆëŠ”(=ì¸¡ì •í•  ìˆ˜ ìˆëŠ”) tapeê°€ ìˆì–´\n[2,3]ë„ ê·¸ëŸ¬ë©´ ì´ ìˆ˜ ìˆì–´.(tapeì˜ ê¸¸ì´ê°€ [2,3]ë³´ë‹¤ëŠ” ê¸¸í…Œë‹ˆê¹Œ?!)\n\nìœ„ì˜ ì˜ˆì‹œëŠ” ë‘ êµ¬ê°„ì´ ê°ê° ì´ ìˆ˜ ìˆë‹¤ë©´ ê²¹ì¹˜ëŠ” êµ¬ê°„ë„ ì´ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤.\nì´ë ‡ê²Œ ë˜ë‹¤ë¥¸ measureì¸ ê¸¸ì´ì—ì„œ sigma fieldë¥¼ ìƒê°í•´ë´¤ì„ë•Œ ê²¹ì¹˜ëŠ” êµ¬ê°„ì— ëŒ€í•´ì„œ ì´ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì€ í•©ë¦¬ì ì…ë‹ˆë‹¤.\ní™•ë¥ ì˜ ì…ì¥ì—ì„œ êµì§‘í•©ì„ ì´ ìˆ˜ ìˆë‹¤ëŠ” ê²ƒì€ í•©ë¦¬ì ì´ì§€ ì•Šì•„ ë³´ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\ní•˜ì§€ë§Œ ì—¬ê¸°ì„œëŠ” ì¼ë°˜í™”ëœ measureì˜ ê°œë…ì—ì„œ ìƒê°í•˜ê¸° ë•Œë¬¸ì— êµì§‘í•©ë„ ê°€ëŠ¥í•˜ë‹¤ê³  ê°€ì •í•©ë‹ˆë‹¤.\në”°ë¼ì„œ ë‘ ì§‘í•©ì´ ê°ê° ì´ ìˆ˜ ìˆë‹¤ë©´ ë‘ ì§‘í•©ì˜ êµì§‘í•©ë„ ì´ ìˆ˜ ìˆë‹¤ëŠ” ë‹¤ìŒì˜ ëª…ì œëŠ” sigma fieldì˜ ì¡°ê±´ì…ë‹ˆë‹¤.\n\n\n\n\n\n\n\nì¡°ê±´5\n\n\n\n\n\\(\\forall A,B \\subset \\Omega : A,B \\in \\mathcal{F} \\Rightarrow A \\cap B \\in \\mathcal{F}\\)\n\n\n\n\nêµì§‘í•©ì„ ì´ ìˆ˜ ìˆë‹¤ë©´ í•©ì§‘í•©ë„ ì´ ìˆ˜ ìˆìŒì€ ë‹¹ì—°í•œ ì‚¬ì‹¤ì…ë‹ˆë‹¤.\në”°ë¼ì„œ ë‹¤ìŒì˜ ëª…ì œë„ ì°¸ì…ë‹ˆë‹¤.\n\n\n\n\n\n\n\nì¡°ê±´6\n\n\n\n\n\\(\\forall A,B \\subset \\Omega : A,B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}\\)\n\n\n\n(ì—„ë°€í•˜ì§„ ì•ŠìŒ)\n\ní™•ë¥ ì˜ ê³µë¦¬ë¥¼ ë³´ë©´ \\(\\sigma\\)-additivityë¼ í•´ì„œ êµì§‘í•©ì´ ì—†ëŠ”(disjoint) ê°ê°ì˜ ì§‘í•©ë“¤ì´ ì…€ ìˆ˜ ìˆëŠ” ì§‘í•©(countable set)ì´ \\(\\mathcal{F}\\)ì— í¬í•¨ë˜ë©´ ê·¸ê²ƒë“¤ì˜ í•©ì§‘í•©ë„ ì´ ìˆ˜ ìˆë‹¤ê³  ì •ì˜í•˜ê³  ìˆìŠµë‹ˆë‹¤\nê³µë¦¬ì— í¬í•¨ëë‹¤ëŠ” ê²ƒì€ ì–´ëŠì •ë„ ìëª…í•œ ì‚¬ì‹¤ì´ë¼ê³  í•©ì˜í•œ ê²ƒì…ë‹ˆë‹¤.(ë‚©ë“ì´ ì˜ ë˜ì§€ ì•Šì§€ë§Œ ì¼ë‹¨ ì´ë ‡ê²Œ í•˜ê¸°ë¡œ í–ˆêµ¬ë‚˜~ ì´ëŸ° ëŠë‚Œìœ¼ë¡œ ë„˜ì–´ê°€ì•¼ í•  ê²ƒ ê°™ìŠµë‹ˆë‹¤.)\n\n\n\n\n\n\n\nì¡°ê±´7\n\n\n\n\n\\(\\forall B_1,B_2,... \\subset \\Omega \\text{ such that} B_1,B_2,... \\text{ are disjoint} : B_1,B_2,... \\in \\mathcal{F} \\Rightarrow \\cup_{i=1}^{\\infty}B_i \\in \\mathcal{F}\\)\n\n\n\n\nì¡°ê±´ 5ì™€ 7ì„ ê²°í•©í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì€ ìƒˆë¡œìš´ ì¡°ê±´ì„ ì´ëŒì–´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ëŠ” disjointì¼ ë¿ë§Œ ì•„ë‹ˆë¼ disjointê°€ ì•„ë‹Œ ì§‘í•©ë“¤ì—ë„ ì ìš©ë˜ëŠ” ë” ë„“ì€ ì¡°ê±´ì…ë‹ˆë‹¤.\n\n\n\n\n\n\n\nì¡°ê±´8\n\n\n\n\\(\\forall A_1,A_2,... \\subset \\Omega : A_1,A_2,...\\in \\mathcal{F} \\Rightarrow \\cup_{i=1}^{i=\\infty}A_i \\in \\mathcal{F}\\)\n\n\n\nì´ì™¸ì—ë„ ì–¼ë§ˆë“ ì§€ ë‹¤ë¥¸ ì¡°ê±´ì„ ë§Œë“¤ì–´ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì˜ˆë¥¼ë“¤ë©´ ë‹¤ìŒê³¼ ê°™ì€ ì¡°ê±´ì´ ìˆìŠµë‹ˆë‹¤.\n\n\n\n\n\n\n\nì¡°ê±´ 9,10,11\n\n\n\n\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} \\Rightarrow A-B \\in {\\cal F}\\) (by 2,5)\n\\(\\forall A,B,C \\subset \\Omega: A,B,C \\in {\\cal F} \\Rightarrow A\\cup B \\cup C \\in {\\cal F}\\) (by 6)\n\\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\cap_{i=1}^{\\infty}A_i \\in {\\cal F}\\) (by 2,7)"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field2.html#ìµœëŒ€í•œ-ê°„ë‹¨íˆ-ì •ë¦¬í•˜ê¸°",
    "href": "posts/stochastic process/SP-sigmafield/sigma field2.html#ìµœëŒ€í•œ-ê°„ë‹¨íˆ-ì •ë¦¬í•˜ê¸°",
    "title": "[Stochestic Process] 5.sigma field(2)",
    "section": " ìµœëŒ€í•œ ê°„ë‹¨íˆ ì •ë¦¬í•˜ê¸°",
    "text": "ìµœëŒ€í•œ ê°„ë‹¨íˆ ì •ë¦¬í•˜ê¸°\n\nì§€ê¸ˆê¹Œì§€ ë‚˜ì˜¨ ëª¨ë“  ì¡°ê±´ë“¤ì€ ì´ë ‡ìŠµë‹ˆë‹¤.\n\n\n\n\n\n\n\nsigma fieldê°€ ê°–ì¶°ì•¼í•  ì¡°ê±´ë“¤\n\n\n\n\n\\(\\Omega,\\emptyset \\in \\mathcal{F}\\)\n\\(\\forall A \\subset \\Omega : A \\in \\mathcal{F} \\Rightarrow A^c \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega \\text{ such that } A \\cap B = \\emptyset : A,B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega \\text{ such that } A \\subset B: A,B \\in \\mathcal{F} \\Rightarrow B - A \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega : A,B \\in \\mathcal{F} \\Rightarrow A \\cap B \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega : A,B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}\\)\n\\(\\forall B_1,B_2,... \\subset \\Omega \\text{ such that} B_1,B_2,... \\text{ are disjoint} : B_1,B_2,... \\in \\mathcal{F} \\Rightarrow \\cup_{i=1}^{\\infty}B_i \\in \\mathcal{F}\\)\n\\(\\forall A_1,A_2,... \\subset \\Omega : A_1,A_2,...\\in \\mathcal{F} \\Rightarrow \\cup_{i=1}^{i=\\infty}A_i \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} \\Rightarrow A-B \\in {\\cal F}\\) (by 2,5)\n\\(\\forall A,B,C \\subset \\Omega: A,B,C \\in {\\cal F} \\Rightarrow A\\cup B \\cup C \\in {\\cal F}\\) (by 6)\n\\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\cap_{i=1}^{\\infty}A_i \\in {\\cal F}\\) (by 2,7)\n\n\n\n\nì—¬ê¸°ì„œë¶€í„°ëŠ” ìœ„ì˜ ëª¨ë“  ì¡°ê±´ë“¤ì„ ì „ë¶€ë‹¤ í¬í•¨í•˜ë„ë¡ ê°€ì¥ ê°„ë‹¨íˆ ëª‡ê°œì˜ ì¡°ê±´ë§Œ ë‚¨ê²¨ì„œ sigma fieldë¥¼ ì •ì˜í•´ë³´ê² ìŠµë‹ˆë‹¤.\n\n\nì¡°ê±´9,10,11ì€ ì¡°ê±´ 2,5,6,7ì— ì˜í•˜ì—¬ ë„ì¶œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ 9,10,11ì€ ì‚­ì œí•©ë‹ˆë‹¤.\nì¡°ê±´1ì€ 2ì— ì˜í•´ì„œ ê³µì§‘í•©ê¹Œì§€ í‘œê¸°í•  í•„ìš”ê°€ ì—†ë„ë¡ ìˆ˜ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì¡°ê±´4ëŠ” 2,5ì— ì˜í•´ì„œ ë„ì¶œí•  ìˆ˜ ìˆëŠ” ì¡°ê±´ì…ë‹ˆë‹¤. ë”°ë¼ì„œ 4ëŠ” ì‚­ì œí•©ë‹ˆë‹¤.\nì¡°ê±´5ëŠ” 2,6ì— ì˜í•´ì„œ ë„ì¶œí•  ìˆ˜ ìˆëŠ” ì¡°ê±´ì…ë‹ˆë‹¤. ë”°ë¼ì„œ 5ëŠ” ì‚­ì œí•©ë‹ˆë‹¤.\nì¡°ê±´6ì€ ì¡°ê±´3ì˜ disjointì§‘í•©ì„ í¬í•¨í•©ë‹ˆë‹¤.. ë”°ë¼ì„œ 3ì„ ì‚­ì œí•©ë‹ˆë‹¤.\nì¡°ê±´8ì€ ì¡°ê±´6ì„ í¬í•¨í•˜ëŠ” ë” ë„“ì€ countable unionì…ë‹ˆë‹¤. ë”°ë¼ì„œ 6ì„ ì‚­ì œí•©ë‹ˆë‹¤.\nì¡°ê±´8ì€ ì¡°ê±´7ì˜ disjoinì§‘í•©ì„ í¬í•¨í•˜ëŠ” countable unionì…ë‹ˆë‹¤. ë”°ë¼ì„œ 7ì„ ì‚­ì œí•©ë‹ˆë‹¤. \n\n\nì‚­ì œì¡°ê±´ : 3,4,5,6,7,9,10,11\në³€ê²½ë  ì¡°ê±´ : 1\në‚¨ì•„ìˆëŠ” ì¡°ê±´ : 1,2,8\nì´ë ‡ê²Œ ë°˜ì˜í•œ ì¡°ê±´ë“¤ì´ ë°”ë¡œ sigma fieldì˜ ì¡°ê±´ì…ë‹ˆë‹¤."
  },
  {
    "objectID": "posts/stochastic process/sp_appendix_set.html",
    "href": "posts/stochastic process/sp_appendix_set.html",
    "title": "[Stochestic Process] ë¶€ë¡ - ìê¾¸ ê±¸ë¦¬ëŠ” ì§‘í•©ê°œë… ì •ë¦¬",
    "section": "",
    "text": "ì§‘í•©ì€ ì›ì†Œë“¤ì˜ ëª¨ì„ì´ë©° ì¤‘ë³µëœ ì›ì†Œë¥¼ í¬í•¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\ní¬ê²Œ ì¤‘ê´„í˜¸ ì•ˆì— ì›ì†Œë¥¼ ë‚˜ì—´í•˜ëŠ” ì›ì†Œë‚˜ì—´ë²•ê³¼ íŠ¹ì •í•œ ì¡°ê±´ì„ ë§Œì¡±í•˜ëŠ” ì›ì†Œë“¤ë¡œ ì´ë£¨ì–´ì§„ ì§‘í•©ì„ ì •ì˜í•˜ëŠ” ì¡°ê±´ì œì‹œë²•ìœ¼ë¡œ ì§‘í•©ì„ ì •ì˜í•  ìˆ˜ ìˆë‹¤.\n\n\n\n\nì§‘í•©ë¡ ì—ì„œ ì§‘í•© Aì˜ ì›ì†Œê°€ ì§‘í•© Bì˜ ëª¨ë“  ì›ì†Œì— ì†í•˜ë©´ ì§‘í•© AëŠ” ì§‘í•© Bì˜ ë¶€ë¶„ì§‘í•©(subset)ì´ë©° ì§‘í•© BëŠ” ì§‘í•© Aì˜ ìƒìœ„ì§‘í•©(superset)ì…ë‹ˆë‹¤.\nì¦‰, ë¶€ë¶„ì§‘í•©ê³¼ ìƒìœ„ì§‘í•©ì€ ë‘ ì§‘í•©ì‚¬ì´ì— ì •ì˜ë˜ëŠ” ê°œë…ìœ¼ë¡œì„œ ê¸°í˜¸ë¡œ \\(A \\subseteq B\\)ë¼ í‘œê¸°í•©ë‹ˆë‹¤.\nì‰½ê²Œ ìƒê°í•´ë³´ìë©´ ë¶€ë¶„ì§‘í•©ì€ ì–´ë–¤ ì§‘í•©ì— ì¡´ì¬í•˜ëŠ” ê°ê°ì˜ ëª¨ë“  ì›ì†Œì—ì„œ íŒë‹¨(ì„ íƒí• ì§€ ë§ì§€)ì„ í†µí•´ ë§Œë“  ë˜ë‹¤ë¥¸ ì§‘í•©ìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nëª¨ë“  ì§‘í•©ì€ ë°˜ë“œì‹œ ìê¸° ìì‹ ì˜ ë¶€ë¶„ì§‘í•©ì…ë‹ˆë‹¤. ì´ëŠ” ê°ê°ì˜ íŒë‹¨ì—ì„œ ëª¨ë“  ì›ì†Œë¥¼ ì„ íƒí–ˆë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nê³µì§‘í•©ì€ ëª¨ë“  ì§‘í•©ì˜ ë¶€ë¶„ì§‘í•©ì…ë‹ˆë‹¤. ì´ëŠ” ê°ê°ì˜ íŒë‹¨ì—ì„œ ëª¨ë“  ì›ì†Œë¥¼ ì„ íƒí•˜ì§€ ì•ŠìŒìœ¼ë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ì—„ë°€í•˜ì§„ ì•ŠìŠµë‹ˆë‹¤.ê³µì§‘í•©ì´ ì™œ ëª¨ë“  ì§‘í•©ì˜ ë¶€ë¶„ì§‘í•©ì¸ì§€ë¥¼ ì‰½ê²Œ ë°›ì•„ë“¤ì—¬ì§€ì§€ ì•Šì•„ì„œ ì´ë¥¼ ê¸°ì–µí•˜ê¸° ìœ„í•¨ì…ë‹ˆë‹¤.)\n\n\n\n\n\nì§‘í•© Aì˜ ì›ì†Œê°€ ì§‘í•© Bì˜ ëª¨ë“  ì›ì†Œì— ì†í•˜ë©´ì„œ ì§‘í•© Aì™€ ì§‘í•© Bê°€ ê°™ì§€ì•Šë‹¤ë©´(not equal) ì§‘í•© AëŠ” ì§‘í•© Bì˜ ì§„ë¶€ë¶„ì§‘í•©(proper or strict subset)ì…ë‹ˆë‹¤.\nê¸°í˜¸ë¡œëŠ” \\(A \\subsetneq B\\)ë¡œ í‘œê¸°í•©ë‹ˆë‹¤.\n\n\n\n\n\nìœ„ì—ì„œ ë¶€ë¶„ì§‘í•© ê¸°í˜¸ëŠ” ë‹¨ìˆœíˆ \\(A\\)ê°€ \\(B\\)ì˜ ë¶€ë¶„ì§‘í•©ì„ì„ ë‚˜íƒ€ë‚´ì—ˆìŠµë‹ˆë‹¤.\nì´ì™¸ì—ë„ ë¶€ë¶„ì§‘í•© ê¸°í˜¸ëŠ” ì°¸ ê±°ì§“ì„ íŒë‹¨í•´ì•¼í•˜ëŠ” ìˆ˜í•™ì  ëª…ì œì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ì œì¼ í—·ê°ˆë ¸ë˜ ë¶€ë¶„ â€¦)\n\\(A \\subseteq B\\)ëŠ” `\\(A\\)ëŠ” \\(B\\)ì˜ ë¶€ë¶„ì§‘í•©ì´ë‹¤`ë¼ëŠ” ëª…ì œë¥¼ ì˜ë¯¸í•˜ê±°ë‚˜\n\\(A \\subsetneq B\\)ëŠ” `\\(A\\)ëŠ” \\(B\\)ì˜ ì§„ë¶€ë¶„ì§‘í•©ì´ë‹¤`ë¼ëŠ” ëª…ì œë¥¼ ì˜ë¯¸í•©ë‹ˆë‹¤.\n\n\n\n\n\n\n\në¶€ë¶„ì§‘í•©ì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ì„œ \\(\\subset\\)ì„ ì‚¬ìš©í•˜ë©° ì´ëŠ” ìœ„ì—ì„œì˜ \\(\\subseteq\\)ì™€ ê°™ì€ ê°œë…ì…ë‹ˆë‹¤.\nì´ëŸ¬í•œ ê²½ìš° \\(A \\subset A\\)ëŠ” ì°¸ì…ë‹ˆë‹¤. ë¶€ë¶„ì§‘í•©ì€ í•­ìƒ ìê¸°ìì‹ ì„ ë¶€ë¶„ì§‘í•©ìœ¼ë¡œ ê°–ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\në˜í•œ ì´ ê²½ìš° \\(\\subseteq\\)ëŠ” ë‹¨ì§€ ë‘ ì§‘í•©ì´ ê°™ì„ ìˆ˜ ìˆìŒì„ ê°•ì¡°í•˜ëŠ” í‘œí˜„ì´ë¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\nì´ì–´ì§€ëŠ” í¬ìŠ¤íŒ…ì—ì„œëŠ” case1ì˜ notationì„ í™œìš©í•©ë‹ˆë‹¤. ì¦‰, ë‹¤ìŒì˜ ê²½ìš°ëŠ” ëª¨ë‘ ë™ì¹˜ì…ë‹ˆë‹¤.\n\n\\(A \\subset B\\) \\(\\Longleftrightarrow A \\subseteq B\\) \\(\\Longleftrightarrow\\) Aê°€ Bì˜ ì§„ë¶€ë¶„ì§‘í•©ì´ê±°ë‚˜ ê°™ë‹¤. \\(\\Longleftrightarrow\\) AëŠ” Bì˜ ë¶€ë¶„ì§‘í•©ì´ë‹¤.\n\n\n\n\n\n\nì§„ë¶€ë¶„ì§‘í•©(strict or proper subset)ì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ì„œ \\(\\subsetneq\\)ëŒ€ì‹ ì— \\(\\subset\\)ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ê²½ìš° \\(\\leq\\),\\(<\\)ì™€ ê°™ì€ ëŠë‚Œì…ë‹ˆë‹¤. \nì´ëŸ¬í•œ ê²½ìš° ìœ„ì™€ ë‹¤ë¥´ê²Œ \\(A \\subset A\\)ëŠ” ê±°ì§“ì…ë‹ˆë‹¤. \\(\\subset\\)ëŠ” ìê¸°ìì‹ ì„ ëº€ ë¶€ë¶„ì§‘í•©ì¸ ì§„ë¶€ë¶„ì§‘í•©ì„ ì˜ë¯¸í•˜ëŠ” ê¸°í˜¸ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\në°˜ëŒ€ë¡œ \\(A \\subseteq A\\)ëŠ” ì°¸ì…ë‹ˆë‹¤. \\(\\subseteq\\)ëŠ” ì§„ë¶€ë¶„ì§‘í•©ì´ê±°ë‚˜ ê°™ìŒ(ìê¸°ìì‹ ì„ í¬í•¨í•˜ì—¬ ë¶€ë¶„ì§‘í•©)ì„ ì˜ë¯¸í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.\nstochastic process í¬ìŠ¤íŒ…ì—ì„œ ì´ notationì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html",
    "href": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html",
    "title": "plotly ì‹œê°í™” ëª¨ìŒ",
    "section": "",
    "text": "Code\n# %pip install plotly (jupyter notebook)\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\npio.renderers.default = \"plotly_mimetype+notebook\"\n\n\n\n\nCode\nimport pandas as pd\ntimesData = pd.read_csv(\"./timesData.csv\")\ntimesData.head(5)\n\n\n\n\n\n\n  \n    \n      \n      world_rank\n      university_name\n      country\n      teaching\n      international\n      research\n      citations\n      income\n      total_score\n      num_students\n      student_staff_ratio\n      international_students\n      female_male_ratio\n      year\n    \n  \n  \n    \n      0\n      1\n      Harvard University\n      United States of America\n      99.7\n      72.4\n      98.7\n      98.8\n      34.5\n      96.1\n      20,152\n      8.9\n      25%\n      NaN\n      2011\n    \n    \n      1\n      2\n      California Institute of Technology\n      United States of America\n      97.7\n      54.6\n      98.0\n      99.9\n      83.7\n      96.0\n      2,243\n      6.9\n      27%\n      33 : 67\n      2011\n    \n    \n      2\n      3\n      Massachusetts Institute of Technology\n      United States of America\n      97.8\n      82.3\n      91.4\n      99.9\n      87.5\n      95.6\n      11,074\n      9.0\n      33%\n      37 : 63\n      2011\n    \n    \n      3\n      4\n      Stanford University\n      United States of America\n      98.3\n      29.5\n      98.1\n      99.2\n      64.3\n      94.3\n      15,596\n      7.8\n      22%\n      42 : 58\n      2011\n    \n    \n      4\n      5\n      Princeton University\n      United States of America\n      90.9\n      70.3\n      95.4\n      99.9\n      -\n      94.2\n      7,929\n      8.4\n      27%\n      45 : 55\n      2011\n    \n  \n\n\n\n\n\n\nCode\ntimesData.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2603 entries, 0 to 2602\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   world_rank              2603 non-null   object \n 1   university_name         2603 non-null   object \n 2   country                 2603 non-null   object \n 3   teaching                2603 non-null   float64\n 4   international           2603 non-null   object \n 5   research                2603 non-null   float64\n 6   citations               2603 non-null   float64\n 7   income                  2603 non-null   object \n 8   total_score             2603 non-null   object \n 9   num_students            2544 non-null   object \n 10  student_staff_ratio     2544 non-null   float64\n 11  international_students  2536 non-null   object \n 12  female_male_ratio       2370 non-null   object \n 13  year                    2603 non-null   int64  \ndtypes: float64(4), int64(1), object(9)\nmemory usage: 284.8+ KB"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#add-markers-and-text",
    "href": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#add-markers-and-text",
    "title": "plotly ì‹œê°í™” ëª¨ìŒ",
    "section": "add markers and text",
    "text": "add markers and text\n\n\nCode\n#1 data frame\ndf = timesData.iloc[:100]\n\n#2 trace and data\ntrace = go.Scatter(\n    x = df.world_rank,\n    y = df.citations,\n    mode = \"lines+markers\", #add marker,\n    marker = dict(color = \"rgba(16,112,2,0.8)\"),\n    text = df.university_name #add text\n)\ndata = [trace]\n#3 layout and data\nlayout = go.Layout(\n    title = \"citation\",\n    xaxis = dict(title = \"World Rank\",ticklen = 5)\n)\n\n#4 create figure\nfig = go.Figure(data = data,layout = layout)\n\n#5 plot figure\nfig.show()\n\n\n\n                                                \n\n\nversion2ê°€ ë­”ê°€ ë” ì¢‹ì„ë“¯?"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#add-markers-and-text-1",
    "href": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#add-markers-and-text-1",
    "title": "plotly ì‹œê°í™” ëª¨ìŒ",
    "section": "add markers and text",
    "text": "add markers and text\n\n\nCode\n#1 data frame\ndf2014 = timesData[timesData.year == 2014].iloc[:100,:]\n\n#2 trace,data\ntrace = go.Scatter(\n    x = df2014.world_rank,\n    y = df2014.citations,\n    mode = \"markers\",\n    #marker = dict(color = \"green\",opacity=0.8), #alpha(ë¶ˆíˆ¬ëª…ë„) ì¡°ì ˆ vs1\n    marker = dict(color = \"rgba(255,128,2,0.8)\"), #alpha(ë¶ˆíˆ¬ëª…ë„) ì¡°ì ˆ vs2\n    text = df2014.university_name,\n\n)\ndata = [trace]\n\n#3 layout\nlayout = go.Layout(xaxis = dict(title = \"World Rank\"),yaxis = dict(title = \"Citation\"))\n\n#4 create figure\nfig = go.Figure(data=data,layout=layout)\n\n#5 plot\nfig.show()"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#ì—¬ëŸ¬ê°œì˜-ì°¨íŠ¸-ê²¹ì²˜-ê·¸ë¦¬ê¸°",
    "href": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#ì—¬ëŸ¬ê°œì˜-ì°¨íŠ¸-ê²¹ì²˜-ê·¸ë¦¬ê¸°",
    "title": "plotly ì‹œê°í™” ëª¨ìŒ",
    "section": "ì—¬ëŸ¬ê°œì˜ ì°¨íŠ¸ ê²¹ì²˜ ê·¸ë¦¬ê¸°",
    "text": "ì—¬ëŸ¬ê°œì˜ ì°¨íŠ¸ ê²¹ì²˜ ê·¸ë¦¬ê¸°\n\nì—¬ê¸°ì„œëŠ” histogramìœ¼ë¡œ í–ˆìœ¼ë‚˜ ë‹¤ë¥¸ì°¨íŠ¸ë“¤ë„ ê°€ëŠ¥\n\n\n\nCode\n#1.dataframe\nx2011 = timesData.student_staff_ratio[timesData.year == 2011]\nx2012 = timesData.student_staff_ratio[timesData.year == 2012]\n#2.trace&data\ntrace1 = go.Histogram(\n    x=x2011,\n    #opacity=0.7, #ë¶ˆíˆ¬ëª…ë„ ì¡°ì ˆ\n    name=\"2011\", #ë²”ë¡€(legend)ë¥¼ ì„¤ì •í•˜ê¸° ìœ„í•œ ì´ë¦„ ì„¤ì •\n    marker=dict(color=\"rgb(171,50,96)\",opacity=0.7)\n)\n\ntrace2 = go.Histogram(\n    x=x2012,\n    name=\"2012\",\n    marker=dict(color=\"blue\",opacity=0.7)\n)\ndata=[trace1,trace2]\n#3.layout\nlayout = go.Layout(\n    barmode = \"overlay\", #trace ê²¹ì³ ê·¸ë¦¬ê¸°\n    xaxis=dict(title=\"students-staff ratio\"),\n    yaxis=dict(title=\"count\"),\n    title = dict(text = \"histogram\",x = 0.5)\n)\n#4 figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()\n\n\n\n                                                \n\n\nì°¸ê³ ìë£Œ - Opacityì™€ alpha? : OpacityëŠ” markerì•ˆíŒì—ì„œ ëª¨ë‘ ì“°ì¼ ìˆ˜ ìˆìœ¼ë©° alphaëŠ” rgbaì™€ ì“¸ë•Œë§Œ ì…ë ¥,ê°™ì€ ì—­í• ì„ í•¨. ë‹¨,Opacityë¥¼ markerì˜ ë°–ì—ì„œ ì…ë ¥í•˜ë©´ traceì•ˆì—ì„œ ë°€ë„ë¥¼ í‘œí˜„ í•˜ì§€ ëª»í•¨. ë‹¤ë¥¸ traceë¼ë¦¬ ê²¹ì¹ ë•Œì—ëŠ” ë°€ë„í‘œí˜„ë¨.(ê°™ì€ traceì—ì„œë§Œ ì•ˆë¨.)\n\n\nCode\n# 1.data frame\ndataframe = timesData[timesData.year == 2015]\n\n#2.trace and data\ndata = []\nfor col in [\"world_rank\",\"citations\",\"income\",\"total_score\"]:\n    _trace = go.Scatter(\n        x = dataframe[\"world_rank\"],\n        y = dataframe[col],\n        mode = \"lines\"\n    )\n    data.append(_trace)\n\n#3. layout\nlayout = go.Layout(\n    xaxis=dict(\n        domain=[0, 0.45]\n    ),\n    yaxis=dict(\n        domain=[0, 0.45]\n    ),\n    xaxis2=dict(\n        domain=[0.55, 1]\n    ),\n    xaxis3=dict(\n        domain=[0, 0.45],\n        anchor='y3'\n    ),\n    xaxis4=dict(\n        domain=[0.55, 1],\n        anchor='y4'\n    ),\n    yaxis2=dict(\n        domain=[0, 0.45],\n        anchor='x2'\n    ),\n    yaxis3=dict(\n        domain=[0.55, 1]\n    ),\n    yaxis4=dict(\n        domain=[0.55, 1],\n        anchor='x4'\n    ),\n    title = 'Research, citation, income and total score VS World Rank of Universities'\n)\n\n#4. fig\nfig = make_subplots(rows=2,cols=2)\n#5. plot\nrow = 1\ncol = 1\nfor trace in data:\n    fig.append_trace(trace,row=row,col=col)\n    col+=1\n    if col > 2:\n        col = 1\n        row+=1\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{\"type\": \"xy\"}, {\"type\": \"polar\"}],\n           [{\"type\": \"domain\"}, {\"type\": \"scene\"}]],\n)\n\nfig.add_trace(go.Bar(y=[2, 3, 1]),\n              row=1, col=1)\n\nfig.add_trace(go.Barpolar(theta=[0, 45, 90], r=[2, 3, 1]),\n              row=1, col=2)\n\nfig.add_trace(go.Pie(values=[2, 3, 1]),\n              row=2, col=1)\n\nfig.add_trace(go.Scatter3d(x=[2, 3, 1], y=[0, 0, 0],\n                           z=[0.5, 1, 2], mode=\"lines\"),\n              row=2, col=2)\n\nfig.update_layout(height=700, showlegend=False)\n\nfig.show()"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#vector-fieldquiver-plot",
    "href": "posts/visualization/plotly_visualization/ploty ì‹œê°í™” ëª¨ìŒ.html#vector-fieldquiver-plot",
    "title": "plotly ì‹œê°í™” ëª¨ìŒ",
    "section": "Vector field(quiver plot)",
    "text": "Vector field(quiver plot)\n\nì‚¬ì „ì¤€ë¹„\n\nnp.meshgrid : xì¢Œí‘œ,yì¢Œí‘œë¥¼ ê°€ì§€ëŠ” ë²¡í„°ë¥¼ ì…ë ¥í–ˆì„ë•Œ, ë‘ ë²¡í„°ë¡œ ë§Œë“¤ ìˆ˜ ìˆëŠ” ê²©ìì˜ ì¢Œí‘œ(x,y)ë¥¼ ì¶œë ¥\n\n\n\nCode\nimport numpy as np\nx_coord = np.arange(0,2,.2)\ny_coord = np.arange(0,2,.2)\nx,y = np.meshgrid(np.arange(0,2,.2),np.arange(0,2,.2))\nprint(x_coord.shape,y_coord.shape)\nprint(x.shape,y.shape)\n\n\n(10,) (10,)\n(10, 10) (10, 10)\n\n\n\nê²©ì(grid,matrix)ì— í•¨ìˆ˜ ì ìš©í•˜ë©´? => matrix(x,y ê°ê°ì˜ ì¢Œí‘œ)ì˜ ëª¨ë“  ìš”ì†Œì— í•¨ìˆ˜ê°€ ì ìš©ë¨\n\n\n\nCode\nprint(np.cos(x).shape,np.sin(x).shape)\n\n\n(10, 10) (10, 10)\n\n\n\në°°ì—´ì˜ ìš”ì†Œ ê°’ ì°¨ë¡€ëŒ€ë¡œ ì½ì–´ë³´ê¸° â€¦\n\n(0,0),(0.2,0),(0.4,0) â€¦ (1.8,0) => (0,0.2),(0.2,0.2),(0.4,0.2)â€¦ xì¢Œí‘œ ë‹¤ ì½ê³  yì¢Œí‘œì¦ê°€ ê·¸ ë‹¤ìŒ xì¢Œí‘œ ë‹¤ ì½ê³  yì¢Œí‘œ ì¦ê°€ â€¦\n\n\nCode\nx,y\n\n\n(array([[0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8]]),\n array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n        [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n        [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n        [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6],\n        [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],\n        [1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],\n        [1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2],\n        [1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4],\n        [1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6],\n        [1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8]]))\n\n\n\n\nGradient Vector Field\n\n\\(\\nabla f = xe^{-x^2-y^2}\\)\n\n\nCode\n#1.prepare data\nx,y = np.meshgrid(np.arange(-2,2,0.2),np.arange(-2,2,.25)) #ì¢Œí‘œ\nz = x*np.exp(-x**2-y**2) #í•¨ìˆ˜\n\ndx=0.2;dy=0.25 #dx,dy\nv,u = np.gradient(z,dx,dy) #í•¨ìˆ˜ì˜ ê·¸ë ˆë””ì–¸íŠ¸(ê°ì¢Œí‘œì—ì„œì˜ ë¯¸ë¶„ê³„ìˆ˜)\n\n\n\n\nCode\n#2.trace and data => ìƒëµ\n#3.fig\nfig = ff.create_quiver(x,y,u,v,scale=.25,arrow_scale=.4,name=\"quiver\",line_width=1)\nfig.add_trace(go.Scatter(x=[-.7,.75],y=[0,0],\n                         mode=\"markers\",\n                         marker_size=12,\n                         name=\"points\"))\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nx = np.linspace(-1,1,100)\ny = np.linspace(-1,1,100)\nxx,yy = np.meshgrid(x,y)\nfor i in range()\n\n\n(array([[-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        ...,\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ]]),\n array([[-1.        , -1.        , -1.        , ..., -1.        ,\n         -1.        , -1.        ],\n        [-0.97979798, -0.97979798, -0.97979798, ..., -0.97979798,\n         -0.97979798, -0.97979798],\n        [-0.95959596, -0.95959596, -0.95959596, ..., -0.95959596,\n         -0.95959596, -0.95959596],\n        ...,\n        [ 0.95959596,  0.95959596,  0.95959596, ...,  0.95959596,\n          0.95959596,  0.95959596],\n        [ 0.97979798,  0.97979798,  0.97979798, ...,  0.97979798,\n          0.97979798,  0.97979798],\n        [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n          1.        ,  1.        ]]))\n\n\n\n\n\n1. ì‹œì \n\nì¢…ì ì€ í™”ì‚´í‘œë¡œ í‘œì‹œí•´ì•¼ í•˜ë¯€ë¡œ ì‹œì ë§Œ ë§Œë“¤ê¸°\n\n\n\nCode\nimport plotly.graph_objs as go\n\n\n\n\nCode\n#1. prepare data\n\n#ì²«ë²ˆì§¸ ë²¡í„°ì˜ ì‹œì  x[0],y[0],z[0] ì¢…ì  x[1],y[1],z[1]\n#ë‘ë²ˆì§¸ ë²¡í„°ì˜ ì‹œì  x[2],y[2],z[2] ì¢…ì  x[2],y[2],z[2]\n#ë‘ ê°œì”© ë¬¶ì„\nx = [10.1219, 10.42579, 15.21396, 15.42468, 20.29639,20.46268, 25.36298, 25.49156]\ny = [5.0545,  5.180104, 5.0545,   5.20337,  5.0545,  5.194271, 5.0545,   5.231627]\nz = [5.2713,  5.231409, 5.2713,   5.231409, 5.2713 ,  5.235852,  5.2713, 5.231627]\n#pairs = [(0,1),(2,3),(4,5),(6,7)]\n[coord for coord in range(0,len(x),2)]\n\n\n[0, 2, 4, 6]\n\n\n\n\nCode\n#2. trace,data(trace set)\ntrace1 = go.Scatter3d(\n    x=[x[coord] for coord in range(0,len(x),2)],\n    y=[y[coord] for coord in range(0,len(y),2)],\n    z=[z[coord] for coord in range(0,len(z),2)],\n    mode = \"markers\",\n    line=dict(color=\"red\")\n)\ndata = [trace1]\n\n#3. Layout\nlayout = go.Layout(title=dict(text = \"vectors\"))\n\n#4. figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()\n\n\n\n                                                \n\n\n\n\n2. ì„  ë§Œë“¤ê¸°\n\n\nCode\n#1.prepare data\nx_lines = list()\ny_lines = list()\nz_lines = list()\n\nfor i in range(len(x)):\n    x_lines.append(x[i])\n    y_lines.append(y[i])\n    z_lines.append(z[i])\n    #plotlyì—ì„œ Scatterì˜ line modeëŠ” ì ê³¼ ì  ì‚¬ì´ì— ì„ ì„ ë§Œë“¦\n    #0,1ë²ˆì§¸ ìë¦¬ì˜ ì¢Œí‘œì—ëŠ” ì‹œì ,ì¢…ì ì„ ë„£ê³  3ë²ˆì§¸ ìë¦¬ì— Noneì„ ì¶”ê°€í•˜ì—¬ ì ì„ ë§Œë“¤ì§€ ì•ŠìŒ \n    #ë”°ë¼ì„œ, ì„ ì´ ìƒê¸°ì§€ ì•ŠìŒ\n    if i % 2 == 1:    \n        x_lines.append(None)\n        y_lines.append(None)\n        z_lines.append(None)\n\n#2.trace and tr_set(=data)\ntrace2 = go.Scatter3d(\n    x=x_lines,\n    y=y_lines,\n    z=z_lines,\n    mode = \"lines\",\n    line = dict(width = 2, color = 'rgb(255, 0,0)')\n)\ndata = [trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\n#ì¤‘ê°„ì²´í¬\ndata = [trace1,trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\n3.ì¢…ì  ë§Œë“¤ê¸°\n\n\nCode\ndata = [trace1,trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nimport plotly.graph_objs as go\n# plotly.offline.init_notebook_mode()\n\nx = [10.1219, 10.42579, 15.21396, 15.42468, 20.29639,20.46268, 25.36298, 25.49156]\ny = [5.0545,  5.180104, 5.0545,   5.20337,  5.0545,  5.194271, 5.0545,   5.231627]\nz = [5.2713,  5.231409, 5.2713,   5.231409, 5.2713 ,  5.235852,  5.2713, 5.231627]\n\npairs = [(0,1), (2,3),(4,5), (6,7)]\n\n## plot ONLY the first ball in each pair of balls\ntrace1 = go.Scatter3d(\n    x=[x[p[0]] for p in pairs],\n    y=[y[p[0]] for p in pairs],\n    z=[z[p[0]] for p in pairs],\n    mode='markers',\n    name='markers',\n    line=dict(color='red')\n)\n\ngo.Figure(data=trace1)"
  },
  {
    "objectID": "posts/temp/jbig.html",
    "href": "posts/temp/jbig.html",
    "title": "Untitled",
    "section": "",
    "text": "\\[\\begin{split}\n\\begin{aligned}\n&\\text{Let } A \\in R^{200 \\times 20},\\text{rank}(A) = 20\\\\\n&A = U\\Sigma V^T\\\\\n&\\Longleftrightarrow\nA =\n\n\\begin{bmatrix}\n\\boxed{\\begin{matrix} \\\\ \\\\ \\\\ \\\\ \\\\ \\, u_1 \\, \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\end{matrix}} \\!\\!\\!\\!&\n\\boxed{\\begin{matrix} \\\\ \\\\ \\\\ \\\\ \\\\ \\, u_2 \\, \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\end{matrix}} \\!\\!\\!\\!&\n\\boxed{\\begin{matrix} \\\\ \\\\ \\\\ \\\\ \\\\ \\, u_3 \\, \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\end{matrix}} \\!\\!\\!\\!&\n\\cdots \\!\\!\\!\\!&\n\\boxed{\\begin{matrix} \\\\ \\\\ \\\\ \\\\ \\\\  u_M  \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\end{matrix}} \\!\\!\\!\\!&\n\\cdots \\!\\!\\!\\!&\n\\boxed{\\begin{matrix} \\\\ \\\\ \\\\ \\\\ \\\\ u_{200} \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\end{matrix}}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boxed{\\sigma_1 \\phantom{\\dfrac{}{}} \\!\\!} & 0 & 0 & \\cdots & 0 \\\\\n0 & \\boxed{\\sigma_2 \\phantom{\\dfrac{}{}} \\!\\!} & 0 & \\cdots & 0 \\\\\n0 & 0 & \\boxed{\\sigma_3 \\phantom{\\dfrac{}{}} \\!\\!} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots \\\\\n0 & 0 & 0 & \\cdots & \\boxed{\\sigma_{20} \\phantom{\\dfrac{}{}} \\!\\!} \\\\\n0 & 0 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots &  & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boxed{\\begin{matrix} & & & v_1^T & & & \\end{matrix}} \\\\\n\\boxed{\\begin{matrix} & & & v_2^T & & & \\end{matrix}} \\\\\n\\vdots \\\\\n\\boxed{\\begin{matrix} & & & v_{20}^T & & & \\end{matrix}} \\\\\n\\end{bmatrix}\n\\tag{3.4.5}\\\\\n&\\Longleftrightarrow\nA = \\begin{bmatrix}\\sigma_1u_1\\ & \\sigma_2u_2,\\dots,\\sigma_{20}u_{20}\\end{bmatrix}\n\\begin{bmatrix}\n\\boxed{\\begin{matrix} & & & v_1^T & & & \\end{matrix}} \\\\\n\\boxed{\\begin{matrix} & & & v_2^T & & & \\end{matrix}} \\\\\n\\vdots \\\\\n\\boxed{\\begin{matrix} & & & v_{20}^T & & & \\end{matrix}} \\\\\n\\end{bmatrix}\n\\\\\n&\\Longleftrightarrow A = \\sigma_1u_1v_1^T + \\sigma_2u_2v_2^T + \\sigma_3u_3v_3^T + \\dots\n\\end{aligned}\n\\end{split}\\]"
  }
]