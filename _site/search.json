[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "전북대학교 IT응용시스템 공학과 신호연"
  },
  {
    "objectID": "hello github.html",
    "href": "hello github.html",
    "title": "Hoyen's Blog",
    "section": "",
    "text": "My python\n\nprint(\"hello github\")\n\nhello github"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "hoyeon’s blog",
    "section": "",
    "text": "딥러닝(1) - 선형회귀\n\n\n\n\n\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\n\nDec 22, 2022\n\n\nsin ho yeon\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n딥러닝 기말고사\n\n\n\n\n\n\n\nDeep learning\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\n신호연\n\n\n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nDec 21, 2022\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nDec 18, 2022\n\n\nTristan O’Malley\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/deep learning final test/dl final.html",
    "href": "posts/deep learning final test/dl final.html",
    "title": "딥러닝 기말고사",
    "section": "",
    "text": "This is a post of 2022 DL final test\n\nDL Final\n\nimport torch \nimport matplotlib.pyplot as plt\nimport torch.nn as nn\n\n\nsoft = torch.nn.Softmax(dim=1)\nsig = torch.nn.Sigmoid()\ntanh= torch.nn.Tanh()\n\n\nhihello\n\n\ntxt = list('hi?hello!!')*100 \ntxt_x = txt[:-1]\ntxt_y = txt[1:]\ntxt_x[:5], txt_y[:5]\n\n(['h', 'i', '?', 'h', 'e'], ['i', '?', 'h', 'e', 'l'])\n\n\n\ntorch.nn.RNN()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n\n\n교수님이 올려주신 것과 매핑이 다릅니다..!\n\n\ndef make_mapping(txt):\n    mapping = {}\n    idx = 0\n    for chr in txt:\n        if chr not in mapping.keys():\n            mapping[chr] = idx\n            idx+=1\n    return mapping\nmapping = make_mapping(txt)\nmapping\n\n{'h': 0, 'i': 1, '?': 2, 'e': 3, 'l': 4, 'o': 5, '!': 6}\n\n\n\ndef int_encoding(txt,mapping):\n    return [mapping[chr] for chr in txt]\nx = torch.nn.functional.one_hot(torch.tensor(int_encoding(txt_x,mapping))).float().to(\"cuda\")\ny = torch.nn.functional.one_hot(torch.tensor(int_encoding(txt_y,mapping))).float().to(\"cuda\")\nprint(x[:5],y[:5],x.dtype,y.dtype,x.shape,y.shape)\n\ntensor([[1., 0., 0., 0., 0., 0., 0.],\n        [0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.]], device='cuda:0') tensor([[0., 1., 0., 0., 0., 0., 0.],\n        [0., 0., 1., 0., 0., 0., 0.],\n        [1., 0., 0., 0., 0., 0., 0.],\n        [0., 0., 0., 1., 0., 0., 0.],\n        [0., 0., 0., 0., 1., 0., 0.]], device='cuda:0') torch.float32 torch.float32 torch.Size([999, 7]) torch.Size([999, 7])\n\n\n\ntorch.manual_seed(5)\nnet = torch.nn.RNN(7,4).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(4,7).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(net.parameters())+list(linr_h2o.parameters()),lr=0.1)\n\n\nh_0 = torch.zeros((1,4)).to(\"cuda\")\nfor epoch in range(500):\n    #1 hidden,out\n    hidden,h_T = net(x,h_0)\n    #print(hidden.shape,h_T.shape)\n    out = linr_h2o(hidden)\n    #2 loss\n    loss = loss_fn(out,y)\n    if epoch % 100 == 0:\n        print(loss)\n    #3 derivative\n    loss.backward()\n    #4 update\n    optimizer.step()\n    optimizer.zero_grad()\n\ntensor(2.0413, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0055, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0025, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0014, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0010, device='cuda:0', grad_fn=<DivBackward1>)\n\n\n\nh_0 = torch.zeros((1,4)).to(\"cuda\")\nhidden,h_T = net(x,h_0)\nout = linr_h2o(hidden)\nyhat = soft(out)\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x1a949f603a0>,\n  <matplotlib.axis.XTick at 0x1a949f60370>,\n  <matplotlib.axis.XTick at 0x1a949f68ac0>,\n  <matplotlib.axis.XTick at 0x1a94ff82040>,\n  <matplotlib.axis.XTick at 0x1a94ff82640>,\n  <matplotlib.axis.XTick at 0x1a94ff82d90>,\n  <matplotlib.axis.XTick at 0x1a94ff87520>],\n [Text(0, 1, 'h'),\n  Text(1, 1, 'i'),\n  Text(2, 1, '?'),\n  Text(3, 1, 'e'),\n  Text(4, 1, 'l'),\n  Text(5, 1, 'o'),\n  Text(6, 1, '!')])\n\n\n\n\n\n\ntorch.nn.RNNCell()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n\n\ntorch.manual_seed(5)\nrnncell = torch.nn.RNNCell(7,4).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(4,7).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(rnncell.parameters()) + list(linr_h2o.parameters()),lr=0.1)\n\n\nT = len(x)\nfor epoch in range(500):\n    h_t = torch.zeros((1,4)).to(\"cuda\")\n    loss = 0.0\n    for t in range(T):\n        x_t,y_t = x[[t]],y[[t]]\n        #1 hidden,out\n        h_t = rnncell(x_t,h_t)\n        o_t = linr_h2o(h_t)\n        #2 loss at timestep t\n        l_t = loss_fn(o_t,y_t)\n        loss += l_t\n    loss = loss/T\n    if epoch % 100 == 0:\n        print(loss)\n    #3 derivative\n    loss.backward()\n    #4 update and clean\n    optimizer.step()\n    optimizer.zero_grad()\n\ntensor(2.0413, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0055, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0025, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0014, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0010, device='cuda:0', grad_fn=<DivBackward0>)\n\n\n\nT = len(x)\nhidden = torch.zeros((T,4)).to(\"cuda\")\nhidden[[0]] = rnncell(x[[0]],torch.zeros((1,4)).to(\"cuda\"))\nfor t in range(1,T):\n    x_t = x[[t]]\n    #print(x_t.shape)\n    hidden[[t]] = rnncell(x_t,hidden[[t-1]])\n\n\nyhat = soft(linr_h2o(hidden))\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x1a94ffb9a90>,\n  <matplotlib.axis.XTick at 0x1a94ffb9a60>,\n  <matplotlib.axis.XTick at 0x1a94ffcc220>,\n  <matplotlib.axis.XTick at 0x1a94ffc1610>,\n  <matplotlib.axis.XTick at 0x1a94ffc1d60>,\n  <matplotlib.axis.XTick at 0x1a94ffc74f0>,\n  <matplotlib.axis.XTick at 0x1a94ffc1af0>],\n [Text(0, 1, 'h'),\n  Text(1, 1, 'i'),\n  Text(2, 1, '?'),\n  Text(3, 1, 'e'),\n  Text(4, 1, 'l'),\n  Text(5, 1, 'o'),\n  Text(6, 1, '!')])\n\n\n\n\n\n\ntorch.nn.Module을 상속받은 클래스를 정의하고 (2)의 결과와 동일한 적합값이 나오는 신경망을 설계한 뒤 학습하라. (초기값을 적절하게 설정할 것)\n\n\nclass Rnn_cell(nn.Module):\n    def __init__(self,input_size,hidden_size):\n        super().__init__()\n        self.linri2h = torch.nn.Linear(input_size,hidden_size)\n        self.linrh2h = torch.nn.Linear(hidden_size,hidden_size) \n        self.tanh = torch.nn.Tanh()\n    def forward(self,x,h_t):\n        return self.tanh(self.linri2h(x) + self.linrh2h(h_t))\n\n\n#동일한 적합값이 나오도록 신경망을 학습시키기 위해서 RNNCell의 파라미터를 가져왔습니다.\ntorch.manual_seed(5)\nrnncell = torch.nn.RNNCell(7,4).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(4,7).to(\"cuda\")\nrnncell.weight_hh.data\n\ntensor([[-0.2361, -0.1094, -0.3339, -0.2364],\n        [-0.4558, -0.0116,  0.2965,  0.2432],\n        [ 0.4697, -0.4391, -0.0615,  0.4868],\n        [ 0.0819, -0.1410, -0.4503,  0.2327]], device='cuda:0')\n\n\n\ntorch.manual_seed(5)\nrnn_cell = Rnn_cell(7,4).to(\"cuda\")\n#파라미터 초기화(from torch.nn.RNNCell)\nrnn_cell.linri2h.weight.data = rnncell.weight_ih.data\nrnn_cell.linri2h.bias.data = rnncell.bias_ih.data\nrnn_cell.linrh2h.weight.data = rnncell.weight_hh.data\nrnn_cell.linrh2h.bias.data = rnncell.bias_hh.data\nlinr_h2o = torch.nn.Linear(4,7).to(\"cuda\")\n\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(rnn_cell.parameters())+list(linr_h2o.parameters()),lr=0.1)\n\n\nT = len(x)\nfor epoch in range(500):\n    h_t = torch.zeros((1,4)).to(\"cuda\")\n    loss = 0.0\n    for t in range(T):\n        x_t,y_t = x[[t]],y[[t]]\n        #1 hidden,out\n        h_t = rnn_cell(x_t,h_t) #rnncell => rnn_cell\n        o_t = linr_h2o(h_t)\n        #2 loss at timestep t\n        l_t = loss_fn(o_t,y_t)\n        loss += l_t\n    loss = loss/T\n    if epoch % 100 == 0:\n        print(loss)\n    #3 derivative\n    loss.backward()\n    #4 update and clean\n    optimizer.step()\n    optimizer.zero_grad()\n\ntensor(2.0413, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0055, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0025, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0014, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0010, device='cuda:0', grad_fn=<DivBackward0>)\n\n\n\nT = len(x)\nhidden = torch.zeros((T,4)).to(\"cuda\")\nhidden[[0]] = rnn_cell(x[[0]],torch.zeros((1,4)).to(\"cuda\")) #rnncell => rnn_cell\nfor t in range(1,T):\n    x_t = x[[t]]\n    #print(x_t.shape)\n    hidden[[t]] = rnn_cell(x_t,hidden[[t-1]]) #rnncell => rnn_cell\n\n\nyhat = soft(linr_h2o(hidden))\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x1a954939c70>,\n  <matplotlib.axis.XTick at 0x1a9549396d0>,\n  <matplotlib.axis.XTick at 0x1a95b902fd0>,\n  <matplotlib.axis.XTick at 0x1a95b937520>,\n  <matplotlib.axis.XTick at 0x1a95b937d00>,\n  <matplotlib.axis.XTick at 0x1a95b93e430>,\n  <matplotlib.axis.XTick at 0x1a95b93eb80>],\n [Text(0, 1, 'h'),\n  Text(1, 1, 'i'),\n  Text(2, 1, '?'),\n  Text(3, 1, 'e'),\n  Text(4, 1, 'l'),\n  Text(5, 1, 'o'),\n  Text(6, 1, '!')])\n\n\n\n\n\n\ntorch.nn.LSTM()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n\n\ntorch.manual_seed(5)\nlstm = torch.nn.LSTM(7,8).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(8,7).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstm.parameters()) + list(linr_h2o.parameters()),lr=0.1)\n\n\nh_0 = torch.zeros((1,8)).to(\"cuda\")\nc_0 = torch.zeros((1,8)).to(\"cuda\")\nfor epoch in range(500):\n    #1 hidden,output\n    hidden,(h_T,c_T)= lstm(x,(h_0,c_0))\n    out = linr_h2o(hidden)\n    #2 loss\n    loss = loss_fn(out,y)\n    if epoch % 100 == 0:\n      print(loss)\n    #3 derivative\n    loss.backward()\n    #4 update & clean\n    optimizer.step()\n    optimizer.zero_grad()\n\ntensor(1.8939, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0009, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0005, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0003, device='cuda:0', grad_fn=<DivBackward1>)\ntensor(0.0002, device='cuda:0', grad_fn=<DivBackward1>)\n\n\n\nh_0 = c_0 = torch.zeros((1,8)).to(\"cuda\")\nhidden,(_,_) = lstm(x,(h_0,c_0))\nout = linr_h2o(hidden)\nyhat = soft(out)\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x7f68d01dd510>,\n  <matplotlib.axis.XTick at 0x7f68d01dd3d0>,\n  <matplotlib.axis.XTick at 0x7f68d0198590>,\n  <matplotlib.axis.XTick at 0x7f68d0198f50>,\n  <matplotlib.axis.XTick at 0x7f68d01a84d0>,\n  <matplotlib.axis.XTick at 0x7f68d01a8a10>,\n  <matplotlib.axis.XTick at 0x7f68d01a88d0>],\n [Text(0, 1, 'h'),\n  Text(0, 1, 'i'),\n  Text(0, 1, '?'),\n  Text(0, 1, 'e'),\n  Text(0, 1, 'l'),\n  Text(0, 1, 'o'),\n  Text(0, 1, '!')])\n\n\n\n\n\n\ntorch.nn.LSTMCell()을 이용하여 다음문자를 예측하는 신경망을 설계하고 학습하라.\n\n\ntorch.manual_seed(5)\nlstmcell = torch.nn.LSTMCell(7,8).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(8,7).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstmcell.parameters()) + list(linr_h2o.parameters()),lr=0.1)\n\n\nT = len(x)\nfor epoch in range(500):\n  h_t = c_t = torch.zeros((1,8)).to(\"cuda\")\n  loss = 0.0\n  for t in range(T):\n    x_t,y_t = x[[t]],y[[t]]\n    #1 hidden,out\n    h_t,c_t = lstmcell(x_t,(h_t,c_t))\n    out = linr_h2o(h_t)\n    #2 loss\n    l_t = loss_fn(out,y_t)\n    loss += l_t\n  loss = loss/T\n  if epoch % 100 == 0:\n    print(loss)\n  #3derivative\n  loss.backward()\n  #4 update & clean\n  optimizer.step()\n  optimizer.zero_grad()\n\ntensor(1.8939, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0009, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)\n\n\n\nT = len(x)\nhidden = torch.zeros((T,8)).to(\"cuda\")\ncell = torch.zeros((T,8)).to(\"cuda\")\n\nh_0 = c_0 = torch.zeros((1,8)).to(\"cuda\") # t = 0일때의 h,c값 초기화\nhidden[[0]],cell[[0]] = lstmcell(x[[0]],(h_0,c_0)) # t = 1일때의 h_1,c_t먼저 기록\nfor t in range(1,T): #t = 2일때부터 기록\n  x_t = x[[t]]\n  hidden[[t]],cell[[t]] = lstmcell(x_t,(hidden[[t-1]],cell[[t-1]]))\n\n\nout = linr_h2o(hidden)\nyhat = soft(out)\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x7f68d010abd0>,\n  <matplotlib.axis.XTick at 0x7f68d010a710>,\n  <matplotlib.axis.XTick at 0x7f68d00bbc50>,\n  <matplotlib.axis.XTick at 0x7f68d00d5650>,\n  <matplotlib.axis.XTick at 0x7f68d00d5b90>,\n  <matplotlib.axis.XTick at 0x7f68d00d5750>,\n  <matplotlib.axis.XTick at 0x7f68d00cc690>],\n [Text(0, 1, 'h'),\n  Text(0, 1, 'i'),\n  Text(0, 1, '?'),\n  Text(0, 1, 'e'),\n  Text(0, 1, 'l'),\n  Text(0, 1, 'o'),\n  Text(0, 1, '!')])\n\n\n\n\n\n\n(5)의 결과와 동일한 적합값을 출력하는 신경망을 직접설계한 뒤 학습시켜라. (초기값을 적절하게 설정할 것)\n\nversion1 - class사용\n\nclass LSTM_cell(nn.Module):\n  def __init__(self,input_size,hidden_size):\n    super().__init__()\n    self.linri2h = torch.nn.Linear(input_size,hidden_size * 4)\n    self.linrh2h = torch.nn.Linear(hidden_size,hidden_size * 4)\n    self.tanh = torch.nn.Tanh()\n    self.sig = torch.nn.Sigmoid()\n    self.hidden_size = hidden_size\n  def forward(self,x,h_t,c_t):\n    ifgo = self.linri2h(x) + self.linrh2h(h_t)\n    i_t = self.sig(ifgo[:,0:self.hidden_size])\n    f_t = self.sig(ifgo[:,self.hidden_size:self.hidden_size*2])\n    g_t = self.tanh(ifgo[:,self.hidden_size*2:self.hidden_size*3])\n    o_t  = self.sig(ifgo[:,self.hidden_size*3:self.hidden_size*4])\n    #print(ifgo.shape,i_t.shape,f_t.shape,g_t.shape,o_t.shape)\n    c_t = f_t * c_t + i_t * g_t\n    h_t = o_t * self.tanh(c_t)\n    return h_t,c_t\n\n\n#동일한 적합값이 나오도록 신경망을 학습시키기 위해서  LSTMCell의 파라미터를 가져왔습니다.\ntorch.manual_seed(5)\nlstmcell = torch.nn.LSTMCell(7,8).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(8,7).to(\"cuda\")\n\n\ntorch.manual_seed(5)\nlstm_cell = LSTM_cell(7,8).to(\"cuda\")\n#파라미터 초기화(from torch.nn.LSTMCELL)\nlstm_cell.linri2h.weight.data = lstmcell.weight_ih.data\nlstm_cell.linri2h.bias.data = lstmcell.bias_ih.data\nlstm_cell.linrh2h.weight.data = lstmcell.weight_hh.data\nlstm_cell.linrh2h.bias.data = lstmcell.bias_hh.data\n\nlinr_h2o = torch.nn.Linear(8,7).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(lstm_cell.parameters()) + list(linr_h2o.parameters()),lr=0.1)\n\n\nT = len(x)\nfor epoch in range(500):\n    h_t = c_t = torch.zeros((1,8)).to(\"cuda\")\n    loss = 0\n    for t in range(T):\n        x_t,y_t = x[[t]],y[[t]]\n        #1\n        h_t,c_t = lstm_cell(x_t,h_t,c_t)\n        out = linr_h2o(h_t)\n        #2\n        l_t = loss_fn(out,y_t)\n        loss += l_t\n    loss = loss/T\n    if epoch % 100 == 0:\n        print(loss)\n    #3\n    loss.backward()\n    #4\n    optimizer.step()\n    optimizer.zero_grad()\n\ntensor(1.8939, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0009, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)\n\n\n\nT = len(x)\nhidden = torch.zeros((T,8)).to(\"cuda\")\ncell = torch.zeros((T,8)).to(\"cuda\")\n\nh_0 = c_0 = torch.zeros((1,8)).to(\"cuda\") # t = 0일때의 h,c값 초기화\nhidden[[0]],cell[[0]] = lstm_cell(x[[0]],h_0,c_0) # t = 1일때의 h_1,c_t먼저 기록\nfor t in range(1,T): #t = 2일때부터 기록\n  x_t = x[[t]]\n  hidden[[t]],cell[[t]] = lstm_cell(x_t,hidden[[t-1]],cell[[t-1]])\n\n\nout = linr_h2o(hidden)\nyhat = soft(out)\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x7f6948c6bc10>,\n  <matplotlib.axis.XTick at 0x7f6948c6b990>,\n  <matplotlib.axis.XTick at 0x7f687c01eb90>,\n  <matplotlib.axis.XTick at 0x7f686f8d8590>,\n  <matplotlib.axis.XTick at 0x7f686f8d8ad0>,\n  <matplotlib.axis.XTick at 0x7f686f8d8210>,\n  <matplotlib.axis.XTick at 0x7f686f8dc5d0>],\n [Text(0, 1, 'h'),\n  Text(0, 1, 'i'),\n  Text(0, 1, '?'),\n  Text(0, 1, 'e'),\n  Text(0, 1, 'l'),\n  Text(0, 1, 'o'),\n  Text(0, 1, '!')])\n\n\n\n\n\nversion2 - class안사용\n\ntorch.manual_seed(5) \nlstm_cell = torch.nn.LSTMCell(7,8).to(\"cuda\")\nlinr_h2o = torch.nn.Linear(8,7).to(\"cuda\")\nloss_fn = torch.nn.CrossEntropyLoss() \noptimizr = torch.optim.Adam(list(lstm_cell.parameters())+list(linr_h2o.parameters()),lr=0.1)\n\n\nT = len(x)\nfor epoch in range(500):\n    h_t = c_t = torch.zeros((1,8)).to(\"cuda\")\n    loss = 0 \n    ## 1~2\n    for t in range(T):\n        x_t,y_t = x[[t]].to(\"cuda\"), y[[t]].to(\"cuda\")\n        \n        ## lstm_cell step1: calculate _ifgo \n        ifgo = x_t @ lstm_cell.weight_ih.T + h_t @ lstm_cell.weight_hh.T + lstm_cell.bias_ih + lstm_cell.bias_hh\n        ## lstm_cell step2: decompose _ifgo \n        i_t = sig(ifgo[:,0:8])\n        f_t = sig(ifgo[:,8:16])\n        g_t = tanh(ifgo[:,16:24])\n        o_t = sig(ifgo[:,24:32])\n        ## lstm_cell step3: calculate ht,ct \n        c_t = f_t * c_t + i_t * g_t\n        h_t = o_t * tanh(c_t)\n        \n        out = linr_h2o(h_t) \n        #print(o_t.shape,y_t.shape)\n        loss = loss + loss_fn(out,y_t)\n    loss = loss / T\n    if epoch % 100 == 0:\n        print(loss)\n    ## 3 \n    loss.backward()\n    ## 4 \n    optimizr.step()\n    optimizr.zero_grad()\n\ntensor(1.8939, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0009, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0005, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0003, device='cuda:0', grad_fn=<DivBackward0>)\ntensor(0.0002, device='cuda:0', grad_fn=<DivBackward0>)\n\n\n\nT = len(x)\nhidden = torch.zeros((T,8)).to(\"cuda\")\ncell = torch.zeros((T,8)).to(\"cuda\")\n\nh_0 = c_0 = torch.zeros((1,8)).to(\"cuda\") # t = 0일때의 h,c값 초기화\nhidden[[0]],cell[[0]] = lstm_cell(x[[0]],(h_0,c_0)) # t = 1일때의 h_1,c_t먼저 기록\n\nfor t in range(1,T): #t = 2일때부터 기록\n  x_t = x[[t]]\n  hidden[[t]],cell[[t]] = lstm_cell(x_t,(hidden[[t-1]],cell[[t-1]]))\n\n\nout = linr_h2o(hidden)\nyhat = soft(out)\nplt.matshow(yhat.to(\"cpu\").data[-40:],cmap=\"bwr\")\nplt.xticks(range(7),labels=['h','i','?','e','l','o','!'])\n\n([<matplotlib.axis.XTick at 0x7f68d005dfd0>,\n  <matplotlib.axis.XTick at 0x7f68d005df90>,\n  <matplotlib.axis.XTick at 0x7f686f82bc10>,\n  <matplotlib.axis.XTick at 0x7f686f830610>,\n  <matplotlib.axis.XTick at 0x7f686f830b50>,\n  <matplotlib.axis.XTick at 0x7f686f830d90>,\n  <matplotlib.axis.XTick at 0x7f686f854650>],\n [Text(0, 1, 'h'),\n  Text(0, 1, 'i'),\n  Text(0, 1, '?'),\n  Text(0, 1, 'e'),\n  Text(0, 1, 'l'),\n  Text(0, 1, 'o'),\n  Text(0, 1, '!')])\n\n\n\n\n\n\n\n2. 다음을 읽고 참 거짓을 판단하여라. (10점)\n\nLSTM은 RNN보다 장기기억에 유리하다. O \ntorch.nn.Embedding(num_embeddings=2,embedding_dim=1)와 torch.nn.Linear(in_features=1,out_features=1)의 학습가능한 파라메터수는 같다. O \n아래와 같은 네트워크를 고려하자.차원이 (n,1) 인 임의의 텐서에 대하여 net(x)와 net.forword(x)의 출력결과는 같다. O\n아래와 같이 a,b,c,d 가 반복되는 문자열이 반복되는 자료에서 다음문자열을 맞추는 과업을 수행하기 위해서는 반드시 순환신경망의 형태로 설계해야만 한다. O (5)RNN 혹은 LSTM 으로 신경망을 설계할 시 손실함수는 항상 torch.nn.CrossEntropyLoss 를 사용해야 한다. X"
  },
  {
    "objectID": "posts/post-with-code/index.html",
    "href": "posts/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code."
  },
  {
    "objectID": "posts/welcome/index.html",
    "href": "posts/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts."
  },
  {
    "objectID": "posts/딥러닝(1) - 선형회귀/Linear Regression.html",
    "href": "posts/딥러닝(1) - 선형회귀/Linear Regression.html",
    "title": "딥러닝(1) - 선형회귀",
    "section": "",
    "text": "선형회귀에 대해서 정리한 글입니다."
  },
  {
    "objectID": "posts/딥러닝(1) - 선형회귀/Linear Regression.html#선형회귀의-loss-function",
    "href": "posts/딥러닝(1) - 선형회귀/Linear Regression.html#선형회귀의-loss-function",
    "title": "딥러닝(1) - 선형회귀",
    "section": "선형회귀의 Loss function",
    "text": "선형회귀의 Loss function\n2번에서 구한 추정값 \\(\\hat{\\bf{W}}\\)이 얼마나 틀린지,부정확한지 알려주는 함수를 Loss function 또는 Cost function이라고 합니다. 선형회귀에서의 Loss function은 MSE를 사용하며 같습니다.\n(Loss function) \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2} = ({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}}) = (\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\)\n위 함수는 추정값 \\(\\hat{\\bf{W}}\\)이 얼마나 틀렸는지를 나타내는 \\(\\hat{\\bf{W}}\\)에 대한 함수입니다. 우리는 위 수식의 값을 가장 작게만들도록 \\(\\bf{\\hat{W}}\\)를 찾아내어 우리가 모르고있는 확률변수사이의 관계를 알아내야합니다."
  },
  {
    "objectID": "posts/딥러닝(1) - 선형회귀/Linear Regression.html#parameter-update",
    "href": "posts/딥러닝(1) - 선형회귀/Linear Regression.html#parameter-update",
    "title": "딥러닝(1) - 선형회귀",
    "section": "Parameter update",
    "text": "Parameter update\nn개의 독립변수를 가지는 다변수 스칼라 함수에 대한 Gradient는 수학적으로 다음과 같습니다.\n\\(\\nabla f(x_1,x_2,...,x_n) = (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\dots,\\frac{\\partial f}{\\partial x_n})\\) 출력값이 스칼라인 함수의 그레디언트는 벡터입니다.그러므로,그레디언트를 벡터를 입력했을 때,벡터를 출력으로 하는 벡터함수라고 생각해도 무방합니다.중요한 사실은 임의의 지점\\(X\\)에서 그레디언트의 방향은 스칼라함수가 가장 급격하게 증가하는 방향이라는 사실입니다. 비슷하게 임의의 지점\\(x\\)에서 -그레디언트의 방향은 스칼라함수가 가장 급격하게 감소하는 방향입니다.(증명생략)\n이러한 사실을 활용하여 우리는 임의의 \\(\\hat{\\bf{W}}\\)에서 Loss function이 가장 급격하게 감소하는 방향을 찾을 수 있습니다. 또한 궁극적인 목적인 틀린정도를 최소화하는 즉,Loss function값이 가장 작은 \\(\\hat{\\bf{W}}\\)를 찾을 수 있습니다."
  }
]