[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "전북대학교 IT응용시스템 공학과 신호연 sinhoyeon0514@gmail.com"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "HIHO",
    "section": "",
    "text": "[Probability & Statistics] Bias Variance Tradeoff\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] Appendix\n\n\n\n\n\n\n\n\n\n \n\n\n\nimplementation\n\n\n\n\n\n\n\n\n\n \n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\n \n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\n \n\n\n\nNamespace\n\n\n\n\n\n\n\n\n\n\n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\n \n\n\n\nBellman Equation\n\n\n\n\n\n\n\n\n\n \n\n\n\nUntitled\n\n\n\n\n\n\n\n\n\n \n\n\n\n\n\n\n\n\n\n\n\n\n[Paper Study] AutoAugment : Learning Augmentation Strategies from Data\n\n\nfield : CV,RLunderstanding : 😃😃\n\n\n\n\n\n\n\n\n\n \n\n\n\n[Paper Study] Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms\n\n\nfield : RLunderstanding : 😃😃\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Algorithm & Datastructure]1-2. Naming, Styling and Comments\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Algorithm & Datastructure]1-2.Procedure-oriented program vs Object-oriented program\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Algorithm & Datastructure]1-1.Programming and Execution Environment\n\n\n\n\n\n\n\n\n\n \n\n\n\n[주저리주저리] 시험보기 1주일 전 꼭 읽어야 하는 것\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Stochestic Process] 5.sigma field(2)\n\n\n\n\n\n\n\n\n\n \n\n\n\n[Stochestic Process] 4.sigma field(1)\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Stochestic Process] 3.무한집합간의 크기 비교\n\n\n\n\n\n\n\n\n\n \n\n\n\n[Stochestic Process] 부록 - 자꾸 걸리는 집합개념 정리\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Coading Test] 문자열 나누기\n\n\nlevel : 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Stochestic Process] 2.Cardinality\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Coading Test] k번째수\n\n\nlevel : 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Paper Study] Neural Machine Translation by jointly learning to align and translate\n\n\nfield : NLPunderstanding : 😃😃😃\n\n\n\n\n\n\n\n\n\n \n\n\n\n[강화학습] Simple Bandit Algorithm Implementation\n\n\nReinforcement Learning: An Introduction 2.4절 중 Simple Bandit Algorithm psuedo code 구현\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Paper-study] Batch Normalization\n\n\nfield : coreunderstanding : 😃😃😃\n\n\n\n\n\n\n\n\n\n\n\n\n\n[pandas] map,apply,applymap 비교\n\n\n\n\n\n\n\n\n\n\n\n\n\nSequence to Sequence Learning with Neural Networks\n\n\n\n\n\n\n\n\n\n\n\n\n\nAuto-Encoding Variational Bayes(작성중)\n\n\n\n\n\n\n\n\n\n \n\n\n\n[강화학습] Introduction\n\n\n\n\n\n\n\n\n\n\n\n\n\nGenarative Adversarial Nets\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 6-2.EVD & Property\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 6-4.Quadratic Form\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 6-3.EVD of symmetric matrix\n\n\n\n\n\n\n\n\n\n\n\n\n\nDQN\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 6-1.Eigenvalue & Eigenvector\n\n\n\n\n\n\n\n\n\n\n\n\n\n[강화학습] 2-1 Markov Decision process and property\n\n\nMDP와 중요한 property 정리\n\n\n\n\n\n\n\n\n\n \n\n\n\n[강화학습] 1 - 강화학습 용어정리\n\n\n강화학습 공부하면서 용어정리\n\n\n\n\n\n\n\n\n\n\n\n\n\n[강화학습] 2-2 Reward & Return & State Value f & Action Value f\n\n\n\n\n\n\n\n\n\n\n\n\n\n[PRML 읽기] 1 - 확률론 개요\n\n\nSum rule, Product rule, Baye’s rule,random variable independence\n\n\n\n\n\n\n\n\n\n \n\n\n\n[PRML 읽기] 2 - 확률론 개요(작성중 …)\n\n\nprobability density, expectation and covariance, Bayesian probabilities,Gausian distribution\n\n\n\n\n\n\n\n\n\n\n\n\n\nDimension Reduction using Auto Encoder with pytorch\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Coading Test] 시저암호\n\n\nlevel : 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Coading Test] 이상한 문자 만들기\n\n\nlevel : 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Coading Test] 크기가 작은 부분 문자열\n\n\nlevel : 1\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 5.Least Squares\n\n\n\n\n\n\n\n\n\n \n\n\n\n확률론 용어정리\n\n\nrandom variable,event,sample space,probability distribution,randomsample,realization\n\n\n\n\n\n\n\n\n\n\n\n\n\nFinite Difference Method with np.gradient\n\n\n\n\n\n\n\n\n\n\n\n\n\nplotly 시각화 모음\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 4.Ax=b의 해의 갯수\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 3.rank & null space(kernel)\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 2.Linear Combination & Span\n\n\n\n\n\n\n\n\n\n\n\n\n\n[Linear Algebra] 1.행렬곱에 대한 여러가지 관점\n\n\n\n\n\n\n\n\n\n \n\n\n\nMultinomial Logistic Regression & Softmax Regression\n\n\n\n\n\n\n\n\n\n \n\n\n\n카테고리 분포\n\n\n\n\n\n\n\n\n\n\n\n\n\nMLE & MAP (작성중)\n\n\n\n\n\n\n\n\n\n \n\n\n\n확률분포에서 ;와|의 사용\n\n\n\n\n\n\n\n\n\n\n\n\n\nLogistic Regression\n\n\n\n\n\n\n\n\n\n\n\n\n\n파이썬 - 변수,할당문,인터닝\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Regression\n\n\n\n\n\n\n\n\n\n \n\n\n\npytorch로 Rnn구현하기\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/coading test/programmersk번째수.html",
    "href": "posts/coading test/programmersk번째수.html",
    "title": "[Coading Test] k번째수",
    "section": "",
    "text": "문제\n\n\n\n나의 풀이\n\ndef drop(array,value):\n    for idx in range(len(array)):\n        if array[idx] == value:\n            i = idx\n            break\n    dropped_array = array[:i] + array[i+1:]\n    return dropped_array\n    \ndef sort(sub_list):\n    sorted_l = []\n    while len(sub_list)>0:\n        drop_element = min(sub_list)\n        sorted_l.append(drop_element)\n        sub_list = drop(sub_list,drop_element)\n    return sorted_l\n\ndef solution(array, commands):\n    answer = []\n    for el in range(len(commands)):\n        command = commands[el]\n        i = command[0];j=command[1];k=command[2]\n        sub_list = array[i-1:j]\n        sorted_list = sort(sub_list)\n        answer.append(sorted_list[k-1])\n    return answer\n\n\n\n고찰\n\n오랜만에 해서 그런지 sorted함수를 구현하는 것을 까먹어 오래걸렸다.\n구현된 내장함수 중 자주 나오는 것은 외워야 한다."
  },
  {
    "objectID": "posts/coading test/programmers부분문자열.html",
    "href": "posts/coading test/programmers부분문자열.html",
    "title": "[Coading Test] 문자열 나누기",
    "section": "",
    "text": "문제\n\n\n\n나의 풀이\n\ndef solution(s):\n    cnt = [0,0]\n    sep_num = 0\n    for idx in range(len(s)):\n        if cnt == [0,0]:\n            x = s[idx]\n            print(\"첫번째 문자:\",x)\n        if s[idx] == x:\n            cnt[0] += 1\n        else:\n            cnt[1] += 1\n        if cnt[0] == cnt[1]:\n            sep_num += 1\n            cnt = [0,0]\n        if idx == len(s) -1 and cnt[0] != cnt[1]:\n            sep_num +=1\n    return sep_num\n\n\n\n고찰\n\n문제에서 “문자열을 분리합니다”라고 나와 있어서 처음에 문자열 분리하는 코드도 적다가 시간초과오류가 났음 .. ㅜㅜ\n문제설명은 이해를 돕기위한 설명일 뿐, 모든 과정을 일일히,그대로 따라할 필요는 없으며 무슨방법이던지 써서 입출력예시를 잘 맞히는데 초점을 두자."
  },
  {
    "objectID": "posts/coading test/programmers시저암호.html",
    "href": "posts/coading test/programmers시저암호.html",
    "title": "[Coading Test] 시저암호",
    "section": "",
    "text": "문제\n\n\n\n나의 풀이\n\ndef solution(s, n):\n    upper_ch = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n    answer = \"\"\n    for el_s in s:\n        #1.맨 마지막 리턴을 위해 원래 문자가 소문자인지 대문자인지 기억 and 대문자에서 검색할것이기 때문에 대문자로 변환\n        #대문자로 변환된 문자열 s의 각각의 문자,대소문자 여부\n        if el_s == \" \":\n            answer += \" \"\n        elif el_s.isupper() == True:\n            was_upper = True\n        else:\n            was_upper = False\n            el_s = el_s.upper()\n        #2.인덱스 숫자가 upper_ch에서 벗어날때 아닐때 처리\n        for idx,up_ch in enumerate(upper_ch):\n            if el_s == up_ch and idx + n <= len(upper_ch)-1:\n                find_idx = idx+n\n                if was_upper == True:\n                    answer += upper_ch[find_idx]\n                else:\n                    answer += upper_ch[find_idx].lower()        \n            elif el_s == up_ch and idx + n > len(upper_ch)-1:\n                find_idx = n-len(upper_ch[idx:])\n                if was_upper == True:\n                    answer += upper_ch[find_idx]\n                else:\n                    answer += upper_ch[find_idx].lower()  \n    return answer\n\n\n\n다른 풀이\n\ndef caesar(s, n):\n    lower_list = \"abcdefghijklmnopqrstuvwxyz\"  \n    #소문자도 리스트로 만듦 \n    #좋은점->대소문자 여부를 기억하는 코드 불필요(조건문)\n    #안좋은점->소문자로 이뤄진 리스트를 만드는데 그만큼의 메모리 필요\n    upper_list = \"ABCDEFGHIJKLMNOPQRSTUVWXYZ\" \n    \n    \n    \n    result = [] \n    #문자열들을 저장할 list를 만듦\n    #mutable로 붙이는 것과 immutable로 붙이는 것 차이\n    #속도 -> \n    #mutable이 더 빠름,\n    #그러나 garbage collector도 고려시 스캔범위가 너무커서 느려질수도?\n    #메모리 -> immutable이 더 적게들을 듯(리스트는 이중포인터같은 구조라서 이렇게 예상됨)\n    for i in s:\n        if i == \" \":\n            result.append(\" \")\n        elif i.islower() is True:\n            new_ = lower_list.find(i) + n\n            result.append(lower_list[new_ % 26]) \n            #나머지로 계산하는 방식,이게 더 간단하고 좋은듯\n            #반복문이 문자열에 대해서 돌다보니 문자열과 문자열의 인덱스 위주로 너무 생각함\n            #어떤 숫자(여기서는 인덱스)보다 크거나 같을때에 다시 0부터 줘야하는 상황? -> 나머지 활용\n        else:\n            new_ = upper_list.find(i) + n\n            result.append(upper_list[new_ % 26])\n    return \"\".join(result)\n\n나중에 볼 링크 링크1 링크2 링크3"
  },
  {
    "objectID": "posts/coading test/programmers이상한문자만들기.html",
    "href": "posts/coading test/programmers이상한문자만들기.html",
    "title": "[Coading Test] 이상한 문자 만들기",
    "section": "",
    "text": "문제\n\n\n\n나의 풀이\n\ndef solution(s):\n    words = s.split(\" \") #스플릿 함수 헷갈리는 부분이 있었음 - 정리\n    answer = \"\"\n    for wd in words:\n        for idx,chr in enumerate(wd):\n            print(idx,chr)\n            if idx % 2 == 0:\n                chr = chr.upper()\n            else:\n                chr = chr.lower()\n            answer += chr\n        answer+= \" \"\n    return answer[:-1]\n\n\n\nsplit 메서드(함수)\n\nt=\"sdsa  sd\"\nhelp(t.split)\n\nHelp on built-in function split:\n\nsplit(sep=None, maxsplit=-1) method of builtins.str instance\n    Return a list of the words in the string, using sep as the delimiter string.\n    \n    sep\n      The delimiter according which to split the string.\n      None (the default value) means split according to any whitespace,\n      and discard empty strings from the result.\n    maxsplit\n      Maximum number of splits to do.\n      -1 (the default value) means no limit.\n\n\n\nstring객체의 인스턴스 즉,문자열에 대해서만 사용할 수 있는 메서드 sep파라미터에 전달한 인수를 구분자로 사용하여 문자열안에 있는 단어들을 가져옴. 가져온 단어들은 리스트의 원소가 되어 반환됨 Parameter - sep - …\nReturn - list[word1,word2,…] (구분자를 통하여 구분된 단어들,기본값은 공백(space))\n\n\n예시\n\n#문자열 내에 구분기준이 없는 경우?\n#sep = \" \"으로 기본값으로 설정되어 있음. 공백이 없으므로 그냥 문자열을 리스트에 넣어서 반환\n_t = \"abcde\"\nanswer = _t.split()\nprint(answer)\n\n['abcde']\n\n\n\n#문자열 내에 구분기준이 있는 경우?\n_t = \"ab cd esda dg\"\nanswer = _t.split()\nprint(answer)\n\n['ab', 'cd', 'esda', 'dg']\n\n\n\n_t = \"abtcdtesdatdg\"\nanswer = _t.split(\"t\")\nprint(answer)\n\n['ab', 'cd', 'esda', 'dg']\n\n\n\n_t = \"ab;;cd;;esd;;atdg\"\nanswer = _t.split(\";;\")\nprint(answer)\n\n['ab', 'cd', 'esd', 'atdg']"
  },
  {
    "objectID": "posts/coading test/programmers크기가작은부분문자열.html",
    "href": "posts/coading test/programmers크기가작은부분문자열.html",
    "title": "[Coading Test] 크기가 작은 부분 문자열",
    "section": "",
    "text": "문제\n\n\n\n나의 풀이\n\ndef solution(t, p):\n    t_len = len(t);p_len = len(p)\n    answer = 0\n    p = int(p)\n    for idx in range(t_len-p_len+1):\n        num = int(t[idx:idx+p_len])\n        if num <= p:\n            answer+=1\n        else:\n            pass\n    return answer"
  },
  {
    "objectID": "posts/DL/Auto Encoder Dimension Reduction.html",
    "href": "posts/DL/Auto Encoder Dimension Reduction.html",
    "title": "Dimension Reduction using Auto Encoder with pytorch",
    "section": "",
    "text": "import torch.nn as nn\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport random\nimport os\nimport numpy as np\nimport torch\ntest_path = \"./test.csv\"\ntrain_path = \"./train.csv\"\n\n\n# %pip install plotly (jupyter notebook)\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\n#pio.renderers.default = 'iframe_connected'\n#pio.renderers.default = \"vscode\"\npio.renderers.default = \"plotly_mimetype+notebook\""
  },
  {
    "objectID": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension3",
    "href": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension3",
    "title": "Dimension Reduction using Auto Encoder with pytorch",
    "section": "encoding dimension=3",
    "text": "encoding dimension=3\n\ntraining autoencoder\n\ntorch.manual_seed(201711375)\nautoencoder_3 = AutoEncoder(47,3)\nloss_fn = torch.nn.MSELoss()\nrelu = torch.nn.LeakyReLU()\noptimizer = torch.optim.Adam(autoencoder_3.parameters(),lr=0.001)\n\n\nfor epoch in range(20000):\n    #1.yhat\n    out = autoencoder_3(X_train_ohe)\n    #2\n    loss = loss_fn(out,X_train_ohe)\n    #3\n    loss.backward()\n    if epoch % 10000 == 0:\n        print(f\"epoch:{epoch} loss:{loss.tolist()}\")\n    #4\n    optimizer.step()\n    optimizer.zero_grad()\n\nepoch:0 loss:0.5274774432182312\nepoch:10000 loss:0.11526338756084442\n\n\n\n\nvisualization\n\nclass_map_inv = {}\nfor key,value in class_map.items():\n    class_map_inv[value] = key\nclass_map_inv\n\n{0: 'A', 1: 'B', 2: 'C'}\n\n\n\ndt_dim3 = pd.DataFrame({\"class\":Y_train_ohe})\ndt_dim3 = pd.concat([pd.DataFrame(np.array(autoencoder_3.encoder(X_train_ohe).tolist())),dt_dim3],axis=1)\ndt_dim3 = dt_dim3.rename(columns = {0:\"x\",1:\"y\",2:\"z\"})\n\n\ncount = 0\ndata = []\nfor cl in dt_dim3[\"class\"].unique():\n    cond = dt_dim3[\"class\"] == cl\n    _data = dt_dim3.loc[cond,:]\n    x = _data.x.tolist()\n    y = _data.y.tolist()\n    z = _data.z.tolist()\n    if count == 0:\n        color = \"red\"\n    elif count == 1:\n        color = \"blue\"\n    else:\n        color = \"black\"\n    trace=go.Scatter3d(\n        x=x,\n        y=y,\n        z=z,\n        mode=\"markers\",\n        marker = dict(color = color,size=2),\n        name = str(class_map_inv[cl])\n        )\n    data.append(trace)\n    count+=1\n\nlayout = go.Layout(title=dict(text = \"3-dimension \"))\n\n#4. figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()"
  },
  {
    "objectID": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension10",
    "href": "posts/DL/Auto Encoder Dimension Reduction.html#encoding-dimension10",
    "title": "Dimension Reduction using Auto Encoder with pytorch",
    "section": "encoding dimension=10",
    "text": "encoding dimension=10\n\ntorch.manual_seed(201711375)\nautoencoder_3 = AutoEncoder(47,10)\nloss_fn = torch.nn.MSELoss()\nrelu = torch.nn.LeakyReLU()\noptimizer = torch.optim.Adam(autoencoder_3.parameters(),lr=0.001)\n\n\nfor epoch in range(40000):\n    #1.yhat\n    out = autoencoder_3(X_train_ohe)\n    #2\n    loss = loss_fn(out,X_train_ohe)\n    #3\n    loss.backward()\n    if epoch % 10000 == 0:\n        print(f\"epoch:{epoch} loss:{loss.tolist()}\")\n    #4\n    optimizer.step()\n    optimizer.zero_grad()\n\nepoch:0 loss:0.3828417658805847\nepoch:10000 loss:0.060432590544223785\nepoch:20000 loss:0.06043253839015961\nepoch:30000 loss:0.060432031750679016"
  },
  {
    "objectID": "posts/DL/Bias Variance tradeoff/bias variance trade off.html",
    "href": "posts/DL/Bias Variance tradeoff/bias variance trade off.html",
    "title": "[Probability & Statistics] Bias Variance Tradeoff",
    "section": "",
    "text": "supervised learning의 목적 중 하나는 unknown data에 대해서 generalization을 잘 하도록 모델을 학습시키는 것입니다.\n이는 expected generalization error에 포함하는 bias와 variance를 줄이도록 학습하는 것과 같습니다..\n그러나 학습과정에서 bias와 variance는 서로간에 상충합니다.\n풀어쓰자면 bias가 작아지면 variance는 커지고 variance가 작아지면 bias가 커지는 bias variance tradeoff 현상이 나타납니다.\nbias와 variance를 조금 구체적으로 살펴보면 다음과 같습니다.\n\nbias\n\nbias는 학습알고리즘에서의 잘못된 가정으로 인해 발생하는 오류입니다.\nHigh bias는 학습알고리즘이 feature와 target간의 regularities 를 놓치게 합니다.(underfitting의 원인)\n이는 unknown data에 대해서 generalization error가 나오게 합니다.\n\nvairance\n\nvariance는 훈련데이터에서의 작은 변동에 학습알고리즘이 민감하게 반응하여 발생하는 오류입니다.\nHigh variance는 학습알고리즘이 feature와 target간의 regularities 를 너무 과하게 학습해서 발생합니다.(overfitting의 결과)\n마찬가지로 이는 unknown data에 대해서 generalization error가 나오게 합니다.\n\n\n1번그림\n\nbias가 작아서 알고리즘이 어떤 방향으로 편향되지 않습니다.\nvariance또한 작아서 데이터의 변화에 의한 학습된 알고리즘의 변동,편차는 거의 없습니다.\n전체적으로 bias,variance가 작기 때문에 학습된 알고리즘은 True값에 가깝습니다.\n\n2번그림\n\nbias가 크기 때문에 오른쪽 방향으로 편향되어 있습니다.\nvariance는 작아서 데이터의 변화에 의한 학습된 알고리즘의 변동,편차는 거의 없습니다.\n\n3번그림\n\nbias가 작아서 어떤 방향으로 편향되지 않습니다.\nvariance는 크기때문에 작은 데이터의 변화에 의한 학습된 알고리즘의 성능변화가 매우 심합니다.\n\n\n\n\n\n\n\n위키피디아의 notation과 강의자료 중 notation이 다르고 이외에도 다른부분이 많이 헷갈리기에 notation을 먼저 정리합니다.\n\\(F^*(x) = f(x) = f,\\hat{F}(x) = \\hat{f}(x;D) = \\hat{f}\\)\n\\(\\mathbb{E}_D = \\mathbb{E}\\)\n\n\n\n\n\n통계학에서 데이터는 다음과 같은 additive error model로부터 sampling됩니다.\n\n\\[\\begin{aligned}\n&y = f(x) + \\epsilon ,\\,\\, \\epsilon \\overset{\\text{i.i.d}}{\\sim} \\mathcal{N}(0,\\sigma^2)\\\\\n&\\text{where} \\\\\n&f(x) : \\text{target function that we are trying to learn,but do not really know} \\\\\n&\\epsilon : \\text{statistical error}\n\\end{aligned}\\]\n\n\\(f(x)\\)는 모집단에서 두 변수 \\(x,y\\)사이의 관계를 대표하는 곡선이며 우리가 학습 알고리즘을 통해 찾고자 하는 target function입니다.\ntarget function을 찾게되면 우리는 unknown data에 대해서도 좋은 결과를 얻을 수 있습니다.(good generalization)\n\n\n출처 : Pilsung Kang 교수님 깃허브\n\n모집단으로부터 sampling되는 data set에 의해서 같은 알고리즘이라도 다르게 학습이 됩니다.\n만약 서로다른 data set가 N개가 존재한다면 true function인 \\(f(x)\\)에 대해 서로다른 N개의 근사값인 \\(\\hat{f}_1(x),\\hat{f}_2(x),\\dots,\\hat{f}_n(x)\\)를 구합니다.\n개인적으로 각각의 datapoint는 \\(p(x,y)\\)를 따르므로 dataset \\(D\\)도 확률변수이며 이로부터 random한 \\(\\hat{f}(x)\\)도 확률변수라고 생각합니다.\n\n\n\n\n\n\nbias-variance decomposition은 bias,variance,irreducible error로 구성되는 학습알고리즘의 expected generalization error를 살펴봄으로서 학습알고리즘을 분석하는 방법입니다."
  },
  {
    "objectID": "posts/DL/Linear Regression.html",
    "href": "posts/DL/Linear Regression.html",
    "title": "Linear Regression",
    "section": "",
    "text": "선형회귀에 대해서 정리한 글입니다."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#assumption-modeling",
    "href": "posts/DL/Linear Regression.html#assumption-modeling",
    "title": "Linear Regression",
    "section": " Assumption & modeling",
    "text": "Assumption & modeling\n\n\nText(0.5, 1.0, 'n=200')\n\n\n\n\n\n우리가 가진 데이터를 관찰해봅시다. 가장크게 눈에 띄는 사실은 x와 y사이의 관계가 선형적이라는 점입니다. 따라서 간단한 선형모형으로 독립변수와 종속변수사이의 관계를 모델링할 수 있을 것 같습니다. 조금 더 세부적으로 들어가서 각각의 x값에 대해서 y값을 관찰해 봅시다. 첫번째로 알 수 있는 점은 동일한 x값에 대해서도 서로다른 y값을 가지는 점들 데이터가 많이 있다는 것입니다. 이로부터 \\(y\\)가 \\(x\\)뿐만아니라 또다른 확률변수 \\(\\epsilon\\)의 값에 의해 결정된다는 것을 알 수 있습니다. 두번째로 x값에 의해서 찍히는 점의 위치(y)의 양상이 다르다는 점을 알 수 있습니다. x가 작으면 일반적으로 y는 낮은위치에서 점이 찍히고 x가 크면클수록 일반적으로 높은 위치에서 점이 찍히는 것을 알 수 있습니다.\n데이터를 관찰하면서 얻은 사실로부터 \\(x\\)와 \\(y\\)사이의 관계를 수학적으로 모델링 해보겠습니다. 첫번째 사실로부터 우리는 동일한 \\(x\\)라도 각각의 데이터에 대해서 어떤 또다른 값이 더해짐을 알 수 있습니다. 이렇게 뽑힐때마다 그 값이 다른 변수는 확률변수 \\(\\epsilon_i\\)를 더해줌으로서 표현할 수 있습니다. 여기서 오차가 따르는 분포에 대해서 가정을 합니다. 오차는 평균이 0이고 분산이 \\(\\sigma^2\\)인 정규분포를 따르는 확률변수라고 가정합니다.\n또한 두번째 사실로부터 우리는 \\(x\\)와 \\(y\\)는 커지면 커지고 작아지면 작아지는 관계임을 알 수 있습니다. 이러한 관계를 표현할 수 있는 방법은 여러가지가 있지만 선형회귀에서는 선형으로 이 관계를 표현합니다.\n\\[\\begin{aligned}\n&Y_i = w_0 + w_1x_1 + \\dots + w_Mx_M + \\epsilon_i={\\bf{x_i}^\\intercal}w + \\epsilon_i\\\\\n&\\text{where , }{\\bf{{\\bf{x_i}^\\intercal}}} = \\begin {bmatrix}0,x_{i,1},\\dots,x_{i,M} \\end {bmatrix} \\in \\mathbb{R^{1 \\times (M+1)},{\\bf{w}} = \\begin {bmatrix} w_0,w_1,\\dots,w_M\\end{bmatrix}}^\\intercal \\in \\mathbb{R^{(M+1)\\times 1}}\\\\\n\\quad\\quad\\quad &\\epsilon_i \\overset{i.i.d}{\\sim} \\mathcal{N}(0,\\sigma^2)\\,\\,(\\text{for } i=1,2,\\dots,N)\n\\end{aligned}\\]\n이렇게 \\({\\bf{x_i}^\\intercal}\\)와 \\(Y_i\\)사이의 관계를 정할 경우 각각의 \\(Y_i\\)안에 확률변수 \\(\\epsilon_i\\)가 \\(Y_i\\)도 마찬가지로 정규분로를 따르는 확률변수로 그 값이 \\(\\epsilon\\)에 의해 독립변수의 값이 동일하더라도 추출할때마다 다를 수 있습니다. \\(Y_i\\)도 확률변수이므로 정규분포를 따르는 확률변수이므로 중심위치와 변동을 확인하기 위해 기댓값과 분산을 구해볼 수 있습니다. 이때 \\(x\\)는 주어져 있으므로 그때의 확률분포에 대해서 계산하면 다음과 같습니다.\n\\[\\begin{aligned}\n&\\mathbb{E}[Y_i] = \\mathbb{E}[w_0 + \\dots + w_Mx_M+\\epsilon_i] = w_0 + w_1x_1 + \\dots + w_Mx_M={\\bf{x_i}^\\intercal}w \\\\\n&\\text{var}[Y_i] = \\text{var}[w_0 + \\dots + w_Mx_M+\\epsilon_i] = \\text{var}[\\epsilon_i] = \\sigma^2\n\\end{aligned}\\]\n위수식으로부터 각각의 \\(Y_i\\)에 대한 확률분포는 정규분포이며(\\(\\epsilon_i\\)에 의해) 기댓값은 \\(x\\)와 \\(w\\)에 의해 정해지지만 분산은 \\(\\sigma^2\\)으로 항상 동일하다는 점을 알 수 있습니다. 모두 정규분포를 따르지만 기댓값만 \\(x\\)에 의하여 달라집니다. 즉 다음과 같습니다.\n\\[\\begin{aligned}\n&Y_i|w;{\\bf{x_i}^\\intercal} \\sim \\mathcal{N}({\\bf{x_i}^\\intercal}{\\bf{w}},\\sigma^2)\\, \\text{ for } i = 1,2,\\dots,N\\\\\n&p(y_i|{\\bf{w}};{\\bf{x_i}^\\intercal}) = 정규분포식\n\\end{aligned}\\]\n이미지\n여기까지 우리가 가진 데이터를 기반으로 독립변수와 종속변사이의 관계를 선형모형을 만들어봤습니다. 그렇다면 궁극적인 목적인 unseen data에 적절하게 종속변수의 값을 예측하려면 어떻게 해야할까요?만약 학습데이터로부터 적절히 가중치인 \\(w\\)를 구할수만 있다면 독립변수가 입력되었때 대하여 종속변수가 따르는 확률분포(정규분포)를 알 수 있고 그 분포에서 가장 확률이 높은 곳의 종속변수의 값(정규분포에서는 xw)을 예측값으로 하면 됩니다. 또 다르게 생각하면 가중치를 구한다는 것은 데이터를 가장 잘 표현하는 직선(평면,초평면)을 얻은것이므로 입력x에 대하여 직선의 값을 읽어서 예측값으로 하면 됩니다. 어찌됐건 두 경우 모두 가중치를 구해야**하므로 우리의 목적은 이제 가중치를 구하는 것입니다."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#point-estimation---mle",
    "href": "posts/DL/Linear Regression.html#point-estimation---mle",
    "title": "Linear Regression",
    "section": " Point Estimation - MLE",
    "text": "Point Estimation - MLE\n그렇다면 가장 적절한 가중치는 무엇일까요? 데이터가 주어질때 가중치에 확률이 다음과 같다고 해봅시다.\n\\[\\begin{aligned}\n&p({\\bf{w}}|D) = p_{{\\bf{w}}|Y_1,Y_2,\\dots,Y_N}(w|y_1,y_2,\\dots,y_N;{\\bf{X}})\\\\\n&\\text{where, } {\\bf{X}} =\n\\begin{bmatrix}\n--{\\bf{x_1^\\intercal}}--\\\\\n--{\\bf{x_2^\\intercal}}--\\\\\n\\vdots\\\\\n--{\\bf{x_N^\\intercal}}--\n\\end{bmatrix}\n\\end{aligned}\\]\n오른쪽식은 왼쪽식에서의 데이터\\(D\\)를 좀더 풀어적은 수식입니다. 만약 위와 같은 확률을 계산했을때 그 값이 작은 가중치는 가능성이 낮은 가중치이므로 우리가 찾는 적절한 가중치는 아닐것입니다. 반대로 확률이 높은 가중치는 가능성이 높은 가중치이기때문에 우리가 찾는 가중치라고 할 수 있겠습니다. 그러면 그냥 “저 확률을 가장 크게 하는 가중치를 찾으면 되겠다”라고 생각이 들지만 안타까운 점은 우리는 위와같은 (조건부)확률(분포)을 바로 알기가 어렵습니다.. 따라서 베이즈정리의 도움을 받습니다. 베이즈 정리는 다음과 같습니다.\n\\[\\begin{aligned}\n&p({\\bf{w}}|D) = \\frac{p(D|{\\bf{w}})p({\\bf{w}})}{p(D)} \\propto (D|{\\bf{w}})p({\\bf{w}})\n\\end{aligned}\\]\n수식에서 분모\\(p(D)\\)는 normalization constant라 하는 상수입니다. 그러므로 분자를 최대화하는 가중치를 구하면 되지만 MLE(maximum likelyhood estimation) 에서는 likelyhood인 \\(L = p(D|w)\\)만을 최대화 하는 것을 목적으로 합니다.\n\\[\\begin{aligned}\nL = p_{Y_1,Y_2,\\dots,Y_N|{\\bf{w}}}(y_1,y_2,\\dots,y_N|w;X)  &= p_{Y_1,Y_2,\\dots,Y_N|{\\bf{w}}}(y_1,y_2,\\dots,y_N|w;X)\\\\\n&= p_{Y_1,{\\bf{w}}}(y_1|{w},{\\bf{X}})\\dots p_{Y_N,{\\bf{w}}}(y_N|{w},{\\bf{X}}) \\\\\n&= \\prod_{n = 1}^{N}p_{Y_n|{\\bf{W}}}(y_n|w;{\\bf{X}}) = \\prod_{n = 1}^{N}\\mathcal{N}(y_n|{\\bf{x_n}^\\intercal}{\\bf{w}},\\sigma^2) \\\\\n&= \\prod_{i=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(y_n-{\\bf{x_n}}^\\intercal{\\bf{w}})^2}{2\\sigma^2} \\right\\}  \\\\\n\\end{aligned}\\]\nlog likelyhood를 구하면 다음과 같습니다. 로그를 취하는 이유는 최댓값을 가지는 \\({\\bf{w}}\\)가 변하지 않고 곱셈을 덧셈을 바꿔서 계산하기 더 편하기 때문입니다.\n\\[\\begin{aligned}\n\\text{lnL} &= \\text{ln}\\prod_{n=1}^N \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\exp\\left\\{-\\frac{(y_n-{\\bf{x_n}}^\\intercal{\\bf{w}})^2}{2\\sigma^2} \\right\\}  \\\\\n&= \\sum_{n=1}^N\\text{ln} \\frac{1}{\\sqrt{2\\pi \\sigma^2}}\\text{exp}\\left\\{-\\frac{(y_n-{\\bf{x_n}}^\\intercal{\\bf{w}})^2}{2\\sigma^2} \\right\\} \\\\\n&= \\sum_{n=1}^N \\text{ln}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} - \\frac{1}{2\\sigma^2} \\sum_{n=1}^{N}(y_n-{\\bf{x_n}}^\\intercal{\\bf{w}})^2 \\\\\n&= C_1 - C_2\\sum_{n=1}^{N}(y_n-{\\bf{x_n}}^\\intercal{\\bf{w}})^2\n\\end{aligned}\\]\n상수를 제외해도 최댓값의 위치는 변하지 않으므로 제외하고 loglikelyhood를 최대화 하는 가중치를 얻은것이 우리의 목표입니다. 이때의 가중치는 가중치에 대한 추정량이므로 \\(\\bf{\\hat{w}}\\)로 표기합니다.\n여기서 시그마에서부터 보면 흔히 경사하강법에서 쓰는 MSE가 보입니다. MSE는 선형회귀의 MLE에서 Negative log likelyhood를 구할때 나오는 항입니다.\npsuedo inverse에 의한 해를 구하면 다음과 같습니다.\n\\[\\begin{aligned}\n\\hat{{\\bf{w}}} = (\\bf{X^TX}^{-1})X^Ty\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#선형회귀의-loss-function",
    "href": "posts/DL/Linear Regression.html#선형회귀의-loss-function",
    "title": "Linear Regression",
    "section": " 선형회귀의 Loss function",
    "text": "선형회귀의 Loss function\n2번에서 구한 추정값 \\(\\hat{\\bf{W}}\\)이 얼마나 틀린지,부정확한지 알려주는 함수를 Loss function 또는 Cost function이라고 합니다. 선형회귀에서의 Loss function은 일반적으로 MSE를 사용하며 주어진 샘플에서 잔차(residual,\\(\\hat{y}_i-y\\))들을 전부 제곱하여 더한 값입니다.\n(Loss function) \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2} = \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}}) = \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\)\nMSE와 같은 Loss function은 우리의 추정이 얼마나 틀렸는지를 나타내는 \\(\\hat{\\bf{W}}\\)에 대한 함수입니다. 그러므로, loss function을 가장 최소화 하는 \\(\\bf{\\hat{W}}\\)을 찾아내면 확률변수사이의 선형관계인 \\(\\bf{W}\\)를 알아낼 수 있습니다.\n\n\nText(110, 15, 'residual')"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#parameter-update",
    "href": "posts/DL/Linear Regression.html#parameter-update",
    "title": "Linear Regression",
    "section": " Parameter update",
    "text": "Parameter update\nn개의 독립변수를 가지는 다변수 스칼라 함수에 대한 Gradient는 수학적으로 다음과 같습니다.\n\\(\\nabla_{X}{f(x_1,x_2,...,x_n)} = (\\frac{\\partial f}{\\partial x_1},\\frac{\\partial f}{\\partial x_2},\\dots,\\frac{\\partial f}{\\partial x_n})\\) 다변수 스칼라 함수에 그레디언트를 취하면 벡터입니다.그러므로,그레디언트를 벡터(다변수)를 입력했을 때,벡터를 출력으로 하는 벡터함수라고 생각해도 무방합니다.중요한 사실은 임의의 공간상의 임의의 point \\(X\\)에서 스칼라함수에 대한 gradient of f = \\(-\\nabla_{X}{f}\\) 방향은 스칼라함수가 가장 급격하게 감소하는 방향이라는 사실입니다.(증명생략)\n위의 사실에 의하면,우리는 임의의 \\(\\hat{\\bf{W}}\\)에서 Loss function이 가장 급격하게 감소하는 방향을 찾을 수 있습니다. 그러므로 감소하는 방향을 찾고 이동하고 감소하는 방향을 찾고 이동하고 반복하다보면… 궁극적인 목적인 틀린정도를 최소화하는 즉,Loss function값이 가장 작은 \\(\\hat{\\bf{W}}\\)를 찾을 수 있습니다. \\(\\bf\\hat{W}\\)를 수정하는 구체적인 수식은 다음과 같습니다.\n(Gradient descent parameter update) \\(\\hat{\\bf{W}}_{t} = \\hat{\\bf{W}}_{t-1} - \\alpha\\times\\nabla_{W}{L}\\)\n\\(\\hat{\\bf{W}}_{t-1}\\)은 수정되기전의 가중치(벡터)이며 \\(\\hat{\\bf{W}_{t}}\\)는 파라미터를 한번 업데이트 한 후의 가중치(벡터)입니다. \\(t-1\\)의 \\(\\hat{\\bf{W_{t-1}}}\\)에 \\(-\\alpha\\times\\nabla_{W}{L}\\)를 더해줌으로서 \\(\\hat{\\bf{W}}_{t-1}\\)은 loss function이 가장 급격히(많이)감소하는 방향으로 이동하며 \\(\\hat{\\bf{W}}_{t}\\)가 됩니다. \\(\\alpha\\)는 학습률(learning rate)입니다. \\(\\hat{\\bf{W}}_{t-1}\\)과 곱해져서 얼마나 많이 또는 적게 움직일지를 결정합니다. 한번에 얼마나 이동할지에 비유한 “보폭”으로 생각할 수 있습니다.\n요약하자면, 경사하강법을 통하여 위와 같이 가중치\\(\\hat{\\bf{W}}\\)를 재귀적으로 업데이트 하면 loss function \\(L\\)이 가장 최소가 되는 지점의 \\(\\hat{\\bf{W}}\\)를 찾을 수 있습니다."
  },
  {
    "objectID": "posts/DL/Linear Regression.html#mse에-대한-더-상세한-전개",
    "href": "posts/DL/Linear Regression.html#mse에-대한-더-상세한-전개",
    "title": "Linear Regression",
    "section": " MSE에 대한 더 상세한 전개",
    "text": "MSE에 대한 더 상세한 전개\nMSE를 더 상세히 전개하면 다음과 같습니다. \\(MSE = \\Sigma_{i=1}^{i=n}(y_i - \\hat{y_i})^{2}\\) \\(= \\frac{1}{n}({\\bf{y} - \\bf{\\hat{y}}})^{T}({\\bf{y} - \\bf{\\hat{y}}})\\) \\(= \\frac{1}{n}(\\bf{y}-X\\hat{\\bf{W}})^{T}(\\bf{y}-X\\hat{\\bf{W}})\\) \\(= \\frac{1}{n}(\\bf{y^T - \\hat{\\bf{W}}^{T}\\bf{X}^{T})(\\bf{y} - \\bf{X}\\bf{\\hat{W}}})\\) \\(= \\frac{1}{n}(\\bf{y^Ty-y^TX\\hat{W}} - \\hat{W}X^Ty + \\hat{W}^TX^TX\\hat{W})\\)\n여기서 \\(\\bf{y^TX\\hat{W}} \\in \\bf{R}^{1 \\times 1}\\) 이므로 \\(\\bf{y^TX\\hat{W}} = (\\bf{y^TX\\hat{W}})^T = (\\bf{\\hat{W}X^Ty})\\)가 성립합니다. 그러므로 MSE를 정리하면 다음과 같습니다. (MSE) \\(MSE = \\frac{1}{n}(\\bf{y^Ty -2\\hat{W}X^Ty + \\hat{W}^TX^TX\\hat{W}})\\)"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#gradient-descent에-대한-더-상세한-전개loss-mse일-경우",
    "href": "posts/DL/Linear Regression.html#gradient-descent에-대한-더-상세한-전개loss-mse일-경우",
    "title": "Linear Regression",
    "section": " Gradient Descent에 대한 더 상세한 전개(\\(Loss\\) = MSE일 경우)",
    "text": "Gradient Descent에 대한 더 상세한 전개(\\(Loss\\) = MSE일 경우)\n(Gradient of MSE) \\(\\nabla{L} = MSE\\) \\(= \\bf{\\frac{1}{n}\\frac{\\partial}{\\partial \\hat{W}}(\\bf{y^Ty - 2\\hat{W}^TX^T + \\hat{W}^TX^TX\\hat{W}})}\\) \\(= \\bf{\\frac{1}{n}}(\\bf{\\frac{\\partial}{\\partial \\hat{W}}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2\\hat{W}^{T}X^{T}y + \\frac{\\partial}{\\partial\\hat{W}}\\hat{W}^{T}X^{T}X\\hat{W})\\) \\(= \\bf{\\frac{1}{n}(\\frac{\\partial}{\\partial \\hat{W}}{y^{T}y} - \\frac{\\partial}{\\partial \\hat{W}}2y^TX\\hat{W} + \\frac{\\partial}{\\partial\\hat{W}}\\hat{W}^TX^TX\\hat{W})}\\) \\(= \\bf{\\frac{1}{n}[0 - 2X^Ty + (X^TX + X^TX)\\hat{W}]}\\) \\(= \\bf{\\frac{2}{n}X^T(X\\hat{W} - y)}\\)\n(parameter update) \\(\\bf{\\hat{W}_{t} = \\hat{W}_{t-1} - \\alpha \\times \\frac{2}{n}X^T(X\\hat{W} - y)}\\)"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#결과해석",
    "href": "posts/DL/Linear Regression.html#결과해석",
    "title": "Linear Regression",
    "section": " 결과해석",
    "text": "결과해석\n200개의 샘플로부터 \\(\\bf{w}\\)를 추정하여 \\(\\hat{\\bf{w}}= (0.125,0.969)\\)를 얻었습니다. population regression model의 \\({\\bf{w}} = (w_0,w_1) = (0,1)\\)을 올바르게 추정했음을 알 수 있습니다. 아주 약간의 차이가 존재하는데 이 차이는 모집단에서 샘플을 더 얻거나 더 세밀하게 업데이트하면 최소화할 수 있습니다.\n\n#plt.title(\"w_1 : {} // w_0: {}\".format(round(W_hat[1].tolist()[0],3),round(W_hat[0].tolist()[0],3)))\nplt.title(\"Linear Regression\")\ntext=f\"What = ({round(t[0].tolist()[0],3)},{round(t[1].tolist()[0],3)})\"\nplt.plot(X[:,1],y,\"bo\",alpha=0.5)\nplt.plot(X[:,1],X@W_hat,\"r--\")\nplt.gca().axes.xaxis.set_visible(False)\nplt.gca().axes.yaxis.set_visible(False)\nplt.title(text)\n\nText(0.5, 1.0, 'What = (-0.125,0.969)')"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#appendix",
    "href": "posts/DL/Linear Regression.html#appendix",
    "title": "Linear Regression",
    "section": " Appendix",
    "text": "Appendix"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#mle의-해와-mse유도",
    "href": "posts/DL/Linear Regression.html#mle의-해와-mse유도",
    "title": "Linear Regression",
    "section": " MLE의 해와 MSE유도",
    "text": "MLE의 해와 MSE유도"
  },
  {
    "objectID": "posts/DL/Linear Regression.html#참고자료",
    "href": "posts/DL/Linear Regression.html#참고자료",
    "title": "Linear Regression",
    "section": " 참고자료",
    "text": "참고자료\nMaximum Likelihood Estimation(MLE) & Maximum A Posterior(MAP)"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html",
    "href": "posts/DL/Logistic Regression.html",
    "title": "Logistic Regression",
    "section": "",
    "text": "로지스틱회귀에 대해서 정리한 글입니다."
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#기댓값에-대한-고찰",
    "href": "posts/DL/Logistic Regression.html#기댓값에-대한-고찰",
    "title": "Logistic Regression",
    "section": "기댓값에 대한 고찰",
    "text": "기댓값에 대한 고찰\n기댓값은 실험 또는 시행을 무한히 반복했을때 확률변수가 취하는 값의 평균으로(또는 샘플링된 값의 평균) 기대되는 값입니다. 확률변수가 베르누이 분포를 따르는 경우 확률변수에 대한 기댓값(\\(E\\,[y|x_{1,i},x_{2,i}\\.,\\dots,x_{m,i}]\\))과 모수\\((p_i)\\)가 같은 값을 가집니다. 그러므로,만약에 주어진 샘플데이터로부터 베르누이분포의 모수를 적절히 추정할 수 있다면 주어진 조건하에서 실험 또는 시행을 무한히 반복할 경우 확률변수가 1인사건과 0인사건중 어떤 사건이 더 많이 발생할지 알 수 있고 이를 바탕으로 종속변수 Y의 값을 결정하는 것은 타당합니다. - e.g.\n\n\\(E\\,[y]\\, = \\hat{p_i}<0.5\\) => 무한히 실행했을때 0인 경우가 더 많을 것임 => 관측치를 0으로 예측 \n\\(E\\,[y]\\, = \\hat{p_i}\\geq0.5\\)=>무한히 실행했을때 1인 경우가 더 많을 것임 => 관측치를 1로 예측"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#concept",
    "href": "posts/DL/Logistic Regression.html#concept",
    "title": "Logistic Regression",
    "section": "concept",
    "text": "concept\n선형회귀에서 추정하고자하는 변수\\(y\\)는 \\(x_0,x_1,...,x_m\\)과 \\(w_0,w_1,...,w_m\\)과의 linear combination이였습니다.위에서 언급했듯이 특정샘플에 대한 모수를 적절하게 추정할 수 있다면 관측치가 어떤 클래스에 속할지 합리적으로 알 수 있으므로,로지스틱회귀에서도 선형회귀에서의 아이디어를 핵심아이디어를 가지고와서 추정하고자 하는 모수\\(p_i\\)를 \\(x_0,x_1,...,x_m\\)과 \\(w_0,w_1,...,w_m\\)의 linear combination로 표현하고자 합니다.\n선형회귀의 아이디어(linear combination) + 모수에 대한 표현이라는 조건을 만족하기 위해서 최종적인 식은 다음과 조건을 만족해야 것입니다. - \\((x_0,x_1,...,x_m)\\,,(w_0,w_1,..,w_m)\\)의 linear combination 식에 있어야 함. - linearcombination = 모수(추정하고자하는값)여야 함.\nwhy linear combination?"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#본격적인-유도",
    "href": "posts/DL/Logistic Regression.html#본격적인-유도",
    "title": "Logistic Regression",
    "section": "본격적인 유도",
    "text": "본격적인 유도\n\n모수를 로지스틱함수로 바꾸기\n\n\\((x_0,x_1,...,x_m)\\,,(w_0,w_1,..,w_m)\\)의 linear combination이 식에 존재해야 합니다. 그러므로 선형방정식을 하나 만듭니다. \\[\\begin{align}\nf(i) = x_{1,i}w_0 + x_{2,i}w_1 + x_2w_2 + ... + x_{m,i}w_m = X_iW \\nonumber \\\\\nwhere,X_i = \\,[x_{1,i},x_{2,i},\\dots,x_{m,i}]\\, ,W = \\,[w_0,w_1,\\dots,w_m]^\\text{T} \\nonumber \\\\\n\\end{align}\\]\n좌변은 예측하고자 하는 값인 모수여야 합니다. 좌변을 바꿔봅니다. \\[p_i = WX_i\\]\n좌변의 베르누이 분포의 모수 \\(p_i\\)는 확률변수 \\(y = 1\\)인 사건이 일어날 확률입니다. 그러므로 \\([0,1]\\)이라는 범위를 가지는 반면 우변의 값\\(WX_i\\)은 \\(\\,[-\\infty,\\infty]\\,\\)에 범위를 가집니다. 여기서 Odss Ratio를 써서 모수 \\(p_i\\)를 포함하며 더 넓은 range를 갖도록 좌변을 수정합니다. \\[\\text{Odds Ratio} = \\frac{p_i}{1-p_i} = WX_i\\]\n좌변을 Odds Ratio로 수정했지만 여전히 좌변의 범위는\\(\\,[0,\\infty]\\,\\)으로 우변에 비해 좁습니다. 따라서 Odds Ratio에 로짓변환을 취하여 좌변의 범위를 \\(\\,[-\\infty,\\infty]\\)로 넓혀줍니다. \\[\\text{logit}(p) = \\text{ln}\\frac{p_i}{1-p_i} = WX_i\\]\n\n위 식을 해석하기 위해 \\(X\\)의 첫번째 요소인 \\(x_1\\)에 대응하는 회귀계수 \\(w_1\\)이 학습결과 3으로 정해졌다고 가정해봅시다.만약 \\(x_1\\)의 값이 1증가한다면 로그오즈비가 3증가합니다.\n\n이제 양변의 범위는 맞춰졌으므로 추정하고자 하는 변수 \\(p_i\\)가 좌변에 오도록 정리해봅시다. \\[p_i = \\frac{1}{\\,(1 + e^{-WX_i})\\, }\\] (전개) \\(\\frac{p_i}{1-p_i} = e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\,(1-p_i)\\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\,e^{WX_i}-p_ie^{WX_i}\\) \\(\\Longleftrightarrow p_i + p_ie^{WX_i} = \\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i\\,(1 + e^{WX_i})\\, = \\,e^{WX_i}\\) \\(\\Longleftrightarrow p_i = \\frac{\\,e^{WX_i}}{\\,(1 + e^{WX_i})\\, }\\) \\(\\Longleftrightarrow p_i = \\frac{1}{\\,(1 + e^{-WX_i})\\, }\\)\n\n최종적으로, 앞서 목적이었던 X와 W의 선형조합이 수식내부에 존재하도록 새롭게 표현한 모수는 다음과 같습니다. \\[p_i(y) = Pr\\,(y = 1|X_i;W)\\, = \\frac{1}{\\,1 + e^{-WX_i}\\,}\\]\n\n\n베르누이 분포의 pmf 정리\n베르누이분포의 모수 \\(p_i\\)가 새롭게 표현되었으므로 확률질량함수도 새롭게 표현할 수 있습니다. 마지막 수식은 베르누이 분포의 확률질량함수가 모수\\(W\\)에 관한 식으로 바뀌었음을 표현합니다.\n\\[\\begin{align}\nBern(y;p_i) = Pr\\,(Y_{i} = y|x_{1,i},x_{2,i},\\dots,x_{m,i};p_i) &=\n\\begin{cases}\np_i & \\text{if}\\,y=1 \\\\\n1-p_i & \\text{if}\\,y=0\n\\end{cases} \\\\\n&= p_i^{y}(1-p_i)^{1-y} \\\\\n&= \\frac{e^{yWX_i}}{1+e^{WX_i}}\n\\end{align}\\]"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#setting",
    "href": "posts/DL/Logistic Regression.html#setting",
    "title": "Logistic Regression",
    "section": "setting",
    "text": "setting\n\nimport torch\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n#sig = lambda z:torch.exp(z)/(1+torch.exp(z))\nsig = torch.nn.Sigmoid()"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#data",
    "href": "posts/DL/Logistic Regression.html#data",
    "title": "Logistic Regression",
    "section": "data",
    "text": "data\n\ntorch.manual_seed(2022)\nn=400\n\n#1 모수 W가정\nW = torch.tensor(\n    [[-0.8467],\n    [0.041]]).float()\n\n#2 각각의 관측치(데이터요소)에서의 모수 p_i시각화(시그모이드 함수 시각화)\n_x = torch.linspace(-150,150,n).reshape(-1,1)\n_one = torch.ones((n,1))\nX = torch.concat((_one,_x),axis=1)\np_i = sig(X@W)\ny = torch.bernoulli(p_i)\nplt.xlim([-150,150])\nplt.plot(X[:,1],y,\"bo\",alpha=0.5)\nplt.plot(X[:,1],p_i,\"r--\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y,$p_i$\")\nplt.title(\"realizations $y_1,\\dots,y_{300}$ from $Bern(p_1),\\dots,Bern(p_{300})$\")\nplt.legend([\"y\",\"p\"])\n\n<matplotlib.legend.Legend at 0x1f2684bfe20>"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#gradient-descent-1",
    "href": "posts/DL/Logistic Regression.html#gradient-descent-1",
    "title": "Logistic Regression",
    "section": "Gradient Descent",
    "text": "Gradient Descent\n\nimport torch\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\n\ntorch.manual_seed(2022)\nn=400\n\n#2 임의의 W에 대한 estimated value(추정치) What 초기화\nWhat = torch.tensor(\n    [[0.],\n    [-0.03]],requires_grad=True)\n\n_x = torch.linspace(-150,150,n).reshape(-1,1)\n_one = torch.ones((n,1))\nX = torch.concat((_one,_x),axis=1)\nyhat = sig(X@What)\n\nplt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\nplt.plot(X[:,1].data,p_i,\"r\")\nplt.plot(X[:,1].data,yhat.data,\"g\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"realizations $y_1,\\dots,y_{60}$ from $Bern(p_1),\\dots,Bern(p_{60})$\")\nplt.legend([\"y\",\"$p_i$\",\"$\\hat{y}(\\hat{p_i})$\"])\n\n<matplotlib.legend.Legend at 0x1f268e11e50>\n\n\n\n\n\n\nloss_fn = torch.nn.BCELoss()\n\"\"\"\ndef BCE_Loss(yhat,y):\n    return torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat))\n\"\"\"\n\n'\\ndef BCE_Loss(yhat,y):\\n    return torch.mean(y * torch.log(yhat) + (1-y) * torch.log(1-yhat))\\n'\n\n\n\n#custom sigmoid + torch.BCELoss 쓰면 오류 발생. 0과 1사이의 범위 아님\n#torch.nn.Sigmoid + custom BCE Loss 써도 오류발생 => nan\nplt.subplots(2,5,figsize=(20,8))\nplt.subplots_adjust(hspace=0.3)\ni=1\n\nfor epoch in range(200):\n    #1 yhat \n    yhat = sig(X@What)\n    #2 loss\n    loss = loss_fn(yhat,y)\n    if epoch % 20 == 0:\n        plt.subplot(2,5,i)\n        #plt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\n        plt.plot(X[:,1].data,p_i,\"r\")\n        plt.plot(X[:,1].data,yhat.data,\"g\")\n        plt.xlabel(\"x\")\n        plt.ylabel(\"y\")\n        #plt.title(\"realizations $y_1,\\dots,y_{60}$ from $Bern(p_1),\\dots,Bern(p_{60})$\")\n        plt.legend([\"p\",\"yhat\"])\n        title = \"loss : {}\".format(round(loss.tolist(),5))\n        plt.title(title)\n        i+=1\n    #3 derivative\n    loss.backward()\n    #4 update & clean\n    What.data = What.data - 0.00005 * What.grad\n    What.grad = None\n\n\n\n\n\nround(loss.tolist(),4)\n\n0.3109\n\n\n\nplt.plot(X[:,1].data,y,\"bo\",alpha=0.3)\nplt.plot(X[:,1].data,p_i,\"r\")\nplt.plot(X[:,1].data,yhat.data,\"g\")\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"Logistic Regression\")\nplt.legend([\"y\",\"$p_i$\",\"$\\hat{y}(\\hat{p_i})$\"])\nplt.gca().axes.xaxis.set_visible(False)\nplt.gca().axes.yaxis.set_visible(False)"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#베르누이-분포-전개",
    "href": "posts/DL/Logistic Regression.html#베르누이-분포-전개",
    "title": "Logistic Regression",
    "section": "1.베르누이 분포 전개",
    "text": "1.베르누이 분포 전개\n\\[\\begin{aligned}\np_i^y(1-p_i)^{1-y} &= \\ (\\frac{1}{\\,1 + e^{-WX_i}\\,})^y\\,\\,(1-\\frac{1}{\\,1 + e^{-WX_i}\\,})^{1-y} \\\\\n&= (\\frac{1}{1+e^{-WX_i}})^{y}(\\frac{e^{-WX_i}}{1+e^{-WXi}})^{1-y} \\\\\n&= (\\frac{1}{1+e^{-WX_i}})^{y}(\\frac{1}{1+e^{WXi}})^{1-y} \\\\\n&= (\\frac{1+e^{WX_i}}{1+e^{-WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= (\\frac{e^{WX_i}+e^{2WX_i}}{1+e^{WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= (\\frac{e^{WX_i}(1+e^{WX_i})}{1+e^{WX_i}})^{y}(\\frac{1}{1+e^{WX_i}}) \\\\\n&= e^{yWX_i}\\frac{1}{1+e^{WX_i}} \\\\\n&= \\frac{e^{yWX_i}}{1+e^{WX_i}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#nll전개with-parameter-w",
    "href": "posts/DL/Logistic Regression.html#nll전개with-parameter-w",
    "title": "Logistic Regression",
    "section": "2.NLL전개(with parameter \\(W\\))",
    "text": "2.NLL전개(with parameter \\(W\\))\n\\[\\begin{aligned}\nLL &= \\text{ln}(\\underset{i=1}{\\overset{n}{\\large{\\prod}}}\\,\\frac{e^{y_iWX_i}}{1+e^{WX_i}}) \\\\\n&=\\overset{n}{\\underset{i=1}{\\large{\\sum}}}(\\text{ln}\\frac{e^{y_iWX_i}}{1+e^{WX_i}}) \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}\\,[\\text{ln}e^{y_iWX_i} - \\text{ln}(1+e^{WX_i})]\\, \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}\\,[y_iWX_i - \\text{ln}(1+e^{WX_i})], \\\\\n&= \\overset{n}{\\underset{i=1}{\\large{\\sum}}}y_iWX_i - \\overset{n}{\\underset{i=1}{\\large{\\sum}}}ln(1+e^{WX_i}) \\\\\n\n\nNLL &= -\\overset{n}{\\underset{i=1}{\\large{\\sum}}}y_iWX_i + \\overset{n}{\\underset{i=1}{\\large{\\sum}}}ln(1+e^{WX_i}) \\\\\n\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Logistic Regression.html#nll전개cross-entropy-유도하기",
    "href": "posts/DL/Logistic Regression.html#nll전개cross-entropy-유도하기",
    "title": "Logistic Regression",
    "section": "3.NLL전개(Cross Entropy 유도하기)",
    "text": "3.NLL전개(Cross Entropy 유도하기)\n임의의 i번째 항에서의 확률변수 \\(Y_i\\)가 따르는 베르누이 분포는 다음과 같습니다. \n\\[\\begin{aligned}\nBern(y|X_i;p_i) = (p_i)^y(1-p_i)^{y-1}\n\\end{aligned}\\]\n모수가 \\(p_i\\)인 각각의 베르누이 분포를 따르는 확률변수 \\(Y_1,Y_2\\dots Y_n\\)으로부 n개의 realization(sample) \\(y_1,y_2\\dots y_n\\)에 대한 NLL는 다음과 같습니다. \n\\[\\begin{aligned}\nNLL &= -\\text{ln}\\prod_{i=1}^{n}p_i^{y_i}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}\\text{ln}p_i^{y_i}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}\\text{ln}p_i^{y_i} + \\text{ln}(1-p_i)^{1-y_i} \\\\\n&= -\\sum_{i=1}^{n}y_i\\text{ln}p_i + (1-y_i)\\text{ln}(1-p_i)\n\\end{aligned}\\]\n참고링크 1. 로지스틱 회귀 전개 2. 위키피디아 - 로지스틱 회귀 3. ratsgo’s blog"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "",
    "text": "[Deep Learning Series - Part3]\n안녕하세요!!😀 이번 포스트에서는 다항로지스틱 회귀와 소프트맥스 회귀에 대해서 정리해보고자 합니다. 공부하면서 생각보다 모르는 내용이 많아서 다시 처음부터 공부하고 복습해야 하는 내용이 많았네요. 잡담은 그만하고 시작해보겠습니다!! 읽어주셔서 감사해요😎"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#가정-1",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#가정-1",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "가정 (1)",
    "text": "가정 (1)\n로지스틱회귀를 복기해보면… 종속변수 \\(y_i\\)는 베르누이분포를 따르는 확률변수\\(Y_i\\)로부터 샘플링된 값으로 가정했습니다. 또한 베르누이분포의 모수\\(W\\)는 주어진 조건인 \\(X_i\\)와 회귀계수(가중치)\\(W\\)의 일차결합으로 가정했습니다. 이렇게 모수를 가정하면서 베르누이분포의 확률질량함수도 새로운 모수\\(W\\)를 가지게되었고 W를 적절히 추정하면 데이터가 0또는1에 속할 확률을 알아내게 되어 확률이 더 높은 클래스를 주어진데이터에 대한 클래스로 예측했었습니다.다항로지스틱회귀와 소프트맥스회귀에서도 이러한 과정 즉,분포를 가정하고 데이터를 기반으로 모수를 추정하여 확률분포를 기반으로 예측하는 매커니즘은 거의 그대로입니다.\n먼저 다항로지스틱회귀와 소프트맥스회귀에서 종속변수에 대한 가정을 해보겠습니다. 다항로지스틱 회귀와 소프트맥스회귀에서 모두 각각의 관측치(each observation)에서 종속변수의 realization인 \\(y_i\\)는 확률변수\\(Y_i\\)로부터 표본추출(sampling)되었다고 가정합니다. 이때 각각의 관측치에서의 확률변수 \\(Y_i\\)가 따르는 분포는 설명변수 \\(x_{1,i},x_{2,i},\\dots,x_{M,i}\\)가 조건으로 주어질 때, 각각의 범주(클래스)에 속할 확률들을 모수로 가지는 카테고리분포를 따릅니다.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\nY_i|x_{1,i},x_{2,i},\\dots,x_{M,i} \\sim \\text{Cat}(y|x_{1,i},x_{2,i},\\dots,x_{M,i};\\mu_i)\n& =\n\\begin{cases}\n\\mu_{1,i} \\text{ if } y = (1,0,\\dots,0,0) \\\\\n\\mu_{2,i} \\text{ if } y = (0,1,\\dots,0,0) \\\\\n\\quad\\quad \\vdots \\\\\n\\mu_{K,i} \\text{ if } y = (0,0,\\dots,0,1) \\\\\n\\end{cases} \\\\\n&= \\mu_{1,i}^{y_1}\\mu_{2,i}^{y_2},\\dots,\\mu_{K,i}^{y_K} \\\\\n&= \\prod_{K=1}^{K}\\mu_{K,i}y_{K,i} \\\\\n\\end{aligned} \\\\\n&\n\\begin{aligned}\n&\\text{where, }\\\\\n&\\mu_i = {\\mu_{1,i},\\mu_{2,i},\\dots,\\mu_{K,i}} \\\\\n&\\mu_{1,i} = Pr(Y_i = (1,0,\\dots,0)|x_{1,i},\\dots,x_{M,i}) \\\\\n&\\mu_{2,i} = Pr(Y_i = (0,1,\\dots,0)|x_{1,i},\\dots,x_{M,i}) \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\vdots \\\\\n&\\mu_{K,i} = Pr(Y_i = (0,0,\\dots,0,1)|x_{1,i},\\dots,x_{M,i}) \\\\\n\\end{aligned}\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#가정-2",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#가정-2",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "가정 (2)",
    "text": "가정 (2)\n각각의 관측치에서 확률변수\\(Y_i\\)가 따르는 카테고리분포의 모수\\(\\mu_i\\)는 데이터포인트마다 다른 설명변수(X_i)와 시행마다 변하지 않는 고정된 회귀계수(W)의 일차결합을 포함하는 수식으로 표현됩니다. 주어진 X값을 W와 일차결합하여 추정하고자 하는 값을 표현하는 선형회귀의 핵심아이디어이자 대부분의 회귀문제에서 사용하는 중요한 아이디어 입니다.\n\\[\\begin{aligned}\n&\\mu_{k,i}  = \\mu_{k,i}(X_i;W_{k,i}) = \\mu_{k,i}(X_iW_{k,i}) =  Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i)\\\\\n&\\text{where},\\\\\n&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n&W_k = [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n&\\mu_{k,i} = \\text{i-th 관측치의 $k$번째 모수}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xw의-선형조합을-포함한-모수의-표현-유도하기",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xw의-선형조합을-포함한-모수의-표현-유도하기",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "X,W의 선형조합을 포함한 모수의 표현 유도하기",
    "text": "X,W의 선형조합을 포함한 모수의 표현 유도하기\n위에서 언급했듯이 대부분의 회귀에서 모델링의 핵심아이디어는 추정하고자 하는 대상을 설명변수와 가중치의 일차결합(선형조합)이 포함되도록 표현하는 것입니다. 다항로지스틱회귀도 추정하고자 하는 모수\\(\\mu_i = (\\mu_{1,i},\\mu_{2,i},\\dots,\\mu_{K,i})\\)를 각각을 설명변수와 가중치의 일차결합으로 표현해야 합니다.이진로지스틱회귀와에서도 이렇게 모수를 표현했었는데 다항로지스틱회귀에서는 일차결합으로 표현해야할 모수가 좀 더 많습니다. -_-;;\n차근차근 한번 유도해보겠습니다. 일단 K개의 모수를 표현하는 일차결합을 만들어줍니다. 이러한 일차결합에서 x는 관측치마다 존재하는 설명변수의 값에 따라서 회귀계수(가중치)인 W는 관측치에 따라서 변하지 않는 일정한 값입니다.\n\\[\\begin{aligned}\n&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i-\\text{ln}Z \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\\n&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i {\\text{ (임의의 k번째 항)}}\\\\  \n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n&where,\\\\\n&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n\\end{aligned}\\]\n 한 가지 유의해야 할 점은 마지막 모수는 일차결합으로 표현하지 않는다는 것입니다. 카테고리분포에서 모수의 총합은 1이기 때문에 마지막 \\(K\\)번째 모수는 1에서 전부 빼면 되기 때문입니다.\n그런데 섣불리 일차결합을 만들다보니 … 좌변에 있는 모수는 \\([0,1]\\)의 범위이고 우변은 \\([-\\infty,\\infty]\\)의 범위이므로 가지므로 양변의 범위가 전혀 맞지 않습니다. 그러므로 좌변을 Odds Ratio(엄밀히 Odds Ratio는 아니지만 통일성을 위해 Odds Ratio라고 하겠습니다.) + Logit transform을 취하여 좌변이 우변과 같은 범위를 가질 수 있도록 확장하여 줍니다. (로그안에 있는 분모가 K번째 클래스에 대한 항임을 유의합니다.) \\[\\text{ln}\\frac{\\mu_{k,i}}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = \\text{ln}\\frac{Pr(Y_i = (0,\\dots,1_{k-th},0,\\dots,0)|X_i;W_k)}{Pr(Y_i = (0,\\dots,0,1)|X_i)} = W_k^TX_i\\]\n원래의 목적은 모수에 대한 일차결합이 포함된 항을 얻는 것이었습니다. 그러므로 정리하여 모수에 대한 표현을 얻습니다.\n\\[\\begin{aligned}\n\\mu_{k,i} = Pr(Y_i = (0,\\dots,0,1_{k-th},0,\\dots,0|X_i;W_k) = Pr(Y_i = K|X_i)e^{X_iW_k}\n\\end{aligned}\\]\n여기까지 해서 모수에 대한 표현을 얻었습니다. 다만 \\(Y_i\\)가 \\(K\\)번째 클래스에 대한 확률은 카테고리분포에서의 모수에 대한 제약조건을 활용하여 더 간단하게 바꿀 수 있습니다.\n\\[\\begin{aligned}\n&Pr(Y_i = K|X_i) = 1- \\sum_{k=1}^{K-1}Pr(Y_i = K|X_i)e^{X_iW_k} = 1-Pr(Y_i = K|X_i)\\sum_{k=1}^{K-1}e^{X_iW_k} \\\\\n&\\Longleftrightarrow Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{k=1}^{K-1}e^{X_iW_k}}\n\\end{aligned}\\]\n더 간단하게 표현된 항으로 다시 정리하여 쓰면 다음과 같습니다.\n\\[\\begin{aligned}\n&\\mu_{k,i}=Pr(Y_i = k|X_i) = Pr(Y_i = K|X_i)e^{X_iW_k} = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}}\\\\\n&\\text{인덱스 겹치므로 시그마의 $k \\rightarrow j$}\n\\end{aligned}\\]\n 최종적으로 카테고리 분포의 모수는 다음과 같습니다. 전개하는 과정이 마지막 \\(K\\)번째 항은 제외한채 진행되었으므로 K번째 항에대한 확률은 따로 써줍니다.\n\\[\\begin{aligned}\n&\\mu_{k,i}=Pr(Y_i = k|X_i) = \\frac{e^{X_iW_k}}{1+\\sum_{j=1}^{K-1}e^{X_iW_j}} \\text{(단, $k != K$)}\\\\\n&\\mu_{K,i}=Pr(Y_i = K|X_i) = \\frac{1}{1+\\sum_{j=1}^{K-1}e^{X_iW_k}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#estimation",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#estimation",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "Estimation",
    "text": "Estimation\n더 공부해 오겠습니다 ^__^;;"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#가정",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#가정",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "가정",
    "text": "가정\n소프트맥스회귀의 가정은 로지스틱회귀의 가정과 습니다. 각 datapoint에서의 종속변수의 값은 카테고리분포를 따르는 확률변수에서 샘플링되었으며 카테고리분포의 모수는 각 datapoint마다 변하는 설명변수와 회귀계수(가중치)의 일차결합으로 표현됩니다."
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xw의-선형조합을-포함한-모수의-표현-유도하기-1",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#xw의-선형조합을-포함한-모수의-표현-유도하기-1",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "X,W의 선형조합을 포함한 모수의 표현 유도하기",
    "text": "X,W의 선형조합을 포함한 모수의 표현 유도하기\n소프트맥스 회귀마찬가지로 추정하고자 하는 모수를 설명변수와 가중치의 일차결합이 포함된 항으로 표현합니다.\n먼저 설명변수와 가중치의 일차결합형태로 모수를 나타냅니다. 임의의 i번째 관측치가 각각의 범주에 \\((1,2,...,K)\\) 속할 확률을 의미하는 모수는 다음과 같습니다.\n\\[\\begin{aligned}\n&\\mu_{1,i} = Pr(Y_i=(1,0,0,\\dots,0)|X_i;W_1)\\quad \\\\\n&\\quad\\,\\,\\, = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n&\\mu_{2,i} = Pr(Y_i=(0,1,0,\\dots,0)|X_i;W_2) = \\\\\n&\\quad\\,\\,\\, = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n&\\mu_{3,i} = Pr(Y_i = (0,0,1,\\dots,0)|X_i;W_2)) = \\\\\n&\\quad\\,\\,\\, = w_{0,3}x_{0,i}+w_{1,3}x_{1,i} + w_{2,3}x_{2,i} + \\dots \\ + w_{M,3}x_{M,i} = W_3^TX_i-\\text{ln}Z \\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th},\\dots,0,0)|X_i;W_k)) \\\\\n&\\quad\\,\\,\\,= w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{m,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i \\\\  \n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\mu_{K-1,i} = Pr(Y_i = (0,0,0,\\dots,1,0)|X_i;W_{K-1})) \\\\\n&\\quad\\,\\,\\,= w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i \\\\ \\\\\n&where,\\\\\n&w_{m,k} : \\text{$k$번째 모수를 표현하기위해 $m$번째 값과 곱해지는 가중치} \\\\\n&x_{m,i} : \\text{i-th 관측치의 $m$번째 독립변수의 값} \\\\\n&X_i = [x_{0,i},x_{1,i},\\dots,x_{M,i}]^{\\text{T}}\\text{ : i-th관측치의 feature vector(단,$x_{0,i}$ = 1)} \\\\\n&W_k : [w_{0,k},w_{1,k},\\dots,w_{M,k}]^{\\text{T}}\\text{ : 카테고리 분포의 임의의 k-th 모수$\\mu_k$를 구하기 위한 가중치를 모아놓은 벡터} \\\\\n\\end{aligned}\\]\n이렇게 나타내고 보니 좌변과 0~1사이의 수만 갖지만 우변은 어떤 수던지 나올 수 있습니다. 범위를 맞춰 주기 위해서 좌변에 로그를 씌워 로그확률로 만들어줍니다. 추가적으로 우변에 \\(-lnZ\\)라는 normalizating factor를 더해줍니다. 다음과정에서 카테고리분포의 모수의 합이 1이되도록 하는 확률질량함수의 특징을 유지하기 위해서 사용합니다.\n\\[\\begin{aligned}\n&\\text{ln}\\mu_{1,i} = w_{0,1}x_{0,i}+w_{1,1}x_{1,i} + w_{2,1}x_{2,i} + \\dots \\ + w_{M,1}x_{M,i} = W_1^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\text{ln}\\mu_{2,i} = w_{0,2}x_{0,i}+w_{1,2}x_{1,i} + w_{2,2}x_{2,i} + \\dots \\ + w_{M,2}x_{M,i} = W_2^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\text{ln}\\mu_{k,i} = w_{0,k}x_{0,i}+w_{1,k}x_{1,i} + w_{2,k}x_{2,i} + \\dots +w_{m,k}x_{M,i} \\dots + w_{M,k}x_{M,i} = W_k^TX_i-\\text{ln}Z \\\\\n\\\\\n&\\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\quad \\quad \\quad \\quad \\quad \\quad  \\quad \\vdots \\\\\n&\\text{ln}\\mu_{K-1,i} = w_{0,K}x_{0,i}+w_{1,K-1}x_{1,i} + w_{2,K-1}x_{2,i} + \\dots \\ + w_{M,K-1}x_{M,i} = W_{K-1}^TX_i-\\text{ln}Z \\\\\n\\\\\n\\end{aligned}\\]\n따라서,임의의 \\(k\\)번째 모수는 다음과 같습니다. \\[\\mu_{k,i} = Pr(Y_i = (0,0,\\dots,1_{k-th}|X_i;W_k) = \\frac{1}{Z}e^{W_k^TX_i}\\]\n카테고리분포의 제약조건 즉,모수는 각각의 범주에 속할 확률을 나타내므로 총합이 1임을 활용합니다. 이를 활용하여 Z를 표현하면 다음과 같습니다.\n\\[\\begin{aligned}\n&\\sum_{k=1}^{K}{\\mu_{k,i}} =\\sum_{k=1}^{K}{Pr(Y_i=k)}= \\frac{1}{Z}\\sum_{k=1}^{K}e^{W_k^TX_i} = 1\\\\\n&\\Longleftrightarrow Z = \\sum_{k=1}^{K}e^{W_k^TX_i}\n\\end{aligned}\\]\n최종적으로, 결과를 정리하면 다음과 같습니다. - 추정하고자하는 카테고리분포의 모수는 \\(\\mu_k\\)는 \\(W_k\\)와 \\(X_i\\)의 일차결합으로 표현되었습니다. 이는 소프트맥스 함수이므로 소프트맥스 회귀라는 이름이 붙었습니다. \\[\\mu_{c,i}(X_i;W) = Pr(Y_i = (0,0,\\dots,1_{c-th},0,\\dots,0)|X_i;W_k) = \\frac{e^{W_c^TX_i}}{\\sum_{k=1}^{K}e^{W_k^TX_i}} = softmax(c,W_1^TX_i,W_2^TX_i,\\dots,W_K^TX_i)\\] - 카테고리분포의 위에서 구한 모수로 다시 정리하면 확률질량 함수는 새로운 모수 \\(W_1,W_2,\\dots,W_K\\)를 가집니다.(인덱스 \\(k->j,c->k\\)) \\[Y_i \\sim Cat(y|X_i;W_1,W_2,\\dots,W_K) = \\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\\]"
  },
  {
    "objectID": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#mle",
    "href": "posts/DL/Multinomial Logistic Regression,Softmatx Regression.html#mle",
    "title": "Multinomial Logistic Regression & Softmax Regression",
    "section": "MLE",
    "text": "MLE\n여기까지의 과정으로부터 카테고리분포의 모수는 설명변수와 가중치(회귀계수)의 일차결합으로 표현되며 또한 확률질량함수가 새로운 모수 \\(W = (W_1,W_2,\\dots,W_K)\\)로 표현되었습니다.만약 카테고리분포의 모수만 추정할 수 있다면 우리는 데이터포인트가 어떤 범주에 속할 확률이 가장 높은지 알 수 있으며 범주를 분류할 수 있습니다. 여기서는 카테고리분포의 모수\\(W\\)를 MLE로 추정합니다.\n확률분포에서 임의의 모수\\(W = (W_1,W_2,\\dots,W_K)\\)를 가정할 때, 확률변수 \\(Y_1,Y_2,\\dots,Y_N\\)으로부터 realization인 \\(y_1,y_2,\\dots,y_N\\)이 나올 가능도는 다음과 같습니다.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\nL({W};X_i|y_1,y_2,\\dots,y_n) &= Pr_{Y_1,Y_2,\\dots,Y_N}(y1,y2,\\dots,y_n|X_i;W)\\\\\n&= \\prod_{i=1}^{N}Pr_{Y_i}(Y_i=y_i|X_i;W) \\\\\n&= \\prod_{i=1}^{N}\\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\\\\\n\\end{aligned}\n\\\\\n&\\text{where } \\{W\\} = \\{W1,W2,\\dots,W_N\\}\n\\end{aligned}\\]\n위와 같은 가능도를 최소화 하는 \\(W\\)를 찾는 것이 목적입니다.다음과 같습니다\n\\[\\begin{aligned}\n\\overset{*}{\\{W\\}} = \\underset{\\{W\\}}{\\text{argmax}} \\prod_{i=1}^{N}\\prod_{k=1}^{K}\\frac{e^{W_k^TX_i}}{\\sum_{j=1}^{K}e^{W_j^TX_i}}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn 구현.html",
    "href": "posts/DL/Pytorch Rnn 구현.html",
    "title": "pytorch로 Rnn구현하기",
    "section": "",
    "text": "hi?hi!가 반복되는 텍스트 데이터에서 다음 문자가 뭐가 나올지 예측하는 RNN모형 만들기"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn 구현.html#vectorization",
    "href": "posts/DL/Pytorch Rnn 구현.html#vectorization",
    "title": "pytorch로 Rnn구현하기",
    "section": "vectorization",
    "text": "vectorization\n\n여러가지 방법이 있으나(tf-idf,dense vector,one-hot encoding 등등…) 여기서는 원핫인코딩 사용\n\n\ndef mapping(txt,map_dict):\n    return [map_dict[chr]for chr in txt]\ntxt_mapped = mapping(txt,map_dict)\nprint(txt_mapped[:10])\n\ndef onehot_encoding(txt_mapped):\n    seq_encoded = torch.nn.functional.one_hot(torch.tensor(txt_mapped))\n    return seq_encoded.float()\nsequence_data_encoded = onehot_encoding(txt_mapped)\nprint(sequence_data_encoded[:10])\n\n[2, 3, 0, 2, 3, 1, 2, 3, 0, 2]\ntensor([[0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [0., 1., 0., 0.],\n        [0., 0., 1., 0.],\n        [0., 0., 0., 1.],\n        [1., 0., 0., 0.],\n        [0., 0., 1., 0.]])\n\n\n데이터 살짝 변형 하나의 긴 sequence data를 RNN의 입력으로 해도 되지만 처리속도,성능을 고려했을 때 자그마한 sequencedata로 분리하여 입력해주는게 더 좋은 방법임. 분리하는 방법도 여러가지가 있을 수 있겠는데 여기서는 다음과 같이 분리함 raw sequence data : hi?hi!hi?hi!hi?hi! ……….. sequence1 : (x,y) = (hi?,h) sequence2 : (x,y) = (i?h,i) sequence3 : (x,y) = (?hi,!) …\n\ndef create_seqdataset(seq_data,seq_length):\n    #x = seq_data[:-1]\n    #y = seq_data[1:]\n    seqs_x = []\n    seqs_y = []\n    for idx in range(0,len(seq_data)-seq_length):\n        seqs_x.append(seq_data[idx:idx+seq_length])\n        seqs_y.append(seq_data[idx+seq_length])\n    return torch.stack(seqs_x),torch.stack(seqs_y)\n    #return seq_x,seq_y\n\nx_data,y_data = create_seqdataset(sequence_data_encoded,3)\nprint(x_data.shape,y_data.shape)\n\ntorch.Size([57, 3, 4]) torch.Size([57, 4])\n\n\n\n왜 저런 shape을 맞춰 주는가?\n여기서 나오는 x_data.shape = \\((57,3,4)\\)가 살짝 난해함.  파이토치 공식문서에 따르면 batch_first = True로 설정할 경우,rnn계열의 모델에 넣어줘야 하는 텐서의 shape은 \\((N,L,H_{in})\\) = (batch size,sequnce length,input_size)이고 dataloader라는 일종의 데이터 중간관리자?를 한 번 거쳐서 모델에 입력됨. dataloader에서 나오는 output.shape = \\((N,L,H_{in})\\)이 되기 위해서는 input.shape = \\((D,L,H_{in}\\)(D는 분리된 시퀀스의 갯수)이어야 함(즉 입력텐서의 차원이 3개여야 출력텐서의 차원도3개이고 차원이 나오는 순서도 저런식이 되어야 함). 따라서 저렇게 설정함.\n\n\n파라미터 잠깐 설명\nbatch size는 배치의 총 갯수(배치안에 있는 원소의 갯수 아님!), sequnce length는 시퀀스데이터의 길이이자 timestemp(시점)의 총 갯수(길이), \\(H_{in}\\)은 each timestep(각 시점)마다 입력되는 벡터의 길이라고 볼 수 있음. 위처럼 원핫인코딩을 한 경우 \\(H_{in}\\)은 시퀀스데이터에 있는 문자의 갯수로 결정되므로 4이고 L은 create_seqdataset함수에서 인수로 넣어준 3(sequnce_length)이고 마지막으로 N(batch_size)은 torch.utils.data.DataLoader안에 인수로 넣어주는 batch_size로 인해서 일정한 갯수로 배치를 나누었을때 나오는 배치들의 총 숫자임.rnn 문서에서 설명하는 batch_size는 torch.utils.dada.DataLoader에서 설정한 batch_size의 갯수만큼 데이터를 모아서 여러개의 배치로 만들었을때 나오는 배치의 총 갯수라고 보면됨.(헷갈리는 부분….)"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn 구현.html#학습-준비하기",
    "href": "posts/DL/Pytorch Rnn 구현.html#학습-준비하기",
    "title": "pytorch로 Rnn구현하기",
    "section": "학습 준비하기",
    "text": "학습 준비하기\n\ndefine architecture,loss,optimizer\ndata check\n\n\n#architecture,loss,optimizer \ntorch.manual_seed(2022)\nrnn = torch.nn.RNN(4,20,batch_first = True)\nlinr = torch.nn.Linear(20,4)\nloss_fn = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(list(rnn.parameters())+list(linr.parameters()),lr=1e-3)\n\n\nds = torch.utils.data.TensorDataset(x_data,y_data)\ndl = torch.utils.data.DataLoader(ds,batch_size=8,drop_last=True)\n\nfor idx,(x,y) in enumerate(dl):\n    if idx ==5:\n        break\n    print(x.shape,y.shape)\n\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\ntorch.Size([8, 3, 4]) torch.Size([8, 4])\n\n\n위에서 언급했듯이 데이터로더를 거쳐서 나오는 텐서는 RNN에 바로 입력될 것임. input.shape = \\((N,L,H_{in}) = (8,3,4)\\)"
  },
  {
    "objectID": "posts/DL/Pytorch Rnn 구현.html#모형학습",
    "href": "posts/DL/Pytorch Rnn 구현.html#모형학습",
    "title": "pytorch로 Rnn구현하기",
    "section": "모형학습",
    "text": "모형학습\n\nfor epoch in range(0,101):\n    for tr_x,tr_y in dl:\n        #1 output\n        hidden,hT = rnn(tr_x)\n        #print(hidden.shape)\n        output = linr(hT[-1])\n        #2 loss\n        loss = loss_fn(output,tr_y)\n        #3 derivative\n        loss.backward()\n        #4 update & clean\n        optimizer.step()\n        optimizer.zero_grad()\n    if epoch % 10 == 0:\n        print(f'epoch : {epoch},loss : {round(loss.tolist(),5)}')\n\nepoch : 0,loss : 1.31779\nepoch : 10,loss : 0.69453\nepoch : 20,loss : 0.19338\nepoch : 30,loss : 0.05891\nepoch : 40,loss : 0.02861\nepoch : 50,loss : 0.01791\nepoch : 60,loss : 0.0126\nepoch : 70,loss : 0.00947\nepoch : 80,loss : 0.00744\nepoch : 90,loss : 0.00602\nepoch : 100,loss : 0.00499\n\n\npytorch의 rnn을 거쳐서 나오는 output은 두 가지임. - hidden : 가장 깊이 위치한 히든레이어의 각각의 시점에서의 출력값을 모아놓은 텐서 - hT : 모든 히든레이어에의 마지막 시점(시점T)에서의 출력값을 모아놓은 텐서 - 외우기! 위치 : 가장깊은 <=> 모든 , 시점 : 각각의 <=> 마지막\n위와같은 설정에서는 가장 깊이 위치한 히든레이어의 마지막시점에서의 출력값만이 우리는 다음에올 문자열을 예측할 때 필요하므로 hT[-1]을 하여 그 값을 가져옴."
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory.html",
    "href": "posts/DL/[PRML 정독하기] Probability Theory.html",
    "title": "[PRML 읽기] 1 - 확률론 개요",
    "section": "",
    "text": "Figure1.9\n\n\n빨강색,파랑색 상자 중 하나를 선택하고 선택한 상자안에서 사과(초록) 또는 오렌지(주황)를 꺼낸다고 합시다. 여러번 반복했을 때, 빨강색 상자와 파랑색 상자가 선택된 비율이 각각 40%,60%라고 알려져 있는 상태입니다. 미래에 선택된 상자를 나타내는 변수를 B라고 하면 실제로 상자를 선택하기 전까지는 각각의 상자를 뽑을 확률(가능성)만이 존재하므로 변수B는 확률변수 입니다. 이 확률변수가 취할 수 있는 값은 r 또는 b로 두 가지 입니다. 마찬가지로 미래에 선택된 과일을 나타내는 확률변수 F를 놓을 수 있고 F가 취할 수 있는 값을 a 또는 o로 놓을 수 있습니다.\n빈도주의 관점에서 어떠한 사건이 발생할 확률은 매우 여러번 시행을 반복했을때, 어떤 사건이 나오는 경우의 비율(fraction,ratio)입니다. 예를 들어 주사위 눈이 3이나올 확률이 50%라고 하면 100번 던졌을때 50번정도는 3이 나오는 것으로 이해할 수 있습니다. 위의 문제에서 여러번 반복했을때 빨강색 또는 파랑색상자인 사건이 발생하는 경우가 각각 전체에서 40%,60%였다고 했으므로 이는 확률입니다. 또한 확률변수 B가 r을 취하는 사건에 대한 확률과 확률변수 B가 b를 취하는 사건에 대한 확률이라고 말하며 다음과 같이 적을 수 있습니다.\n\\[\\begin{aligned}\np(B = r) = 0.4 \\\\\np(B = b) = 0.6\n\\end{aligned}\\]\n각각의 상자를 선택하는 사건의 확률은 확률의 정의에 의해서 [0,1]사이의 구간에만 존재합니다. 또한 각각의 상자를 선택하는 사건은 상호베타적이면서 시행으로부터 나올 수 있는 모든 결과들 입니다. 그러므로 확률의 합은 1입니다.\n몇 가지 궁금한 점이 생겼습니다. “사과(초록)이나 오렌지(주황)가 나올 확률은?” 또는 “사과를 뽑았을 때 어떤 상자를 선택할 가능성이 높은지?”에 대해서 궁금합니다. 이는 sum rule과 product rule을 알아야 합니다.\n\n\n\n\n\n\nFigure1.9\n\n\n확률변수 \\(X,Y\\)가 존재하고 \\(X\\)가 취할 수 있는 값은 \\(x_i(i=1,2,\\dots,M)\\) \\(Y\\)가 취할 수 있는 값은 \\(y_j(j=1,2,\\dots,L)\\)라고 합시다. 총 \\(N\\)번을 시행했다고 할 때,시행의 결과 중 \\(X = x_i\\)이면서 동시에 \\(Y=y_i\\)인 경우는 \\(n_{ij}\\)번 나왔으며 \\(X = x_i\\)인 경우는 \\(c_i\\)번 나왔고 확률변수 \\(Y = y_j\\)인 경우는 \\(r_j\\)번이 나왔습니다.\n확률변수 \\(X = x_i\\)이고 \\(Y=y_j\\)인 사건이 동시에 발생할 확률을 \\(X=x_i,Y=y_j\\)일 때의 결합확률(joint probability)라고 합니다.\n\\[p(X = x_i,Y = y_j)\\]\n결합확률은 매우 여러번 시행했을 때, \\(X=x_i,Y=y_j\\)인 사건이 나오는 경우의 \\(n_{ij}\\) 비율입니다.\n\\[N \\rightarrow \\infty,\\,\\, p(X=x_i,Y=y_j) = \\frac{n_{ij}}{N}\\]\n시행으로부터 확률변수 \\(X = x_i\\)인 사건이 몇 번 나왔는지 알기위해서는 \\(c_i\\)는 \\(X = x_i,Y = y_j(\\text{for } j=1,2,\\dots,L)\\)인 사건이 발생하는 모든 경우를 전부 다 더해야 합니다. 예를 들어서 사과를 선택하는 \\(F = a\\)이 경우는 사과를 선택하고 상자가 파랑색 상자인 \\(F=a,B=r\\) 사건이 발생한 경우와 사과를 선택하고 상자가 빨강색 상자인\\(F=a,B=b\\) 사건이 발생한 경우이므로 시행으로부터 두 가지 케이스에 해당하는 모든 경우를 모두 세어야 합니다.\n\\[c_i = \\sum_{j=1}^{L}n_{ij}\\]\n결과적으로 ,\\(X = x_i\\)인 사건의 확률은 다음과 같습니다.\n\\[\\begin{aligned}\np(X = x_i) &= \\frac{c_{i}}{N} \\\\\n&=\\frac{\\sum_{j=1}^{L}n_{ij}}{N}\\\\\n&= p(X=x_i,Y=y_1) + p(X=x_i,Y=y_2) + \\dots + p(X=x_i,Y=y_L) \\\\\n&= \\sum_{j=1}^{L}p(X=x_i,Y=y_j) \\\\\n\\end{aligned}\\]\n위와 같이 하나의 확률변수에 대한 확률을 구할 때, 다른 확률변수와의 모든 결합확률을 더하여 구하는 법칙을 sum rule of probability라고 합니다. 이때 다른 확률변수와 결합확률을 marginalizing 또는 summing out하여 구하므로 marginal probability라고 합니다.\n시행의 결과가 \\(X=x_i\\)인 특정 조건을 만족하는 경우에 대해서만 고려해본다고 합시다. 조건에 맞는 경우안에서 \\(Y = y_j\\)인 사건이 나오는 횟수의 비는 \\(p(Y=y_j|X=x_i)\\)라고 표기하며 다음과 같습니다.\n\\[p(Y=y_j|X=x_i) = \\frac{n_{ij}}{c_j}\\]\n이를 \\(X=x_i\\)로 주어졌을 때, \\(Y=y_j\\)인 사건에 대한 조건부확률이라고 합니다. 조건부 확률의 경우 기호\\(|\\) 다음에 조건이 오며 분모에는 조건에 해당하는 사건이 나오는 경우가 몇 번인지 그 횟수에 값이 오지만 조건부확률이 아닌 그냥 확률의 경우 분모는 몇 번 시행했는지 입니다. 이러한 이유는 조건부확률은 일반적인 확률과 다르게 어떤 특정한 조건안에서 다른사건이 나오는 비율이기 때문입니다.\n위에서 정의한 조건부확률로 결합확률을 다시 적어보면 다음과 같습니다.\n\\[p(X=x_i,Y=y_j) = \\frac{n_{ij}}{N} = \\frac{n_{ij}}{c_i}\\frac{c_i}{N} = p(Y=y_j|X=x_i)p(X = x_i)\\]\n즉 결합확률은 \\(X=x_i\\)인 사건이 발생한 확률과 발생한 사건을 조건\\(X=x_i\\)으로하고 \\(Y=y_j\\)가 발생할 확률의 곱과 같습니다. 이는 어느정도 직관과 일치한다고 볼 수 있는데 예를 들자면 빨강색상자에서 사과를 뽑을 가능성은 먼저 빨강색상자를 고르고 그 다음 사과를 뽑을 가능성이기 때문입니다.\n\n\n\n위에서 적은 결합확률로부터 다음과 같은 식을 얻어낼 수 있습니다.\n\\[p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)}\\]\n이를 베이즈정리 라고 합니다. 베이즈 정리에서 \\(p(Y|X)\\)는 posterior probability(사후확률)로 어떤 조건 또는 증거가 발견되었을때의 확률입니다. \\(p(Y)\\)는 prior probability로 어떤 증거 또는 조건이 발견되기전의 확률입니다. 베이즈 정리로부터 우리는 posterior와 prior의 관계 즉,조건 또는 증거가 발견되기 전,후의 확률사이의 수식을 알 수 있습니다. 그러므로 사전확률을 알고 있다면 그 확률을 통하여 사후확률을 구할 수 있습니다.\n\\[p(Y|X) = \\frac{p(X|Y)p(Y)}{p(X)} = \\frac{p(X|Y)p(Y)}{\\sum_{Y}p(X,Y)} = \\frac{p(X|Y)p(Y)}{\\sum_{Y}p(X|Y)p(Y)}\\]\n분모를 sum rule과 product rule에 의하여 더 전개하면 위와 같습니다. 사후확률 \\(p(Y|X)\\)는 \\(X\\)라는 증거,조건이 주어질 때 결과 \\(Y\\)에 대한 확률이었습니다. 위의 수식을 곰곰히 보면 … \\(X\\)가 조건으로 주어질때 결과\\(Y\\)에 대한 조건부 확률을 구하기 위하여 \\(Y\\)가 주어질때의 \\(X\\)에 대한 조건부 확률로 계산합니다. 여기서 나타나는 베이즈 정리에서 핵심은 어떤 조건이 주어지고 결과에 대한 확률을 구할 때, 결과를 조건으로 조건을 결과로 역으로 바꾼 확률을 사용한다는 점입니다. 즉 \\(X\\)가 조건일 때, \\(Y\\)에 대한 조건부 확률이 잘 구해지지 않는다면 이를 뒤집어서 \\(Y\\)가 조건이고 \\(X\\)가 결과일때의 확률을 이용할 수 있습니다.\n베이즈 정리에서 분모는 normalization 상수로 모든 \\(Y\\)에 대하여 확률의 합이 1이 되도록 합니다.\n예시로 돌아가서 오렌지 또는 사과가 나올 확률과 사과를 골랐을 때 어떤 상자를 골랐을 확률이 높은지를 계산해봅시다. 각각의 경우에 대해 확률을 정리하면 다음과 같습니다.\n\\[\\begin{align}\n&p(B = r) = 0.4 \\\\\n&p(B = b) = 0.6 \\\\\n&p(F = a | B = r) = \\frac{1}{4}\\\\\n&p(F = o | B = r) = \\frac{3}{4}\\\\\n&p(F = a | B = b) = \\frac{3}{4}\\\\\n&p(F = o | B = b) = \\frac{1}{4}\\\\\n\\end{align}\\]\n(3)(4),(5)(6) 각각의 합은 normalization constant로 인해 합이 1이 되는것을 알 수 있습니다.\n이어서 원래 궁금했던 첫번째 문제인 사과 또는 오렌지가 나올 확률을 Product rule로 계산해볼 수 있습니다.\n\\[\\begin{aligned}\n&\n\\begin{aligned}\np(F = a) &= p(F=a,B=r) + p(F=a,B=b) \\\\\n&=p(F=a|B=r)p(B=r) + p(F=a|B=b)p(B=b) \\\\\n&=\\frac{1}{4}\\times\\frac{4}{10} + \\frac{3}{4}\\times\\frac{6}{10}\\\\\n&= \\frac{11}{20}\\\\\n\\end{aligned}\n\\\\\n&p(F=b) = 1-\\frac{11}{20}= \\frac{9}{20}\n\\end{aligned}\\]\n또다른 문제인 오렌지 또는 사과를 뽑았을때 어떤 박스를 선택했는지 알고 싶습니다. 즉 알고싶은 확률은 \\(p(B|F=a)\\) 또는 \\(p(B|F=o)\\)입니다. 그런데 우리에게 주어진 확률들은 사전확률인 \\(p(F)\\)와 조건과 결과가 역으로 뒤집힌 확률인 \\(p(F|B)\\)입니다. 그러므로 ,Bayes Rule을 사용하여 원하는 확률을 구할 수 있습니다.\n\\[\\begin{align}\n&p(B|F) = \\frac{p(F|B)p(F)}{p(B)} \\\\\n&p(B=r|F=o) = \\frac{p(F=o|B=r)p(B=r)}{p(F=o)} = \\frac{2}{3}\\\\\n&\\leftrightarrow p(B=b|F=o) = 1-\\frac{2}{3} = \\frac{1}{3}\n\\end{align}\\]\n오렌지를 확인하기전까지는 빨강색 박스일 확률이 절반이 안되는 0.4 였는데 오렌지를 확인하고는 \\(\\frac{2}{3}\\)로 확률이 상승했습니다. 이는 주어지는 정보,주건이 확률에 큰 영향을 미치는 것을 확인할 수 있습니다. 또한 구한 확률이 직관적으로 그림에서 확인할 수 있는 사실과도 일치함을 알 수 있습니다.\n\n\n\n두 확률변수가 독립이면 다음과 같습니다.\n\\[\\text{X and Y are independent random variables} \\iff p(X,Y) = p(X)p(Y)\\]\n예시에서 확인해봅시다.\n\\[\\begin{aligned}\n&p(B=r|F=o) = \\frac{2}{3} ,p(B=r) = \\frac{4}{10}\\\\\n&p(B=r|F=o) \\not = p(B=r) \\Longleftrightarrow \\text{X and Y are dependent}\n\\end{aligned}\\]\n만약 두 박스안에 들어있는 오랜지와 사과가 같은 비율로 들어있다고 한다면…\n\\[\\begin{aligned}\np(F=o | B=r) = p(F = o) \\\\\np(F=a | B=r) = p(F = a)  \\\\\np(F=o | B=b) = p(F = o)  \\\\\np(F=a | B=b) = p(F = a)\n\\end{aligned}\\]\n따라서 상자안에 있는 과일의 갯수가 같을 경우, 두 확률변수는 독립입니다.\n\n\n\n\n빈도주의 관점에서 확률은 매우여러번 시행했을 때, 어떤 사건이 발생하는(나오는) 비율(ratio)입니다.\nSum rule은 여러확률변수에 대한 결합확률이 주어질 때, 특정한 하나의 변수에 대한 확률(marginal probability)을 구함\n\n\\[p(X) = \\sum_Yp(X,Y)\\]\n\nProduct rule은 조건부확률과 주변확률사이의 곱으로 결합확률을 구함\n\n\\[p(X,Y) = p(Y|X)p(X) = p(X|Y)p(Y)\\]\n\n베이즈정리는 posterior(사후확률)를 prior(사전확률)과 조건과 결과가 바뀌었을때의 확률을 통해서 구합니다. 또한 사전확률과 사후확률사이의 관계(수식)입니다.\n\n\\[p(X|Y) = \\frac{p(Y|X)p(Y)}{p(X)} = \\frac{p(Y|X)p(Y)}{\\sum_Yp(X,Y)p(Y)}\\]\n\n두 확률변수가 독립일 경우 , 결합확률(분포)는 각각의 변수에 대한 (주변)확률의 곱입니다\n\n\\[\\text{X and Y are independent random variables} \\iff p(X,Y) = p(X)p(Y)\\]\n\n\n\n\n\n확률변수가 독립이라면 \\(Y\\)의 조건부확률은 조건\\(X\\)가 어떤 값이던간에 전혀 영향이 없습니다(독립적입니다).\n\\[\\begin{aligned}\n&p(Y|X) = p(Y) \\\\\n&\\leftrightarrow p(Y|X) = \\frac{p(X,Y)}{p(X)} = p(Y)\\\\\n&\\leftrightarrow p(X,Y) = p(X)p(Y)\n\\end{aligned}\\]\n반대로 해도 성립합니다.\n\n\n\n\n가장 쉽게 이해하는 베이즈 정리(Bayes’ Law)\nIndependent Random Variables\nPRML 1.2-probability theory"
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory2.html",
    "href": "posts/DL/[PRML 정독하기] Probability Theory2.html",
    "title": "[PRML 읽기] 2 - 확률론 개요(작성중 …)",
    "section": "",
    "text": "PRML을 읽고 정리한 내용입니다."
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory2.html#probability-density-function",
    "href": "posts/DL/[PRML 정독하기] Probability Theory2.html#probability-density-function",
    "title": "[PRML 읽기] 2 - 확률론 개요(작성중 …)",
    "section": "Probability density function",
    "text": "Probability density function\n확률밀도함수는 연속확률변수가 미소구간안에 속하는 사건에 대한 확률을 미소구간의 길이로 나눈 확률밀도값을 함숫값으로 가지는 확률함수로 정의합니다.\n\\[p(x) \\overset{\\Delta}{=} \\lim_{\\Delta x \\rightarrow 0}\\frac{p(x<X\\leq x+\\Delta x))}{\\Delta x}\\]\n확률밀도함수를 정적분하면 확률변수가 임의의 구간안에 속하는 사건에 대한 확률을 얻을 수 있습니다.(증명)\n\\[\\begin{aligned}\nP(a < X \\leq b) = \\int_{a}^{b}p(u)d(u)\n\\end{aligned}\\]\n확률밀도함수는 다음의 두 가지 조건을 만족해야 합니다. 첫번째 식은 확률(밀도)는 반드시 0보다 크거나 같음을 의미합니다. 두번째 식에서 확률변수는 반드시 \\((-\\infty,\\infty]\\)인 구간안에 속함을 의미합니다.\n\\[\\begin{align}\np(x) \\geq 0 \\\\\n\\int_{-\\infty}^{\\infty}f(t)dt = 1\n\\end{align}\\]"
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory2.html#probabiltiy-variable-transform",
    "href": "posts/DL/[PRML 정독하기] Probability Theory2.html#probabiltiy-variable-transform",
    "title": "[PRML 읽기] 2 - 확률론 개요(작성중 …)",
    "section": "Probabiltiy variable transform",
    "text": "Probabiltiy variable transform\n이 부분의 내용은 PRML에 있는 내용을 각색한 부분입니다. 틀린부분이 있다면 알려주세요!!\n연속확률변수 \\(X\\)의 확률밀도함수를 \\(p_X(x)\\)라 할 때, 변수를 변환하여 \\(X\\)를 \\(Y\\)에 관한 식\\(X = g(Y)\\)로 표현했다고 해봅시다. 목적은 확률변수 \\(Y\\)의 확률밀도함수 \\(p_Y(y)\\)를 얻는 것입니다. \\(\\Delta x\\rightarrow 0 \\Delta y \\rightarrow 0\\)이라고 한다면 다음이 성립합니다.\n\\[\\begin{aligned}\n&\\lim_{\\Delta x \\rightarrow 0}\\frac{p(x < X \\leq x + \\Delta x)}{\\Delta x} \\times \\Delta x \\overset{\\sim}{=} \\lim_{\\Delta y \\rightarrow 0}\\frac{p(y < Y \\leq y + \\Delta y)}{\\Delta y} \\times \\Delta y \\\\\n&\\Longleftrightarrow p_X(x)dx \\overset{\\sim}{=} p_Y(y)dy\n\\end{aligned}\\]\n윗식은 Jacobian factor에 의해 등식으로 바꿀 수 있습니다.\n\\[\\begin{aligned}\np_Y(y) &= p_X(x) \\begin {vmatrix} \\frac{dx}{dy} \\end {vmatrix} \\\\\n&= p_X(g(y))|g^{'}(y)|\n\\end{aligned}\\]\n확률변수의 변환은 확률분포함수를 최대화 하는 문제에서 유용하게 사용할 수 있다고 합니다. 변환할 변수를 선택하면 최대화해야하는 확률함수를 바꿀 수 있습니다."
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory2.html#sum-rule-product-rule-of-continuous-variable",
    "href": "posts/DL/[PRML 정독하기] Probability Theory2.html#sum-rule-product-rule-of-continuous-variable",
    "title": "[PRML 읽기] 2 - 확률론 개요(작성중 …)",
    "section": "Sum rule & Product rule of continuous variable",
    "text": "Sum rule & Product rule of continuous variable\n이산확률변수에 대해서는 Sum rule과 Product rule을 살펴봤었지만 연속확률변수 대해서는 보지 않았었습니다. 연속확률변수의 경우 다음과 같습니다. 엄밀한 증명은 measure theroy로 증명해야 하므로 .. 생략하겠습니다.(간략한 증명)\n\\[\\begin{aligned}\n&p(x) = \\int_y f(x,y)dy \\\\\n&p(x,y) = p(y|x)p(x)\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory2.html#expectations-and-variances",
    "href": "posts/DL/[PRML 정독하기] Probability Theory2.html#expectations-and-variances",
    "title": "[PRML 읽기] 2 - 확률론 개요(작성중 …)",
    "section": "Expectations and Variances",
    "text": "Expectations and Variances\n함수의 기댓값(또는 평균)은 함숫값이 어떤 값을 중심으로 분포하는지를 알려줍니다. 가능한 모든\\(x\\)에 대하여 함숫값과 그때의 확률분포의 값을 곱하여 얻은 가중평균입니다.\n\\[\\begin{aligned}\n&\\mathbb{E}[f] = \\sum_x p(x)f(x) \\quad \\text{If X is a discrete R.V} \\\\\n&\\mathbb{E}[f] = \\int_x p(x)f(x)dx \\quad \\text{If X is a continuous R.V}\n\\end{aligned}\\]\n표본의 크기가 무한할 경우, 표본으로 부터 구한 함숫값의 평균과 기댓값은 값이 같습니다. 이를 통해서 확률분포의 기댓값을 알 수 있다면 표본이 적당히 크기가 클 경우 함숫값이 어느정도 일지 대략적으로 예측할 수 있습니다.\n\\[\\mathbb{E}[f] = \\lim_{N \\rightarrow \\infty}\\frac{1}{N}\\sum_{n=1}^{N}f(x_n)\\]\n다변수함수는 여러개의 변수를 가지는 함수입니다. 따라서 각각의 변수가 따르는 확률분포중에서 하나를 선택하여 그때의 확률분포와 함숫값의 기댓값을 구할 수 있습니다. 이때 기댓값은 나머지 확률변수에 대한 함수가 됩니다.\n\\[\\mathbb{E}_x[f(x,y)] = f(y)\\]\n함수의 조건부 기댓값은 조건부 확률분포와의 가중평균으로 정의할 수 있습니다. \\(y\\)가 조건으로 주어질 때, \\(x\\)의 조건부 기댓값은 다음과 같습니다.\n\\[\\mathbb{E}_x[f|y] = \\sum_x{p(x|y)}{f(x)}\\]\n확률변수 \\(f(x)\\)의 분산(variance)는 함수가 기댓값을 중심으로 얼마나 퍼져있는지 알려줍니다. 편차제곱의 기댓값(평균)으로 정의합니다.\n\\[\\begin{aligned}\n&\\mathbb{E}[x] = \\int_xxp(x)dx \\text{ or } \\sum_xxp(x)\\\\\n&\n\\begin{aligned}\n\\text{var}[x]\n&= \\mathbb{E}[(x - \\mathbb{E}[x])^2] \\\\\n&= \\mathbb{E}[x^2] - \\mathbb{E}[x]^2\\\\\n\\end{aligned}\n\\end{aligned}\\]\n두 개의 확률변수에 대해서 공분산은 다음과 같습니다.(2번째 식에 대한 전개)\n\\[\\begin{align}\n\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}] \\\\\n&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}[x]\\mathbb{E}[y]\n\\end{align}\\]"
  },
  {
    "objectID": "posts/DL/[PRML 정독하기] Probability Theory2.html#appendix",
    "href": "posts/DL/[PRML 정독하기] Probability Theory2.html#appendix",
    "title": "[PRML 읽기] 2 - 확률론 개요(작성중 …)",
    "section": "Appendix",
    "text": "Appendix\n\n확률밀도함수에 관한 여러가지 증명\n누적분포함수는 연속확률변수가 \\((-\\infty,x]\\)인 구간안에 속할 확률입니다.\n\\[F(x) = P(-\\infty<X\\leq x)\\]\n따라서,연속확률분포의 분자를 누적분포함수로 나타낼 수 있습니다. 이는 누적분포함수의 도함수가 확률밀도함수이며 누적분포함수의 기울기,변화율이 확률밀도함수임을 나타냅니다.\n\\[p(x) = \\lim_{\\Delta x \\rightarrow 0}\\frac{p(x<X\\leq x+\\Delta x))}{\\Delta x} = \\lim_{\\Delta x \\rightarrow 0}\\frac{F(x+\\Delta x) - F(x)}{\\Delta x} = \\frac{dF}{dx}\\]\n누적분포함수의 도함수가 확률밀도함수이므로 확률밀도함수의 적분은 누적분포함수입니다.\n\\[\\int_{-\\infty}^{x}f(t)dt = F(x) = P(-\\infty<X\\leq x)\\]\n임의의 구간 \\((a,b]\\)사이에 확률변수 \\(X\\)가 속하는 사건에 대한 확률은 다음과 같습니다.\n\\[\\begin{aligned}\nP(a < X \\leq b) &= P(-\\infty < X \\leq b) - P(-\\infty < X \\leq a) \\\\\n&= F(b) - F(a) \\\\\n&= \\int_{-\\infty}^{b}f(t)dt - \\int_{-\\infty}^{a}f(t)dt \\\\\n&= \\int_{a}^{b}f(t)dt\n\\end{aligned}\\]\n\n\n공분산 전개하기\n\\[\\begin{aligned}\n\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[\\{x-\\mathbb{E}[x]\\}\\{y-\\mathbb{E}[y]\\}] \\\\\n&=\\mathbb{E}_{x,y}[xy - x\\mathbb{E}[y] - y\\mathbb{E}[x] + \\mathbb{E}[x]\\mathbb{E}[y]]\\\\\n&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}_{x,y}[x\\mathbb{E}[y]] - \\mathbb{E}_{x,y}[y\\mathbb{E}[x]] + \\mathbb{E}[x]\\mathbb{E}[y]]\\\\\n\\end{aligned}\\]\n여기서 \\(\\mathbb{E}_{x,y}[x\\mathbb{E}[y]]\\)는 다음과 같다.\n\\[\\begin{aligned}\n\\int_{\\infty}^{\\infty}\\int_{\\infty}^{\\infty}x\\mathbb{E}[y]p(y)p(x)dydx &=  \\int_{\\infty}^{\\infty}x\\mathbb{E}[y]p(x)\\bigg(\\int_{\\infty}^{\\infty}p(y)dy\\bigg)dx \\\\\n&= \\int_{\\infty}^{\\infty}x\\mathbb{E}[y]p(x)dx \\\\\n&= \\mathbb{E}[y]\\int_{\\infty}^{\\infty}xp(x)dx \\\\\n&= \\mathbb{E}[y]\\mathbb{E}[x]\n\\end{aligned}\\]\n마찬가지로 \\(\\mathbb{E}_{x,y}[y\\mathbb{E}[x]]\\)도 같은 값을 가진다. 따라서 다음과 같다.\n\\[\\begin{aligned}\n\\text{cov}[x,y] &= \\mathbb{E}_{x,y}[xy] - \\mathbb{E}_{x,y}[x\\mathbb{E}[y]] - \\mathbb{E}_{x,y}[y\\mathbb{E}[x]] + \\mathbb{E}[x]\\mathbb{E}[y]]\\\\\n&= \\mathbb{E}_{x,y}[xy] - \\mathbb{E}[y]\\mathbb{E}[x] - \\mathbb{E}[x]\\mathbb{E}[y] + \\mathbb{E}[x]\\mathbb{E}[y] \\\\\n&=\\mathbb{E}_{x,y}[xy] - \\mathbb{E}[x]\\mathbb{E}[y]\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/jujurery/시험보기 1주일 전 체크해야 할 것.html",
    "href": "posts/jujurery/시험보기 1주일 전 체크해야 할 것.html",
    "title": "[주저리주저리] 시험보기 1주일 전 꼭 읽어야 하는 것",
    "section": "",
    "text": "이 글은 확률과정론 시험을 보고 충격받아서 다시는 이런 실수를 반복하지 않기위해 쓰는 글이다. 언젠가 또 이 글을 볼 미래의 나에게 말해주자면 만약 이러한 것들 중 하나라도 딱 하나라도 지금 너가 하고 있다면 그 시험(시험,발표 등등 고난)은 매우 위험하며 결코 성공하지 못할 것이다. 그 순간 넌 이미 자신에게 지나치게 관대한 인간이며 열심히 하는 척 연기하는 사람이다.\n\n절대로 다 안다고 생각하지 말기(자만,가장 위험한 생각)\n\n아마 공부를 좀 해봤을 수도 있다.\n그래서 “이 정도면 열심히 했다.”,“이 정도면 다 알지”,“그건 좀 쉽잖아” 라는 생각이 들 수 있다.\n가장 위험한 생각이다.\n이런 생각은 내 마음속에 들어가있는 순간부터 앞으로 매 순간 최선을 다 하지 못하게 만들기 때문이다.\n제일 버려야 할 생각이며 가장 쓸모없다.\n대충,어느정도만 하게 만들며 계속해서 쌓이다가 결국 나중에 큰 문제가 되어 터지고 만다.\n또한 가장 가소로운 생각이기도 하다.\n“그건 좀 쉽잖아?” 내가 뭘 안다고 그걸 평가하는가? 내가 뭘 평가할 수준은 되는가?\n“더 이상 할거 없는데”라고 생각이 들 수 있다. 그럴때는 이번에 완전 처음 공부해 본 개념 or 많이 막혔던 내용 or 바로 대답 못하고 머릿속을 한 번 거쳐야 하는 내용 or 밑바닥이 되는 내용 or 문제 만들어서 풀기 등등… 모든 수단을 가동해서 더 공부해야 할 부분을 찾는다,\n\n\n\n시험기간은 원래 지옥이다.\n\n시험기간은 모두에게 다 힘든 시간이다. 근데 자꾸 벗어나서 편하게 하려하지 말자.\n시험기간인데 편하고 싶어? 그럼 시험 끝나고가 지옥일 거야…\n그냥 미리 맞는거라 생각하자. 그래도 성적 잘 맞고 지옥인게 좋잖아?\n\n\n\n허용된 부분하에서 최대한 쉽게 시험 만들기\n\n뭐든지 간에 어느정도 까지 허용된 부분이 있을 것이다.(확률과정론의 경우 내가 20분을 소요했던 1,2번이 그냥 집에서 적어올 수 있는 문제였다.)\n허용된 부분을 최대한 활용하자. 최대한 쉽게 시험을 쉽게 만들자.\n어디까찌 허용이 되는지 세세히 알기 위해서라도 평소에 열심히 듣는 것이 중요하다. 자만은 금물이며 시간을 헛되이 보내지 말자\n\n\n\n 몇 번씩 봤는지 체크하기\n\n어디를 몇 번씩 보며 체크해야 할지가 좀 추상적일 수 있다.\n시험이라면 봐야할 범위에 있는 여러 단원,부분,문제가 있고 발표준비라면 ppt의 여러 슬라이드가 될 수 있고 등등 뭐냐에 따라서 여러가지 섹션별로 나눠진다.\n그 중 가장 최소가 되는 단위로 나눠서 반드시 체크한다.\n이렇게 해야 어디를 많이 봤고 어디를 적게 봤는지 알 수 있다.\n또한 잘 했던 것과 못했던 것 나눠서 다른 모양으로 체크하자. 이렇게 하면 어디가 많이 막혔는지,잘 안받아 들여지는지 알 수 있다.\n\n\n\n 3~4일전 무조건 전자기기 멀리하기\n\n뇌를 공부이외의 다른 것들로부터 비우는 것이 목적이다.\n시험때 자꾸 노래 생각나서 망함(HOT 캔디 확률과정론 시험 때 자꾸 생각나서 멸망…)\n유튜브가 문제인 듯 함. 유튜브에서 노래가 우연히 나와가지고 좋아서 계속들음\n그냥 아예 전자기기들 자체를 멀리 하자.\n쓸 수 있는 경우는 강의 들을 때이거나 연락온 것 체크해야 할때(이것도 반드시 시간을 정해놓자)\n사실 3~4일 이라고 해놨지만 중요하게 급히 처리해야 할 일이 있을때는 그냥 멀리하는게 답이다.\n\n\n\n 평소에 가장 시야에 뭔가 없는 곳에 앉자\n\n뒷 자리나 사람들이 있는 곳에 앉지마라. 앞 자리에 앉아라\n집중력을 분산시키고 특히 시험때 다른사람이 빨리 풀거나 하는 것을 보면 조급해지기 때문이다.\n최대한 아무도 없는 온전히 집중할 수 있는 곳을 찾아서 그 장소에서 풀자.(잘 푸는 사람 옆이나 뒤는 특히 반드시 피하자.)"
  },
  {
    "objectID": "posts/Linear Algebra/Ax = b/Ax = b.html",
    "href": "posts/Linear Algebra/Ax = b/Ax = b.html",
    "title": "[Linear Algebra] 4.Ax=b의 해의 갯수",
    "section": "",
    "text": "유투브 - 혁펜하임님의 선형대수학강의를 정리하기 위해 작성한 글입니다.Rank와 Null space로 Ax=b의 해의 갯수를 파악합니다.\n연립일차방정식은 행렬 Ax=b로 나타내고 행렬의 랭크와 열공간을 기반으로 해의 갯수를 나타낼 수 있습니다.\n\n열공간을 기반으로 한 Ax=b의 해석\n행렬\\(A = \\begin{bmatrix}a_1 & a_2 & \\dots & a_n\\end{bmatrix} \\in \\mathbb{R}^{m \\times n}\\)와 벡터\\(x = \\begin{bmatrix}x_1,x_2,\\dots,x_n\\end{bmatrix}^T\\in \\mathbb{R}^{n \\times 1}\\)의 곱을 생각해보자. 행렬과 벡터의 곱은 다음과 같이 열벡터의 일차결합으로 바라볼 수 있다.\n\\[\\begin{aligned}\nAx = \\begin{bmatrix}a_1 & a_2 & \\dots & a_n\\end{bmatrix}\n\\begin{bmatrix}\nx_1\\\\\nx_2\\\\\n\\vdots\\\\\nx_n\n\\end{bmatrix}\n= a_1x_1 + a_2x_2 + \\dots a_nx_n\n\\end{aligned}\\]\n행렬 A의 열공간은 열벡터들의 선형생성이다.\n\\[\\begin{aligned}\n&\\text{C}(A) = \\text{span}(a_1,a_2,\\dots,a_n) = \\{c_1a_1 + c_2a_2 + \\dots c_na_n|c_1,c_2,\\dots,c_n \\in K\\}\n\\end{aligned}\\]\n그러므로, 행렬 \\(A\\)의 열공간은 임의의 \\(x\\)에 대하여 가능한 \\(Ax\\)의 모든 집합과 같다. 가능한 모든 스칼라\\(x_1,x_2,\\dots,x_n\\)로 벡터의 일차결합이 이루는 집합은 생성(span)과 같기 때문이다.\n\\[\\text{C}(A) = \\text{span}(a_1,a_2,\\dots,a_n) = \\{Ax |x\\in K^n\\}\\]\n만약 \\(Ax = b\\)인 방정식의 해 \\(x\\)를 구하려 한다 하자. \\(\\text{C}(A)\\)는 가능한 \\(Ax\\)의 모든 집합이였으므로 만약 \\(\\text{C}(A)\\)가 \\(b\\)를 포함한다면(즉,\\(b\\)가 열공간의 원소라면) \\(Ax=b\\)인 \\(x\\)가 존재하여 방정식의 해가 존재하고 \\(\\text{C}(A)\\)가 \\(b\\)를 포함하지 않는다면 \\(Ax = b\\)인 \\(x\\)값이 존재하지 않고 따라서 방정식의 해가 존재하지 않는다.\n\n\nCase1 : A가 full column rank일 때\n문제 : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) 일때, \\(Ax = b\\)를 만족하는 x의 갯수는?\n 위의 문제에서 A는 full column rank이다. full column rank일 경우 \\(Ax\\)의 모양은 위와 같고 열공간은 m차원 벡터공간의 부분공간이자 n차원 벡터공간이다.앞에서 “m차원 벡터공간의 부분공간인 n차원 벡터공간” 이라 했는데 그 이유는 (열)벡터가 m차원 벡터공간의 원소(벡터)이기 때문이다. m차원 벡터공간에 있는 (열)벡터 n개를 기저로 생성된 공간이기 때문에 m차원 벡터공간의 부분공간이면서 동시에 n차원 벡터공간이 된다. \n위에서 언급했듯이 열공간은 임의의 x에 대하여 가능한 \\(Ax\\)의 집합이기에 방정식의 해가 존재하려면 b가 열공간안에 있으면 되고 아니면 바깥에 있으면 된다. 그렇다면 이 경우 b의 위치는 어떻게 될까?\n\n가능한 b의 위치는 2가지이다. 1의 경우는 열공간안에 b가 없는 경우다. 이 경우 b는 m차원 벡터공간에는 있으면서(\\(b \\in R^{m \\times 1}\\)이기에 당연하다) 부분공간인 n차원 열공간안에는 없게된다. 따라서 이 경우 해는 존재하지 않는다. 2의 경우는 열공간안에 b가 있는 경우다. 이 경우 b는 m차원 벡터공간에 속하면서 동시에 부분공간인 n차원 벡터공간에도 속한다.\n이렇게 생각하면 끝난 것 같은데 한가지 더 생각해야 할 것이 있다. 바로 null space이다. 만약 Ax = b를 만족하는 하나의 해인 \\(x_{particular}\\)가 있다고 해보자. 이때 \\(Ax=0\\)을 만족하는 널공간의 임의의 원소인 \\(x\\)를 \\(x_{null}\\in N(A)\\)이라고 하면 다음이 성립한다.\n\\[A(x_{particular} + x_{null}) = Ax_{particular} + Ax_{null} = b\\]\n윗식에 의해서 null space의 원소인 \\(x_{null}\\)과 \\(x_{particular}\\)의 합도 방정식의 해이므로 해를 구할때 null space도 확인해야 한다. null space에 대한 x를 추가한 완전해는 다음과 같다. \\[x_{complete} = x_{particular} + x_{null}\\]\n그렇다면 x_{null}을 어떻게 확인할 수 있을까? 랭크-널리티 정리로 확인할 수 있는데 위와같이 full column rank인 경우는 다음과 같다.\n\\[\\begin{aligned}\n&\\text{rank}(A) + \\text{nulity(A)} = n\\\\\n&\\Longleftrightarrow \\text{nulity(A)} = n - rank(A) = n - n = 0\n\\end{aligned}\\]\n랭크-널리티 정리에 의하여 null space의 차원이 0임을 확인했다. 차원이 0인 널공간(벡터공간)은 \\(\\{\\bf{0}\\}\\)이므로 \\(x_{null} \\in N(A)\\)인 \\(x_{null} ={\\bf 0}\\)이다. 따라서 2번의 경우, \\(x_{particular}\\)가 존재하여 \\(Ax = 0\\)일 경우의 완전해는 다음과 같다. \\[\\therefore  x_{total} = x_{particular} + x_{null} = x_{particular} + {\\bf 0} = x_{particular}\\]\n결론적으로, full column rank일 경우 해가 존재하지 않거나 단 하나 존재한다.\n\n\nCase2 : A가 full row rank일 때\n문제 : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = m<n\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) 일때, \\(Ax = b\\)를 만족하는 x의 갯수는?\n 위의 문제에서 A는 full row rank이다. 위해서 한 것처럼 먼저 \\(C(A)\\)와 \\(b\\)가 어떻게 위치하고 있을지 파악하고 null space를 따져 완전해를 구하는 것이다. 먼저 \\(C(A)\\)를 생각해보자. \\(C(A)\\)는 행렬의 랭크가 m이기에 n개의 (열)벡터 중 선형독립인 column vector는 m개 뿐이다. 따라서 벡터의 span인 열공간은 m차원 벡터공간이다. (n개의 열벡터가 존재하지만,선형종속이기떄문이다.)\n이전문제와 다른점도 존재하는데 바로 열공간이 (열)벡터가 존재하는 벡터공간의 부분공간이 아니라는 점이다. 이 문제에서 각각의 열벡터는 m차원 공간의 벡터이고 열벡터의 생성도 m차원 벡터공간이기때문에 열벡터가 존재하는 바로 그 공간이 열공간이다.\n위와 같이 열공간에 대해서 생각했으면 이제 b에 대해서 생각해볼 차례다. b의 위치는 어떻게 될까? b는 A의 열공간에 속하는 벡터일까 아닐까?\n\nb의 경우 \\(b \\in \\mathbb{R}^{m \\times 1}\\)이기에 열공간에 속하는 벡터이다. 그러므로 full row rank인 경우는 반드시 해가 존재한다.\n여기서 완전해를 구하기 위해서 null space도 생각해야한다. 랭크-널리티 정리에 의해 다음과 같다.\n\\[\\begin{aligned}\n&rank(A) + nulity(A) = n \\\\\n&\\Longleftrightarrow nulity(A) = n - rank(A) =  n - m\n\\end{aligned}\\]\n랭크정리에 의해서 null space는 n-m차원의 벡터공간이다. 이 경우 가능한 \\(x_{null}\\)은 무한히 많이 존재하므로 해가 무수히 많다. 완전해는 다음과 같다.\n\\[\\begin{aligned}\n\\therefore\\,\\,  &x_{complete} = x_{particular} + x_{null}\\\\\n&\\text{where, } x_{null} \\in N(A) = \\mathbb{R}^{n-m}\n\\end{aligned}\\]\n결론적으로, full row rank의 경우 해는 무수히 많다.\n\n\nCase3 : A가 full rank일 때\n문제 : \\(A \\in \\mathbb{R}^{m \\times m},\\text{rank}(A) = m\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) 일때, \\(Ax = b\\)를 만족하는 x의 갯수는?\n\n사실 이 문제의 해는 다음과 같다. \\[x = A^{-1}b\\] 그러므로,full rank인 경우 해의 갯수가 1개이다.\n위처럼 간단하게 해를 구할 수 있지만 … 그래도 기하학적으로 생각해보기 위에서 했던 것처럼 따져보자. column space는 m차원 벡터공간이다. column vector는 column space 그 자체인 m차원 벡터공간의 원소이다. \\(b\\in \\mathbb{R}^{m \\times 1}\\)의 경우 m차원 벡터공간의 원소이다. 그러므로, column space는 반드시 b를 포함하며 그림으로 표현하면 다음과 같다.\n\n널공간을 따지기 위해 랭크-널리티 정리를 사용해 보면 다음과 같다.\n\\[\\begin{aligned}\n&\\text{rank}(A)+\\text{nulity}(A) = m\\\\\n&\\Longleftrightarrow \\text{nulity}(A) = m - \\text{rank}(A) = m - m = 0\n\\end{aligned}\\]\nnull space는 차원이 0이므로 \\(N(A) = \\{{\\bf 0}\\}\\)이고 다음과 같다.\n\\[x_{complete} = x_{particular} + x_{null} = x_{particular} + {\\bf 0} = x_{particular}\\]\n결과적으로, full rank인 경우 해는 반드시 존재하며 갯수는 1개이다.\n\n\nCase4 : A가 rank deficient 일 때\n문제 : \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = \\alpha < \\text{min}(m,n)\\,,x = \\in \\mathbb{R}^{n \\times 1}\\,,b\\in \\mathbb{R}^{m \\times 1}\\) 일때, \\(Ax = b\\)를 만족하는 x의 갯수는?\n\n위의 예시에서는 A가 rank deficient인 경우 중, \\(m<n\\) 인 경우를 생각해보자. 행렬 A의 column space는 m차원 공간의 부분공간이자 \\(\\alpha\\)차원의 벡터공간이다.\\(b \\in \\mathbb{R}^{m \\times 1}\\)이므로 가능한 경우는 아래와 같다.\n\n1번의 경우라면 b는 column space의 원소가 아니므로 방정식의 해는 존재하지 않는다. 만약 2번의 경우라면 해가 존재한다. 이때에는 null space를 고려한 완전해를 따져야하므로 랭크-널리티 정리를 확인한다.\n\\[\\begin{aligned}\n&\\text{rank}(A) + \\text{nulity}(A) = \\alpha + \\text{nulity}(A) = n \\\\\n&\\Longleftrightarrow \\text{nulity}(A) = n - \\alpha > 0\n\\end{aligned}\\]\n랭크정리에 의해서 null space는 \\(n - \\alpha\\)차원의 벡터공간이다. 이 경우 null space의 임의의 원소인 \\(x_{null}\\)는 무한히 많이 존재하므로 해가 무수히 많다. 완전해는 다음과 같다.\n\\[\\begin{aligned}\n\\therefore\\,\\,  &x_{complete} = x_{particular} + x_{null}\\\\\n&\\text{where, } x_{null} \\in N(A) = \\mathbb{R}^{n-\\alpha}\n\\end{aligned}\\]\n결론적으로, rank deficient의 경우 해는 무수히 많거나 해는 존재하지 않는다.\n\n\n정리\n\n\n\n\n\n\n\n\nrank type\nexpression\n해의 갯수\n\n\n\n\nfull column rank\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) = n<m\\)\n해가 없거나 해가 한개만 존재한다.\n\n\nfull row rank\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) = m<n\\)\n해가 무수히 많다.\n\n\nfull rank\n\\(A \\in \\mathbb{R}^{m \\times m}\\),\\(\\text{rank}(A) = m\\)\n해가 한개만 존재한다.\n\n\nrank deficient\n\\(A \\in \\mathbb{R}^{m \\times n}\\),\\(\\text{rank}(A) < \\text{min}(m,n)\\)\n해가 없거나 해가 무수히 많다.\n\n\n\n\n\n참고자료**\n혁펜하임 - [선대] 2-11강. Ax=b 의 해의 수 알아내기 프린키피아"
  },
  {
    "objectID": "posts/Linear Algebra/derivative/벡터,행렬미분.html",
    "href": "posts/Linear Algebra/derivative/벡터,행렬미분.html",
    "title": "[Linear Algebra] Appendix",
    "section": "",
    "text": "다변수 스칼라함수를 생각해보자. 즉 2개이상의 변수가 input이며 ouput은 하나의 스칼라인 함수이다.\n만약 함수를 이루는 모든 변수가 각각 조금씩 바뀐다면? \\(\\to\\) 함숫값도 작지만 아주 조금 변화할 것이다.\n전미분은 위와 같은 상황 즉, 다변수 함수의 모든 변수가 조금씩 변화했을때 함숫값의 변화량을 근사하는 양이다.(from wikipedia)\n\n다변수함수의 미소증분\n\n\n\n\n\n\n\n\nDefinition of total derivative\n\n\n\n\\[\\begin{aligned}\n&df = \\frac{\\partial{f}}{\\partial{x_1}}dx_1+\\frac{\\partial{f}}{\\partial{x_2}}dx_2+\\dots+\\frac{\\partial{f}}{\\partial{x_n}}dx_n \\\\\n&\\text{where } f : \\mathbb{R}^n \\rightarrow \\mathbb{R}\n\\end{aligned}\\]\n\n\n추가설명(엄밀한 notation 제외)\n\n편미분 \\(\\frac{\\partial{f}}{\\partial{x}}\\)는 임의의 변수 x의 단위변화량이다.\n\nx를 매우 미소하게 증가시키고 : \\(x \\overset{+dx}{\\to} x+dx\\)\n이에 따라서 f가 얼마나 변화했는지 그 양을 측정한 뒤 : \\(f(x+dx) - f(x)\\)\n“x가 1만큼 증가하면 얼마나 f는 변화해?”를 측정한 값(비율)이다. : \\(\\frac{f(x+dx) - f(x)}{dx}\\)\n\n하나의 변수 x의 매우 조그마하고 순간적인 변화에 의해 함숫값의 변화가 아주 조금 변할 때\n이러한 조그마한 변화가 변수가 1만큼 증가하는 것을 기준으로 삼는 경우에는 얼마정도 일지 측정한 단위(길이에 대한)변화량이다.\n느낌 : 이런 순간적인 변화를 1만큼 계속 유지하면 얼마나 변화할까?\n\n정리하자면 임의의 단일변수의 미소변화에 대한 단위변화량이다.(딱딱한 정의)\n여러개의 변수가 있지만 하나의 변수만 조금 변화시킨뒤 측정한다.(그래서 편미분이다.)\n사실 식이 엄밀하진 않다.위의 표현은 상미분에 대한 표현이다.\n자세한 표현식은 wiki를 참조하자.\n\n단위변화량을 어떤 값과 곱해주면 변화량이 나온다.\n\n밥을 100g먹으면 몸무게가 5kg 찐다고 하자.(ㅜㅜ억울한 상황이다.)\n그렇다는 것은 1g먹으면 0.05kg이 찐다는 것을 암시하며\n이는 밥(x) 1g에 대한 몸무게(f)의 단위변화량은 0.05kg이다. \\(\\Longleftrightarrow= \\frac{\\partial f}{\\partial x} = 0.05\\)\n\n밥 300g 먹으면? \\(\\frac{\\partial f}{\\partial x}300 = 0.05 \\times 300 = 15\\)\n밥 0.03g 먹으면? \\(\\frac{\\partial f}{\\partial x}0.03 = 0.05 \\times 0.03 = 0.0015\\)\n밥 \\(db\\)만큼(진짜조금) 먹으면? \\(\\frac{\\partial f}{\\partial x}db = 0.05 \\times db\\)\n\n\n변화를 시키는 여러가지 요인을 전부 고려하면?\n\n밥(\\(x_1\\))도 조금먹고 물도 조금(\\(x_2\\))먹고 조금 자다가(\\(x_3\\)) 운동도 조금(\\(x_4\\))하면 몸무게(f)가 어떻게 될까??\n\\(df = \\frac{\\partial{f}}{\\partial{x_1}}dx_1 + \\frac{\\partial{f}}{\\partial{x_2}}dx_2 + \\frac{\\partial{f}}{\\partial{x_3}}dx_3 + \\frac{\\partial{f}}{\\partial{x_4}}dx_4\\)\n\n\n\n\n\n추후 정리"
  },
  {
    "objectID": "posts/Linear Algebra/derivative/벡터,행렬미분.html#definition",
    "href": "posts/Linear Algebra/derivative/벡터,행렬미분.html#definition",
    "title": "[Linear Algebra] Appendix",
    "section": "Definition",
    "text": "Definition\n\n다음과 같이 길이가 2인 벡터가 input이고 하나의 값을 output하는 스칼라함수를 생각해보자.\n이 함수는 다른 시각으로 2개의 변수를 input으로 받는 다변수 스칼라 함수로 생각할 수도 있다.\n\n\\[\\begin{aligned}\nf\\big(\\begin{bmatrix}x_1 \\\\ x_2\\end{bmatrix}\\big)\n= f(x_1,x_2)\n\\end{aligned}\\]\n\n위와 같은 스칼라함수의 벡터에 대한 미분은 다음과 같이 정의된다.\n\n\n\n\n\n\n\n다변수 스칼라함수의 벡터미분에 대한 정의\n\n\n\n\\[\\begin{aligned}\n&\\frac{\\partial{f}}{\\partial{x^T}} := \\begin{bmatrix}\\frac{\\partial{f}}{\\partial{x_1}} & \\frac{\\partial{f}}{\\partial{x_2}} & \\dots & \\frac{\\partial{f}}{\\partial{x_n}}\\end{bmatrix}\\\\\n&\\text{where } f:\\mathbb{R}^n \\to \\mathbb{R}\\,,x = \\begin{bmatrix}x_1,x_2,\\dots,x_n\\end{bmatrix}^T\n\\end{aligned}\\]\n\n\n\n왜 \\(\\partial{x^T}\\)일까?\n\ninput은 column vector인데 output은 row vector이다.\n그래서 눕혀서 transpose\n\n그런데 사실 많은 곳에서 notation을 헷갈리게 한다.\n\n미분의 결과를 column vector로 적는 곳도 많고\n\\(\\partial{x^T}\\)대신 \\(\\partial{x}\\)로 적는 곳도 많다.\n그래서 그냥 함수를 이루는 모든 변수에 대해서 미분해본뒤 모아놓은 거구나 이렇게 기억해놓는게 좋다."
  },
  {
    "objectID": "posts/Linear Algebra/derivative/벡터,행렬미분.html#쉽게-미분-구하기",
    "href": "posts/Linear Algebra/derivative/벡터,행렬미분.html#쉽게-미분-구하기",
    "title": "[Linear Algebra] Appendix",
    "section": "★쉽게 미분 구하기★",
    "text": "★쉽게 미분 구하기★\n\n정석적으로 구하려면 위의 정의에 해당하는 모든 component를 각각 편미분 취해서 구해줘야 한다.\n이건 너무 번거로운 과정이며 더 쉽게 구하는 방법이 있다.\n\\(df\\)는 정의는 다음과 같다\n\n\\[\\begin{aligned}\n&df := f(x+ dx) - f(x)\\\\\n&\\text{where } f:\\mathbb{R}^n \\to \\mathbb{R},x = \\begin{bmatrix}x_1,x_2,\\dots,x_n\\end{bmatrix}^T,dx = \\begin{bmatrix}dx_1,dx_2,\\dots,dx_n\\end{bmatrix}^T\n\\end{aligned}\\]\n\n또한 \\(df\\)의 total derivative는 정의에 의해 다음과 같다.(두 번째 정의라고 생각해도 무방하다.)\n\n\\[\\begin{aligned}\ndf &:= \\frac{\\partial{f}}{\\partial{x_1}}dx_1+\\frac{\\partial{f}}{\\partial{x_2}}dx_2+\\dots+\\frac{\\partial{f}}{\\partial{x_n}}dx_n \\\\\n&= \\begin{bmatrix}\\frac{\\partial{f}}{\\partial{x_1}} & \\frac{\\partial{f}}{\\partial{x_2}} & \\dots & \\frac{\\partial{f}}{\\partial{x_n}}\\end{bmatrix}\n\\begin{bmatrix}dx_1\\\\dx_2\\\\ \\vdots \\\\ dx_n \\end{bmatrix}\\\\\n&= \\frac{\\partial{f}}{\\partial{x}^T}dx\n\\end{aligned}\\]\n\n두 정의를 모아서 정리해보자.\n\n\\[\\begin{align}\ndf &= f(x+ dx) - f(x) \\\\\n&= \\frac{\\partial{f}}{\\partial{x}^T}dx\n\\end{align}\\]\n\n그래서 어떻게 구할까?\n\n(1)을 통해서 \\(df\\)를 계산한 후\n(2)와 같은 형식을 어떻게든 만들고\ndx앞에 있는 항이 원하는 미분이다.\n\n주의\n\n\\(f(x+dx)\\)를 계산하는 과정에서 \\(dx^Tdx,dx_1dx_2\\)와 같은 2차 이상의 항이 나올 수 있다.\n이건 그냥 제외시키면 된다. 즉, 1차항만을 남기고는 싹 지우면 된다.\n왜 그럴까?(엄밀하지 않은 증명)\n\n\\(df = f(x+dx) - f(x) = dx^Tdx = dx^2\\)라 해보자.(dx^2처럼 쓸 수 있다.)\n여기서 \\(dx\\)가 벡터가 아니라 스칼라라고 생각해보면 \\(dx\\)로 양변을 나눌 수 있다.\n양변을 \\(dx\\)로 나누고 (2)에 의하여 \\(\\frac{df}{dx} = dx = \\frac{\\partial f}{\\partial x^T}\\)이다.\n미분의 정의에 의해 \\(dx \\to 0\\) 이므로 \\(dx^2\\)와 비슷한 2차이상의 항에 대한 \\(\\frac{\\partial{f}}{\\partial{x^T}} = 0\\)이다.\n따라서 2차이상의 항이 나오면 \\(\\frac{\\partial{f}}{\\partial{x^T}} = 0\\)이 될 것이므로 제외를 시켜줘도 된다.\n사실 벡터미분이기 때문에 나눠줄 수 없어서 엄밀한 증명은 아니다.\n\n\n\n\n예시\n\n\\(f(x) = (y-Ax)^T(y-Ax)\\)일때 \\(\\frac{\\partial{f}}{\\partial{x^T}}\\)를 구해보자."
  },
  {
    "objectID": "posts/Linear Algebra/eigendecomposition.html",
    "href": "posts/Linear Algebra/eigendecomposition.html",
    "title": "[Linear Algebra] 6-2.EVD & Property",
    "section": "",
    "text": "유투브 - 혁펜하임님의 선형대수학강의를 정리하기 위해 작성한 글입니다."
  },
  {
    "objectID": "posts/Linear Algebra/eigendecomposition.html#대각화-관련-동치명제-정리",
    "href": "posts/Linear Algebra/eigendecomposition.html#대각화-관련-동치명제-정리",
    "title": "[Linear Algebra] 6-2.EVD & Property",
    "section": "대각화 관련 동치명제 정리",
    "text": "대각화 관련 동치명제 정리\n\n\\(A \\in \\mathbb{R}^{n \\times n}\\)이라 했을때 다음의 명제는 동치이다.\n\n\\[\\begin{aligned}\n&A\\text{가 diagonalizable하다.} \\\\\n&\\longleftrightarrow A \\text{가 eigen decomposition이 가능하다.}\\\\\n&\\longleftrightarrow V^{-1}\\text{를 구할 수 있다.} \\\\\n&\\longleftrightarrow \\text{det(V)가 존재한다.} \\\\\n&\\longleftrightarrow \\text{V에 존재하는 n개의 vector는 선형독립이다.} \\\\\n&\\longleftrightarrow A \\text{에 대한 선형독립인 eigenvector가 n개 존재한다.}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Linear Algebra/eigenvalue eigenvector/eigenvalue eigenvector.html",
    "href": "posts/Linear Algebra/eigenvalue eigenvector/eigenvalue eigenvector.html",
    "title": "[Linear Algebra] 6-1.Eigenvalue & Eigenvector",
    "section": "",
    "text": "유튜브 - 혁펜하임님의 선형대수학 강의 정리용 입니다."
  },
  {
    "objectID": "posts/Linear Algebra/eigenvalue eigenvector/eigenvalue eigenvector.html#example",
    "href": "posts/Linear Algebra/eigenvalue eigenvector/eigenvalue eigenvector.html#example",
    "title": "[Linear Algebra] 6-1.Eigenvalue & Eigenvector",
    "section": "Example",
    "text": "Example\n\n\n그림과 같은 선형변환을 생각해보자.\n대부분의 경우 선형변환된 다음 그 방향,크기는 제멋대로 변한다.\n그러나 보라색벡터와 초록색벡터는 각각 3,1만큼 크기만 늘어나며 방향이 바뀌지 않는 단순한 스칼라배임을 알 수 있다.\n따라서 보라색벡터 \\([-1,1]^T\\),초록색벡터 \\([1,1]^T\\)는 \\(A\\)의 eigenvector이며 대응하는 eigenvalue는 각각 3,1임을 알 수 있다.\n위에서는 초록색 보라색만 찾았지만 사실 이 경우 eigenvector는 무한히 많다.\n또다른 어떤 선형변환의 경우에는 아예 존재하지 않는 경우도 있다.\n그림으로 보면 아래와 같다.(사실 영상으로 보는게 훨씬 이해가 잘된다. 혁펜하임님 유튜브 가서 한번 보길 추천해요)\n\n\n\n\n\n\n\nbefore\n\n\n\n\n\n\n\nafter"
  },
  {
    "objectID": "posts/Linear Algebra/Least Squares/Least Squares.html",
    "href": "posts/Linear Algebra/Least Squares/Least Squares.html",
    "title": "[Linear Algebra] 5.Least Squares",
    "section": "",
    "text": "\\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m,x \\in \\mathbb{R}^{n \\times 1},b \\in \\mathbb{R}^{m \\times 1}\\) 가 주어지고 방정식 \\(Ax = b\\)를 만족하는 해인 \\(x\\)를 구할 수 없을 때, \\(Ax\\)가 \\(b\\)와 가장 비슷하게 하는 \\(x\\)를 찾는 것이 목적입니다."
  },
  {
    "objectID": "posts/Linear Algebra/Least Squares/Least Squares.html#projection-matrix",
    "href": "posts/Linear Algebra/Least Squares/Least Squares.html#projection-matrix",
    "title": "[Linear Algebra] 5.Least Squares",
    "section": "projection matrix",
    "text": "projection matrix\n위해서 구한 \\(\\hat{x}\\)를 \\(A\\hat{x}\\)에 대입하면 다음과 같습니다. \\[A\\hat{x} = (A^TA)^{-1}A^Tb\\]\n위 식은 우변의 \\(b\\)에 \\(A(A^TA)^{-1}A^T\\)를 곱하여 \\(C(A)\\)에서 \\(b\\)와 가장 비슷하면서(거리가 가장 가까우면서) \\(b\\)를 \\(C(A)\\)에 정사영(projection) 한 벡터 \\(A\\hat{x}\\)을 얻음을 의미합니다. 따라서 \\(A(A^TA)^{-1}A^T\\)를 projection matrix라 부르고 \\(p_A\\)로 표기합니다."
  },
  {
    "objectID": "posts/Linear Algebra/nmf/non-negative-matrix-factorization.html",
    "href": "posts/Linear Algebra/nmf/non-negative-matrix-factorization.html",
    "title": "HIHO",
    "section": "",
    "text": "# Import basic library\nfrom matplotlib import pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport cv2\nfrom PIL import Image\nfrom os import listdir\nimport time\nfrom sklearn.datasets import fetch_olivetti_faces\nfrom sklearn import cluster\nfrom sklearn import decomposition\nfrom numpy.random import RandomState\nimport matplotlib.pyplot as plt\n\n\nrng = RandomState(0)\nfaces, _ = fetch_olivetti_faces(return_X_y=True, shuffle=True, random_state=rng)\nn_samples, n_features = faces.shape\n\ndownloading Olivetti faces from https://ndownloader.figshare.com/files/5976027 to C:\\Users\\22668\\scikit_learn_data\n\n\n\nfaces.shape\n\n(400, 4096)\n\n\n\nimg_shape = (64, 64)\nplt.imshow(faces[0].reshape(img_shape),cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x1e86be33160>\n\n\n\n\n\n\nimplementation\n\ndef nmf(X,p,n_iter=None):\n    m = X.shape[0]; n = X.shape[1]\n    W = np.random.rand(m,p) #W.shape = (m,p),rand함수 사용해서 반드시 0보다 큰 값으로 초기화 해야함!\n    H = np.random.rand(p,n) #H.shape = (p,n)\n    #print(X.shape);#print(W.shape);#print(H.shape)\n    if n_iter is not None:\n        for i in range(n_iter):\n            W = W * ((X@H.T)/(W@H@H.T))\n            H = H * ((W.T@X)/(W.T@W@H))\n            \n    return X,H,W\n    \nX,H,W = nmf(faces,p = 40,n_iter = 2000)\n\n\nplt.imshow(H[16].reshape(img_shape),cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x1e8701cfca0>\n\n\n\n\n\n\nplt.imshow(X[0].reshape(img_shape),cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x1e8517cd1f0>\n\n\n\n\n\n\n_t = np.zeros(img_shape[0]*img_shape[1])\nfor i in range(len(W[0])):\n    _t = _t + W[0][i] * H[i]\n\nplt.imshow(_t.reshape(img_shape),cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x1e851836430>"
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html",
    "href": "posts/Linear Algebra/orthonormal matrix.html",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "",
    "text": "수튜브님의 직교대각화가능(orthogonal diagonalizability)을 보고 정리한 내용입니다."
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#orthonormal-matrix",
    "href": "posts/Linear Algebra/orthonormal matrix.html#orthonormal-matrix",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "Orthonormal matrix",
    "text": "Orthonormal matrix\nIn linear algebra, an orthogonal matrix, or orthonormal matrix, is a real square matrix whose columns and rows are orthonormal vectors. (from wikipedia)\n\n행벡터들과 열벡터들이 orthogonal(직교) + normal(크기가 1)인 정사각행렬을 orthonormal matrix라 한다.\northogonal matrix 또는 orthonormal matrix는 같은 표현이다.\n그러나 직교 + 크기가 1인 벡터가 열벡터이기에 orthogonal + normal = orthonormal이 더 의미를 살리는 듯하여 여기서는 orthonormal matrix용어를 사용한다.\n\n\nexpression 1\n\northonormal matrix \\(Q\\)을 나타내는 첫 번째 표현은 아래와 같다.\n\n\\[\\begin{aligned}\n&Q^TQ = QQ^T = I \\\\\n&\\text{where } I \\text{ is the identity matrix}\n\\end{aligned}\\]\n\n왜 저런 표현식이 나올까?\n이는 정의를 함축하고 아주 잘 함축하고 있는 표현이다.\n임의의 \\(Q \\in \\mathbb{R}^{n \\times n}\\)가 다음과 같다고 해보자.\n\n\\[\\begin{aligned}\n&Q =\n\\begin{bmatrix}\nq_1 & q_2 & \\dots q_n\n\\end{bmatrix}\n\\end{aligned}\\]\n\n여기서 \\(q_i \\in \\mathbb{R}^{n \\times 1}(i=1,2,\\dots,n)\\)은 \\(Q\\)의 column vector이다.\n먼저 \\(Q^TQ = I\\)를 확인해보자.\n\n\\[\\begin{aligned}\nQ^TQ =\n\\begin{bmatrix}\nq_1^T \\\\ q_2^T \\\\ \\vdots \\\\ q_n^T\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1 & q_2 & \\dots & q_n\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nq_1^T,q_1 & q_1^Tq_2 & \\dots & q_1^Tq_n \\\\\nq_2^T,q_1 & q_2^Tq_2 & \\dots & q_2^Tq_n \\\\\n\\vdots & \\vdots & \\vdots & \\vdots \\\\\nq_n^T,q_1 & q_n^Tq_2 & \\dots & q_n^Tq_n\\\\\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n1 & 0 & \\dots & 0\\\\\n0 & 1 & \\dots & 0\\\\\n\\vdots & \\vdots & \\ddots & \\vdots \\\\\n0 & 0 & \\dots & 1\\\\\n\n\n\\end{bmatrix}\n\\end{aligned}\\]\n\n좌우변이 같기때문에 아래가 유도된다.\n\n\\[\\begin{aligned}\nq_i^Tq_j =\n\\begin{cases}\n1 \\quad (i = j)\\\\\n0 \\quad (\\text{otherwise})\n\\end{cases}\n\\text{for all }i \\in (1,\\dots,n),\\,j \\in (1,\\dots,n)\n\\end{aligned}\\]\n\n이는 같은 벡터끼리 내적을 하면 1이고 다른벡터끼리 내적하면 0임을 의미한다.\n즉, 임의의 벡터\\(q_i\\)의 크기는 1인 unit vector이며 열벡터들은 orthogornal 하다는 것이다.\n\n\\[\\begin{aligned}\n&\\text{for all }i \\in (1,...,n), j \\in (1,...,n) \\\\\n&|q_i| = \\sqrt{q_i} = \\sqrt{dot(q_i,q_i)} = 1 \\\\\n&dot(q_i,q_j)=0 \\longleftrightarrow q_i\\perp q_j\n\n\\end{aligned}\\]\n\n동일한 과정을 행벡터에 대해서도 수행하면 행벡터들도 orthonormal하다는 것을 보일 수 있다.\n\n\n\nexpression 2\n\nexpression1으로부터 orthonormal matrix에 관한 다음의 식도 끌어낼 수 있다.\n\n\\[\\begin{aligned}\n&Q^T = Q^{-1} \\\\\n\\end{aligned}\\]\n\n즉,어떤 행렬의 transpose가 그것의 inverse와 같으면 orthonormal matrix라는 것이며 이것또한 orthonormal matrix의 다른 표현이다.\n왜 그런가 하면 expression1에서 \\(Q^TQ = QQ^T = I\\)였다. 그러므로 \\(Q\\)의 역행렬\\(Q^{-1}\\)는 \\(Q^T\\)와 같다. (Q의 역행렬은 \\(Q^{-1}Q = QQ{-1} = I\\)를 만족하는 행렬이다.)"
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#orthogonally-similar",
    "href": "posts/Linear Algebra/orthonormal matrix.html#orthogonally-similar",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "orthogonally similar",
    "text": "orthogonally similar\n\n다음과 같은 식을 만족하면 C는 A와 orthogonally similar(직교닮음)이다.\n\n\\[\\begin{aligned}\n&C = Q^{-1}AQ \\\\\n&\\text{where } Q^{-1} = Q^T\n\\end{aligned}\\]\n\n여기서 \\(Q^{-1} = Q^T\\)는 orthonomal matrix의 정의이다.\n윗 식에 의해서 A도 C와 orthogonally similar임을 이끌어낼 수 있다.\n\n\\[\\begin{aligned}\n&A = QCQ^{-1} \\\\\n&\\text{where } Q^{-1} = Q^T\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#orthogonally-diagonalize",
    "href": "posts/Linear Algebra/orthonormal matrix.html#orthogonally-diagonalize",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "orthogonally diagonalize",
    "text": "orthogonally diagonalize\n\n임의의 정사각행렬 \\(A,D\\)에 대해서 다음이 만족한다고 해보자.\n\n\\[\\begin{aligned}\n&\n\\begin{aligned}\nD &= Q^{-1}AQ \\\\\n&= Q^TAQ \\\\\n\\end{aligned}\n\\\\\n&\\text{where } D = diag(d_1,d_2,\\dots,d_n) , Q^{-1} = Q^T\n\\end{aligned}\\]\n\n이와 같이 \\(A\\)와 직교닮음인 대각행렬 \\(D\\)가 존재할때 행렬\\(A\\)가 직교대각화 되었다 라고 한다.\n또한 이때 정사각행렬 \\(A\\)는 \\(P^{-1}AP\\)가 대각행렬\\(D\\)가 되게하는 가역행렬이자 orthornomal matrix인 \\(P\\)가 존재하기때문에 “행렬\\(A\\)는 orthogonally diagonalizable하다”라고 말한다.\n\\(Q^T\\)와 \\(Q\\)의 자리를 바꿔서 써도 된다. 즉 \\(Q^{-1}\\)를 새로운 Q로 본다면 다음과 같이 적을 수도 있다.\n\n\\[\\begin{aligned}\nD &= QAQ^{-1}\\\\\n&= QAQ^T\\\\\n&\\text{where } D = \\text{diag}(d_1,d_2,\\dots,d_n),Q^{-1} = Q^T\n\\end{aligned}\\]\n\n단지 notation의 차이일 뿐이다. 역행렬의 관계이기 때문이다."
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#명제1-a가-orthogonally-diagonalizable-rightarrow-a-at",
    "href": "posts/Linear Algebra/orthonormal matrix.html#명제1-a가-orthogonally-diagonalizable-rightarrow-a-at",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "명제1 : \\(A\\)가 orthogonally diagonalizable \\(\\rightarrow A = A^T\\)",
    "text": "명제1 : \\(A\\)가 orthogonally diagonalizable \\(\\rightarrow A = A^T\\)\n\n행렬\\(A\\)가 orthogonally diagonalizable하다고 하자. 즉 \\(D = P^TAP\\)를 만족하는 직교행렬 \\(P\\)가 존재한다. 이때 \\(A^T\\)를 전개하면 다음과 같다.\n\n\\[\\begin{aligned}\n&A = PDP^{-1} = PDP^T\\\\\n&A^T = (PDP^T)^T = PD^TP^T = PDP^T = A\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#명제2-a-at-rightarrow-a는-orthogonally-diagonalizable",
    "href": "posts/Linear Algebra/orthonormal matrix.html#명제2-a-at-rightarrow-a는-orthogonally-diagonalizable",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "명제2 : \\(A = A^T \\rightarrow\\) \\(A\\)는 orthogonally diagonalizable",
    "text": "명제2 : \\(A = A^T \\rightarrow\\) \\(A\\)는 orthogonally diagonalizable\n\n수학적 귀납법으로 아래의 두 가지를 증명하면 된다.\n\n\\(A \\in \\mathbb{R}^{1 \\times 1}\\) 일때 직교대각화가 가능하다.(1)\n\\(A \\in \\mathbb{R}^{(n-1)\\times (n-1)}\\)때 직교대각화가 가능하다고 가정하고 \\(A \\in \\mathbb{R}^{n \\times n}\\)때 직교대각화가 가능함을 보인다.(2)\n임의의 \\(n\\) 그리고 \\(n-1\\)에 대해서 성립함을 증명했고 1에대해서 증명했기 때문에 …\n\n\n\\[\\begin{aligned}\n&\\mathbb{R}^{1 \\times 1} \\rightarrow \\mathbb{R}^{2 \\times 2}\\\\\n&\\mathbb{R}^{2 \\times 2} \\rightarrow \\mathbb{R}^{3 \\times 3}\\\\\n&\\mathbb{R}^{3 \\times 3} \\rightarrow \\mathbb{R}^{4 \\times 4}\\\\\n&\\quad\\quad\\quad\\vdots\\\\\n&\\mathbb{R}^{n-1 \\times n-1} \\rightarrow \\mathbb{R}^{n \\times n}\\\\\n&\\quad\\quad\\quad\\vdots\\\\\n\n\\end{aligned}\\]\n\n이렇게 증명하면 모든 \\(n\\) 결국은 모든 대각행렬에 대해서 직교대각화가 가능함을 보일 수 있다.\n느낌\n\n하나의 증명을 위해서 모든 자연수에 대해 딸려있는 증명들을 해야되네?! \\(\\rightarrow\\) 작은 것부터 차례차례 쓰러뜨려보자! \\(\\rightarrow\\) 작은거 하나하나 다하기 힘드니까 임의의 n에 대해서 해보자.!\n(1)을 증명하고 (2)만 증명하면 모든 n에 대해서 증명할 수 있어서 (2)를 증명해서 뚫는 느낌.\n도미노 : 하나를 쓰러뜨리면 그 다음이 쓰러지는 것을 알고있어(증명했어). 증명해야 될 문제는 맨 처음부터 맨 마지막까지 전부 쓰러뜨리면 되는문제야. 맨 처음이 무너진다면 다음에 오는 모든 것들이 무너질꺼야.\n\n\n먼저 \\(A \\in \\mathbb{R}^{1 \\times 1}\\) 일때 직교대각화가 가능함을 보인다. 먼저 \\(A\\)와 \\(P\\)행렬을 잡는다.\n\\[\\begin{aligned}\n&A = \\begin{bmatrix}a\\end{bmatrix}\n\\in \\mathbb{R}^{1 \\times 1} \\\\\n&P = \\begin{bmatrix}1\\end{bmatrix}\n\\in \\mathbb{R}^{1 \\times 1}\n\\end{aligned}\\]\n여기서 \\(A=A^T\\)인 대칭행렬이며 \\(P\\)의 경우 \\(P^{-1} = P^T\\)가 성립하는 orthonomal matrix이다.\n\\[\\begin{aligned}\nD = P^{-1}AP = \\begin{bmatrix}a\\end{bmatrix}\n\\end{aligned}\\]\nD는 대각행렬이다. 따라서 \\(D = P^{-1}AP\\)를 만족하는 가역행렬이자 직교행렬인\\(P\\)와 대각행렬\\(D\\)가 존재하므로 행렬\\(A \\in \\mathbb{R}^{1 \\times 1}\\)는 orthogonally diagonalizable하다.\n\nD는 대각행렬이다. 따라서 \\(D = P^-1AP\\)를 만족하는 가역행렬이자 직교행렬인\\(P\\)와 대각행렬\\(D\\)가 존재하므로 행렬\\(A \\in \\mathbb{R}^{1 \\times 1}\\)는 orthogonally diagonalizable하다.\n정리2 자세한 증명 생략…\n따라서 어떤 행렬이 대칭행렬 \\(\\Longleftrightarrow\\)직교대각화가 가능하다."
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#symmetric-matrix의-evd와-orthogonally-diagonalizable의-연관성",
    "href": "posts/Linear Algebra/orthonormal matrix.html#symmetric-matrix의-evd와-orthogonally-diagonalizable의-연관성",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "Symmetric matrix의 EVD와 orthogonally diagonalizable의 연관성",
    "text": "Symmetric matrix의 EVD와 orthogonally diagonalizable의 연관성\n\n잠깐 넘어가기 전에 중요한 중요한 사실 하나를 짚고 넘어가자.\n우리는 정리를 통해서 Symmetric matrix는 직교대각화가 가능하다는 사실을 알고 있다.\n하지만 이러한 사실만 알고있을뿐 실제로 어떻게 \\(P\\)와 \\(D\\)를 구하는지는 알지 못했다.\n대칭행렬의 경우 직교대각화는 EVD를 통해 구현할 수 있다. 이는 정방행렬의 경우와 유사하다.(정방행렬의 EVD와 orthogonally diagonalizable 참고)\n\n정방행렬의 경우 대각화는 EVD를 통해서 구현할 수 있었다.\n대칭행렬의 경우 직교대각화는 EVD에서 \\(V^{-1} = V^T\\)인 V가 구해지기에 구현할 수 있다."
  },
  {
    "objectID": "posts/Linear Algebra/orthonormal matrix.html#신기하고-중요한-사실들",
    "href": "posts/Linear Algebra/orthonormal matrix.html#신기하고-중요한-사실들",
    "title": "[Linear Algebra] 6-3.EVD of symmetric matrix",
    "section": "신기하고 중요한 사실들",
    "text": "신기하고 중요한 사실들\n\n신기한 사실1 : 대칭행렬 A는 rank1 매트릭스들의 가중합으로 표현할 수 있다.\n\nEVD를 계속해서 전개해보면 다음과 같다.\n\n\\[\\begin{aligned}\nA &= V\\Lambda V^T =\n\\begin{bmatrix}\nq_1 & q_2 & q_3\n\\end{bmatrix}\n\\begin{bmatrix}\n\\lambda_1 & 0 & 0 \\\\\n0 & \\lambda_2 & 0 \\\\\n0 &  0 & \\lambda_3\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1^T \\\\ q_2^T \\\\ q_3^T\n\\end{bmatrix}\n=\n&\\begin{bmatrix}\n\\lambda_1q_1 & \\lambda_2q_2 & \\lambda_3q_3\n\\end{bmatrix}\n\\begin{bmatrix}\nq_1^T \\\\ q_2^T \\\\ q_3^T\n\\end{bmatrix}\\\\\n&= \\lambda_1q_1q_1^T + \\lambda_2q_2q_2^T + \\lambda_3q_3q_3^T =\n\\sum_{i=1}^3 \\lambda_iq_iq_i^T\n\\end{aligned}\\]\n\n여기서부터 재미난 사실을 알 수 있다.\n\\(q_iq_i^T\\)는 rank가 1인 matrix이다.\n마지막 수식은 이러한 rank-1 matrix들을 적절하게 상수배하고 더하여 합을 취하면 \\(A\\)가 나온다는 것이다. 상수가 각각에 대응하는 rank-1 matrix \\(q_iq_i^T\\)의 중요도,기여도를 고려한 가중치라고 생각한다면?\n다음과 같은 해석을 할 수 있다.\n\n어떤 정방행렬\\(A\\)는 여러개의 rank-1 matrix들이 섞인거야.\n그런데 그냥 섞인건 아니고 중요한건 많이 안중요한건 적게 섞은 것이 정방행렬이야!\n그때의 중요도는 \\(\\lambda_i\\)가 알려줘.\n\n그림으로 보면 이런느낌이다.! 그림\n만약에 데이터의 크기가 너무 커서 압축을 해야하는 상황이라면 이를 응용할 수 있다.\n\n이미지의 크기가 너무크다. 조금 사이즈를 줄이고 싶다.\n중요도 \\(\\lambda_i\\)가 상대적으로 작은 matrix들은 조금 제외를 해도 괜찮겠지?\n실제로 된다. 조금 줄여도 여전히 인식할 수 있는 정도이다.(조금 화질이 안좋아지긴 한다.)\n\n\n\n\n신기한 사실2 : 선형변환이 대칭행렬이라면 입력벡터는 직교하는 고유벡터로 분해하고 적절하게 줄이고 늘려서 재조합한 된다.\n\n\n위의 그림과 같이 선형변환이 Symmetric matrix \\(A = A^T\\)인 경우를 고려해보자.\n\\(A\\)를 EVD하고 rank-1 matrix의 가중합으로 표현했을때 \\(x\\)에 대한 선형변환 \\(Ax\\)는 다음과 같다.\n\n\\[Ax = \\lambda_1q_1q_1^Tx + \\lambda_2q_2q_2^Tx + \\lambda_3q_3q_3^Tx\\]\n\n\\(x \\in \\mathbb{R}^3\\)에 대하여 \\(q_i^Tx\\)는 각각의 직교하는 3차원 공간의 또다른 정규직교기저에 대한 \\(x\\)의 성분을 의미한다.\n\\(q_i(i=1,2,3)\\)로 표현된 \\(x\\)는 다음과 같다.\n\n\\[\\begin{aligned}\nx = (q_1^Tx)q_1 + (q_2^Tx)q_2 + (q_3^Tx)q_3 = \\begin{bmatrix}q_1^Tx \\\\ q_2^Tx \\\\ q_3^Tx\\end{bmatrix}\n\\end{aligned}\\]\n\n간단하게 말해서 \\(q_1,q_2,q_3\\)를 포함하도록 \\(x\\)를 분해했다고 보면된다.\n\n\n\n그럼 \\(Ax\\)는?? \\(\\to Ax\\)는 직교기저로 \\(x\\)를 분해하고 각 성분에 대해 적절한 스칼라배를 취해서 다시 조합하여 섞은 것으로 이해할 수 있다.\n\n첫번째 성분인 \\((q_1^Tx)q_1\\)은 \\(\\lambda_1\\)만큼 곱한다. \\(\\to \\lambda_1q_1q_1^Tx\\)\n두번째 성분인 \\((q_2^Tx)q_2\\)은 \\(\\lambda_2\\)만큼 곱한다. \\(\\to \\lambda_2q_2q_2^Tx\\)\n세번째 성분인 \\((q_3^Tx)q_3\\)은 \\(\\lambda_3\\)만큼 곱한다. \\(\\to \\lambda_3q_3q_3^Tx\\)\n곱한것들을 +해서 섞으면 \\(x\\)를 대칭행렬로 선형변환한 \\(Ax\\)이다.\n\n즉, \\(Ax = \\lambda_1q_1q_1^Tx + \\lambda_2q_2q_2^Tx + \\lambda_3q_3q_3^Tx\\)이다."
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "",
    "text": "딥러닝공부방,soohee410님의 블로그을 읽고 정리한 내용입니다."
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html#definition",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html#definition",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "Definition",
    "text": "Definition\n벡터\\({\\bf{x}}\\)의 이차형식(Quadratic form)은 다음과 같이 정의한다.\n\n\n\n\n\n\nDefinition of quadratic form\n\n\n\n\\[\\begin{aligned}\n&Q({\\bf{x}}) := {\\bf{x}}^TA{\\bf{x}}\\\\\n&\\text{where }\n\\begin{aligned}\n&A = A^T \\in \\mathbb{R}^{n \\times n},x \\in \\mathbb{R}^{n \\times 1}\n\\end{aligned}\n\\end{aligned}\\]\n\n\n\n행렬\\(A\\)를 이차형식의 행렬(matrix of quadratic form)이라고 하며 symmetric matrix이다."
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html#example",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html#example",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "Example",
    "text": "Example\n\\({\\bf{x}} = [x_1,x_2]^T\\) ,\\(A = \\begin{bmatrix}3 & -2 \\\\ -2 & 7\\end{bmatrix}\\)일때, quadratic form을 구해보자.\n\\[\\begin{aligned}\n&Q({\\bf{x}}) = {\\bf{x}}^TA{\\bf{x}} =\n\\begin{bmatrix}\nx_1 & x_2\n\\end{bmatrix}\n\\begin{bmatrix}3 & -2 \\\\ -2 & 7\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\ x_2\n\\end{bmatrix}\n= \\begin{bmatrix}\nx_1 & x_2\n\\end{bmatrix}\n\\begin{bmatrix}\n3x_1-2x_2 \\\\ -2x_1 + 7x_2\n\\end{bmatrix}\n= 3x_1^2 -4x_1x_2 + 7x_2^2\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html#problem-setting",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html#problem-setting",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "Problem Setting",
    "text": "Problem Setting\n\n위의 그림은 이차형식을 좌표계에서 표현한 것이다. 각각의 이차형식에서 원점으로부터 거리가 가장 먼 좌표를 찾아보자.\n\n초록색 quadtratic form의 경우 직관적으로 \\(x_2 = 0\\)을 대입하여 거리가 가장 먼 좌표를 바로 구할 수 있다.\n그러나 빨강색 quadtratic form의 경우 거리가 가장 먼 곳의 좌표를 구하기가 쉽지않다.\ncrossproduct term인 \\(4x_1x_2\\)가 존재로 인해서 현재의 좌표축을 기준으로 회전된 타원이 존재하기 때문이다.\n\n\n만약 빨강색 quadratic form이 위와 같이 새로운 축을 기준으로 표현된다면 어떨까?\n\n새로운 축을 기준으로 빨강색 타원은 회전되지 않은 형태이기에\ncross product항이 제거되어 가장 먼 곳의 좌표를 직관적으로 \\(y2=0\\)을 대입하여 쉽게 구할 수 있다.\n이와 같이 축을 바꿔서 하여 cross product항을 제거하면 최소 또는 최대가 되는 좌표를 바로 보다 쉽게 구할 수 있게 해줘서 최적화 관점에서 이점이 많다.\n\n그렇다면 주어진 식을 새로운 축을 기준으로 간다하게 바꾸려면 어떻게 해야 뭘까?\n\n이는 이차형식의 변수변환을 활용하면 된다."
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html#method",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html#method",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "Method",
    "text": "Method\n\n이차형식의 변수변환에서 \\(\\bf{x}\\)는 \\(\\bf{{\\bf{y}}}\\)와 대칭행렬 \\(A\\)의 고유벡터의 모음인 \\(P\\)의 곱이라고 가정한다.\n이때 \\(P\\)는 행렬\\(A\\)가 대칭행렬이며 따라서 직교대각화가 가능하므로 \\(P\\)는 \\(P^{-1} = P^T\\)를 만족하는 orthonormal matrix이다.\n\n\\(A=A^T\\)인 대칭행렬은 직교대각화가 가능하며 고유벡터들은 서로간에 직교한다는 사실을 기억하자.\n\n\n\\[\\begin{aligned}\n&{\\bf{x}} = P{\\bf{{\\bf{y}}}} \\\\\n&{\\bf{{\\bf{y}}}}=P^{-1}{\\bf{x}}\\\\\n&\\text{where }P^{-1} = P^T\n\\end{aligned}\\]\n\n고유벡터들의 집합인 \\(B = \\{p_1,p_2,\\dots,p_n\\}\\)를 구성해보자.\n\\(B\\)는 \\(\\mathbb{R}^n\\)공간의 표준기저 \\(\\{e_1,e_2,\\dots,e_n\\}\\)가 아닌 또다른 정규직교기저이다.(B의 span은 \\(\\mathbb{R}^n\\)이기 때문이다.)\n\n그렇다면 여기서 \\({\\bf{y}}\\)는 뭘까?\n\n\\({\\bf{x}}\\)를 나타낼 수 있는 정규직교기저이자 \\(A\\)의 고유벡터 집합인 \\(B\\)로 표현한 좌표\\([{\\bf{x}}]_B\\)이다.\n\n\\[\\begin{aligned}\n&{\\bf{x}} = P{\\bf{y}}\\\\\n&\\Longleftrightarrow {\\bf{x}} = p_1y_1 + p_2y_2 + \\dots + p_ny_n\\\\\n&\\Longleftrightarrow {\\bf{y}} = [{\\bf{x}}]_B\n\\end{aligned}\\]\n위에서 가정한 사실을 이차형식에 대입해보자\n\\[\\begin{aligned}\n&{\\bf{x}}^TA{\\bf{x}} = (P{\\bf{y}})^TA(P{\\bf{y}}) = {\\bf{y}}^TP^TAP{\\bf{y}} = {\\bf{y}}^TD{\\bf{y}}\\\\\n&\\text{where } D = P^TAP,P^T = P^{-1}\n\\end{aligned}\\]\n\n\\(A\\)는 symmetric하므로 직교대각화\\(D = P^TAP\\)가 가능하다.\n따라서 벡터\\({\\bf{x}}\\)의 quadratic form은 벡터\\({\\bf{y}}\\)의 quadratic form이 되고\n\\(A\\)의 고윳값을 모아놓은 대각행렬 \\(D\\)가 quadratic form의 matrix가 됨을 알 수 있다.\n\n\\[\\begin{aligned}\n{\\bf{x}}^TA{\\bf{x}}= {\\bf{y}}^TD{\\bf{y}} =\n\\begin{bmatrix}\ny_1^T \\\\\ny_2^T \\\\\n\\vdots \\\\\ny_n^T\n\\end{bmatrix}\n\\begin{bmatrix}\n\\lambda_1 & 0 &\\dots & 0 \\\\\n0 & \\lambda_2 &\\dots & 1 \\\\\n\\vdots & \\vdots &\\ddots & \\vdots \\\\\n0 & 0 & \\dots & \\lambda_n \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\ny_1 & y_2 & \\dots & y_n\n\\end{bmatrix}\n= \\lambda_1y_1^2 + \\lambda_2y_2^2 + \\dots + \\lambda_ny_n^2\n\\end{aligned}\\]\n\n조금 더 전개해보면 위와 같은 \\(y\\)에 관한 식으로 나타난다.\n\n변환결과 \\(D\\)가 대각행렬이기에 행렬곱의 결과로 crossproduct term이 나타나지 않았으며\n\\(B = \\{p_1,p_2,\\dots,p_n\\}\\)를 기저로 하는 새로운 좌표축에서 y에 관한 방정식이 표현되었다.\n\n따라서 원점에서 거리가 가장 먼 지점을 비교적 쉽게 구할 수 있다.\n\n\\({\\bf{y}}^TD{\\bf{y}} = \\lambda_1y_1^2 + \\lambda_2y_2^2\\)와 같은 타원이라면\n$y_2 = 0 $을 대입하여 쉽게 구할 수 있다.\n물론 고윳값,고윳벡터를 구하는 과정이 필요함을 잊지말자."
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html#example-orthogonally-diagonalize",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html#example-orthogonally-diagonalize",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "Example : orthogonally diagonalize",
    "text": "Example : orthogonally diagonalize\n\\(A =\\begin{bmatrix}4 & 2 & 2 \\\\ 2 & 4 & 2 \\\\ 2 & 2 & 4\\end{bmatrix}\\)인 symmetric matrix를 직교대각화해라.\n먼저 행렬 A의 고윳값과 고유벡터를 구해보자. 다음과 같은 방정식을 만족하는 \\(\\lambda\\)가 고윳값이다.\n\\[\\begin{aligned}\n&\\text{det}(\\lambda I - A) = 0 \\\\\n&\\Longleftrightarrow\n\\begin{vmatrix}\n\\lambda -4 & -2 & -2 \\\\\n-2 & \\lambda -4 & -2 \\\\\n-2 & -2 & \\lambda -4\n\\end{vmatrix} = 0 \\\\\n&\\Longleftrightarrow(\\lambda-2)^2(\\lambda-8) = 0\n\\end{aligned}\\]\n따라서 고윳값은 2와 8이다. 각각의 고윳값에 대한 단위고유벡터를 구해보면 다음과 같다.\n\n이때 \\(p_2\\)가 바로 구해지지는 않으며 아직 공부하지 못한 방법이다.\n서로간에 직교인 고유벡터들을 구성하는 방법은 나중에 공부를 더 해야할 듯 하다.(그람슈미트?직교여공간?)\n\n\\[\\begin{aligned}\n&\n\\lambda=2 : p_1 = \\begin{bmatrix}-1/\\sqrt{2} & 1/\\sqrt{2}& 0\\end{bmatrix}^T,p_2 = \\begin{bmatrix}-1/\\sqrt{6}&-1/\\sqrt{6}&2/\\sqrt{6}\\end{bmatrix}^T \\\\\n&\\lambda=8 : p_3 = \\begin{bmatrix}1/\\sqrt{3} & 1/\\sqrt{3} & 1/\\sqrt{3}\\end{bmatrix}^T\n\\end{aligned}\\]\n대각행렬 D를 구해보면 다음과 같다.\n\\[\\begin{aligned}\nD = P^{-1}AP = \\text{diag}(2,2,8)\n\\end{aligned}\\]\n\nimport torch\nA = torch.FloatTensor([\n    [4,2,2],\n    [2,4,2],\n    [2,2,4]\n])\np_1 = torch.FloatTensor(\n    [[-1/2**(1/2),1/2**(1/2),0]]\n).T\np_2 = torch.FloatTensor(\n    [[-1/6**(1/2),-1/6**(1/2),2/6**(1/2)]]\n).T\np_3 = torch.FloatTensor(\n    [[1/3**(1/2),1/3**(1/2),1/3**(1/2)]]\n).T\nP = torch.concat([p_1,p_2,p_3],axis=1)\nP_inv = P.inverse()\n#대각화\nD = P_inv @ A @ P\nprint(\"대각행렬 D\\n\",D.round())\n\n대각행렬 D\n tensor([[2., 0., -0.],\n        [-0., 2., -0.],\n        [0., 0., 8.]])"
  },
  {
    "objectID": "posts/Linear Algebra/quadratic form/quadratic forms.html#example-change-of-variable-in-quadratic-form",
    "href": "posts/Linear Algebra/quadratic form/quadratic forms.html#example-change-of-variable-in-quadratic-form",
    "title": "[Linear Algebra] 6-4.Quadratic Form",
    "section": "Example : Change of Variable in Quadratic Form",
    "text": "Example : Change of Variable in Quadratic Form\n위에서 구한 \\({\\bf{x}} = [x_1,x_2]^T\\) ,\\(A = \\begin{bmatrix}3 & -2 \\\\ -2 & 7\\end{bmatrix}\\)일때, quadratic form을 변수변환해보자.\n\\[\\begin{aligned}\n&Q({\\bf{x}}) = {\\bf{x}}^TA{\\bf{x}}\n= 3x_1^2 -4x_1x_2 + 7x_2^2\n\\end{aligned}\\]\n먼저 고윳값과 고유벡터를 구하면 다음과 같다.\n\n이때 고유벡터는 모두 단위벡터이자 orthonormal한 벡터로 잡자.\n\n\\[\\begin{aligned}\n&\n\\lambda=3 : p_1 = \\begin{bmatrix}2/\\sqrt{5} & -1/\\sqrt{5}\\end{bmatrix}^T\\\\\n&\\lambda=-7 : p_2 = \\begin{bmatrix}1/\\sqrt{5} & 2/\\sqrt{5} \\end{bmatrix}^T\n\\end{aligned}\\]\n대각화를 위해 \\(A\\)를 대각화시키는 행렬 \\(P\\)를 구성하자.이때 \\(P\\)에 대해서 몇 가지를 기억하자.\n\n\\(P\\)는 \\(A\\)의 고유벡터들을 모아놓은 행렬이자\n\\(P^{-1} = P^T\\)를 만족하는 orthonormal matrix이다.(A가 symmetric matrix이기에 가능하다.)\n\n\\[P = \\begin{bmatrix}2/\\sqrt{5} & 1/\\sqrt{5}\\\\-1\\sqrt{5} & 2/\\sqrt{5}\\end{bmatrix}\\]\n따라서 대각화를 해보면 다음과 같다.\n\\[D = P^{-1}AP = \\begin{bmatrix}3 & 0 \\\\ 0 & -7\\end{bmatrix}\\]\n이제 위에서 구한 \\(P\\)로 변수변환을 할 수 있다. \\({\\bf{x}}\\)=\\(P{\\bf{y}}\\)를 quadratic form에 대입해보자.\n\\[\\begin{aligned}\nQ({\\bf{x}}) &= x^TAx = (Py)^TA(Py) = y^TP^TAPy = y^TP^TAPy\\\\\n&=y^TDy = 3y_1^2 - 7y_2^2\n\\end{aligned}\\]\n이러한 변수변환을 시각화 해보면 다음과 같다고 한다.\n\n\nx에 대한 quadratic form인 \\({\\bf{x}}^TA{\\bf{x}}\\)가 존재하며 그 값은 16이다.\n\\({\\bf{x}} = P{\\bf{y}}\\)를 x에 대한 2차형식에 대입하여 y에 대한 quadratic form으로 변수를 바꿨으며 그 값도 마찬가지로 16이다.\n\\({\\bf{x}}^TA{\\bf{x}} = {\\bf{y}}^TD{\\bf{y}} = 16\\)으로 같은 값에 mapping됨을 기억하자.(변수변환을 해도 나타내는 형태(타원)는 그대로이다.)"
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html",
    "title": "[Linear Algebra] 3.rank & null space(kernel)",
    "section": "",
    "text": "유투브 - 혁펜하임님의 선형대수학강의를 정리하기 위해 작성한 글입니다."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#예제",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#예제",
    "title": "[Linear Algebra] 3.rank & null space(kernel)",
    "section": "예제",
    "text": "예제\n\\[\\begin{pmatrix}\n1&2&3\\\\\n0&0&0\n\\end{pmatrix}\\]\n위에 있는 행렬에서 선형독립인 열벡터의 갯수 = 1이다. 또한 선형독립인 열벡터의 갯수 = 선형독립인 행벡터의 갯수 = 열공간의 차원 = 행공간의 차원이므로 랭크정리도 성립한다.\n위와 같은 \\(2 \\times 3\\) 행렬은 랭크보다 행,열의 갯수가 많이 부족하므로 rank-deficient 라고 한다.\n\\[\\begin{pmatrix}\n1&0&1\\\\\n0&1&1\n\\end{pmatrix}\\]\n위에 있는 행렬에서 선형독립인 열벡터의 갯수 = 2이다. 또한 선형독립인 열벡터의 갯수 = 선형독립인 행벡터의 갯수 = 열공간의 차원 = 행공간의 차원이므로 랭크정리도 성립한다.\n위와 같은 행렬은 행의 갯수만큼 랭크가 다 차있으므로 full row rank라 한다."
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#용어정리",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#용어정리",
    "title": "[Linear Algebra] 3.rank & null space(kernel)",
    "section": "용어정리",
    "text": "용어정리\n\\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n<m \\Rightarrow\\) 열벡터가 모두 선형독립,full column rank,위아래로 길쭉하고 양옆은 좁은 직사각행렬 \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) = n>m \\Rightarrow\\) 행벡터가 모두 선형독립,full row rank,위아래가 좁고 좌우 양옆으로 길쭉한 직사각행렬 \\(A \\in \\mathbb{R}^{n \\times n},\\text{rank}(A) = n \\Rightarrow\\) 행백터,열벡터가 모두 선형독립,full rank,위,아래,양,옆의 길이가 모두 같은 정사각행렬 \\(A \\in \\mathbb{R}^{m \\times n},\\text{rank}(A) < \\text{min}(n,m)\\) = 선형종속인 행벡터,열벡터 반드시 존재,rank deficient"
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#예제1",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#예제1",
    "title": "[Linear Algebra] 3.rank & null space(kernel)",
    "section": "예제1)",
    "text": "예제1)\n행렬 A = \\(\\begin{pmatrix}1&0&1 \\\\0&1&1\\end{pmatrix}\\)일 때, 행렬A의 영공간은?\n먼저 만족하는 \\(x\\)를 찾아보자. 어떤 방법이던 사용가능하지만 문제가 간단하므로 직관적으로 풀이한다.\n\\[\\begin{aligned}\n&Ax = x_1\\begin{pmatrix}1\\\\0\\end{pmatrix} + x_2\\begin{pmatrix}0 \\\\ 1\\end{pmatrix} + x_3\\begin{pmatrix}1\\\\1\\end{pmatrix} = {\\bf 0} \\\\\n&\\Longleftrightarrow\n\\begin{cases}\nx_1 + x_3 = 0 \\\\\nx_2 + x_3 = 0\n\\end{cases}\n\\\\\n&\\Longleftrightarrow x_3 = t,x_2 = -t,x_1 = t \\\\\n&\\Longleftrightarrow x = t\\begin{pmatrix}1\\\\-1\\\\1\\end{pmatrix} \\\\\n&\\therefore \\text{N}(A) = \\Bigg\\{t\\begin{pmatrix}1\\\\-1\\\\1 \\end{pmatrix}|t \\in \\mathbb{R}\\Bigg\\}\n\\end{aligned}\\]\n방정식의 해를 구해보니 1)\\([1,-1,1]^T\\)의 span이 영공간이며 2)영공간은 행렬곱에 의해서(중간에서 차원일치 \\(2 \\times 3 과 3 \\times 1\\)) 행벡터가 존재하는 차원인 3차원 벡터공간의 부분공간인 1차원 벡터공간을 생성함을 알 수 있다.\n영공간의 원소(방정식의 해)는 무수히 많음이 자명한데 왜냐하면 \\(Ax = 0\\)을 만족하는 임의의 x벡터에 대한 스칼라배에 대해 다음이 성립하기 때문이다.\n\\[\\begin{aligned}\n&Ax = {\\bf 0} \\\\\n&\\Longleftrightarrow cAx = c{\\bf 0} = {\\bf 0} \\\\\n&\\Longleftrightarrow A(cx) ={\\bf 0} \\\\\n\\end{aligned}\\]\n따라서 \\(x\\)의 스칼라배인 \\(cx\\)도 \\(A\\)와 곱해져서 \\({\\bf 0}\\) 만들기 \\(x\\)의 스칼라배도 영공간의 원소이다. \nrank-nulity theoreom도 성립함을 확인할 수 있다. 영공간의 차원은 영공간의 기저의 갯수인데 \\([1,-1,1]^T\\)이 기저의 조건인 1)선형생성 = 벡터공간 2)선형독립 이라는 두 조건을 만족하므로 차원은 1이다.랭크는 다른방식으로도 구할 수 있지만 행렬이 간단해서 바로2임을 확인할 수 있으므로 다음과 같다 \\[\\text{nulity}(A) + \\text{rank}(A)= n   \\Longleftrightarrow  1 + 2 = 3\\]"
  },
  {
    "objectID": "posts/Linear Algebra/rank and null space/rank and null space.html#예제2",
    "href": "posts/Linear Algebra/rank and null space/rank and null space.html#예제2",
    "title": "[Linear Algebra] 3.rank & null space(kernel)",
    "section": "예제2)",
    "text": "예제2)\n행렬 A = \\(\\begin{pmatrix}1&2&3 \\\\0&0&0\\end{pmatrix}\\)일 때, 행렬A의 영공간은?\n마찬가지로 만족하는 \\(x\\)를 먼저 찾아보자.\n\\[\\begin{aligned}\n&Ax = x_1\\begin{pmatrix}1\\\\0\\end{pmatrix} + x_2\\begin{pmatrix}2 \\\\ 0\\end{pmatrix} + x_3\\begin{pmatrix}3\\\\0\\end{pmatrix} = {\\bf 0} \\\\\n&\\Longleftrightarrow\nx_1 + 2x_2 + 3x_3 = 0 \\\\\n&\\Longleftrightarrow x_3 = t,x_2 = q,x_1 = -2q -3t \\\\\n&\\Longleftrightarrow x = \\begin{pmatrix}-2q-3t\\\\q\\\\t\\end{pmatrix} = q\\begin{pmatrix}-2\\\\1\\\\0\\end{pmatrix} + t\\begin{pmatrix}-3\\\\0\\\\1\\end{pmatrix} \\\\\n&\\therefore \\text{N}(A) = \\Bigg\\{q\\begin{pmatrix}-2\\\\1\\\\0 \\end{pmatrix} + t\\begin{pmatrix}-3\\\\0\\\\1\\end{pmatrix}|t,q \\in \\mathbb{R}\\Bigg\\}\n\\end{aligned}\\]\n방정식의 해를 구해보니 1)두 벡터의 span이 영공간이며 2)생성된 영공간은 행렬곱에 의해서(중간에서 차원일치 \\(2 \\times 3\\) 과 \\(3 \\times 1\\)) 행벡터가 존재하는 차원인 3차원 벡터공간안에서 부분공간인 2차원 벡터공간을 생성함을 알 수 있다.\n\\(Ax = 0\\)을 만족하는 2개의 x의 선형조합이 모두 영공간의 원소임을 확인해보자.\n\\[\\begin{aligned}\n&Ax_1 = {\\bf 0},Ax_2 = {\\bf 0} \\\\\n&\\Longleftrightarrow cAx_1 = c{\\bf 0}={\\bf 0},cAx_2 = c{\\bf 0}={\\bf 0} \\\\\n&\\Longleftrightarrow A(cx_1) + A(cx_2) = A(cx_1 + cx_2) = {\\bf 0} \\\\\n\\end{aligned}\\]\n따라서 \\(x\\) 방정식을 만족하는 두 벡터의 선형조합인 \\(c(x_1 + cx_2)\\)도 \\(A\\)와 곱해져서 영공간의 원소임을 알 수 있다. \n마찬가지로 rank-nulity theoreom도 성립함을 확인할 수 있다. 영공간의 차원은 영공간의 기저의 갯수이고 \\([-2,1,0]^T,[-3,0,1]^T\\)가 기저의 조건인 1)선형생성 = 벡터공간(여기서 영공간) 2)선형독립 이라는 두 조건을 만족하므로 영공간의 기저는 \\([-2,1,0]^T,[-3,0,1]^T\\)이고 차원은 2이다.랭크는 다른방식으로도 구할 수 있지만 행렬이 간단해서 바로1임을 확인할 수 있으므로 다음과 같다 \\[\\text{nulity}(A) + \\text{rank}(A)= n   \\Longleftrightarrow  2 + 1 = 3\\]"
  },
  {
    "objectID": "posts/Linear Algebra/span과 columns space.html",
    "href": "posts/Linear Algebra/span과 columns space.html",
    "title": "[Linear Algebra] 2.Linear Combination & Span",
    "section": "",
    "text": "유투브 - 혁펜하임님의 선형대수학강의를 참고했습니다.\n\nLinear Combination\n벡터 \\(\\bf{v_1,v_2,\\dots,v_n}\\)의 선형결합(또는 일차결합)은 다음과 같다. \\[w_1{\\bf v_1} + w_2{\\bf v_2} + \\dots + w_n{\\bf v_n}\\] 선형결합의 결과는 여러가지 벡터들의 결합이다. 여기서 각각의 \\(w_1,w_2,\\dots,w_n\\)은 스칼라이며 대응하는 \\(x_1,x_2,\\dots,x_n\\)를 결합에 사용하는 정도를 의미한다. 만약 \\(w_1 = 0.0001\\)이면 여러 벡터들을 결합하지만 그 결합 중 \\(\\bf v_1\\)이 아주 사용하여 결합하는 것이고 \\(w_2 = 120\\)이라면 결합에서 \\(\\bf v_2\\)를 아주 많이 사용하는 것이다.\n\n\nspan\n벡터 \\(v_1,v_2,\\dots,v_n\\)의 선형결합에서 \\(w_1,w_2,\\dots,w_n\\)(스칼라,결합에 사용하는 정도)를 바꿨을때 가능한 모든 벡터들의 집합이며 즉,벡터의 선형결합으로 가능한 모든 집합들이며 정의는 다음과 같다. \\[\\text{span}({\\bf{v_1,v_2,\\dots,v_n}}) := \\{w_1{\\bf{v_1}} + w_2{\\bf{v_2}} + \\dots + w_n{\\bf{v_n}}:w_1,w_2,\\dots,w_n \\in K\\}\\]  span은 선형결합으로 만들어지는 또다른 벡터공간이다. 여기서 K는 field를 의미하는데 스칼라는 field라는 또다른 집합으로부터 가져온 원소이기때문에 그렇다.임의의 벡터들의 span은 어떨까? 아래의 그림을 확인해보자.\n<참고> span은 동사로도 사용한다. ex : 벡터공간을 생성한다. 열공간은 열벡터들이 생성하는 공간이다.(=열공간은 열벡터의 생성이다.).기저들이 벡터공간을 생성한다.\n:는 조건을 의미합니다.\n\n\nCode\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport random\nplt.style.use(\"ggplot\")\n\ndef linrcmb(v1,v2):\n    linrcomb_x= []\n    linrcomb_y= []\n\n    _w = np.linspace(-50,50,300).tolist()\n    for i in range(2000):\n    #w1,w2를 -100~100사이의 임의의 숫자로\n        w1 = random.sample(_w,1)\n        w2 = random.sample(_w,1)\n        #print(w1,w2)\n    #선형결합 계산\n        linrcomb = w1 * v1 + w2 * v2\n    #시각화를 위해 선형결합의 x값 y값 따로 모아놓기\n        linrcomb_x.append(linrcomb[0][0])\n        linrcomb_y.append(linrcomb[1][0])\n    return linrcomb_x,linrcomb_y\n\nv1 = np.array([[0],[0]])\nv2 = np.array([[0],[0]])\nx,y = linrcmb(v1,v2)\nfig,ax = plt.subplots(figsize=(30,5))\nplt.subplot(1,5,1)\nplt.title(\"$v_1 = [0,0]^{T},v_2 = [0,0]^T$\")\nplt.scatter(x,y,color=\"black\")\n\nv1 = np.array([[1],[0]])\nv2 = np.array([[-3],[0]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,2)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [-3,0]^T$\")\nplt.scatter(x,y)\n\nv1 = np.array([[1],[1]])\nv2 = np.array([[0],[0]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,3)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [0,0]^T$\")\nplt.scatter(x,y,color=\"green\",alpha=1)\n\n\nv1 = np.array([[1],[0]])\nv2 = np.array([[0],[1]])\nx,y = linrcmb(v1,v2)\nplt.subplot(1,5,4)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [0,1]^T$\")\nplt.scatter(x,y,color=\"purple\",alpha=1)\n\ndef linrcmb2(v1,v2):\n    linrcomb_x= []\n    linrcomb_y= []\n\n    _w = (np.linspace(-250,250,300)).tolist()\n    for i in range(2000):\n        w1 = random.sample(_w,1)[0] * 100 #그래프를 그리기 위한 값 조절\n        w2 = random.sample(_w,1)[0] \n        #선형결합 계산\n        linrcomb = w1 * v1 + w2 * v2\n        #시각화를 위해 선형결합의 x값 y값 따로 모아놓기\n        linrcomb_x.append(linrcomb[0][0])\n        linrcomb_y.append(linrcomb[1][0])\n    \n    return linrcomb_x,linrcomb_y\nlinrcmb2(v1,v2)\n\nv1 = np.array([[-1],[0]])\nv2 = np.array([[2],[2]])\nx,y = linrcmb2(v1,v2)\nplt.subplot(1,5,5)\nplt.title(\"$v_1 = [1,0]^{T},v_2 = [2,2]^T$\")\nplt.scatter(x,y,color=\"blue\",alpha=1)\nplt.subplots_adjust(wspace=0.4,hspace=0.5)\nplt.suptitle(\"span$(v_1,v_2)$\",y=1.02,fontsize=20)\n\n\nText(0.5, 1.02, 'span$(v_1,v_2)$')\n\n\n\n\n\n점 하나는 벡터 하나를 나타낸다. 1번째 그림에서 벡터들의 생성(span)은 2차원 벡터공간의 부분공간이자 0차원 벡터공간(점)이며 2,3번째 그림에서 벡터들의 생성(span)은 2차원 벡터공간의 부분공간이자 1차원 벡터공간(직선)이다. 이와는 다르게 3번째 그림에서의 벡터들의 생성(span)은 3차원 벡터공간 그 자체인데 그 이유는 3차원 벡터공간의 모든 점을 표현할 수 있기 때문이다.\n\n\ncolumn Space\n열공간(columns space)은 행렬의 열벡터들의 span 즉, 행렬의 열벡터들로 가능한 모든 선형조합(벡터)의 집합입니다. \\(v_1,v_2,\\dots,v_n\\)이 행렬A의 열벡터들이라고 할 때, 열공간은 다음과 같습니다. \\[\\text{C}(A) = \\text{span}(v_1,v_2,\\dots,v_n) = \\{w_1{\\bf v_1} + w_2{\\bf v_2} + \\dots + w_n{\\bf v_n}:w_1,w_2,\\dots,w_n \\in K\\}\\] 열공간은 방정식 \\(Ax = b\\)의 해의 갯수를 파악하는데 쓰입니다.\n\n\n참고자료\n[선형대수] 벡터공간(vector space), 벡터 부분공간(vector subspace), 생성공간(span), 차원(dimension) 위키피디아-Linear span 위키피디아-Row and columns spaces 혁펜하임 - [선대] 2-6강. span 과 column space (열공간) 직관적 설명"
  },
  {
    "objectID": "posts/Linear Algebra/행렬곱에 대한 여러가지 시각.html",
    "href": "posts/Linear Algebra/행렬곱에 대한 여러가지 시각.html",
    "title": "[Linear Algebra] 1.행렬곱에 대한 여러가지 관점",
    "section": "",
    "text": "유튜브 - 혁펜하임님의 선형대수학을 참고했습니다.\n\n행렬곱은 내적이다.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},B \\in \\mathbb{R}^{n \\times p} \\\\\n&AB =\n\\begin{bmatrix}\na_1^T\\\\\na_2^T\\\\\n\\vdots\\\\\na_m^T\\\\\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1 & b_2 & \\dots & b_p\n\\end{bmatrix}=\n\\begin{bmatrix}\na_1^Tb_1 & a_1^Tb_2 & \\dots & a_1^Tb_p \\\\\na_2^Tb_1 & a_2^Tb_2 & \\dots & a_2^Tb_p \\\\\n\\vdots & \\vdots  & \\vdots & \\vdots \\\\\na_{m}^Tb_1 & a_m^Tb_2 & \\dots & a_m^Tb_p \\\\\n\\end{bmatrix}\n\\end{aligned}\\]\n\n\n행렬곱은 rank-1 matrix의 합이다.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},B \\in \\mathbb{R}^{n \\times p} \\\\\n&AB =\n\\begin{bmatrix}\na_1 & a_2 & \\dots & a_n\n\\end{bmatrix}\n\\begin{bmatrix}\nb_1^T \\\\\nb_2^T \\\\\n\\vdots \\\\\nb_n^T\n\\end{bmatrix}\n= a_1b_1^T + a_2b_2^T + \\dots + a_nb_n^T\n\\end{aligned}\\]\n\n\n행렬과 벡터의 곱은 열공간에 속한 임의의 벡터이다.\n\\[\\begin{aligned}\n&\\text{Let }A \\in \\mathbb{R}^{m \\times n},x \\in \\mathbb{R}^{n \\times 1} \\\\\n&Ax =\n\\begin{bmatrix}\na_1 & a_2 & \\dots & a_n\n\\end{bmatrix}\n\\begin{bmatrix}\nx_1 \\\\\nx_2 \\\\\n\\vdots \\\\\nx_n\n\\end{bmatrix}\n= a_1x_1 + a_2x_2 + \\dots + a_nx_n\n\\end{aligned}\\]\n\n\\(a_1,a_2,\\dots,a_n\\)은 벡터 \\(x_1,x_2,\\dots,x_n\\)은 스칼라이다.\n행렬곱은 위와같이 열공간의 기저(행렬\\(A\\)의 열벡터)와 스칼라(미지수벡터\\(x\\)의 원소)와의 일차결합이므로 기저인 열벡터가 생성(span)하는 열공간의 원소이다. 이는 방정식 \\(Ax=b\\)의 해의 갯수를 알아내는데에 사용하는 중요한 개념이다.(참고 : 방정식 Ax = b의 해의 갯수 알아내기)\n열공간(column space) : 행렬에서 (열)벡터의 일차결합으로 생성되는 벡터공간. 열벡터의 span\n\n\n\n행벡터와 행렬의 곱은 row space(행공간)에 속한 임의의 벡터이다.\n\\[\\begin{aligned}\n&\\text{Let }x \\in \\mathbb{R}^{1 \\times n},X \\in \\mathbb{R}^{n \\times p} \\\\\n&xA =\n\\begin{bmatrix}\nx_1 & x_2 & \\dots & x_n\n\\end{bmatrix}\n\\begin{bmatrix}\na_1^T \\\\\na_2^T \\\\\n\\vdots \\\\\na_n\n\\end{bmatrix}\n= x_1a_1^T + x_2a_2^T + \\dots + x_na_n^T\n\\end{aligned}\\]\n\n\\(a_1^T,a_2^T,\\dots,a_n^T\\)은 벡터 \\(x_1,x_2,\\dots,x_n\\)은 스칼라이다. 행렬곱은 위와같이 행공간의 기저(행렬\\(A\\)의 행벡터)와 스칼라(\\(x\\)의 원소)와의 일차결합으로 기저인 행벡터가 생성하는 행공간의 원소이다.\n행공간 : 행렬에서 행벡터의 일차결합으로 생성되는 벡터공간. 행벡터의 span\n\n\n\n참고문헌\n혁펜하임 - 선대 2-5강. 행렬의 곱셈과 네 가지 관점 (열공간 (column space) 등)"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html",
    "title": "[pandas] map,apply,applymap 비교",
    "section": "",
    "text": "sdasdaksldjklsdjklasjdklj dasmkdlmsakldmklsadmkls\n\nimport numpy as np\nimport pandas as pd"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#data",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#data",
    "title": "[pandas] map,apply,applymap 비교",
    "section": " Data",
    "text": "Data\n\ns = pd.Series([\"cat\",\"dog\",np.nan,\"rabbit\"])"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#arg가-dict인-경우",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#arg가-dict인-경우",
    "title": "[pandas] map,apply,applymap 비교",
    "section": " arg가 dict인 경우",
    "text": "arg가 dict인 경우\n\ns.map({\"cat\":\"kitten\",\"dog\":\"puppy\"})\n\n0    kitten\n1     puppy\n2       NaN\n3       NaN\ndtype: object\n\n\n\nSeries의 값 중 dict의 key와 일치하는 값이 없으면 NaN으로 바뀜."
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#arg가-series인-경우",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#arg가-series인-경우",
    "title": "[pandas] map,apply,applymap 비교",
    "section": " arg가 Series인 경우",
    "text": "arg가 Series인 경우\n\ns2 = pd.Series([\"lion\",\"elephant\",\"dog\",np.nan])\ns.map(s2)\n\n0    NaN\n1    NaN\n2    NaN\n3    NaN\ndtype: object"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#arg가-function인-경우",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#arg가-function인-경우",
    "title": "[pandas] map,apply,applymap 비교",
    "section": " arg가 function인 경우",
    "text": "arg가 function인 경우\n\ns.map(lambda x : \"I am a {}\".format(x))\n\n0       I am a cat\n1       I am a dog\n2       I am a nan\n3    I am a rabbit\ndtype: object"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#잘못된-사용-예시",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#잘못된-사용-예시",
    "title": "[pandas] map,apply,applymap 비교",
    "section": " 잘못된 사용 예시",
    "text": "잘못된 사용 예시\ns = pd.Series([0.1,3,2,0.4])\ns.map(lambda x : sum(x))\n>>> TypeError: 'float' object is not iterable\n\n각각의 값(여기선 float)이 따로따로 함수(mapping)이 입력된다. 하지만 sum함수는 float형을 input으로 할 수 없으므로 오류 발생함.(float형은 iterable하지 않음)\n\n\ns = pd.Series([0.3,2,0.4,5])\ns.map(np.var)\n\n0    0.0\n1    0.0\n2    0.0\n3    0.0\ndtype: float64\n\n\n\n오류가 뜨지 않으나 원하던 동작이 아니다. 각각의 변수에 대해서 평균값을 구하려 했겠지만 각각의 값에 mean이 되므로 그 값 자체이다."
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#data-1",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#data-1",
    "title": "[pandas] map,apply,applymap 비교",
    "section": " Data",
    "text": "Data\n\nn1 = pd.Series([0.1,4,0.35,2])\nn2 = pd.Series([0.3,0.4,3,-5])\ndf = pd.DataFrame({\"n1\":n1,\"n2\":n2})\ndf\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.10\n      0.3\n    \n    \n      1\n      4.00\n      0.4\n    \n    \n      2\n      0.35\n      3.0\n    \n    \n      3\n      2.00\n      -5.0"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#column에-function-적용",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#column에-function-적용",
    "title": "[pandas] map,apply,applymap 비교",
    "section": " column에 function 적용",
    "text": "column에 function 적용\n\ncolumn vector에 함수 적용한다고 생각하자.\n\n\ndf.apply(func = lambda x : sum(x)/len(x),axis=0)\n\nn1    1.6125\nn2   -0.3250\ndtype: float64\n\n\n\ndf.apply(np.mean,axis=0)\n\nn1    1.6125\nn2   -0.3250\ndtype: float64\n\n\n\n#vector에 elementwise**2\n#np.array를 **2하면 모든 원소가 제곱되는 것과 같다.\ndf.apply(func = lambda x : x ** 2,axis=0)\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.0100\n      0.09\n    \n    \n      1\n      16.0000\n      0.16\n    \n    \n      2\n      0.1225\n      9.00\n    \n    \n      3\n      4.0000\n      25.00\n    \n  \n\n\n\n\n\n#vector에 elementwise로 sin취함.\ndf.apply(func = lambda x : np.sin(x),axis=0)\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.099833\n      0.295520\n    \n    \n      1\n      -0.756802\n      0.389418\n    \n    \n      2\n      0.342898\n      0.141120\n    \n    \n      3\n      0.909297\n      0.958924"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#row에-function-적용",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#row에-function-적용",
    "title": "[pandas] map,apply,applymap 비교",
    "section": " row에 function 적용",
    "text": "row에 function 적용\n\nrow vector에 함수 적용.\n\n\ndf.apply(sum,axis=1)\n\n0    0.40\n1    4.40\n2    3.35\n3   -3.00\ndtype: float64\n\n\n\ndf.apply(np.sin,axis=1)\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.099833\n      0.295520\n    \n    \n      1\n      -0.756802\n      0.389418\n    \n    \n      2\n      0.342898\n      0.141120\n    \n    \n      3\n      0.909297\n      0.958924\n    \n  \n\n\n\n\n\n#vector에 elementwise**2\n#np.array를 **2하면 모든 원소가 제곱되는 것과 같다.\ndf.apply(func = lambda x : x ** 2,axis=1)\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.0100\n      0.09\n    \n    \n      1\n      16.0000\n      0.16\n    \n    \n      2\n      0.1225\n      9.00\n    \n    \n      3\n      4.0000\n      25.00"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#data-2",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#data-2",
    "title": "[pandas] map,apply,applymap 비교",
    "section": " Data",
    "text": "Data\n\nn1 = pd.Series([0.1,4,0.35,2])\nn2 = pd.Series([0.3,0.4,3,-5])\ndf = pd.DataFrame({\"n1\":n1,\"n2\":n2})\ndf\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.10\n      0.3\n    \n    \n      1\n      4.00\n      0.4\n    \n    \n      2\n      0.35\n      3.0\n    \n    \n      3\n      2.00\n      -5.0"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#올바른-사용",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#올바른-사용",
    "title": "[pandas] map,apply,applymap 비교",
    "section": " 올바른 사용",
    "text": "올바른 사용\n\ndf.applymap(lambda x : \"값은 {}\".format(x))\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      값은 0.1\n      값은 0.3\n    \n    \n      1\n      값은 4.0\n      값은 0.4\n    \n    \n      2\n      값은 0.35\n      값은 3.0\n    \n    \n      3\n      값은 2.0\n      값은 -5.0\n    \n  \n\n\n\n\n\ndf.applymap(lambda x : len(str(x)))\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      3\n      3\n    \n    \n      1\n      3\n      3\n    \n    \n      2\n      4\n      3\n    \n    \n      3\n      3\n      4\n    \n  \n\n\n\n\n\ndf.applymap(lambda x : x ** 2)\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.0100\n      0.09\n    \n    \n      1\n      16.0000\n      0.16\n    \n    \n      2\n      0.1225\n      9.00\n    \n    \n      3\n      4.0000\n      25.00"
  },
  {
    "objectID": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#잘못된-사용",
    "href": "posts/numpy pandas/pandas applymap apply map/pandas apply map applymap.html#잘못된-사용",
    "title": "[pandas] map,apply,applymap 비교",
    "section": " 잘못된 사용",
    "text": "잘못된 사용\ndf.applymap(lambda x : sum(x))\n>>> TypeError: 'float' object is not iterable\n\nSeries의 map method와 마찬가지로 DataFrame의 applymap method도 각각의 값(여기선 float)이 따로따로 함수(mapping)에 입력된다. 그러므로 sum함수에서 오류가 발생한다.\n\n\ndf.applymap(lambda x : np.mean(x))\n\n\n\n\n\n  \n    \n      \n      n1\n      n2\n    \n  \n  \n    \n      0\n      0.10\n      0.3\n    \n    \n      1\n      4.00\n      0.4\n    \n    \n      2\n      0.35\n      3.0\n    \n    \n      3\n      2.00\n      -5.0\n    \n  \n\n\n\n\n\n오류가 뜨지 않으나 원하던 동작이 아니다. 각각의 변수에 대해서 평균값을 구하려 했겠지만 각각의 값에 mean이 되므로 그 값 자체이다."
  },
  {
    "objectID": "posts/numpy pandas/수치미분 & np.gradient/수치미분 & np.gradient.html",
    "href": "posts/numpy pandas/수치미분 & np.gradient/수치미분 & np.gradient.html",
    "title": "Finite Difference Method with np.gradient",
    "section": "",
    "text": "\\({\\bf x} =\\begin{bmatrix}x_1&x_2&\\dots&x_m\\end{bmatrix}^T\\)일 때, \\(x\\)에 대한 다변수함수 \\(f({\\bf x})\\)의 gradient는 다음과 같다.\n\\[\\begin{aligned}\n&\\text{gradient of }f({\\bf{x}}) = \\frac{\\partial f}{\\partial {\\bf x}} = \\nabla f(\\bf{x}) =\n\\begin{pmatrix}\n\\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial }{\\partial x_m}\n\\end{pmatrix}\n\\end{aligned}\\]\n함수가 가지는 모든 변수에 대해서 편미분 한 뒤 모아놓은 벡터라고 생각하면 된다. 함수의 수식을 알고 미분이 가능하면 우리는 해석적으로 미분해서(미분공식써서) 그레디언트를 구하고 각각의 어떤 point에서의 편미분계수들도 구할 수 있다. 그러나 우리가 주어진 데이터는 함수f의 함숫값들이 주어진다. 예를 들면 다음과 같다.\n\nimport numpy as np\nf = np.array([1,2,4,7,11,17],dtype = float)\nprint(\"f(x)\")\nprint(f)\n\nf(x)\n[ 1.  2.  4.  7. 11. 17.]\n\n\n위와 같은 함숫값들만 주어질때에는 원래의 함수를 알기는 불가능하다. 따라서 도함수를 통한 정확한 미분계수를 구하기가 불가능하므로 주어진 데이터로 \\(\\bf x\\)에서의 미분계수의 값을 근사적으로 구할 수 있는데 이를 수치미분이라 한다."
  },
  {
    "objectID": "posts/numpy pandas/수치미분 & np.gradient/수치미분 & np.gradient.html#차원-배열의-경우",
    "href": "posts/numpy pandas/수치미분 & np.gradient/수치미분 & np.gradient.html#차원-배열의-경우",
    "title": "Finite Difference Method with np.gradient",
    "section": "1차원 배열의 경우",
    "text": "1차원 배열의 경우\n\nEx) \\(dx\\) = 1\n넘파이 1차원 배열이 다음과 같이 주어져 있다고 하자.\n\nfrom IPython.display import display, Markdown\nf = np.array([1,2,4,7,11,16],dtype=float)\nprint(f)\n\n[ 1.  2.  4.  7. 11. 16.]\n\n\nnp.gradient함수는 1차원 배열의 내부에 있는 각각의 값들은 \\(x\\)값이 거리가 \\(dx\\)=1씩 변화할때마다의 함숫값\\(f(x)\\)들로 이해한다. 즉,다음과 같다.\n\nfor i in range(len(f)):\n    display(Markdown(rf'$x_{i+1}$에서의 함숫값 $f(x_{i+1})$ = {f[i]}'))\n\n\\(x_1\\)에서의 함숫값 \\(f(x_1)\\) = 1.0\n\n\n\\(x_2\\)에서의 함숫값 \\(f(x_2)\\) = 2.0\n\n\n\\(x_3\\)에서의 함숫값 \\(f(x_3)\\) = 4.0\n\n\n\\(x_4\\)에서의 함숫값 \\(f(x_4)\\) = 7.0\n\n\n\\(x_5\\)에서의 함숫값 \\(f(x_5)\\) = 11.0\n\n\n\\(x_6\\)에서의 함숫값 \\(f(x_6)\\) = 16.0\n\n\n위에서 넘파이의 그래디언트는 끝값을 제외한 내부의 요소에는 중앙차분근사 양끝값에 대해서는 후향차분 또는 전향차분을 사용한다고 언급했었다. 계산한,\\(x_2,x_5\\)에서 미분계수의 2차중앙차분근사는 다음과 같다.\n\\[\\begin{aligned}\n&f^{'}(x_2) \\overset{\\sim}{=} \\frac{f(x_3)-f(x_1)}{2h} = \\frac{4-1}{2} = 1.5 \\\\\n&f^{'}(x_5) \\overset{\\sim}{=} \\frac{f(x_6)-f(x_4)}{2h} = \\frac{16-7}{2} = 4.5 \\\\\n&\\text{where, } h = x_3-x_2 = x_2-x_1 = 1\n\\end{aligned}\\]\n1차원 배열의 가장 처음에 오는 값에 전향차분근사를 사용하고 가장 마지막에 오는 값에서는 후향차분근사를 사용한다.\n\\[\\begin{aligned}\n&f^{'}(x_1) = \\frac{f(x_2) - f(x_1)}{h} = \\frac{2-1}{1} = 1 \\\\\n&f^{'}(x_6) = \\frac{f(x_6) - f(x_5)}{h} = \\frac{16-11}{1} = 5\n\\end{aligned}\\]\n계산한 값과 실제로 일치하는지 확인.\n\nnum_diff = np.gradient(f)\nprint(\"np.gradient의 출력값\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$에서의 도함수의 근삿값 $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradient의 출력값\n[1.  1.5 2.5 3.5 4.5 5. ]\n\n\n\\(x_1\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 1.0\n\n\n\\(x_2\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 1.5\n\n\n\\(x_3\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 2.5\n\n\n\\(x_4\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 3.5\n\n\n\\(x_5\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 4.5\n\n\n\\(x_6\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 5.0\n\n\n\n\nEx) \\(dx \\not = 1\\) (default가 아닐 경우)\n거리\\(dx=2\\)일때 계산한,\\(x_2,x_5\\)에서 미분계수의 2차중앙차분근사는 다음과 같다.\n\\[\\begin{aligned}\n&f^{'}(x_2) \\overset{\\sim}{=} \\frac{f(x_3)-f(x_1)}{2h} = \\frac{4-1}{4} = 0.75 \\\\\n&f^{'}(x_5) \\overset{\\sim}{=} \\frac{f(x_6)-f(x_4)}{2h} = \\frac{16-7}{4} = 2.25 \\\\\n&\\text{where, } h = x_3-x_1 = x_6-x_4 = 2\n\\end{aligned}\\]\n거리가 \\(dx=2\\)일때 배열의 양 끝값에서 전향,후향차분근사를 통한 미분계수의 값은 다음과 같다.\n\\[\\begin{aligned}\n&f^{'}(x_1) = \\frac{f(x_2) - f(x_1)}{h} = \\frac{2-1}{2} = 0.5 \\\\\n&f^{'}(x_6) = \\frac{f(x_6) - f(x_5)}{h} = \\frac{16-11}{2} = 2.5\n\\end{aligned}\\]\nx값 사이의 거리\\(dx\\)를 바꾸고 싶다면? => 두번째 인수에 스칼라 대입하면 된다.\n\ndx = 2\nnum_diff = np.gradient(f,dx)\nprint(\"np.gradient의 출력값\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$에서의 도함수의 근삿값 $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradient의 출력값\n[0.5  0.75 1.25 1.75 2.25 2.5 ]\n\n\n\\(x_1\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 0.5\n\n\n\\(x_2\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 0.75\n\n\n\\(x_3\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 1.25\n\n\n\\(x_4\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 1.75\n\n\n\\(x_5\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 2.25\n\n\n\\(x_6\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 2.5\n\n\n\n\nEx) x값의 좌표를 직접 정해주는 경우\n이전에는 각각의 인덱스간의 거리는 모두 동일하게 기본값 1이거나 다른값을 사용했다. 그러지 않고 \\(x_1,x_2,\\dots,x_6\\)의 좌표를 직접 지정해주는 것도 가능하다. 함수의 2번재 인수에 좌표를 직접 넣어주면 된다.\n먼저 x값의 좌표를 다음과 같다고 해보자.\n\nx = np.array([0., 1., 1.5, 3.5, 4., 6.], dtype=float)\nf = np.array([1,2,4,7,11,16],dtype=float)\nprint(\"각각의 좌표와 함숫값\")\nfor i in range(len(x)):\n    display(Markdown(rf'$x_{i+1}$ = {x[i]}, $f(x_{i+1})$ = {f[i]}'))\n\n각각의 좌표와 함숫값\n\n\n\\(x_1\\) = 0.0, \\(f(x_1)\\) = 1.0\n\n\n\\(x_2\\) = 1.0, \\(f(x_2)\\) = 2.0\n\n\n\\(x_3\\) = 1.5, \\(f(x_3)\\) = 4.0\n\n\n\\(x_4\\) = 3.5, \\(f(x_4)\\) = 7.0\n\n\n\\(x_5\\) = 4.0, \\(f(x_5)\\) = 11.0\n\n\n\\(x_6\\) = 6.0, \\(f(x_6)\\) = 16.0\n\n\n각각의 좌표에서 도함수의 근삿값을 구하면 아래와 같다.(수식 계산은 잘 모르겠네요 … 추후에 더 공부하겠습니다!)\n\nnum_diff = np.gradient(f,x)\nprint(\"np.gradient의 출력값\")\nprint(num_diff)\nfor i in range(len(num_diff)):\n    display(Markdown(rf'$x_{i+1}$에서의 도함수의 근삿값 $\\frac{{dy}}{{dx}}|_{{x = x_{i+1}}}$ ~= {num_diff[i]}'))\n\nnp.gradient의 출력값\n[1.  3.  3.5 6.7 6.9 2.5]\n\n\n\\(x_1\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_1}\\) ~= 1.0\n\n\n\\(x_2\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_2}\\) ~= 2.9999999999999996\n\n\n\\(x_3\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_3}\\) ~= 3.5\n\n\n\\(x_4\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_4}\\) ~= 6.700000000000001\n\n\n\\(x_5\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_5}\\) ~= 6.899999999999999\n\n\n\\(x_6\\)에서의 도함수의 근삿값 \\(\\frac{dy}{dx}|_{x = x_6}\\) ~= 2.5"
  },
  {
    "objectID": "posts/numpy pandas/수치미분 & np.gradient/수치미분 & np.gradient.html#차원-배열의-경우-1",
    "href": "posts/numpy pandas/수치미분 & np.gradient/수치미분 & np.gradient.html#차원-배열의-경우-1",
    "title": "Finite Difference Method with np.gradient",
    "section": "2차원 배열의 경우",
    "text": "2차원 배열의 경우\n2차원 배열의 경우 axis=0(세로축)과 axis=1(가로축) 두 축방향으로 계산한 도함수의 근삿값을 반환한다.axis=0일 경우 각각의 열마다 따로따로 독립적으로 \\(x_1,x_2...\\)에 대한 함숫값\\(f(x_1),f(x_2),\\dots\\)이 있다고 생각하면 되고 axis=1일 경우 각각의 행마다 따로따로 독립적으로 \\(x_1,x_2...\\)에 대한 함숫값\\(f(x_1),f(x_2),\\dots\\)이 있다고 생각하면 된다.또한 1차원 배열과 유사하게 각각의 행,열의 끝값에는 전향or후향차분근사를 행,열의 내부에 있는 값은 중앙차분근사를 사용한다.\n\nEx) \\(dx=1,dy=1\\)\n2차원 배열은 다음과 같다.\n\nnp.array([[1, 2, 6], [3, 4, 5]], dtype=float)\n\narray([[1., 2., 6.],\n       [3., 4., 5.]])\n\n\n\nax0_difcoef,ax1_difcoef= np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float))\nprint(f'axis = 0 방향으로 도함수의 근삿값 계산 \\n{ax0_difcoef}')\nprint(f'axis = 1 방향으로 도함수의 근삿값 계산 \\n{ax1_difcoef}')\n\naxis = 0 방향으로 도함수의 근삿값 계산 \n[[ 2.  2. -1.]\n [ 2.  2. -1.]]\naxis = 1 방향으로 도함수의 근삿값 계산 \n[[1.  2.5 4. ]\n [1.  1.  1. ]]\n\n\n\n\nEx) \\(dx \\not = 1,dy \\not = 1\\) (default가 아닌 경우)\n각각의 행,열마다 거리를 따로 설정해주고 싶은 경우? => 스칼라 2개 인수로 전달.\n\ndx = 2;dy = 2\nax0_difcoef,ax1_difcoef= np.gradient(np.array([[1, 2, 6], [3, 4, 5]], dtype=float),dx,dy)\nprint(f'axis = 0 방향으로 도함수의 근삿값 계산 \\n{ax0_difcoef}')\nprint(f'axis = 1 방향으로 도함수의 근삿값 계산 \\n{ax1_difcoef}')\n\naxis = 0 방향으로 도함수의 근삿값 계산 \n[[ 1.   1.  -0.5]\n [ 1.   1.  -0.5]]\naxis = 1 방향으로 도함수의 근삿값 계산 \n[[0.5  1.25 2.  ]\n [0.5  0.5  0.5 ]]"
  },
  {
    "objectID": "posts/numpy pandas/수치미분 & np.gradient/수치미분 & np.gradient.html#전향차분근사-유도",
    "href": "posts/numpy pandas/수치미분 & np.gradient/수치미분 & np.gradient.html#전향차분근사-유도",
    "title": "Finite Difference Method with np.gradient",
    "section": "전향차분근사 유도",
    "text": "전향차분근사 유도\n\\(x_1,x_2,\\dots,x_{i-1},x_i,x_{i+1},\\dots,x_n\\)과 각각에 대응하는 함숫값 \\(f(x_1),f(x_2),\\dots,f(x_{i-1}),f(x_i),f(x_{i+1}),\\dots,f(x_n)\\) 주어진 데이터라고 가정하자. 목적은 x_i에서의 미분계수를 구하는 것이다. \\(a = x_i\\)에서 함수\\(f(x)\\)의 테일러 급수 근사는 다음과 같다. \\[f(x) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x-x_i) + \\frac{f^{''}(x_i)}{2!}(x-x_i)^2 + \\dots \\]\n\\(x=x_{i+1}\\)에서의 함숫값은 다음과 같다. \\[f(x_{i+1}) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x_{i+1}-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x_{i+1}-x_i) + \\frac{f^{''}(x_i)}{2!}(x_{i+1}-x_i)^2 + \\dots \\]\n\\(f'(x_i)\\)가 포함된항만 남겨두고 나머지는 이항하면 다음과 같다. \\[f^{'}(x_i)(x_{i+1}-x_i) = f(x_{i+1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}(x_{i+1}-x_i)^2 + \\dots\\]\n\\(h = x_{i+1}-x_i\\)로 두고 양변을 h로 나누면 다음과 같다. \\[f'(x_i) = \\frac{f(x_{i+1})}{h} - \\frac{f(x_i)}{h} - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!}\\]\n여기서 우변의 두개의 항만 남겨두고 \\(O(h) = - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\)라 하면 다음과 같다. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i+1})-f(x_i)}{h}\\\\\n&O(h) = - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\\\\n&\\text{where, } h = x_{i+1} - x_i \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/numpy pandas/수치미분 & np.gradient/수치미분 & np.gradient.html#후향차분근사-유도",
    "href": "posts/numpy pandas/수치미분 & np.gradient/수치미분 & np.gradient.html#후향차분근사-유도",
    "title": "Finite Difference Method with np.gradient",
    "section": "후향차분근사 유도",
    "text": "후향차분근사 유도\n\\(x_1,x_2,\\dots,x_{i-1},x_i,x_{i+1},\\dots,x_n\\)과 각각에 대응하는 함숫값 \\(f(x_1),f(x_2),\\dots,f(x_{i-1}),f(x_i),f(x_{i+1}),\\dots,f(x_n)\\) 주어진 데이터라고 가정하자. 목적은 x_i에서의 미분계수를 구하는 것이다. \\(a = x_i\\)에서 함수\\(f(x)\\)의 테일러 급수 근사는 다음과 같다. \\[f(x) = \\sum_{n=0}^{\\infty}\\frac{f^{n}(x_i)}{n!}(x-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x-x_i) + \\frac{f^{''}(x_i)}{2!}(x-x_i)^2 + \\dots \\]\n\\(f(x_i)\\)는 다음과 같다. \\[f(x_{i-1}) = \\sum_{n=0}^{\\infty}\\frac{f^n(x_i)}{n!}(x_{i-1}-x_i)^n = f(x_i) + \\frac{f^{'}(x_i)}{1!}(x_{i-1}-x_i) + \\frac{f^{''}(x_i)}{2!}(x_{i-1}-x_i)^2+\\dots \\]\n1차미분이 포함된 항만 남기고 나머지는 이항하면 다음과 같다. \\[f^{'}(x_i)(x_{i-1}-x_i) = f(x_{i-1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}(x_{i-1}-x_i)^2-\\frac{f^{'''}(x_i)}{3!}(x_{i-1}-x_i)^3-\\dots \\]\n\\(h = x_{i} - x_{i-1}\\)로 놓으면 다음과 같다. \\[f^{'}(x_i)(-h) = f(x_{i-1}) - f(x_i) - \\frac{f^{''}(x_i)}{2!}h^2+\\frac{f^{'''}(x_i)}{3!}h^3-\\dots \\]\n양변을 \\(-h\\)로 나누면 다음과 같다.\n\\[\\begin{aligned}\nf^{'}(x_i) &= \\frac{f(x_{i-1})}{-h} + \\frac{f(x_i)}{h} + \\frac{f^{''}(x_i)}{2!}h-\\frac{f^{'''}(x_i)}{3!}h^2+\\dots \\\\\n&=\\frac{f(x_i)-f(x_{i-1}) }{h} +  \\frac{f^{''}(x_i)}{2!}h-\\frac{f^{'''}(x_i)}{3!}h^2+\\dots\n\\end{aligned}\\]\n마찬가지로 우변의 두개 항만 남겨두고 \\(O(h) = \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} + \\dots\\)라 하면 다음과 같다. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i})-f(x_{i-1})}{h}\\\\\n&O(h) = \\frac{h}{2!}f^{''}(x_i) - \\frac{h^2}{3!}f{'''}(x_i)+\\dots \\\\\n&\\text{where, } h = x_{i} - x_{i-1}, \\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/numpy pandas/수치미분 & np.gradient/수치미분 & np.gradient.html#중앙차분근사-유도",
    "href": "posts/numpy pandas/수치미분 & np.gradient/수치미분 & np.gradient.html#중앙차분근사-유도",
    "title": "Finite Difference Method with np.gradient",
    "section": "중앙차분근사 유도",
    "text": "중앙차분근사 유도\n전향차분근사와 후향차분근사의 유도과정에서의 테일러 전개식은 다음과 같다. \\[f'(x_i) = \\frac{f(x_{i+1})}{h} - \\frac{f(x_i)}{h} - \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} - ...\\] \\[f'(x_i) = \\frac{f(x_{i})}{h} - \\frac{f(x_{i-1})}{h} + \\frac{hf^{''}(x_i)}{2!} - \\frac{h^2f^{'''}(x_i)}{3!} + ...\\]\n두 식을 더해주면 다음과 같다.\n\\[\\begin{aligned}\n&2f^{'}(x_i) = \\frac{f(x_{i+1})-f(x_{i-1})}{h} - \\frac{2h^2f^{'''}(x_i)}{3!} \\\\\n&\\Leftrightarrow f^{'}(x_i) = \\frac{f(x_{i+1})-f(x_{i-1})}{2h} - \\frac{h^2f^{'''}(x_i)}{3!} - ...\n\\end{aligned}\\]\n마찬가지로 우변의 두개 항만 남겨두고 절단오차\\(O(h^2) = -\\frac{h^2f^{'''}(x_i)}{3!} - \\dots\\)라 하면 다음과 같다. \\[\\begin{align}\n&f'(x_i) \\overset{\\sim}{=} \\frac{f(x_{i+1})-f(x_{i-1})}{2h}\\\\\n&O(h) = - \\frac{h^2}{3!}f{'''}(x_i)+\\dots \\\\\n&\\text{where, } h = x_{i} - x_{i-1}\\,\\,\\text{or}\\,\\, h = x_{i+1} - x_i\\nonumber\n\\end{align}\\]"
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html",
    "href": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html",
    "title": "Auto-Encoding Variational Bayes(작성중)",
    "section": "",
    "text": "variational auto-encoder는 generative model로서 intractable한 posterior를 포함하는 latent variable이 있고 large dataset에도 잘 동작한다. 이는 1)variational lower bound를 reparamet-erization을 통해 샘플링하고 SGD를 사용하며 2)posterior를 variational inference로 구한 뒤 encoder(inference model)로 학습시키기에 가능하다. 이번 포스트에서는 논문에서 사용된 방법들에 대해서 자세하게 수식적으로 설명한다."
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#latent-variable-model",
    "href": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#latent-variable-model",
    "title": "Auto-Encoding Variational Bayes(작성중)",
    "section": "Latent Variable Model",
    "text": "Latent Variable Model\nlatent variable model은 관측가능한 데이터를 변수와 관련짓는 통계학적 모델이다. 이때의 변수를 latent variable이라고 하며 데이터가 어떻게 생성되는지에 영향을 미친다. latent variable model에서 데이터가 만들어지는 과정은 다음과 같다.\n(Data Generation Process) 1. latent variable \\(\\bf{z}\\)가 \\(p(\\bf{z})\\)에서 먼저 sampling된다.  2. observed data \\(\\bf{x}\\)가 그 후 \\(p({\\bf{x}}|{\\bf{z}}) = p({\\bf{x}}|f_{\\theta}({\\bf{z}}))\\)에서 sampling된다. \n\n\\(\\bf{z}\\)의 dimension은 일반적으로 \\(\\bf{x}\\)의 dimension보다 작다고 가정한다.\n여기서 \\(f_\\theta(\\bf{z})\\)는 \\(\\theta\\)를 parameter로 갖는 vector function으로 여러개(또는 하나의)의 output을 가집니다.\n\n예를 들어 학교에 “고양이”를 주제로 하여 과제를 제출해야 한다고 가정해보자. latent variable은 교수님이 지정한 글자크기,글자간격,글자수,주제(고양이)로 비유할 수 있으며 제출할 과제는 이러한 잠재변수들이 결과를 미쳤을 것이라 볼 수 있다. 특히 주목할 점은 latent variable \\(\\bf{z}\\)가 그대로 영향을 미칠 수도 있고 적절한 transform(function)을 거친 \\(f_{\\theta}(\\bf{z})\\)가 영향을 미칠 수도 있다는 것이다. 이는 주제가 “고양이”로 선정되었어도 누군가는 “고양이에게 줘야하는 음식”으로 다른 누군가는 “고양이의 행동분석”으로 과제를 제출하는 것에 비유할 수 있다."
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#generative-model",
    "href": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#generative-model",
    "title": "Auto-Encoding Variational Bayes(작성중)",
    "section": "Generative Model",
    "text": "Generative Model\nGenerative Model(생성모델)은 observed data와 유사하면서도 다른 new samples을 생성해내는 것이 목적이다. 이를 다시말하면 Generative model의 목적은 \\(p_{data}(x)\\)를 구하는 것이라고 할 수 있다. 관측된 데이터를 생성하는 확률분포 \\(p_{data}(\\bf{x})\\)를 알면 sampling을 통해서 생성해낼 수 있기 때문이다."
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#variational-inference",
    "href": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#variational-inference",
    "title": "Auto-Encoding Variational Bayes(작성중)",
    "section": "Variational Inference",
    "text": "Variational Inference\n\\(p(\\bf{{\\bf{z|x}}})\\)는 어떻게 구할 수 있을까? 논문에서는 variational infernece를 사용한다.\n\\[p({\\bf{z|x}}) = \\frac{p({\\bf{x|z}})p({\\bf{z}})}{\\int_{\\bf{z}}p({\\bf{x|z}})p(\\bf{z})d{\\bf{z}}}\\]\nvariational inference에서 풀고자 하는 문제는 posterior를 구하는 것이다. posterior인 \\(p(\\bf{z|x})\\)는 일반적으로 intractable하다. 왜냐하면 분모에 있는 \\(\\bf{z}\\)에 대한 적분이 일반적으로 너무 고차원이여서 계산이 불가능하기 때문이다. variational inference는 \\(p(\\bf{z|x})\\)를 근사적으로라도 구하기 위해 \\(q_{\\boldsymbol{\\phi}}(\\bf{z|x})\\)를 정의한 후 이를 \\(p(\\bf{z|x})\\)에 충분히 가깝게 다가가도록 하는 방식을 취한다. 이는 수학적으로 두 분포간의 차이를 알려주는 KL-divergence를 최소화하는 \\(q_{\\boldsymbol{\\phi}}(\\bf{z|x})\\)를 구하는 문제이다. 여기서 \\(q_{\\boldsymbol{\\phi}}(\\bf{z|x})\\)는 임의적으로 정할 수 있는 확률분포임에 주목하자. \\(q_{\\boldsymbol{\\phi}}(\\bf{z|x})\\)는 우리가 잘 알고있고 더 적은 파라미터를 가지며 계산이 쉬운 편리한 함수로 가정하여 문제를 쉽게 풀 수 있게 해준다.\n(definition of KL-divergence)\n\\[\\begin{aligned}\n\\text{D}_{KL}[q_{\\boldsymbol{\\phi}}({\\bf{z|x}})||p({\\bf{z|x}})] &:= \\int_zq_{\\boldsymbol{\\phi}}({\\bf{z|x}})\\text{log}\\frac{q_{\\boldsymbol{\\phi}}({\\bf{z|x}})}{p({\\bf{z|x}})}dz \\\\\n\\end{aligned}\\]\n(variational inference)\n\\[\\begin{aligned}\n&{\\hat{q_{\\boldsymbol{\\phi}}}} = \\underset{q_{\\boldsymbol{\\phi}}}{\\text{argmin}}\\,\\text{D}_{KL}[q_{\\boldsymbol{\\phi}}({\\bf{z|x}})||p({\\bf{z|x}})]\n\\end{aligned}\\]\n여기서 \\(q_{\\boldsymbol{\\phi}}\\)는 \\(\\boldsymbol{\\phi}\\)가 parameter이므로 위의 KL-divergence를 최소화하는 \\(q_{\\boldsymbol{\\phi}}\\)를 찾는 문제는 최소화하는 \\(\\boldsymbol{\\phi}\\)를 찾는것과 동일하다. 즉,다음과 같다.\n(variational inference)\n\\[\\begin{aligned}\n&{{\\boldsymbol{\\phi}}} = \\underset{{\\boldsymbol{\\phi}}}{\\text{argmin}}\\,\\text{D}_{KL}[q_{\\boldsymbol{\\phi}}({\\bf{z|x}})||p({\\bf{z|x}})]\n\\end{aligned}\\]\nKL-divergence를 log likelyhood에서 찾을 수 있다. log likelyhood를 전개하면 다음과 같다.\n(log likelyhood expansion)\n\\[\\begin{aligned}\n\\text{log}\\,(p_{\\boldsymbol{\\theta}}({\\bf{x}}))& = \\int_z\\text{log}\\,p_{\\boldsymbol{\\theta}}({\\bf{x}})q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}}\\quad\\leftarrow \\int_z q_{\\boldsymbol{\\phi}}(\\bf{z|x})d{\\bf{z}}= 1\\\\\n&=\\int_z\\text{log}\\left(\\frac{p(\\bf{z,x})}{p(\\bf{z|x})}\\right)q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}}\\quad\\leftarrow p_{\\boldsymbol{\\theta}}({\\bf{x}}) = \\frac{p({\\bf{x,z}})}{p(\\bf{z|x})}\\\\\n&=\\int_z\\text{log}\\left(\\frac{p(\\bf{z,x})}{q_{\\boldsymbol{\\phi}}(\\bf{z|x})}\\cdot\\frac{q_{\\boldsymbol{\\phi}}({\\bf{z|x}})}{p({\\bf{z|x}})}\\right)q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}}\n\\\\\n&= \\int_z\\text{log}\\left(\\frac{p(\\bf{z,x})}{q_{\\boldsymbol{\\phi}}(\\bf{z|x})}\\right)q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}} + \\int_z\\text{log}\\left(\\frac{q_{\\boldsymbol{\\phi}}(\\bf{z,x})}{p(\\bf{z|x})}\\right)q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}}\\\\\n&= \\mathcal{L(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}})} + D_{KL}(q_{\\boldsymbol{\\phi}}({\\bf{z|x}})||p({\\bf{z|x}}))\n\n\\end{aligned}\\]\n\nELBO(\\(\\mathcal{L}\\))는 evidence lower bound의 약자로 likelyhood(evidence,용어만다르다)의 하한(lower bound)이다. 즉,모든 x에서 evidence는 모든 ELBO보다 크다.\n\\(\\text{log}\\,p({\\bf{x}})\\) = evidence = log likelyhood이다.(헷갈리지 말자…)\n자세한 ELBO에 대한 증명은 Appendix 참고\n\n윗 식을 보면 결국 KL-divergence를 maximize하는 \\({\\boldsymbol{\\phi}}\\)를 찾는 것은 ELBO를 minimize하는 \\({\\boldsymbol{\\phi}}\\)를 찾는 것과 같은 문제다.\n\\[\\begin{aligned}\n\\hat{\\boldsymbol{\\boldsymbol{\\phi}}} = \\underset{\\boldsymbol{\\boldsymbol{\\phi}}}{\\text{argmax}}\\,\\mathcal{L(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}})}\n\\end{aligned}\\]\n정리하자면 목적은 \\(p(\\bf{z|x})\\)를 구하는 것이었다. 이는 variational inference에서는 우리가 알고 있는 계산하기 편리한 형태인 \\(q_{\\boldsymbol{\\phi}}\\)를 \\(p\\)와 비슷하게 만드는 방법이었고 수학적으로 이는 KL-divergence를 minimize하는 \\(q_{\\boldsymbol{\\phi}}\\) 또는 \\(\\boldsymbol{\\phi}\\)를 찾는 것과 같았으며 계속해서 수식전개하면 결국에는 ELBO를 maximize하는 \\(\\boldsymbol{\\boldsymbol{\\phi}}\\)를 찾는 문제와 같았다. 결과적으로 보면이제 ELBO를 maximize하는 optimization문제가 되었음을 알 수 있다. 여기까지 3번 MLE로 발생하는 문제들을 막기위해 이상적인 sampling함수(posterior)를 얻는 과정을 풀었다. 이제 MLE를 풀기만 하면 된다."
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#maximum-likleyhood-estimation",
    "href": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#maximum-likleyhood-estimation",
    "title": "Auto-Encoding Variational Bayes(작성중)",
    "section": "Maximum Likleyhood Estimation",
    "text": "Maximum Likleyhood Estimation\n위에서 사실은 살짝 MLE로 \\(\\theta\\)를 언급했었다. 구체적으로 하려는 것은 다음과 같다.\n\\[\\begin{aligned}\n\\hat{\\theta} = \\underset{{\\bf{\\theta}}}{\\text{argmax}}\\,\\text{log}\\,p_{\\boldsymbol{\\theta}}(\\bf{x})\n\\end{aligned}\\]\n\nlog likelyhood를 maximize하도록 바꿈.\n\n여기사 다시 나타나는 문제는 \\(p{\\bf{({\\bf{x}})}} = \\int_z p({\\bf{z}})p({\\bf{x|z}})d{\\bf{z}}\\)이다. 위에서도 언급했듯이 이는 \\(\\bf{z}\\)에 대한 매우 고차원 적분으로 explict하게 구할 수 없으며 Monte carlo method의 경우에도 차원이 너무 커서 추정하는데 매우 오랜시간이 걸리므로 거의 불가능에 가깝다. 여기서 문제를 약간 우회하여 \\(\\text{log}\\,p(\\bf{x})\\)의 lowerbound인 ELBO를 maximize하는 문제로 바꿔보자. evidence의 lowerbound인 ELBO를 maximize하면 evidence자체도 어느정도 maximize될 것이다.\n\\[\\begin{aligned}\n&\\hat{\\theta} = \\underset{{\\bf{\\theta}}}{\\text{argmax}}\\,\\text{log}\\,p(\\bf{x}) \\approx \\underset{{\\bf{\\theta}}}{\\text{argmax}}\\,\\mathcal{L}({\\boldsymbol{\\theta}},\\boldsymbol{\\boldsymbol{\\phi}};{\\bf{x}}) \\\\\n\\end{aligned}\\]\n여기서 알 수 있는 것은 문제가 하나의 ELBO에 대한 \\(\\phi,\\theta\\)의 maximization 문제로 바뀌다는 것이다. 정리하자면 이 문제만 풀게되면 우리는 **\\(q_{\\phi}({\\bf{x|z}})\\approx p({\\bf{x|z}})\\)인 이상적인 샘플링 함수인 posterior를 얻을 뿐만 아니라 MLE도 풀 수 있게 된다.\n\\[\\begin{aligned}\n&\\boldsymbol{\\theta,\\boldsymbol{\\phi}}=\\underset{{\\boldsymbol{\\theta,\\boldsymbol{\\phi}}}}{\\text{argmax}}\\,\\mathcal{L}(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};\\bf{x})\\\\\n\\end{aligned}\\]\n그러나 ELBO도 아직은 intractable한 적분이 포함되기 때문에 얼핏보면 풀 수 없는 상태처럼 보이지만 자세히 보면 ELBO는 우리가 가정한 여러개의 계산이 편리한 함수이므로 optimization이 가능한 형태임을 알 수 있다. 여기서 부터는 직접 ELBO를 풀어서 optimization을 해보자."
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#solving-optimization-problem",
    "href": "posts/paper study/auto-encoding variational bayes/Auto-Encoding Variational Bayes.html#solving-optimization-problem",
    "title": "Auto-Encoding Variational Bayes(작성중)",
    "section": "Solving Optimization Problem",
    "text": "Solving Optimization Problem\n먼저 ELBO와 우리가 알고 있는 사실들을 정리하자. 위에서 \\(q_{\\phi}\\)는 임의적으로 정할 수 있는 계산하기 편리한 함수이며 여기서는 정규분포로 가정해보자.\n(Goal)\n\\[\\begin{aligned}\n&\\boldsymbol{\\theta,\\boldsymbol{\\phi}}=\\underset{{\\boldsymbol{\\theta,\\boldsymbol{\\phi}}}}{\\text{argmax}}\\,\\mathcal{L}(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};\\bf{x})\\\\\n\\end{aligned}\\]\n(ELBO)\n\\[\\begin{aligned}\n\\mathcal{L(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}})}&:= \\int_z\\text{log}\\left(\\frac{p(\\bf{z,x})}{q_{\\boldsymbol{\\phi}}(\\bf{z|x})}\\right)q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}}\\\\\n\\end{aligned}\\]\n(Assumption)\n\\[\\begin{aligned}\n&p({\\bf{z}}) = \\mathcal{N}({\\bf{z}}|0,{\\bf{I}})\\\\\n&p({\\bf{x|z}}) = \\mathcal{N}({\\bf{x}}|f_{\\theta}(\\bf{z}),\\boldsymbol{\\sigma^2}{\\bf{I}})\\\\\n&q_{\\boldsymbol{\\phi}}({\\bf{z|x}}) = \\mathcal{N}({\\bf{z}};\\boldsymbol{\\mu,\\sigma^2}\\bf{I})\\\\\n\n\\end{aligned}\\]\n여기서 ELBO는 다음과 같이 전개할 수 있다.(자세한 증명은 Appendix 참고)\n(ELBO expansion)\n\\[\\begin{aligned}\n\\mathcal{L(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}})}&= \\int_z\\text{log}\\left(\\frac{p(\\bf{z,x})}{q_{\\boldsymbol{\\phi}}(\\bf{z|x})}\\right)q_{\\boldsymbol{\\phi}}({\\bf{z|x}})d{\\bf{z}}\\\\\n&=\\mathbb{E}_{q_{\\phi}({\\bf{z|x}})}\\left[\\text{log}\\,(p({\\bf{x}}|f_{\\theta}({\\bf{z}})))\\right] - D_{KL}(q_{\\phi}({\\bf{z|x}})||p({\\bf{z}}))\\\\\n\n\\end{aligned}\\]\n여기서 잠깐 전개된 ELBO를 살펴보면 많은 insight를 준다. 윗 식의 RHS의 첫번째 항을 \\(\\phi,\\theta\\)에 대해 maximize한다는 것은 \\(\\theta,\\phi\\)를 적절히 학습하여 \\(q_{\\phi}\\)에서 sampling된 \\(\\bf{z}\\)로부터 \\(x\\)를 다시 복원할 확률을 최대화 한다는 것으로 일반적인 Auto-encoder에서 reconstruction error를 최소화 하는것과 같다.** 여기서 두번째 항은 Regularization의 역할을 하는데 이는 posterior를 근사한 \\(q_{\\phi}(\\bf{z|x})\\)가 \\(p(\\bf{z})\\)와 비슷하도록 유지하게 해주며 따라서 우리는 학습을 완료한 decoder에서 그냥 \\(q_{\\phi}\\)인 encoder를 떼버리고 정규분포 \\(p(\\bf{z})\\)에서 \\(\\bf{z}\\)를 샘플링 한 후 Network의 input으로 주면 된다.\n\\[\\begin{aligned}\n\\mathcal{L}(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}},{\\bf{x}}) &= \\mathbb{E}_{Z\\sim q}\\left[\\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}}) \\right] \\\\\n&= \\int_z\\left[\\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}})\\right]q({\\bf{z|x}})d{\\bf{z}}\n\\end{aligned}\\]\n이전에도 계속 문제였던 \\(d\\bf{{z}}\\)가 또 등장한다. 그러므로 우리는 expectation을 구할 수 없다. 따라서 samping을 통해 expectation을 근사적으로 구하는 Monte carlo method를 사용한다. 이는 다음과 같다.\n\\[\\begin{aligned}\n\\mathcal{L}(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}}) &= \\mathbb{E}_{Z\\sim q}\\left[\\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}}) \\right] \\\\\n&= \\int_z\\left[\\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}})\\right]q({\\bf{z|x}})d{\\bf{z}} \\\\\n&\\approx \\frac{1}{L}\\sum_{l=1}^L[\\text{log}\\,p({\\bf{x,z}}^{(l)}) - \\text{log}\\,q_{\\boldsymbol{{\\boldsymbol{\\phi}}}}({\\bf{z}}|{\\bf{x}}^{(l)})]\n\\end{aligned}\\]\n\\[\\text{where } {\\bf{z}}^{(l)} \\sim q_{\\boldsymbol{\\phi}}({\\bf{z}})\\]\nELBO를 근사적으로 구했으므로 최적화를 위해 gradient를 구해야 한다. 먼저 \\(\\theta\\)에 대한 gradient를 구해보자.\n\\[\\begin{aligned}\n\\nabla_{\\theta}\\mathcal{L}(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}}) &= \\mathbb{E}_{Z\\sim q}\\left[\\nabla_\\theta\\text{log}\\,p({\\bf{x,z}})\\right]\\\\\n&\\approx \\frac{1}{L}\\sum_{l=1}^L\\nabla_\\theta\\text{log}\\,p({\\bf{x,z}}^{(l)})\n\\end{aligned}\\]\n\n뒷항은 \\(\\theta\\)에 대해 parameterize되어있지 않으므로 미분과정에서 제거됨.\n\n\\(\\theta\\)에 대한 gradient는 전혀 문제없이 잘 구해짐을 알 수 있다. 하지만 문제는 \\(\\boldsymbol{\\phi}\\)에 대한 gradient를 구할 때 발생한다. \\(f({\\bf{z}}) = \\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}})\\)라 할 때 gradient는 다음과 같이 구해진다.(증명생략)\n\\[\\begin{aligned}\n\\nabla_{\\boldsymbol{\\phi}}\\mathbb{E}_{q_{\\boldsymbol{\\phi}}({\\bf{z})}}[f({\\bf{z}})] &= \\mathbb{E}_{q_{\\boldsymbol{\\boldsymbol{\\phi}}(\\bf{z})}} \\nabla_{q_{\\boldsymbol{\\boldsymbol{\\phi}}(\\bf{z})}}\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z}}) \\\\&\\approx \\frac{1}{L}\\sum_{l=1}^L f({\\bf{z}})\\nabla_{q_{\\boldsymbol{\\boldsymbol{\\phi}}({\\bf{z}})}}\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z}}^{(l)})\n\\end{aligned}\\]\n갑자기 로그가 들어간 형태로 gradient가 구해진다. 이러한 gradient에 대한 추정량은 unbiased estimator이지만 high variance를 가진다고 한다. 그러므로 샘플링을 아주많이(거의무한하게)취하는 것이 아니면 이 값은 수렴하지 않는 값이므로 적당한 샘플링을 통해서 문제를 풀어야 하는 우리의 방식에는 맞지않는다. 그러므로 샘플을 대체적으로 취하는 방식인 reparameterization을 사용한다. 이를 사용하면 unbiased estimator를 얻을뿐만 아니라 low variance를 가진다. (더 자세한 논의는 링크에서 확인 가능)\n(reparameterization trick) \\({\\bf{z}}\\)가 conditional distribution인 \\(q_{\\boldsymbol{\\phi}}(\\bf{z|x})\\)를 따르는 continuous 또는 discrete random-variable일때 \\({\\bf{z}} = g_{\\boldsymbol{\\phi}}(\\boldsymbol{\\epsilon},\\bf{x})\\)인 g에 대해 deterministic한 random variable로 나타낼 수 있다는 것이다.(where, \\({\\bf{\\epsilon}}\\sim p(\\boldsymbol{\\epsilon})\\))\n그러므로 우리는 reparameterization trick을 사용해서 ELBO를 Monte carlo method를 통해 다르게 생각할 수 있다. 이렇게 ELBO를 구하면 Gradient estimator는 unbiased일 뿐만 아니라 low variance를 가진다.\n\\[\\begin{aligned}\n\\tilde{\\mathcal{L}}^A(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}}) &= \\mathbb{E}_{Z\\sim q}\\left[\\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}}) \\right] \\\\\n&= \\int_z\\left[\\text{log}\\,p({\\bf{x,z}})-\\text{log}\\,q_{\\boldsymbol{\\phi}}({\\bf{z|x}})\\right]q({\\bf{z|x}})d{\\bf{z}} \\\\\n&\\approx \\frac{1}{L}\\sum_{l=1}^L[\\text{log}\\,p({\\bf{x,z}}^{(l)}) - \\text{log}\\,q_{\\boldsymbol{{\\boldsymbol{\\phi}}}}({\\bf{z}}|{\\bf{x}}^{(l)})]\n\\end{aligned}\\]\n$$ {}^{(l)} = q_{}({}^{(l)},{}),{}p()\n\nA는 type A를 의미함.\n\n논문에는 이러한 estimator말고도 다른 방법으로 구한 것도 있다. 이는 다음과 같다.\n\\[\\begin{aligned}\n\\tilde{\\mathcal{L}}^B(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{x}}) = -D_{KL}(q_{\\boldsymbol{\\phi}}({\\bf{z|x}})||p_\\theta({\\bf{z}})) + \\frac{1}{L}\\sum_{l=1}^L(\\text{log}\\,p_\\theta({\\bf{x}}|{\\bf{z}}^{(l)})\n\\end{aligned}\\]\n$$ {}^{(l)} = q_{}({}^{(l)},{}),{}p()\n두번째 estimator를 통해서 ELBO를 maximization하는 최적화 문제를 푸는것이 왜 auto-encoder와 연결되는지 알 수 있다. ELBO를 maximize하려면 첫번째 term을 가능한 작게 해야 하는데 이는 observation을 보고 latent variable가 따르는 예측한 값이 얼마나 차이가 나느냐이다.(encoder의 성능?인 것 같다.)두번째 텀은 decoder가 얼마나 latenr variable에서 input space로 mapping을 잘 하느냐를 의미한다.(decoder의 성능?)\n여기까지 단 한개의 관측치에 대해서 모든 과정을 수행해봤다. 사실은 한 개의 관측치가 아니라 data set의 크기가 m인 mini-batch에 대하여 위의 과정을 수행해줘야 하므로 이것을 고려한 ELBO는 다음과 같다.\n\\[\\begin{aligned}\n\\mathcal{L}(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{X}}) \\approx\n\\tilde{L}^M(\\boldsymbol{\\boldsymbol{\\phi}},{\\boldsymbol{\\theta}};{\\bf{X}})=\\frac{N}{M}\\sum_{i=1}^M\\tilde{L}(\\boldsymbol{\\theta},\\boldsymbol{\\boldsymbol{\\phi}};{\\bf{x}}^{(i)})\n\\end{aligned}\\]\n\\[\\text{where } {\\bf{z}}^{(l)} \\sim q_{\\boldsymbol{\\phi}}({\\bf{z}})\\]"
  },
  {
    "objectID": "posts/paper study/auto-encoding variational bayes implement/Auto-Encoding Variational Bayes.html",
    "href": "posts/paper study/auto-encoding variational bayes implement/Auto-Encoding Variational Bayes.html",
    "title": "Untitled",
    "section": "",
    "text": "Import & Data Load\n\n# prerequisites\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms,datasets\nimport matplotlib.pyplot as plt\nfrom torch.distributions.multivariate_normal import MultivariateNormal\nfrom torch.distributions.normal import Normal\n\nbs = 100\n# MNIST Dataset\ntrain_dataset = datasets.MNIST(root='./mnist_data/', train=True, transform=transforms.ToTensor(), download=True)\ntest_dataset = datasets.MNIST(root='./mnist_data/', train=False, transform=transforms.ToTensor(), download=False)\n\n# Data Loader (Input Pipeline)\ntrain_loader = torch.utils.data.DataLoader(dataset=train_dataset, batch_size=bs, shuffle=True)\ntest_loader = torch.utils.data.DataLoader(dataset=test_dataset, batch_size=bs, shuffle=False)\n\n\nsig = torch.nn.Sigmoid()\n\n\nplt.figure(figsize=(2,1))\nplt.imshow(train_dataset[0][0].permute(1,2,0).numpy(),cmap=\"gray\")\n\n<matplotlib.image.AxesImage at 0x1efcc237df0>\n\n\n\n\n\n\n\n Modeling\n\nclass Encoder(nn.Module):\n    def __init__(self,x_dim,latent_dim): #latent space의 dimension은 output_dim // 2\n        super().__init__()\n        self.output_dim = latent_dim * 2 #hidden layer output dimension (= latent space dim * 2)\n        self.latent_dim = latent_dim     #latent variable dimension (= last hidden dim // 2)\n        \n        self.l1 = torch.nn.Linear(x_dim,x_dim // 2)\n        self.tanh = torch.nn.Tanh()\n        self.l2 = torch.nn.Linear(x_dim // 2,self.output_dim)\n        #relu\n        \n    def forward(self,x):\n        out = self.tanh(self.l1(x)) #l1 out\n        out = self.tanh(self.l2(out)) #l2 out\n        #Note : Dnn's output are mean,log variance\n        #half of last hidden layers output => mean\n        #half of last else hidden layers output => log variance        \n        mean = out[:,:self.latent_dim]    #mean\n        log_var = out[:,self.latent_dim:] #log_variance\n        return mean,log_var\n\n\nclass Z_Sampler(nn.Module):\n    def __init__(self,latent_dim):\n        super().__init__()\n        self.latent_dim = latent_dim\n    def forward(self,mean,log_var):\n        \"\"\"\n        Sampling z with reparameterization trick\n        \"\"\"\n        eps_sampler = MultivariateNormal(loc = torch.tensor([0]*(self.latent_dim)).float(),covariance_matrix = torch.eye(self.latent_dim))\n        eps_realizations = eps_sampler.sample()\n        #reparameterization trick z = mu + std * epsilon\n        #std = exp(ln(1/2 * variance))\n        z = mean + torch.exp(0.5 * log_var) * eps_realizations  \n        return z\n\n\nclass Decoder(nn.Module):\n    def __init__(self,latent_dim,out_dim):\n        super().__init__()\n        self.latent_dim = latent_dim\n        self.out_dim = out_dim \n        \n        self.l3 = torch.nn.Linear(latent_dim,latent_dim * 2)\n        self.tanh = torch.nn.Tanh()\n        self.l4 = torch.nn.Linear(latent_dim * 2,out_dim)\n        #softmax + cross entropy loss\n    def forward(self,z):\n        out = self.tanh(self.l3(z))\n        out = self.l4(out)\n        return out\n\n\nclass VAE(nn.Module):\n    def __init__(self,x_dim,latent_dim): #decoder추가해야함\n        super().__init__()\n        self.x_dim = self.out_dim = x_dim\n        self.latent_dim = latent_dim\n        \n        self.encoder = Encoder(x_dim,latent_dim)\n        self.z_sampler = Z_Sampler(latent_dim)\n        self.decoder = Decoder(latent_dim,x_dim)\n    def forward(self,x):\n        mean_1,log_var_1 = self.encoder(x) # input : x // output : parameter phi of q(z|x;\\phi)\n        z = self.z_sampler(mean_1,log_var_1) # input : parameter phi // output : realization of z\n        xhat=self.decoder(z)\n        return xhat\n\n\n_tmp = train_dataset[0][0].reshape(-1,28*28)\nvae = VAE(x_dim = 28 * 28,latent_dim = 10)\nprint(vae(_tmp).shape)\nplt.imshow(sig(vae(_tmp)).reshape(28,28).data,cmap = \"gray\")\n\ntorch.Size([1, 784])\n\n\n<matplotlib.image.AxesImage at 0x1efcc45adf0>"
  },
  {
    "objectID": "posts/paper study/autoaugmentation/autoaugmentation.html",
    "href": "posts/paper study/autoaugmentation/autoaugmentation.html",
    "title": "[Paper Study] AutoAugment : Learning Augmentation Strategies from Data",
    "section": "",
    "text": "한 줄 요약\n\nimage Augmentation에 강화학습 \\(\\to\\) sota!\n\n\n\nIntro & Abstract\n\nData augmentation은 image classifier의 성능을 향상시키는 쉬운 방법.\n왜 image augmentatio이 성능을 향상? \\(\\to\\) 데이터의 invariance들을 충분히 학습\n\ninvariance(불변성)란? : 차이가 있거나 변환이 적용된 후에도 무언가가 동일하게 유지되는 속성,상태를 의미함\n예를 들어 자동차,사과\n\n그러나 데이터셋마다 이미지의 분포가 다르기 때문에 수동적으로 augmentation전략을 다뤄줘야 했음.\n\n과일이 많은 데이터에 색에 대한 transform을 많이 적용하면? \\(\\to\\) 사과가 사과가 아니게 되겠죠?\n\n이 논문에서는 데이터마다 적절한 augmentation policy을 자동적으로 찾기위해 강화학습을 사용함.\nImagenet과 CIFAR-10에서 Sota를 달성했음. 후에 끝판왕 느낌의 efficient net이 나오는데 그 때의 augmentation에도 활용.\n\n\n\nMethod\n\n\n\nFig1\n\n\n\n크게 두 가지의 파트로 구분\n\nController(RNN) : Polict가 있는 Search Space에서 하나의 augmentation policy를 sampling.\nChild network : 이미지 분류기 sampling된 policy로 학습 후 validation accuracy R(reward)을 계산.\n\n과정(충분히 반복) policy sampling(by controller) \\(\\to\\) fitting classifier,calculating reward \\(\\to\\) contoller update,policy sampling \\(\\to\\) fitting classifier,calculating reward \\(\\to\\) contoller update,policy sampling  \\(\\to\\) fitting classifier,calculating reward \\(\\quad\\quad\\quad\\quad\\quad\\quad\\quad \\vdots\\) \\(\\to\\) optimal policy(converge to best augmentation strategy)\n\n\n\nSVHN dataset에 적용한 예시\n하나의 Policy는 5가지의 subpolicy로 구성됨.\nsubpolicy는 operation 2개와 probability,magnitude로 구성됨\n\noperation : 이미지 변형하는 방법 (Rotate,Brightness,ShearX,Inver 등등… 총 16가지 방법이 존재함)\nprobability : 얼마나 많이 적용할거냐\nmagnitude : 어느정도 강도로 할거냐\n\n위의 예시에서 배치는 총 15개. 각각의 배치마다 균일분포로 어떤 subpolicy가 할당됨.(그림에서는 아예 33333씩 할당되었지만 실제는 아닐 수 있음.이해를 돕기 위함)\nNote\n\n동일한 subpolicy가 적용되는 배치들이라도 각 배치마다 다른 operation이 적용될 수 있음. 이는 probability 또한 subpolicy에 있기 떄문임\n\n\n\n\nImageNet에 적용한 예시 vs SVHN에서 적용한예시\nSVHN에는 기하학적 변환이 많이 높은 확률로 첫번째 변환으로 적용되게 되어 있음(Shear X,Shear Y)\n\nSVHN은 숫자 이미지 dataset $\\(숫자가 비틀려있거나 왜곡되어 있는 경우가 많음.\\)$ 강인함을 policy는 기하학적 변환을 많이 포함.\n또한 색변환도 어느정도 포함되어 있음.(이는 데이터에서 확인가능,데이터 자체에 반전되어 있는 경우도 많음)\n\nImagenet에는 색상 변환이 많이 높은 확률로 첫번째 변환으로 적용되게 되어 있음(Shear X,Shear Y)\n\nImageNet은 다양한 물체들을 포함하는 dataset \\(\\to\\) 다양한 색상들이 포함되어 있음. \\(\\to\\) 강인함을 가지기 위해 policy는 색상 변환이 많이 포함\n\n\n\n\nResult\nResult on many datasets\n\nResult on Imagenet dataset\n\n\n\nConclusion\n\n다양한 데이터셋에 강화학습을 사용한 autoaugmentation 전략을 적용 \\(\\to\\) Sota!!\n추가적으로, 데이콘,캐글 등에서 위에서 나온 augmentation전략들이 많이 사용됨. 적용하기 쉬움\n링크 내꺼 코드"
  },
  {
    "objectID": "posts/paper study/batchnormalization/batchnormalization.html",
    "href": "posts/paper study/batchnormalization/batchnormalization.html",
    "title": "[Paper-study] Batch Normalization",
    "section": "",
    "text": "Deep learning에서는 각 레이어의 input의 분포가 계속해서 변화합니다.\n이는 네트워크의 학습에 어려움을 가져옵니다.\n논문에서는 Batch단위의 input을 normalization,shifting,scailing,하여 분포를 어느정도 일정하게 유지시킬 수 있는 방법을 제안합니다.\n이를 당시의 sota모델에 적용했더니 동일한 정확도를 14배 적은 training step으로부터 얻을 수 있었으며 상당한 격치를 두고 원래모델을 능가했습니다.\n또한 동일한 방법을 적용한 앙상블 네트워크를 사용하여 ImageNet classification에서 가장 좋은 결과를 낼 수 있었습니다.(4.9% top5 validation error,4.8% test error)"
  },
  {
    "objectID": "posts/paper study/batchnormalization/batchnormalization.html#intuition",
    "href": "posts/paper study/batchnormalization/batchnormalization.html#intuition",
    "title": "[Paper-study] Batch Normalization",
    "section": "Intuition",
    "text": "Intuition\n\nBatchNormalization - 출처 : JINSOL KIM\n\n직관적으로 internal covariate shift를 막기 위해 분포가 고정되게 하려면 위와 같이 각 히든레이어의 output에 normalization을 취할 수 있습니다. 이는 논문의 알고리즘에서 설명하는 방법입니다.\n그러나 찾아본 흔히 Fully connected-layer와 activation function사이에 batchnormalization layer를 놓습니다. 이는 논문의 실험에서 사용한 방법입니다.\n정리하자면 normalization을 적용하는 위치는 문제마다 다르지만 흔히들 위와 같이 Fully connected layer와 activation function사이에 놓는게 일반적이며 이는 비교적 자유로운 편이라 할 수 있습니다.\n위와 같은 방법으로 normalization만 취하게 된다면 네트워크의 표현력을 감소시킬 수 있습니다. 시그모이드의 linear regime에 값들이 대다수 위치하기 때문입니다.(뉴럴넷은 선형+비선형 변환을 통해서 높은 표현력을 지닙니다.단순히 normalization만 취하면 비선형함수의 역할이 감소하게 됩니다.)\n\n\n\n\nfigure3 - DNN with learnable parameter\n\n\n\n따라서 normalization된 값을 적절하게 shifting,scailing하도록 각 뉴런에 붙는 learnable parameter \\(\\gamma,\\beta\\)를 추가합니다.\n\n\n정리하자면 BatchNormalization은 Batch단위로 normalization을 통해 internal covariate shift를 막고 동시에 learnable parameter로 shifting,scailing함으로서 nonlinearity를 유지하여 gradient vanishing(exploding),학습의 어려움,표현력의 감소와 같은 문제를 해결했다고 할 수 있습니다."
  },
  {
    "objectID": "posts/paper study/batchnormalization/batchnormalization.html#implementation",
    "href": "posts/paper study/batchnormalization/batchnormalization.html#implementation",
    "title": "[Paper-study] Batch Normalization",
    "section": "Implementation",
    "text": "Implementation\n\nTraining\n\n\n\nFigure4 - BN Algorithm\n\n\nnotation\n\n논문의 알고리즘 부분에서는 Batchnormalization은 activation function바로 다음에 위치하는 것을 기준으로 설명합니다.\n\\(\\mathcal{B} = \\{x_{1...m}\\}\\)는 크기가 m인 batch를 입력했을때 임의의 노드에서 출력된 m개의 scalar값이다.(activation function을 통과한 후이다.)m개의 output입니다.\n\\(\\mu_{\\mathcal{B}}\\),\\(\\sigma^2_{\\mathcal{B}}\\)는 각각 \\(\\mathcal{B}\\)의 평균,분산을 의미합니다.\n\\(\\hat{x_i}\\)는 \\(\\mathcal{B}\\)에 속하는 임의의 원소 \\(x_i\\)에 normalization한 값입니다.\n여기서 \\(\\epsilon\\)은 매우작은 값을 의미하며 분산이 0일때의 연산이 불안정해지는 것을 막습니다.\n\\(y_i\\)는 learnable parameter인 \\(\\gamma,\\beta\\)에 대한 값이며 \\(\\text{BN}_{\\gamma,\\beta}(x_i)\\)를 계산한 결과입니다.\n\nexplanation - 먼저 크키가m인 batch에 대해서 어떤 노드에서 m개의 스칼라값인 \\(\\mathcal{B}\\)가 출력됩니다. - \\(\\mathcal{B}\\)의 평균,분산을 계산합니다. - \\(\\forall x_i \\in \\mathcal{B}\\)에 대하여 normalization을 취하고 learnable parameter인 \\(\\gamma,\\beta\\)를 곱합니다.\n학습된 \\(\\gamma\\),\\(\\beta\\)의 예시 Normalization연산이 필요없다고 학습한 경우,nonlinearity를 유지하는 것이 좋은 경우,identity를 유지하는게 좋은 경우  \\[\\gamma \\approx \\sqrt{var[x]},\\beta \\approx \\mathbb{E}[x] \\rightarrow \\hat{x_i}\\approx x_i  \\] Normalization연산이 필요하다고 학습한 경우,linearity를 가지는 것이 경우,identity를 버리는게 좋은 경우  \\[\\gamma \\approx 1,\\beta \\approx 0 \\rightarrow \\hat{x_i} \\approx \\frac{x_i-\\mu_\\mathcal{B}}{\\sqrt{\\sigma_\\mathcal{B}^2-\\epsilon}}\\]\n\n\nTest or Inference\n\ntraining에서는 minibatch단위로 평균,분산을 구하여 normalization할 수 있지만 test에서는 이와는 다르게 minibatch단위로 data가 입력되지 않을뿐더러 또한 입력되는 데이터가 한개여도 올바르게 예측해야 원합니다.\n따라서 이때에는 training에서 각각의 배치들로부터 얻은 평균들과 분산들을 저장해놓고 test에서는 이 값들로 다시 평균을 취하여(평균들의 평균) normalization을 취합니다.\n이때 단순한 평균을 취하는 것이 아니라 어느정도 학습된 네트워크에서 얻어진 minibatch들의 데이터를 더 많이 고려하기 위해서 movingaverage나 exponentialaverage를 사용합니다.\nmovingaverage는 학습단계에서 얻어진 값(평균,분산)의 일부를 직접 지정하여 평균을 구하고 exponentialaverage는 어느정도 안정된 상태의 값(나중값)들에 가중치를 부여하여 평균,분산을 구하는 방법입니다.\n\n\\[\\begin{aligned}\n&\\hat{x} = \\frac{x - \\mathbb{E}[x]}{\\text{Var}[x] + \\epsilon}\\\\\n&y = \\frac{\\gamma}{\\sqrt{\\text{var}[x] + \\epsilon}}\\cdot x + (\\beta - \\frac{\\gamma\\mathbb{E}[x]}{\\sqrt{\\text{Var}[x] + \\epsilon}})\\\\\n&\\text{where }E[x] = E_\\mathcal{B}[\\mu_\\mathcal{B}],\\text{Var}[x] = \\frac{m}{m-1}E_\\mathcal{B}[\\sigma_\\mathcal{B}^2]\n\\end{aligned}\\]\n\n\\(\\frac{m}{m-1}\\)은 unbiased estimate를 위하여 곱해진 값이며 \\(E_{\\mathcal{B}}\\)는 moving average 또는 exponential average를 의미합니다.\ntest에서의 normalization은 단순한 linear transform으로 취급할 수 있는데 이는 training이 끝난 후 사전에 이 값을 계산하여 단순히 곱하고 더하는 것으로 계산할 수 있기 때문입니다."
  },
  {
    "objectID": "posts/paper study/batchnormalization/batchnormalization.html#experiments",
    "href": "posts/paper study/batchnormalization/batchnormalization.html#experiments",
    "title": "[Paper-study] Batch Normalization",
    "section": "Experiments",
    "text": "Experiments\n Figure5 - Mnist experiment\n\n(a)는 BN을 사용한 네트워크와 사용하지 않은 네트워크를 비교, (b,c)는 각 네트워크의 hiddenlayer의 sigmoid의 input 3개를 비교한 것\n네트워크는 각각 100개의 activation을 가지며 3개의 hiddenlayer가 존재\n(a)를 보면BN을 사용한 네트워크가 훨씬 빠른속도로 수렴하고 있음을 알 수 있음\n(b,c)를 보면 BN을 사용한 네트워크에서 값이 훨씬 안정적임을 알 수 있음(internal covariate shift가 적음)\n\n\n\nBN사용한 모델이 좋았다~"
  },
  {
    "objectID": "posts/paper study/dqn/DQN.html",
    "href": "posts/paper study/dqn/DQN.html",
    "title": "DQN",
    "section": "",
    "text": "당시 neural network가 발전함에 따라서 RL에도 그대로 DL을 적용하고자 시도함\n그러나 여러가지 문제점이 많이 존재\n\nThe delay between actions and resulting rewards, which can be thousands of timesteps long, seems particularly daunting when compared to the direct association between inputs and targets found in supervised learning.  => 강화학습은 보상을 기반으로 학습을 하나 보상을 받는 시점이 정해지지 않음. 이는 딥러닝과는 다름\nAnother issue is that most deep learning algorithms assume the data samples to be independent, while in reinforcement learning one typically encounters sequences of highly correlated states. => 딥러닝은 변수들이 독립임을 가정하지만 강화학습에서 해결하고자 하는 데이터(시퀀스)가 높은 상관성을 가진채로 존재함\nFurthermore, in RL the data distribution changes as the algorithm learns new behaviours, which can be problematic for deep learning methods that assume a ﬁxed underlying distribution. => 딥러닝은 fixed underlying function(확률분포)를 가정하지만 강화학습의 경우 distribution이 변화함.\n\n논문에서는 위와 같은 문제점들을 극복하여 neural network를 RL(Q-learning)에 적용"
  },
  {
    "objectID": "posts/paper study/dqn/DQN.html#experience-replay",
    "href": "posts/paper study/dqn/DQN.html#experience-replay",
    "title": "DQN",
    "section": " experience replay",
    "text": "experience replay\n\nstore the agent’s experiences at each time-step, et = (st , at , rt , st+1 ) in a data-set D = e1 , …, eN\nDuring the inner loop of the algorithm, we apply Q-learning updates, or minibatch updates, to samples of experience, e ∼ D, drawn at random from the pool of stored samples.\nAfter performing experience replay, the agent selects and executes an action according to an -greedy policy."
  },
  {
    "objectID": "posts/paper study/dqn/DQN.html#algorithm",
    "href": "posts/paper study/dqn/DQN.html#algorithm",
    "title": "DQN",
    "section": " algorithm",
    "text": "algorithm"
  },
  {
    "objectID": "posts/paper study/gan/gan.html",
    "href": "posts/paper study/gan/gan.html",
    "title": "Genarative Adversarial Nets",
    "section": "",
    "text": "이 글은 Generative Adversarial Networks과 이후에 나온 튜토리얼인 NIPS 2016 Tutorial: Generative Adversarial Networks를 읽고 정리한 내용입니다.\n\nIntroduction\nGAN은 생성모델로 관측된 데이터를 생성한 확률분포를 구하여 데이터를 생성하는 것이 목적입니다. GAN 이전의 생성모델은 풀기가 어려운 확률계산이 있다는 단점이 있습니다. GAN은 이와 달리 Discrminator와 Generator를 서로 적대적으로 경쟁시켜서 학습합니다. Generator는 관측 데이터를 생성한 분포와 점점 가까워지며 학습이 끝나면 데이터를 생성한 확률분포를 근사적으로 얻습니다.\nGAN은 게임이론에서의 two-player game에서 유래되었습니다. 게임에서 각각의 player는 상대방의 전략을 매순간 인지하고 있으며 각각 상대방의 전략에 대응하여 번갈아가며 최선의 전략을 취합니다. 이렇게 최선의 전략을 번갈아가며 계속해서 취했을때 서로가 더이상 다른전략을 세울 필요가 없기에 고정된 전략만 취하는 지점인 내시균형(nash-equlibirum)에 다다릅니다. (자세한 설명 참조)\n예를 들어 Discrminator는 경찰 Generator는 사기꾼인 게임에 비유할 수 있습니다. 사기꾼은 최대한 진짜같은 가짜화폐를 유통시키려 하며 경찰은 이러한 가짜화폐를 잡아내려 합니다. 사기꾼은 경찰에게 걸리지 않기위해 점점 가짜를 진짜같이 만들며 경찰은 더 정확히 진짜와 가짜를 구분하려고 발전합니다. 마지막에 경찰은 사기꾼이 만들어내는 가짜화폐를 진짜화폐를 전혀 구분하지 못하는 nash-equilibrium에 다다릅니다.\n\n\n\nProblem Setting\n관측된 데이터는 \\(p_{data}\\)라는 확률분포에서 샘플링 됩니다. GAN은 여타 다른 생성모델과 마찬가지로 관측된 데이터를 통해서 역으로 데이터를 생성해낸 \\(p_{data}\\)를 알아내고자 합니다. \\(p_{data}\\)만 알아낸다면 sampling을 통해 학습된 데이터와 유사한 데이터를 생성할 수 있을 것입니다.\n\n\n\nMethod\nD,G로 구성된 two-player가 참여하는 minimax game의 value function은 다음과 같습니다.\n\\[\\begin{aligned}\n&\\underset{G}{\\text{min}}\\,\\underset{D}{\\text{max}}\\,V(D,G) = \\mathbb{E}_{{\\bf{x}}\\sim p_{data}}\\left[\\text{log\\,D({\\bf{x}})}\\right] + \\mathbb{E}_{{\\bf{z}}\\sim p_z(z)}\\left[\\text{log}\\,(1-D(G(\\bf{z})))\\right] \\\\\n&p_{data} : \\text{(observed) data generating Distribution}\\\\\n&p_z : \\text{prior distribution}\\\\\n&\\text{Generator }G : \\text{mapping from latent space to data(input) space}\\\\\n&\\text{Discriminator }D : \\text{probability that input came from the data rather than } p_g\\\\\n\\end{aligned}\\]\nD에 대한 value function만 따로보면 다음과 같습니다.\n\\[\\begin{aligned}\n&\\underset{D}{\\text{max}}\\,V(D,G) = \\mathbb{E}_{{\\bf{x}}\\sim p_{data}}\\left[\\text{log\\,D({\\bf{x}})}\\right] + \\mathbb{E}_{{\\bf{z}}\\sim p_z(z)}\\left[\\text{log}\\,(1-D(G(\\bf{z})))\\right] \\\\\n\\end{aligned}\\]\n우변의 첫번째 항은 Discriminator가 관측된 real 데이터를 \\(x\\)를 real 데이터라고 분류할 확률입니다. 또한 -가 붙은 우변의 두번째 항은 Discriminator가 생성된 fake 데이터 \\(G({\\bf{z}})\\)를 fake 데이터라고 분류할 확률입니다. 따라서 이와 같은 objective function을 maximize하는 D를 구하는 것은 진짜와 가짜를 잘 분류하도록 Discriminator를 학습시키는 것을 의미합니다.\nG에 대한 value function만 따로보면 다음과 같습니다.\n\\[\\begin{aligned}\n&\\underset{G}{\\text{min}}\\,V(D,G) = \\mathbb{E}_{{\\bf{z}}\\sim p_z(z)}\\left[\\text{log}\\,(1-D(G(\\bf{z})))\\right] \\\\\n\\end{aligned}\\]\n우변은 Discriminator가 관측된 가짜데이터인 \\(G({\\bf{z}})\\)를 진짜로 분류할 확률입니다. 따라서 objective function을 minimize하는 G를 구하는 것은 Generator가 Discriminator를 더 잘 속일 수 있도록 진짜와 같은 데이터를 생성하도록 학습하는 것입니다.\n\n\n\ngan-figure1\n\n\n\n검은색 점선 : \\(p_{data}\\)\n초록색 곡선 : \\(p_g\\)\n파랑색 곡선 : \\(D(\\bf{x})\\)\n수평선 z : latent space\n수평선 x : data space\n\nobjective function을 최적화 하는 과정을 시각적으로 나타내면 위와 같습니다.\n\n\n: \\(p_{data}\\)와 \\(p_g\\)의 분포가 어느정도 차이를 보입니다. 또한 \\(D(\\bf{x})\\)도 어느정도 불안정하게 분류를 하는 모습입니다.\n\n\n: \\(D(\\bf{x})\\)가 먼저 올바르게 분류할 수 있도록 학습합니다. 이때 \\(G(z)\\)로부터 만들어진 가짜 데이터가 사용됩니다.\n\n\n: \\(G(z)\\)가 점점 더 진짜 데이터를 생성하도록 학습합니다. 이때 \\(D(\\bf{x})\\)를 얼마나 잘 속이고 있는지가 사용됩니다. 또한 이전보다 조금 더 \\(p_g\\)가 \\(p_{data}\\)와 비슷해졌습니다. \\(\\vdots\\)\n\n\n: G가 거의 완벽하게 D를 속이며 진짜같은 데이터를 생성할 수 있습니다. \\(p_g \\approx p_{data}\\)\n\n\n\n\n\nImplementation\n\n\n\nalgorithm 1\n\n\n\n실제로는 목적함수를 D,G에 번갈아가며 numerical,iterative method를 사용하여 최적화함.\n충분한 성능을 가져서 더 이상 개선되지 않을 때(nash equilibrium에 도달했을 때), G의 확률분포 \\(p_g \\approx p_{data}\\)\n\n(Detail) - 주어진 G에 대해서 D를 끝까지 최적화 하는 것은 비효율적이며 과적합을 발생시킴 \\(\\rightarrow\\) Discriminator를 업데이트 횟수를 k로 제한(논문에서 \\(k\\) = 1) - 학습 초기에 Generator G가 좋지 못할 경우 \\(\\text{log}(1-D(G(\\bf{z})))\\)는 기울기가 거의 없음 => 학습 초기에만 \\(\\text{log}(1-D(G(\\bf{z})))\\)를 minimize하지 않고 \\(\\text{log}D(G(z))\\)를 maximize 하는 방식을 취함 => 더 큰 gradient가 flow\n\n\n Proof\nGAN은 \\(p_g\\)로 관측 데이터를 생성하는 확률분포 \\(p_{data}\\)얻는 것이 목적입니다. 그러나 아직까지 위와 같은 GAN의 알고리즘으로 정말로 \\(p_{data}\\)를 얻을 수 있는지는 증명하지 않았기에 확실하지 않습니다. 논문에서는 이와 관련하여 증명합니다.\n\nGlobal minimum에서 \\(p_g = p_{data}\\)\nGlobal minimum으로 수렴할 수 있는가?(아직 잘 모르겠네요 ㅜㅜ)\n\n1을 증명하기 위해서 value function을 조금 풀어쓰면 다음과 같습니다.\n\\[\\begin{aligned}\n&\\underset{G}{\\text{min}}\\,\\underset{D}{\\text{max}}\\,V(D,G) = \\underset{G}{\\text{min}}\\left[-\\text{log(4)} + 2\\cdot\\text{JSD}(p_{data}||p_g)\\right]\\\\\n&p_{data} : \\text{(observed) data generating Distribution}\\\\\n&p_g : \\text{(implict) generator's distribution}\\\\\n&\\text{Generator }G : \\text{mapping from latent space to data(input) space}\\\\\n&\\text{Discriminator }D : \\text{probability that input came from the data rather than } p_g\\\\\n\n\\end{aligned}\\]\n우변을 보면 jenson-Shannon divergence(JSD)가 존재합니다. JSD는 두 확률분포간의 차이를 측정하며 항상 0보다 크거나 같으며 두 확률분포 \\(p_g,p_{data}\\)가 같을때 최솟값 0을 가집니다. 그러므로, global optimal에서 \\(p_g = p_{data}\\)입니다."
  },
  {
    "objectID": "posts/paper study/MARL Selective overview/MARL Selective overview.html",
    "href": "posts/paper study/MARL Selective overview/MARL Selective overview.html",
    "title": "[Paper Study] Multi-Agent Reinforcement Learning: A Selective Overview of Theories and Algorithms",
    "section": "",
    "text": "Abstract\n\n최근 강화학습은 순차적인 의사결정 문제를 해결하는데에 상당한 성공을 거두었다.\n이러한 성공들에도 불구하고 multi agent reinforcement learning(MARL)에 대한 이론적인 기초는 상대적으로 부족하다.\n따라서 이 논문에서는 몇 가지 MARL 알고리즘을 전체적으로 설명하고 이론적으로 분석한다.\n뿐만아니라 중요하지만 다소 도전적인 난관들도 제시한다.\n논문의 궁극적인 목표는 현재의 MARL 분야에 대한 의견,평가을 제시하는 것을 넘어서 유익한 연구방향을 설정할 수 있도록 돕는 것이다.\n\n\n\nIntroduction\n\n최근 강화학습은 복잡한 함수를 근사할 수 있는 딥러닝의 발전과 더불어 RL은 놀랍게 진보했다.\n\n예를 들면 playing real-time strategy games, playing car games, etc.\n\n대부분의 성공에는 하나의 agent가 존재하는 것이 아니라 하나 이상의 다수의 agent가 참여하며 이는 MARL로 모델링 된다.\nMARL은 구체적으로 뭘까?\n\n다수의 자립하여 움직이는 agent들이 존재하고\n이러한 agent가 공통적인 환경(environment)에 놓여있을 때의\n순차적인 의사결정문제를 다룬다.\n각각의 agent는 환경,다른 agent들과의 상호작용을 통해서 그들이 얻는 각각의 return을 최대화 하려고 노력한다.\n\n크게 MARL은 다음의 세 가지로 구분된다.\n\nfully cooperative \\(\\to\\) agent들은 공동의 return을 최적화 하기 위해서 협력한다.\n\nex) 로봇이 물건을 지정된 장소에 운반하는 경우, 여러대의 자율주행차가 목적지에 도착.\n\nfully competitive \\(\\to\\) agent들은 합이 0인 return을 서로 최적화 하기 위해서 경쟁한다.\n\nex) 바둑,체스\n\na mix of two \\(\\to\\) agent가 보편적인 return을 최적화 하기위해 협력 or 경쟁할 수 있다.(협력도 경쟁도 모두 가능하다.)\n\nex) 축구,농구\n\n\nMARL이 무조건 좋아보이는데? \\(\\to\\) 어려운 점이 많다.(복습)\n\n각 에이전트들은 각각의 return을 최대화 하려함.\n\n균형점에서 비효율적일 수 있으며(내시균형에서 발생하는 문제)\n통신,협력이 효율적으로 이루어질 수 있는가에 대한 추가적인 기준이 필요함.\n적대적 agent에 대한 robustness는 충분한가?\n\n모든 agent는 저마다의 policy를 계속해서 향상(수정).\n\nB라는 agent가 이전과 다른 action을 하게 되면 environment가 변하게 되고\nA라는 agent가 직면하는 environment는 non-stationary해지는 것을 의미.\n\n모든 agent에 대한 action들의 조합,결합은 agent한명이 증가할수록 지수적으로 증가.\n각 agent는 다른 agent가 무엇을 관측했는지는 정확히,모든것을 알 수 없음.\n\n다른 agent의 관측에 대한 제한된 정보만을 가지고 각 agent는 결정을함.\n최적의 결정이 아닌 suboptimal한 결정을 가져옴."
  },
  {
    "objectID": "posts/paper study/seq2seq/seq2seq.html",
    "href": "posts/paper study/seq2seq/seq2seq.html",
    "title": "Sequence to Sequence Learning with Neural Networks",
    "section": "",
    "text": "Deep Nueral Network는 복잡한 task에 놀라운 성능을 보여왔지만 sequence에서 sequence를 mapping에서는 task에서는 잘 사용되지 못했습니다.\n논문에서는 sequence의 구조에 대한 최소한의 가정만으로 학습하는 end-to-end approach를 소개합니다."
  },
  {
    "objectID": "posts/paper study/seq2seq/seq2seq.html#intuition",
    "href": "posts/paper study/seq2seq/seq2seq.html#intuition",
    "title": "Sequence to Sequence Learning with Neural Networks",
    "section": " Intuition",
    "text": "Intuition\n\n\n\nhttps://gaussian37.github.io/dl-concept-attention/\n\n\n(Process)\n\n먼저 LSTM에 Input sequence를 each time step마다 각각 입력합니다.\n마지막 토큰인 \\(\\text{<EOS>}\\)이 입력되고 나면 마지막 hidden state인 context vector(\\(\\bf{v}\\))를 얻습니다. \n\\(\\bf{v}\\)를 또 다른 LSTM의 입력으로 사용합니다.\nLSTM은 each time step마다 예측 y를 얻은 뒤 다음 state의 입력으로 활용합니다.\noutput sequence는 <EOS>를 만날때 더 이상 출력하지 않습니다.\n\n(context vector란?)\n\ncontext \\(\\bf{v}\\)는 input sequence보다 적은 차원을 가지며 모든 input에 대해서 고정된 크기를 가지는 벡터이다.\n\\(\\bf{v}\\)는 input sequence의 핵심적인 의미를 포착합니다.(비슷한 의미를 가지는 문장은 거리가 가깝고 완전 다른 의미라면 거리가 멀다.)\n\n(왜 하필 LSTM?)\n\nLSTM는 당시에는 긴 문장을 처리할 수 있는(long range temproal dependencies) state of the art였기 때문에 LSTM 위주로 설명되어 있습니다.\n더 성능이 잘 나온다면 얼마든지 다른 모델도 사용할 수 있습니다.(ex GRU 등등…)\n\n(Note)\n\ncontext vector를 lstm에 입력하여 \\(\\text{<EOS>}\\)토큰을 만날때까지 출력하기 때문에 input과 output sequence의 길이는 다릅니다. (장점)\n고정된 크기의 context vector로 mapping되기 때문에 output sequence를 내놓기 위해 필요한 정보가 사라질 수 있습니다. (단점)"
  },
  {
    "objectID": "posts/paper study/seq2seq/seq2seq.html#modeling",
    "href": "posts/paper study/seq2seq/seq2seq.html#modeling",
    "title": "Sequence to Sequence Learning with Neural Networks",
    "section": " Modeling",
    "text": "Modeling\n(vanila RNN)\n\\[\\begin{aligned}\n&h_t = \\text{sigm}(W^{hx}x_t + W^{hh}h_{t-1})\\\\\n&y_t = W^{yh}h_t\n\\end{aligned}\\]\n\nRNN에서 사용되는 수식은 위와 같습니다.\n이론적으로만 보면 RNN은 길이가 긴 sequence에서도 사용할 수 있습니다.\n하지만 sequence의 길이가 길어지며 long term dependency 가지는 경우 성능이 좋지 못합니다.\n따라서 vanila RNN대신 LSTM을 사용합니다.(수식 생략)\n\n(The goal of lstm)\n\\[\\begin{aligned}\n&p(y_1,y_2,\\dots,y_{T'}|x_1,x_2,\\dots,x_T) = \\prod_{t=1}^{T'}(y_t|v,y_1,\\dots,y_{t-1})\\\\\n&\\hat{{\\bf{y}}} = \\underset{{\\bf{y}}}{\\text{argmax}}\\,p(y_1,y_2,\\dots,y_{T'}|v,x_1,x_2,\\dots,x_T)\n\\end{aligned}\\]\n\nLSTM의 목적은 왼쪽의 output \\(y_1,y_2,\\dots,y_{T'}\\)에 대한 conditional probability distribution을 얻는 것입니다.\n즉, each time step t마다 softmax를 통과한 output인 conditional distribution을 모두 메모리에 저장한 뒤 전부 곱하는 것을 의미합니다.\noutput은 가장 높은 확률을 가질때의 \\(\\bf{y}\\)라고 생각할 수 있습니다.\n이렇게 sequence를 출력하는 알고리즘을 exhaustive search라 하며 매우 높은 시간복잡도를 가진다는 단점이 있습니다.\n\n각각의 \\(y_1,y_2\\dots\\)가 가질 수 있는 값들이 너무 다양하며(vocab의 크기) 모든조합에 대해 곱하여 확률분포를 계산해야 합니다.\n확률분포를 다 계산했다 하더라도 가장 높은 확률을 가지는 \\(\\bf{y}\\)를 탐색해야 하는 search space가 너무 큽니다.\n\n따라서 다른 알고리즘을 통해서 conditional probability distribution의 max값을 구해야 합니다.\n\n\n Searching Algorithm\n\n Greedy-search Algorithm\n\\[\\begin{aligned}\n\n&\\text{until } \\hat{y_t} \\text{ is not }\\text{<EOS>}\\\\\n&\\hat{y_t} = \\underset{y_t}{\\text{argmax}}\\,p(y_t|v,y_1,y_2,\\dots,y_{t-1})\\\\\n\\end{aligned}\\]\n\nGreedy Algorithm은 Each timestep t에서 얻은 각각의 condional distribution의 max값을 찾는 알고리즘입니다.\nExhaustive Algorithm과 다르게 search space가 그렇게 크지 않습니다. 따라서 더 적은 시간복잡도를 가집니다.\n\n\n\n윗 그림은 각각의 timestep t마다 \\(y_t\\)마다 conditional distribution을 나타낸 그림입니다.\nGreedy Algorithm은 각 t마다 conditional distribution을 maximize하는 토큰만 outputsequence로 출력합니다.\n\\(p(y_1,y_2,y_3,y_4) = 0.5 \\times 0.4 \\times 0.4 \\times 0.6 = 0.048\\)\n\n(Note)\n\nsearch space of Greedy algorithm < searchspace of Exhaustive Algorithm\n다만 Greedy algorithm의 conditional distribution의 max값을 보장하지 못합니다.(다 계산하고 최댓값을 보는게 아니라 따로따로 max를 계산한걸 가져오기 때문)\n\n\n\n위의 그림은 t=2에서만 greedy하게 진행하지 않은 경우를 나타냅니다.\n\\(p(y_1,y_2,y_3,y_4) = 0.054\\)로 오히려 더 높은 확률을 가집니다.\nGreedy Algorithm이 optimal sequence를 보장하지 못한다는 사실을 알 수 있습니다.\n\n\n\n Beam-search Algorithm\n\n요약하자면 Beam-search는 Greedy Search를 \\(K\\)개 하는 것입니다.\n\n\n\nBeam-searh Algorithm은 Greedy Algorithm이 optimal sequence를 잘 찾지 못한다는 점을 보완한 알고리즘입니다.\n처음 \\(k\\)개의 beam을 통하여 Greedy Search를 수행하는 알고리즘 입니다.\n\n(Beam-Algorithm Process)\n\n먼저 hyparparameter인 Beamsize(\\(k\\))를 입력으로 받습니다.\ntimestep = 1에서 가장 확률이 높은 \\(k\\)개의 Beam을 선택합니다.\n각각의 Beam에서 계속해서 conditional probability를 계산하며 Greedy하게 탐색합니다.\n각각의 Beam은 \\(\\text{<EOS>}\\)를 만났을때 탐색을 종료하며 candidate에 추가됩니다.\ncandidate에 있는 모든 Beam에 대해서 score를 계산합니다.\n\n(score)\n\\[\\begin{aligned}\n\\frac{1}{L^{\\alpha}}\\text{log}\\,p(y_1,\\dots,y_L|{\\bf{c}}) = \\frac{1}{L^{\\alpha}}\\sigma_{t=1}^{L}\\text{log}\\,p(y_t|y_1,\\dots,t_{t-1}|{\\bf{c}})\n\\end{aligned}\\]\n\n\\(L\\)은 문장의 길이 \\(\\alpha\\)는 보통 0.75로 긴 시퀀스에 대해서 패널티를 주기위해서 사용합니다."
  },
  {
    "objectID": "posts/paper study/seq2seq/seq2seq.html#experiments",
    "href": "posts/paper study/seq2seq/seq2seq.html#experiments",
    "title": "Sequence to Sequence Learning with Neural Networks",
    "section": " Experiments",
    "text": "Experiments\n\n Training Details\n\n4-layer가 있는 LSTM, 1000 cell있으며 1000차원의 워드 임베딩\n문장을 뒤집어서 input으로 사용했음.(자세한 이유x)\nLSTM’s parameters with the uniform distribution between -0.08 and 0.08\nSGD without momentum, 초기 lr = 0.7로 고정. 5 epochs뒤에 lr을 반절로 계속 나누었으며 총 7.5 epochs.\n각 batch는 128 sequence들로 이루어져 있으며 gradient를 계산했으며 계산한 gradient를 배치사이즈인 128로 나눔.\ngradient에 constraint있음\n\n\n\n Experiments Results\n\n\nSMT기반의 방법만큼이나 LSTM기반의 방법도 도 좋은 결과를 냄\n\n\n\n\ncontext vector를 2차원상에 투영한 그림\n\n\n\ncontext vector를 PCA로 2차원상에 투영한 그림\n비슷한 의미의 문장은 가깝고 다른 의미의 문장은 거리가 멀더라 => LSTM기반의 네트워크가 의미를 잘 포착함을 알 수 있습니다."
  },
  {
    "objectID": "posts/paper study/seq2seq/seq2seq.html#conclusion",
    "href": "posts/paper study/seq2seq/seq2seq.html#conclusion",
    "title": "Sequence to Sequence Learning with Neural Networks",
    "section": " Conclusion",
    "text": "Conclusion\n\n입력순서를 역전시키니 결과가 잘 나왔다.(그러나 개인적으로 이게 의미있는 결과인지는 모르겠다.)\nLSTM기반의 방법이 생각보다 좋은 성능을 내더라.\nvocabulary가 제한되며 structure에 대한 제한이 없음에도 불구하고 SMT-based system만큼이나 LSTM기반의 방법이 좋은 성능을 보인다.\n간단하고,쉬운 방법이 SMT기반의 방법을 넘었다. 그러므로 ,계속해서 연구한다면 이러한 접근법이 sequence to sequence 문제에도 충분히 적용가능하다."
  },
  {
    "objectID": "posts/paper study/seq2seq with attention/seq2seq with attention.html",
    "href": "posts/paper study/seq2seq with attention/seq2seq with attention.html",
    "title": "[Paper Study] Neural Machine Translation by jointly learning to align and translate",
    "section": "",
    "text": "당시 인공신경망을 활용한 기계번역에서는 대부분 encoder와 decoder를 포함한 모델을 사용했습니다\n이러한 모델은 고정된 길이의 context vector를 사용하여 길이가 긴 문장에서 성능저하를 가져왔습니다.\n따라서 논문에서는 각각의 target world에 대해 서로다른 context vector를 사용함으로서 길이가 긴 문장에 대한 성능저하를 개선합니다.\n이러한 새로운 접근 방식을 통해 그 당시의 state of the art인 phrase-based system과 비슷한 번역 성능을 달성했습니다."
  },
  {
    "objectID": "posts/paper study/seq2seq with attention/seq2seq with attention.html#intuition",
    "href": "posts/paper study/seq2seq with attention/seq2seq with attention.html#intuition",
    "title": "[Paper Study] Neural Machine Translation by jointly learning to align and translate",
    "section": " Intuition",
    "text": "Intuition\n 출처 : paper-Figure1\n\n위의 그림은 논문에서 제시한 모델로 위는 decoder 아래는 encoder입니다.\ndecoder를 보면 기존의 seq2seq 아키텍쳐와 다를것이 거의 없습니다만 새로운 정보(휘어져 들어가는 화살표) 들어가며 이는 그림의 아래쪽에 있는 encoder에서 만들어짐을 알 수 있습니다.\nencoder는 bidirectional RNN을 사용합니다. 정방향과 역방향으로 읽어들이면서 input sequence에 대해 전체적이면서도 특히 i-th 단어(토큰)과 연관된 정보 \\(h_t\\)를 만듭니다.\n이렇게 만들어진 input sequence의 각 시점에서의 정보는 각각의 target을 만드는데 얼마나 중요한 정보인지를 의미하는 값인 \\(\\alpha\\)와 곱하여 모두 더해집니다.\n즉, 더해진 값은 input sequence에서 모든 정보를 target을 예측하는데 얼마나 중요한지,관련있는지를 고려해서 재조합한 새로운 정보라고 할 수 있습니다. 이렇게 더해진 값은 새로운 출력값 \\(y_t\\)를 만들기 위한 정보인 decoder의 hidden state \\(s_t\\)를 구하는데 사용됩니다.\n\n\n양방향 vs 단방향 RNN\n\n\n단방향 RNN에서 hidden state \\(h_t\\)\n\n어떻게 생성? \\(\\rightarrow\\) input word \\(x_t\\)와 이전에 읽어들인 sequence에 대한 정보 \\(h_{t-1}\\)를 합쳐서 새로운 정보 \n지금 단어 \\(x_t\\)까지 입력된 sequence까지의 대한 정보이자 특히 마지막으로 입력된 단어를 많이 고려한 정보 \n지금의 input인 \\(x_t\\)다음에 오는 sequence는 고려하지 않음\n\n\n\n양방향 RNN에서의 hidden state \\(h_t\\)\n\n어떻게 생성? \\(\\rightarrow\\) input sequence를 정방향,역방향으로 서로다른 RNN을 통과하여 생성\n정방향에서는 지금의 단어 \\(x_t\\)까지 순차적으로 입력된 sequence에 대한 정보이자 특히 마지막 \\(x_t\\)를 많이 고려한 정보를 뽑아냄.\n반대로 역방향에서는 끝에서부터 시작하여 반대로 \\(x_t\\)까지 입력된 sequence 대한 정보이자 특히 마지막 \\(x_t\\)를 많이 고려한 정보를 뽑아냄.\n정방향이던 역방향이던 다음에 오는 sequence에 대한 정보는 고려하지 않으나 이 둘을 합쳐서 전체적인 정보 + 특정시점 \\(x_t\\)에 고려한 정보를 얻을 수 있음.\n\n\n\n\n직관적인 정리\n\n\n\n\n\n\n\n\n\n\n\n\n기존의 seq2seq 모델은 encoder에서 고정된 길이의 context vector로 바꾼 정보만을 decoder에서 사용하여 긴 문장에 대해서는 정보의 손실이 일어났으며 또한 성능이 좋지 못했습니다.\n따라서 논문에서는 decoder에서 target(word)을 만들 때 input sequnce의 각각의 위치에서 나오는 모든 정보를 중요도를 반영하여 재조합한 새로운 정보를 만들고 이를 활용합니다.\n이렇게 만든 새로운 정보는 decoder가 output sequence의 각각의 target을 예측할때 가중치 \\(\\alpha\\)를 통해 특정위치 근처의 문맥에 주목,집중한 값 이기때문에 집중,주의를 의미하는 attention이라는 용어를 따와서 attention mechanism이라고 합니다.\n이렇게 attention mechanism을 사용함으로서 encoder가 source sentence의 모든정보를 하나의 고정된 길이의 context vector로 인코딩해야 하는 부담을 줄여줍니다."
  },
  {
    "objectID": "posts/paper study/seq2seq with attention/seq2seq with attention.html#modeling",
    "href": "posts/paper study/seq2seq with attention/seq2seq with attention.html#modeling",
    "title": "[Paper Study] Neural Machine Translation by jointly learning to align and translate",
    "section": " Modeling",
    "text": "Modeling\n\n Decoder\n\n위와 같은 새로운모델에서 chainrule에서 각각의 conditional probability는 다음과 같습니다.\n\n\\[\\begin{aligned}\np(y_i|y_1,\\dots,y_{i-1},{\\bf{x}}) = g(y_{i-1},s_{i-1},c_i)\n\\end{aligned}\\]\n\n기존의 seq2seq모델에서는 context vector \\(c\\)는 target \\(y_i\\)가 바뀌어도 고정된 바뀌지 않는 값이었습니다.\n논문에서 제시된 모델은 이와는 다르게 각각의 target \\(y_i\\)를 계산하기 위해서 서로다른 context vector \\(c_i\\)를 사용합니다.\n여기서 \\(c_i\\)는 decoder의 hidden state인 \\(s_{i}\\)를 계산하기 위해서 사용됩니다. 즉,다음과 같습니다.\n\n\\[s_i = f(s_{i-1},y_{i-1},c_i)\\]\n\n\\(c_i\\)는 target \\(y_i\\)를 예측하기 위해서 input sequence에서 나온 정보 \\(h\\)를 중요도 \\(a\\)에 따라 재조합한 정보입니다. 구체적으로 다음과 같습니다.\n\n\\[\\begin{aligned}\nc_i = \\sum_{j=1}^{T_x}\\alpha_{i,j}h_{j}\n\\end{aligned}\\]\n\n각각의 annotation 즉 hidden state \\(h_j\\)는 전체문장의 정보를 담고 있으나 특히 \\(j\\)번째 poistion근처의 문맥정보를 많이 담고 있습니다.(bidirectional RNN에 의한 결과입니다.)\n참고 - \\(i\\)값이 바뀌더라도 즉, 또 다른 target을 예측하더라도 참고하는 input sequence의 정보인 annotations는 바뀌지 않음(저장해놨다가 각각의 \\(y_i\\)값을 구하는데 사용할 수 있음,구현시 유의)\n각각의 가중치 \\(\\alpha\\)는 다음과 같이 계산할 수 있습니다.\n\n\\[\\begin{aligned}\n\\alpha_{ij} = \\frac{\\text{exp}(e_{ij})}{\\sum_{k=1}^{T_x}\\text{exp}(e_{ik})}\\\\\n\\text{where},\\,e_{ij} = a(s_{i-1},h_j)\n\\end{aligned}\\]\n\n\\(\\alpha\\)는 softmax의 함숫값이며 \\(e\\)를 0과1사이의 값으로 scailing 했음을 알 수 있습니다.\n여기서 \\(a\\)는 alignment model로 input sequence의 j번째 position과 output sequence의 i번째 position이 얼마나 일치하는지,연관되어있는지,관련있는지 알려주는 값이며 이는 feedforward nueral network로부터 계산됩니다.\n이와같이 (soft)alignment(일치,정렬)를 직접적으로 계산함으로서 alignment가 잠재적(보이지않던,숨겨져있었던)이었던 기존의 기계번역 모델들과는 다르게 alignment를 더 잘 학습하도록 gradient가 backpropagation 될 수 있으며 따라서 input과 output sequnce에서의 alignment를 기존모델보다 더 잘 학습할 수 있습니다.\n\n\n\n Encoder\n\n인코더에서는 정방향,역방향으로 input sequence를 모두 읽어들이는 bidirectional RNN을 사용합니다.\nforward RNN \\(\\overset{\\rightarrow}{f}\\)은 \\(x_1\\)에서 \\(x_{T_x}\\)까지 forward hidden states인 \\((\\overset{\\rightarrow}{h_1},\\dots,\\overset{\\rightarrow}{h_{T_x}})\\)를 계산합니다.\nbackward RNN \\(\\overset{\\leftarrow}{f}\\)은 \\(x_{T_x}\\)에서 \\(x_1\\)까지 bacward hidden states인 \\((\\overset{\\leftarrow}{h_{T_x}},\\dots,\\overset{\\leftarrow}{h_{T_1}})\\)을 계산합니다.\n양방향,정방향의 hidden state를 모두 결합하여 다음과 같은 결합된 hidden state값을 계산합니다. \\[h_j = \\left[\\overset{\\rightarrow}{h}_j^{\\,\\,T};\\overset{\\leftarrow}{h}_j^{\\,\\,T}\\right]^T\\]\n\\(h_j\\)는 input sequence 전체의 정보를 모두 갖지만 특히 position j근처에 집중된 정보를 가집니다."
  },
  {
    "objectID": "posts/Probability Statistics/categori distribution.html",
    "href": "posts/Probability Statistics/categori distribution.html",
    "title": "카테고리 분포",
    "section": "",
    "text": "카테고리 분포에 대한 정리\n\n카테고리 분포\n카테고리분포는 시행의 한번의 시행(또는 실험)으로부터 나올 수 있는 사건이 K개인 확률분포를 모델링할때 쓰이며 다음과 같습니다.\n\\[\\begin{aligned}\n&Cat({\\bf{x};\\bf{\\mu}}) =\n\\begin{cases}\n\\mu_1\\, (\\text{if } x = (1,0,0,0,\\dots,1)) \\\\\n\\mu_2\\, (\\text{if } x = (0,1,0,0,\\dots,1)) \\\\\n\\mu_3\\, (\\text{if } x = (0,0,1,0,\\dots,1)) \\\\\n\\vdots \\\\\n\\mu_k\\, (\\text{if } x = (0,0,0,0,\\dots,1)) \\\\\n\\end{cases}\n\\\\\n&\\text{where, }x = (x_1,x_2,\\dots,x_K),\\mu = (\\mu_1,\\mu_2,\\dots,\\mu_k)\n\\end{aligned}\\]\n카테고리분포의 변수\\(\\bf{X}\\)는 K개의 원소를 가지는 원핫인코딩(one-hot encoded)된 벡터이며 각원소는 indicate number(어떤 클래스에 속하는지 나타내는)인 1또는0입니다. 모수(벡터)\\(\\mu\\)도 K개의 원소를 가지며 각각의 원소는 카테고리 확률분포로부터 대응하는 결과값(원핫벡터)에 대한 확률입니다. 즉,각각의 원핫벡터가 표본추출될 가능성(확률)을 알려줍니다.\n위와 같은 사실로부터 다음과 같은 4가지의 제약조건이 존재합니다.\n\n\\(\\mu_i\\)는 원핫벡터가 나올 확률입니다.\n\n\\[0\\leq\\mu_i\\leq1\\]\n\n확률의 합은 1입니다.\n\n\\[\\sum_{i=1}^{K}\\mu_i = 1\\]\n\n원핫벡터의 각 원소는 indicate number인 1또는 0입니다.\n\n\\[\\begin{aligned}\nx_i =\n\\begin{cases}\n0\\\\\n1\n\\end{cases}\n\\end{aligned}\\]\n\n원핫벡터의 모든 원소의 합은 1입니다. \\[\\sum_{i=1}^{K}x_i = 1\\]"
  },
  {
    "objectID": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html",
    "href": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html",
    "title": "MLE & MAP (작성중)",
    "section": "",
    "text": "HYEONGMIN LEE’S WEBSITE - MLE & MAP를 공부하고 정리한 글입니다."
  },
  {
    "objectID": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#problem-setting",
    "href": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#problem-setting",
    "title": "MLE & MAP (작성중)",
    "section": "Problem Setting",
    "text": "Problem Setting\n실제 \\(N\\)명의 사람들로부터 키(\\(x\\))와 몸무게(\\(t\\))를 조사한 다음과 같은 data set \\(D\\)를 가지고 있다고 해보자.\n\\[ D = \\{(x_1,t_1),(x_2,t_2,),\\dots,(x_N,t_N)\\}\\]\n키로 몸무게를 예측하는 모형을 만들고자 한다. 어떻게 만들 수 있을까?"
  },
  {
    "objectID": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#modeling",
    "href": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#modeling",
    "title": "MLE & MAP (작성중)",
    "section": " Modeling ",
    "text": "Modeling \n가장 단순한 접근은 키(\\(x\\))를 입력으로 하고 몸무게(\\(t\\))를 출력하는 함수\\(y(x|\\theta)\\)를 만드는 것이다.\n\\[t = y(x|\\theta)\\]\n\n\\(\\theta\\)는 함수의 parameter를 나타낸다.\n\n파라미터\\(\\theta\\)를 학습하여 키-몸무게 사이의 관계를 표현하는 거의 완벽하게 표현하는 함수를 얻었다고 해보자. 그렇다면 키\\(x\\)를 입력으로 하여 몸무게에 대한 예측값 \\(t\\)하나를 얻을 수 있다. 예를 들어 키가 175cm인 사람의 몸무게가 함수를 통하여 얻은 예측값이 72kg이라고 하자. 이 결과를 말로하면 “키가 175cm인 사람의 몸무게는 72kg이야.”라고 하는 것과 같다.\n이렇게 키\\(x\\)에 대해서 몸무게에 대한 하나의 예측값\\(y\\)를 하나만 돌려주는 방법은 옳을까? 결론적으로 말하면 그렇지 않다. 왜냐하면 실제 data의 형태는 키\\(x\\)에 대해 하나의 몸무게\\(t\\)만을 가지지 않기이다. 예를 들자면 \\((175,62),(175,72),(175,83),(175,88)\\dots\\)등등 다양한 키-몸무게 다양한 조합이 가능하지만 함수로 표현할 경우 하나의 입력은 하나의 출력에만 mapping되며 이 밖에 나올 수 있는 다른 값들에 대한 설명은 전혀 하지 않기 때문이다. 그렇다면 어떠한 방법이 더 좋을까? 키가 175cm인 사람의 몸무게는 아마도 72kg 꺼야 정도의 말을 할 수 있다면 가장 좋을 것이다.\n이러한 해석은 키가 175cm인 사람에 대해서 몸무게에 대한 예측이 거의 72kg이라고 확신하지만 이와 동시에 몸무게의 예측값이 불확실하며 키가 동일하다 할지라도 어느정도는 랜덤하다는 것을 내포한다.이렇게 불확실하며 랜덤한 가지는 값은 randomvariable라고 하며 문제에서 몸무게\\(t\\)를 random-variable로 취급하면 위와 같은 해석이 가능하다. 예를들어 키가 \\(x\\)인 사람의 몸무게\\(t\\)가 randomvariable이며 다음과 같은 정규분포를 따른다고 가정하자.\n\\[\\begin{aligned}\n&t\\sim\\mathcal{N}(y(x|\\theta),\\sigma^2) \\\\\n&\\longleftrightarrow p(t|x,\\theta,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(\\frac{-(t-y(x|\\theta))^2}{2\\sigma^2})\n\\end{aligned}\\]\n위와 같다면 \\(y(x|\\theta)\\)에사 가장 높은 확률을 가지는 정규분포이므로 “키가 \\(x\\)인 사람의 몸무게\\(t\\)는 아마도 \\(y(x|\\theta)\\)일꺼야”라는 것을 의미하며 또한 다른 값들에 대한 확률들도 어느정도 담고 있으므로 \\(t\\)가 불확실하며 랜덤하다는 것도 확실하게 표현함을 알 수 있다.\n여기서 \\(\\sigma^2\\)는 우리가 한 예측에 대해서 얼마나 불확실인지를 나타내는 정도이다. \\(\\sigma^2\\)가 커서 확률분포의 변동이 크다면 모든 값에 대하여 일정한 확률을 부여하므로 예측값 \\(t\\)에 대한 확신이 없다는 것이고 변동이 작다면 어떤 특정 값에만 확률을 몰아서 부여한 것이므로 예측값\\(t\\)에 대한 확신이 크다는 것이다\n예를 들어 \\(x = 175,\\,y(x|\\theta) = 72,\\,t\\sim\\mathcal{N}(72,\\sigma^2)\\)인 두 개의 정규분포를 따른다면 모양은 다음과 같다.\n\n\n<matplotlib.legend.Legend at 0x19bf519daf0>\n\n\n\n\n\n이와 같이 키\\(x=175\\)에 일때 몸무게\\(t\\)는 randomvariable이며 평균이72 확률이 가장 높으므로 키가 175cm인 사람의 몸무게는 아마도 72kg 꺼야라는 말을 할 수 있으며 또한 다른 값으로 나올 확률도 빠짐없이 표현되어 있으므로 이와 같은 몸무게\\(t\\)는 확률변수로 표현하는 적절하다. 또한 빨간색 그래프는 파란색그래프보다 \\(\\sigma^2\\)가 더 크며 \\(t=72\\)에서 더 높은 확률을 부여하므로 예측에 대한 확신이 더 크다는 것을 나타낸다.\n지금까지 몸무게 \\(t\\)를 randomvariable이며 확률분포 \\(\\mathcal{N}(y(x|\\theta),\\sigma^2)\\)를 따른다고 하였다. 예시에서는 확률분포를 parameter를 가정하고 그래프를 그려서 비교했지만 실제로는 확률분포의 종류도 모르거니와 확률분포의 parameter도 모르기에 위와 같은 해석을 할 수 없다. 그러므로 여기서부터는 확률분포를 추정하는 것에 관한 얘기를 해보고자 한다."
  },
  {
    "objectID": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#maximum-likelyhood-estimation",
    "href": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#maximum-likelyhood-estimation",
    "title": "MLE & MAP (작성중)",
    "section": "Maximum Likelyhood Estimation",
    "text": "Maximum Likelyhood Estimation\n주어진 데이터에 맞는 확률분포는 어떻게 결정할 수 있을까? 크게 두 단계로 나뉜다. - 확률분포의 종류 결정 - 확률분포의 파라미터 추정\n확률분포의 종류를 결정하는 일은 문제마다 다르다. 위에서 예시로 들었던 몸무게와 같은 연속적인 확률변수일 경우 연속확률분포 중 하나를 가정하고 이산확률변수일 경우 이산확률분포 중 하나를 가정한다. 위의 문제에서는 정규분포를 몸무게\\(t\\)의 분포로 가정했다. 그러나 이는 문제에 따라 다르며 문제에 맞는 적절한 확률분포를 먼저 가정해야 한다.\n확률분포를 가정했다면 다음은 확률분포의 parameter를 구하면 된다.(추정한다고 말한다.)parameter를 추정할 경우 우리는 확률분포에 대한 완전한 식을 얻을 수 있으며 따라서 확률분포로부터 여러가지 정보를 얻을 수 있다. 여기서는 MLE를 통해서 확률분포의 파라미터를 추정한다.\n앞선 문제를 풀기전에 앞서서 간단한 예시로 예를 MLE에 대한 감을 잡아보자.예를 들어 대한민국 20대 남성들 중 한명의 키를 골라서 얻은 data set \\(D = \\{65\\}\\)가 주어졌다고 가정 해보자. 분포의 모양은 먼저 정규분포로 가정한다.\n\n\n<matplotlib.legend.Legend at 0x19bf1d77f10>\n\n\n\n\n\n평균만 다른 정규분포 3개를 고려해보자. 이 중 어떤 분포에서 \\(D\\)가 나왔을까? 주어진 3개의 분포중에는 \\(\\mu=63\\)인 파랑색에 해당하는 분포에서 \\(D=\\{65\\}\\)를 나왔을 가능성이 가장 크다. 왜냐하면 파랑색 확률분포에서 dataset에 속한 하나의 datapoint가 나올 확률이 가장 크기때문이다. 그러므로 3개의 정규분포중에서는 \\(\\mu =63\\)인 경우가 \\(D\\)에 가장 적합하다고 할 수 있으며 그 다음은 보라색 그 다음은 초록색 분포의 모수가 적합하다.\n이번에는 2개의 관측치가 존재하는 data set \\(D = \\{62,65\\}\\)가 주어졌다고 가정 해보자.\n\n\n<matplotlib.legend.Legend at 0x19bf7255e80>\n\n\n\n\n\n어떤 확률분포에서 \\(D=\\{62,65\\}\\)가 나왔을까? 마찬가지로 파랑색 분포가 주어진 분포중에서는 가장 가능성이 커보이는데 왜냐하면 마찬가지로 dataset에 속한 2개의 각각의 datapoint를 얻을 확률이 가장 커보이기 때문이다. 그러므로 위와 동일하게 3개의 정규분포중에서는 \\(\\mu =63\\)인 경우가 \\(D\\)에 가장 적합하다고 할 수 있으며 이번에는 초록색,보라색 순서로 모수가 적합하다.\n이번에는 크기가 \\(N\\)인 \\(D\\)가 주어져있다고 가정해보자.\n\n\n\n\n\n\n그래프는 크기가 30인data set이지만 N이라고 보자..!\n\n이번에도 어떤 확률분포에서 \\(D\\)가 나왔을지 한번 구분해보자. 그러나 이전에는 \\(D\\)의 크기가 1또는 2여서 직관적으로 보였지만 이번에는 데이터가 너무 많기 때문에 바로 보이지 않는다.\n그러나 이전에 했던 과정들 중에서 \\(D\\)에 있는 각각의 datapoint를 취할 확률을 모두 고려한뒤에 그 값이 가장 큰 parameter를 주어진 \\(D\\)에 가장 적합한 parameter 로 결정했었는데 이 사실은 그대로 사용할 수 있다. 왜냐하면 사실 각각의 datapoint를 모두 취할 확률은 결합확률이기 때문이다. 또한 각각의 datapoint는 모두 독립적이며 동일한 분포로부터 뽑힌 값이기 때문에 결합확률은 각 datapoin를 모두 동시에 취할 확률이다.\n\\[\\begin{align}\np(D|\\theta) &= p_{X_1,X_2,\\dots,X_N}(x_1,x_2,\\dots,x_N;\\theta) \\\\\n&= p_{X_1}(x_1|\\theta)\\cdot p_{X_2}(x_2|\\theta)\\dots  \\cdot p_{X_N}(x_N|\\theta) \\\\\n&= \\prod_{i=1}^{N}p_{X_i}(x_i;\\theta)\n\\end{align}\\]\n\n\\(\\theta\\)는 parameter를 의미한다. 정규분포의 parameter이므로 \\(\\mu,\\sigma^2\\)라 생각하면 된다.\n결합확률은 likelyhood라고도 한다.\n\n그러므로 3개의 서로다른 \\(\\theta\\)에 대한 확률분포에 대하여 모두 likelyhood를 구한뒤 그 중 가장 큰 값을 parameter로 하면 된다. 그런데 사실 우리의 비교 대상은 단 3개의 \\(\\theta\\)가 아니다. 가능한 모든 확률분포 즉 가능한 모든 \\(\\theta\\)에 대하여 likelyhood의 가장 큰 값을 구해야 한다. 그러므로 \\(\\theta\\)를 변수로 생각하여 likelyhood를 가장크게 하는 \\(\\theta\\)를 찾는다. 따라서 maximum likelyhood estimation이며 이를 통하여 나온 parameter \\(\\theta\\)에 대한 추정량은 다음과 같다.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MLE} &= \\underset{\\theta}{\\text{argmax}}\\,p_{X_1,X_2,\\dots,X_n}(x_1,x_2 \\dots x_n|\\theta) \\\\\n&=  \\underset{\\theta}{\\text{argmax}}\\,p_{X_1,X_2,\\dots,X_n}(x_1,x_2,\\dots,x_n|\\theta) \\\\\n&= \\underset{\\theta}{\\text{argmax}}\\,\\prod_{i=1}^{N}p_{X_i}(x_i;\\theta)\n\\end{aligned}\\]\n정리하자면 결국 MLE는 파라미터를 추정하는 방법으로 likelyhood(\\(D\\)에 있는 각각의 datapoint가 모두 나올 확률)를 최대화하는 파라미터\\(\\theta\\)를 추정량으로 한다는 것이다.\n이제 기존문제로 돌아가보자. 우리에게 주어진 data set \\(D\\)는 다음과 같았으며 정규분포로 몸무게는 \\(t\\)로 가정했었다.\n\\[\\begin{aligned}\n&D = \\{(x_1,t_1),(x_2,t_2,),\\dots,(x_N,t_N)\\} \\\\\n&t\\sim\\mathcal{N}(y(x|\\theta),\\sigma^2) \\\\\n&\\longleftrightarrow p(t|x,\\theta,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(\\frac{-(t-y(x|\\theta))^2}{2\\sigma^2})\n\\end{aligned}\\]\n\\(D\\)에 있는 각각의 datapoint를 모두 나올 확률은 키가 \\(x_1\\)일 때 실제 몸무게가 \\(t_1\\)이고, 키가 \\(x_2\\)일 때 실제 몸무게가 \\(t_2\\)이고, \\(x_N\\)일 때 실제 몸무게가 \\(t_N\\)일 확률과 같다.\n\\[\\begin{aligned}\np(D|\\theta) &= p(t_1,t_2,\\dots,t_N|X,\\theta,\\sigma^2) \\\\\n&= \\prod_{n=1}^Np(t_i|x_i,\\theta,\\sigma^2) \\\\\n&= \\prod_{n=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(\\frac{-(t_i-y(x_i|\\theta))^2}{2\\sigma^2})\n\\end{aligned}\\]\nMLE는 likelyhood를 최대화 하는 파라미터를 구하는 방법이므로 \\(\\theta\\)는 다음과 같다.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MLE} = \\underset{w}{\\text{argmax}}&\\prod_{n=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(\\frac{-(t_i-y(x_i|\\theta))^2}{2\\sigma^2})\n\\end{aligned}\\]\n위의 식으로부터 직접 최대화 하는 \\(\\theta\\)를 찾기는 어려우므로 likelyhood에 \\(-\\text{ln}\\)를 곱해준 NLL을 사용한다. \\(\\text{ln}\\)함수는 곱해도 최댓값(또는 최솟값)의 위치가 변하지 않으며 곱셈연산을 덧셈연산으로 바꿔 계산을 간단하게 해준다. 또한 -를 곱해줌으로서 최소지점을 찾는 문제로 바뀌게 된다.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MLE} &= \\underset{w}{\\text{argmin}}-\\text{ln}\\prod_{n=1}^N\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(\\frac{-(t_i-y(x_i|\\theta))^2}{2\\sigma^2})\\\\\n&= \\underset{w}{\\text{argmin}}-\\sum_{n=1}^N\\text{ln}\\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(\\frac{-(t_i-y(x_i|\\theta))^2}{2\\sigma^2}) \\\\\n&= \\underset{w}{\\text{argmin}}-\\sum_{n=1}^{N}\\bigg[\\text{ln}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} + \\text{ln}\\,\\text{exp}(\\frac{-(t_i-y(x_i|\\theta))^2}\n{2\\sigma^2})\\bigg]\\\\\n&= \\underset{w}{\\text{argmin}}-\\sum_{n=1}^{N}\\bigg[\\text{ln}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} - \\frac{(t_i-y(x_i|\\theta))^2}{2\\sigma^2}\\bigg]\n\\end{aligned}\\]\n여기서 \\(\\theta\\)의 관점에서 상수(\\(\\pi,\\sigma^2\\))를 제거하고 보면 다음과 같다.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MLE} = \\underset{w}{\\text{argmin}}\\sum_{n=1}^{N}(t_i-y(x_i|\\theta))^2\n\\end{aligned}\\]\n어디선가 많이 본 식인데 바로 L2 Loss이다. 이렇게 datapoint가 정규분포를 따른다고 가정하고 MLE를 통해 L2 Loss를 나옴을 밝힘으로써 회귀문제에서 L2 Loss를 왜 사용하는지 그 이유를 설명할 수 있고 역으로 회귀문제에서 L2 Loss를 사용한다면 datapoint가 정규분포를 따른다고 가정하고 MLE를 통해서 파라미터를 구하는 것이겠구나 하고 알 수 있다.\n참고적으로 classification에서는 datapoint가 베르누이분포를 따른다고 가정한다면 binary cross entropy loss를 얻을 수 있다고 한다.\n간단정리 - Dataset이 어떠한 확률분포를 따르는지 결정하기 위해서는 parameter를 구해야 한다. - MLE은 Likelyhood를 가장 크게 만드는 parameter를 찾고 그 값을 parameter에 대한 추정량으로 한다."
  },
  {
    "objectID": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#maximum-a-posterior",
    "href": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#maximum-a-posterior",
    "title": "MLE & MAP (작성중)",
    "section": "Maximum a Posterior",
    "text": "Maximum a Posterior\nMLE는 likelyhood인 \\(p(D|\\theta)\\)를 maximize하는 \\(\\theta\\)를 구하였다. MAP는 반대로 posterior인 \\(p(\\theta|D)\\)를 maximize하는 \\(\\theta\\)를 구한다. 한마디로 설명하자면 주어진 데이터셋 \\(D\\)에 확률이 가장 큰 \\(\\theta\\)를 선택하는 방식이라고 할 수 있다.\nMLE대신 MAP를 통해 파라미터를 구함으로써 얻을 수 있는 장점은 뭘까? 먼저 posterior를 bayes rule에 의해서 적어보면 다음과 같다.\n\\[\\begin{aligned}\n&p(\\theta|D) = \\frac{p(D|\\theta)p(\\theta)}{p(D)} \\propto p(\\theta|D)p(\\theta)\\\\\n&\\text{where }  \\\\\n&\n\\begin{aligned}\n&p(\\theta|D) : \\text{posterior}\\\\\n&p(D|\\theta) : \\text{likelyhood}\\\\\n&p(\\theta)   : \\text{prior} \\\\\n&p(D) : \\text{normalization constant}\n&\\end{aligned}\n\\end{aligned}\\]\n주목할 점은 posterior에는 우리가 정할 수 있는 prior가 포함되어 있다는 것이다. 따라서 prior를 통하여 가지고 있는 사전지식을 posterior에 충분히 반영할 수 있고 사전지식이 없다고 할지라도 prior를 통해 overfitting을 방지할 수 있다. overfitting은 \\(|\\theta|\\)가 커서 신경망의 표현력이 과할 경우 일어나는 현상이므로 prior에 평균이 0인 분포를 가정해준다면 우리는 posterior를 통해서 비교적 작은 크기를 갖는 \\(\\theta\\)를 얻을 수 있다.\n여기서는 \\(\\theta\\)가 정규분포를 따른다고 가정하고 overfitting을 막아주는 L2 Regularization(weight decay)를 유도해보자.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MAP} &= \\underset{\\theta}{\\text{argmax}}\\,P(\\theta|D) \\\\\n&= \\underset{\\theta}{\\text{argmax}}\\,p(\\theta|D)p(\\theta)\\\\\n&= \\underset{\\theta}{\\text{argmax}}\\,\\text{ln}\\,p(\\theta|D)p(\\theta)\\\\\n&= \\underset{\\theta}{\\text{argmax}}\\,\\text{ln}\\,p(\\theta|D) + \\text{ln}\\,p(\\theta)\\\\\n&= \\underset{\\theta}{\\text{argmin}}\\, -\\text{ln}\\,p(\\theta|D) - \\text{ln}\\,p(\\theta)\n\\end{aligned}\\]\n여기서 \\(-\\text{ln}p(\\theta|D)\\)는 MLE에서의 negative log likelyhood와 같으며 \\(\\text{ln}p(\\theta)\\)는 우리가 정하는 prior에 의한 값이다. \\(\\theta\\)에 대한 prior가 정규분포를 따른다고 가정하고 구해보면 다음과 같다.\n\\[\\begin{aligned}\n&\\theta \\sim \\mathcal{N}(\\theta|0,\\sigma^2) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}}\\text{exp}(-\\frac{\\theta^2}{2\\sigma^2_\\theta})\\\\\n&-\\text{ln}p(\\theta) = -\\text{ln}\\frac{1}{\\sqrt{2\\pi\\sigma^2}} + \\frac{\\theta^2}{2\\sigma^2_{\\theta}}\n\\end{aligned}\\]\n상수값을 제거하고 대입하면 MAP를 통해 구한 parameter는 다음과 같다.\n\\[\\begin{aligned}\n\\hat{\\theta}_{MAP} &= \\underset{\\theta}{\\text{argmin}}\\sum_{i=1}^{N}(t - y(x_i|w))^2 + \\frac{w^2}{2\\sigma_{\\theta}^2}\\\\\n&= \\underset{\\theta}{\\text{argmin}}\\sum_{i=1}^{N}(t - y(x_i|w))^2 + \\alpha w^2 \\quad(\\alpha = \\frac{1}{2\\sigma^2})\n\n\\end{aligned}\\]\n이 또한 어디선가 많이 본 식이며 바로 L2 Loss에 L2 Regularization을 추가한 것이다. 이와 같이 parameter의 prior가 정규분포라고 가정하고 MAP를 통해 loss function을 구한다면 우리가 사용하고자 하는 모델의 loss function에서 regularization term이 왜 저렇게 나오는가를 설명할 수 있다. 또한 역으로 L2 Regularization term이 포함된 loss function을 사용한다면 paraemter에 대한 prior를 정규분포임로 가정했음을 알 수 있다."
  },
  {
    "objectID": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#베이즈-정리에서-바라본-mle",
    "href": "posts/Probability Statistics/Maximum likelyhood estimation/MLE.html#베이즈-정리에서-바라본-mle",
    "title": "MLE & MAP (작성중)",
    "section": "베이즈 정리에서 바라본 MLE",
    "text": "베이즈 정리에서 바라본 MLE\n\\(D=(t_1,t_2,\\dots,t_n)\\)는 샘플(데이터셋), \\(\\theta\\)는 우리가 알고싶은 확률분포의 모수라고 합시다. 베이즈정리는 증거 또는 조건이 주어지기 전의 사전확률 \\(p(D)\\)와 주어진 후의 사후확률 \\(p(\\theta|D)\\)사이의 관계를 알려줍니다.\n여기서 사후확률은 \\(\\theta\\)에 대한 확률분포로 데이터(샘플)이 주어질때 확률분포의 임의의 모수\\(\\theta\\)가 얼마나 가능한지 또는 불확실한지 그 정도를 알려주는 확률을 함숫값으로 가지는 확률함수입니다. 그러므로 사후확률을 최대로 하는 모수\\(\\theta\\)가 데이터셋이 주어져있을 때 가장 확률이 높은,가능성이 높은 모수이므로 그때의 값을 확률분포의 모수로 추정하면 됩니다.\n문제는 왼쪽의 확률분포는 바로 알기가 쉽지 않다는 점입니다. 따라서 베이즈정리를 통하여 우변의 식을 최대화 하는 값을 구합니다. 우변의 식에서 분모는 주어진 데이터에 의하여 고정된 상수(normalization constant라고 합니다)입니다. 그러므로 최댓값을 구하는데 영향을 주지 않습니다. 우리는 분자에 있는 \\(p(D|\\theta)p(\\theta)\\)를 최대화하는 \\(\\theta\\)를 찾으면 됩니다.\n여기서 \\(p(D|\\theta)\\)를 가능도(likelyhood)라 합니다. MLE에서는 분자에서 가능도만 최대로 하는 \\(\\theta\\)를 구합니다. MAP라는 다른 방법은 분자에 있는 \\(p(D|\\theta)p(\\theta)\\)를 최대화 하는 \\(\\theta\\)를 구한다고 합니다.\n가능도함수를 최대로 하는 모숫값은 175.75입니다. 따라서 MLE에 의한 모수에 대한 추정값은 175.75입니다. \\[\\hat{\\theta}_{MLE} = 175.75\\]\n링크1(random sample vs random variable) 링크2(추정,추정량,추정값) 링크3(mle) 링크4(mle)"
  },
  {
    "objectID": "posts/Probability Statistics/notation.html",
    "href": "posts/Probability Statistics/notation.html",
    "title": "확률분포에서 ;와|의 사용",
    "section": "",
    "text": "확률분포에서 ;와|에 관한 notation 정리\n\n;의 사용\n; 함수에서 특정 변수를 고정시켜놓을때 사용할 수 있다. 다음과 같은 이변수함수를 생각해보자 \\[f(x,y)\\] 독립변수 x와 y를 갖는 함수이며 두 변수 모두에 대해서 미분가능하다. 여기서 변수 y가 어떤 어떤 값으로 고정된것이 알려지거나 또는 고정된상황을 가정한 후 x에 대해서만 관심이 있다고 해보자(미분,극한등을 아마도?취하고 싶다..아마도…) 이때는 이변수함수로 표기하는것이 아닌 일변수함수로 표기해야한다. 그때는 다음과 같이 표기하면 된다.\n\\[\nf(x;y)\n\\]\n이렇게 표기하면 y값은 이제 어떤 값으로 이미 설정(setting)되거나 고정(fixed)되어있음을 의미한다. 비슷하게 그냥 고정된 파라미터가 이 다음에도 올 수 있다. 베르누이분포의 확률질량함수는 다음과 같다.\n\\[f_X(x;p) = p^x(1-p)^{1-x}\\]\nsetting,fixed된 파라미터인 p를 ;다음에 표시했다.\n\n\n|의 사용\n|는 given that,if이라는 의미이며 |의 다음에는 보통 먼저 깔고가거나 주어지는 전제,상황,조건,증거 등이 나온다. 확률,통계 분야에서만 쓰이며 조건부 확률이나,베이지안에서 많이 쓰이는 기호라고 한다. 중요한 점은 특히 |다음에 오는 상황,조건,증거와 같은 것들은 어떤 관점이냐에 따라서 상수나 확률변수의 관점으로 생각될 수 있다는 것이다. 특히 베이지안에서는 확률변수로 생각한다.\n조건부 확률분포(조건부확률밀도함수,조건부 확률함수)는 다음과 같다. \\[\np_{X|Y}(x|y) = \\frac{p_{XY}(x,y)}{p_Y(y)}\\\\\np_{X|Y}(y|x) = \\frac{p_{XY}(x,y)}{p_X(x)}\n\\]  가장 윗식을 해석해보면 다음과 같다. - 좌변 : 확률변수 Y가 주어진(given)상황에서 확률변수X의 확률(또는 확률밀도)를 나타내는 함수이며 시행으로부터 확률변수 X가 취할 수 있는 값 x를 입력으로 받는다. 조건부 확률분포에서 Y의 값은 y로 주어져 있다고 가정하므로 입력하는 변수가 아니다. 따라서 확률변수 Y는 변수가 아니라 모수(parameter)이다. - 우변 : 조건부확률분포 공식.\n##  결론 ; vs |\n\\[p_{\\theta}(x) = p(x;\\theta) = p(x|\\theta)\\]  ;는 수학의 모든 분야에서 쓰이지만 |는 확률통계분야에서만 쓰인다. ;다음에는 이미 설정되거나 고정된 값이 나오며 |다음에는 먼저 주어지는 조건이 나온다. 근데 ;를 많이 쓰지 않고 |뒤에 그냥 고정된 값을 써버리는 경우가 많다. 이는 지양해야 할 표현이다.\n참고링크 링크1(;의 사용법) 링크2(조건부확률분포) 링크3(표기법의 혼용)"
  },
  {
    "objectID": "posts/Probability Statistics/sample space and random variable.html",
    "href": "posts/Probability Statistics/sample space and random variable.html",
    "title": "확률론 용어정리",
    "section": "",
    "text": "Experiment and trial\n가능한 결과들이 미리 정해져있고 무한히 반복가능한 과정을 확률실험(experiment) 또는 시행(trial)이라고 합니다.\n\n\nsample space & event\n어떤 임의의 시행을 가정해본다고 합시다. 시행으로부터 나올 수 있는 모든 결과들의 집합을 우리는 표본공간(sample space)라고 합니다. 사건은 표본공간이라는 집합의 부분집합입니다. 여러개의 원소(결과)들이 모여서 하나의 사건이 될 수 있으며 단 하나의 원소로도 사건이 될 수 있습니다. 시행의 결과가 어떠한 사건(부분집합)에 속하는 경우 우리는 “~인 사건이 발생했다!”라고 표현합니다.\n\n\nrandom variable\n확률변수는 확률실험 또는 시행으로부터 나올 수 있는 결과를 대신 나타내는 변수입니다. 시행을 하기전까지는 그 값이 정해지지 않고 확률분포만 존재하며 시행을 하면 확률분포에 의해서 결과가 정해지고 실수가 부여됩니다. 수학적으로는 표본공간의 원소를 정의역으로 하여 실수를 대응시키는 “함수”입니다.\n\n\nprobability distribution(function)\n확률분포(확률함수)란 확률변수가 취할 수 있는 실수값에 각각의 실수를 취할 가능성인 확률 또는 확률밀도를 대응시키는 함수입니다.\n\n\n예시\n동전을 두번 던지는 시행을 3번 반복하여 크기가 3인 표본을 얻었다고 가정해봅시다. 동전의 앞면을 H(head)라 하고 뒷면을 T(tail)이라고 할 때 표본공간은 다음과 같습니다.\n\\[\\Omega = \\{HH,HT,TH,TT\\}\\]\n표본공간에 있는 결과들 중에서 동전의 앞면이 1개라도 있는 경우 사건A 동전의 앞면이 하나도 없는 경우를 사건B라 합시다. 사건A와 B는 다음과 같습니다.\n\\[A = \\{HH,HT,TH\\},B=\\{TT\\}\\]\n\\(X_1,X_2,X_3\\)는 각각 첫번째 두번째 세번째 시행의 결과를 나타내는 변수이며 확률분포는 다음과 같다고 합시다.\n\n\n\n                                                \n\n\n위의 확률분포를 반영하여 시행의 결과가 결정됩니다. 시행으로부터 얻은 표본은 \\((1,1,0)\\)이며 \\(X_1 = 1 ,X_2 =1 ,X_3 =0\\) 입니다. 시행의 결과 얻은 실제로 관찰된 표본은 \\((x_1,x_2,x_3)\\) 이런식으로 각각의 원소를 소문자인 미지수로 표현할 수도 있습니다.\n\n\ni.i.d & randomsample & realization\n위의 동전던지기 실험에서 각각의 확률변수는 이전에 던진 결과가 이후에 던지는 결과에 영향을 미치지 않고(즉,확률분포에 영향을 미치지 않고) 동일한 분포를 따르는 확률분포였습니다. 이와 여러개의 확률변수가 서로간에 독립이며 동일한 분포를 따르는 확률변수들을 Independent and identically distributed random variables라고 합니다. 위의 분포는 베르누이 분포를 따르므로 다음과 같이 표현할 수 있습니다.\n\\[X_1,X_2,X_3 \\overset{i.i.d}{\\sim} \\text{Bernoulli(p = 0.7)}\\]\nrandomsample은 i.i.d인 여러개의 확률변수의 모음입니다. \\(X1,X2,X3\\)를 말합니다.\nrealization은 관찰된 결과 각각을 말합니다. \\(x1\\)도 realization \\(x2\\)도 realization \\(x3\\)도? 모두 realization입니다. \\(x1,x2,x3\\)를 모아서 확률변수 \\(X_1,X_2,X_3\\)의 realizations이라고 합니다.\n\n\n참고자료\nwikipedia - Experiment (probability theory) StackExchange - What is the difference between random variable and random sample? wikipedia - i.i.d 정보통신기술용어해설"
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "",
    "text": "파이썬 변수,할당문,인터닝\n전북대학교 최규빈 교수님의 딥러닝 deepcopy-shallowcopy 특강 중,변수와 할당문 부분을 재구성한 글입니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-변수",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-변수",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "파이썬에서의 변수",
    "text": "파이썬에서의 변수\n\n파이썬의 변수는 메모리상에 저장된 객체를 참조(reference)합니다.\n따라서 변수는 자바에서 참조변수와 같습니다.\n변수는 객체(object)를 부르는 별칭(다른이름),객체에 붙여진 포스트잇,또다른 레이블 이라고 생각하는 것이 비유적으로 맞습니다.\n앞으로 객체에 접근하기 위해서는 a라는 변수(별칭,객체)를 찾으면 됩니다.\n마치 C언어의 포인터와 유사하게 동작합니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-할당문",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#파이썬에서의-할당문",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "파이썬에서의 할당문(=)",
    "text": "파이썬에서의 할당문(=)\n\n=은 파이썬(뿐만아니라 대부분의 프로그래밍언어)에서 할당문입니다.\n할당문을 실행하면 = 오른쪽에 있는 객체를 먼저 메모리 상에 생성하거나 가져옵니다.\n=의 왼쪽에는 변수가 존재하며 파이썬은 변수에 생성된 객체를 할당합니다.\n변수는 객체에 붙여지는 별칭,레이블로 표현할 수 있습니다. 이렇게 객체에 변수가 할당되고나면 할당된 변수와 객체에 대해서 “변수가 객체에 바인딩(묶이다)되어있다”고 표현합니다.\n\n\na = [1,2,3]\nprint(id(a))\n\n1819154486912\n\n\n위 코드에서 객체[1,2,3]에 변수 a가 바인딩 되었습니다.내부동작은 다음과 같습니다. 1. 메모리 주소(1819161042880)에 리스트 객체[1,2,3]을 생성합니다 2. 생성된 리스트객체를 변수 a에 할당합니다. a는 객체[1,2,3]에 붙여지는 별칭,레이블이라고 할 수 있습니다.\n\n에일리어싱\n에일리어싱은 하나의 객체를 여러개의 변수가 참조하게 하는 것입니다. 하나의 객체에 여러개의 별칭,별명,레이블을 붙이는 것이라고도 할 수 있습니다.\n\nb = a\nprint(id(a));print(id(b))\nprint(a);print(b)\n\n1819154486912\n1819154486912\n[1, 2, 3]\n[1, 2, 3]"
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#idvalue",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#idvalue",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "id,value",
    "text": "id,value\nid는 객체가 가지는 메모리상의 고유한 주소입니다. 서로다른 객체는 다른 값을 가질수도 같은값을 가질수도 있습니다.\n\na=[1,2,3]\nb=a\na.append(4)\nc=[1,2,3,4]\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\n\na:[1, 2, 3, 4] b:[1, 2, 3, 4] c:[1, 2, 3, 4]\nid(a):1819154849280,id(b):1819154849280,id(c):1819155097408\n\n\n변수a,b,c는 모두 같은 value(값)을 가집니다.a와b는 같은 객체에 바인딩되어있지만 c는 또다른 객체에 바인딩되어 있습니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#is-vs",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#is-vs",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "is vs ==",
    "text": "is vs ==\n- is는 객체비교연산자로 두 변수가 동일한 객체를 참조하는지 아니면 다른객체를 참조하는지 확인한 후 True or False를 반환합니다. 파이썬은 내부적으로 동일한 객체인지 아닌지를 판단할때에는 메모리주소를 확인한다고 합니다. - ==는 값비교연산자로 두 변수가 참조하는 객체의 값이 같은지 아니면 값이 다른지를 확인한 후 True or False를 반환합니다. - 참조하다는 뭔가 잘 와닿는데 참조`라는 용어는 잘 와닿지가 않습니다…변수가 참조하는(또는 가리키는,지칭하는)객체(object) 그 자체입니다.\n\ncode1\n\na=[1,2,3] #1\nprint(\"append하기 전 id(a):\",id(a))\nb=a #2 에일리어싱,동일한 객체를 가리키도록 함.\na.append(4) #3\nc=[1,2,3,4] #4\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\nprint(\"a의 참조(reference)와 b의 참조는 동일한 객체인가요??\",a is b)\nprint(\"a의 참조와 b의 참조는 값(value)이 같나요?\",a == b)\nprint(\"a의 참조(reference)와 c의 참조는 동일한 객체인가요??\",a is c)\nprint(\"a의 참조와 c의 참조는 값(value)이 같나요?\",a == c)\n\nappend하기 전 id(a): 1819155097408\na:[1, 2, 3, 4] b:[1, 2, 3, 4] c:[1, 2, 3, 4]\nid(a):1819155097408,id(b):1819155097408,id(c):1819155103296\na의 참조(reference)와 b의 참조는 동일한 객체인가요?? True\na의 참조와 b의 참조는 값(value)이 같나요? True\na의 참조(reference)와 c의 참조는 동일한 객체인가요?? False\na의 참조와 c의 참조는 값(value)이 같나요? True\n\n\n코드설명 1. 변수a에 [1,2,3]을 할당합니다.a는 [1,2,3]을 참조합니다. 2. 에일리어싱으로 변수b도 [1,2,3]을 참조합니다. 3. 변수a가 참조하는 리스트객체[1,2,3]에 4를 추가합니다. 4. 변수c에 [1,2,3,4]를 할당합니다.\n\n\ncode2 - 살짝 심화\n\na=[1,2,3] #1\nprint(\"재할당 하기 전 id(a):\",id(a))\nb=a #2에일리어싱,동일한 객체를 가리키도록 함\na=[1,2,3]+[4] #3재할당\nc=[1,2,3,4] #4\nprint(f'a:{a} b:{b} c:{c}')\nprint(f'id(a):{id(a)},id(b):{id(b)},id(c):{id(c)}')\nprint(\"a의 참조(reference)와 b의 참조는 동일한 객체인가요??\",a is b)\nprint(\"a의 참조와 b의 참조는 값(value)이 같나요?\",a == b)\nprint(\"a의 참조(reference)와 c의 참조는 동일한 객체인가요??\",a is c)\nprint(\"a의 참조와 c의 참조는 값(value)이 같나요?\",a == c)\n\n재할당 하기 전 id(a): 1819155102592\na:[1, 2, 3, 4] b:[1, 2, 3] c:[1, 2, 3, 4]\nid(a):1819184303168,id(b):1819155102592,id(c):1819184334272\na의 참조(reference)와 b의 참조는 동일한 객체인가요?? False\na의 참조와 b의 참조는 값(value)이 같나요? False\na의 참조(reference)와 c의 참조는 동일한 객체인가요?? False\na의 참조와 c의 참조는 값(value)이 같나요? True\n\n\n코드설명 1. 변수a에 [1,2,3]을 할당합니다.a는 [1,2,3]을 참조합니다. 2. 에일리어싱으로 변수b도 [1,2,3]을 참조합니다. 3. 변수a에 리스트객체[1,2,3,4]를 재할당합니다. 4. 변수c에 [1,2,3,4]를 할당합니다.\na,b,c 각각의 값은 code1과 code2에서 모두 같습니다. 차이점은 code1에서는 a가 참조하는 리스트[1,2,3]에 4를 추가하고 code2에서는 a에 리스트[1,2,3,4]를 재할당한다는 점입니다.중요한 차이점은 다음과 같습니다.\n- code1에 append전 후의 a가 참조하는 객체는 주소는 변하지 않은 것으로 보아 동일한 객체에 원소만 추가되었음을 알 수 있습니다. - 반면 code2에서 할당문 전 후의 a가 참조하는 객체의 주소가 변합니다. - 이전에 없었던 1)객체가 메모리에 생성되고 2)변수a는 이전의 [1,2,3]을 더 이상 참조하지 않고 생성된 객체[1,2,3,4]를 참조**하는 것을 알 수 있습니다."
  },
  {
    "objectID": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#인터닝",
    "href": "posts/python/(1) allocation,variable,interning/python - allocation,variable,interning.html#인터닝",
    "title": "파이썬 - 변수,할당문,인터닝",
    "section": "인터닝",
    "text": "인터닝\n인터닝이란 이미 생성된 객체를 재사용하는 것을 말합니다. 객체의 빠른 재사용을 가능하게 하며 메모리를 절약한다고 합니다. 내부적으로는 다음과 같이 구현됩니다. 1. 임의의 할당문을 실행합니다. 2. = 오른쪽에 있는 객체가 Intern 컬렉션에 등록되어 있는지 아닌지 확인합니다. 3. 등록되어 있는 객체의 경우 그 객체를 그대로 참조합니다. 등록되지 않은 경우 메모리에 객체를 생성하며 변수는 생성된 객체의 주소를 참조합니다\n자주 사용하는 객체의 경우 직접 Intern 컬렉션에 등록할 수 있고 빠르게 재사용할 수 있습니다. -5~256사이의 정수이거나 20자 미만의 문자열은 할당문을 실행하면 자동으로 Inter 컬렉션에 등록됩니다. 따라서 해당하는 정수나 문자열을 또다른 할당문에 실행하면 변수는 같은 객체를 참조합니다.\n예제1-인터닝 X\n\na=1+2021\nb=2023-1\nc=2022\nprint(id(a),id(b),id(c))\nprint(a,b,c)\n\n1819155040400 1819155039600 1819155040688\n2022 2022 2022\n\n\na,b,c는 서로다른 객체를 참조하며 객체들은 모두 같은 값을 가집니다.\n예제2-인터닝 O\n\na=1+2 \nb=4-1\nc=3\nprint(id(a),id(b),id(c))\nprint(a,b,c)\n\n1819075897712 1819075897712 1819075897712\n3 3 3\n\n\na,b,c는 모두같은 객체를 참조합니다. 내부적인 동작은 다음과 같습니다. 1. a=1+2 할당문을 실행합니다. 2. 할당되는 객체가 -5~256사이의 정수이므로 자동으로 Intern 컬렉션에 등록됩니다. 3. b와c에도 정수 3을 할당합니다. 3은 Intern 컬렉션 등록되어있는 객체이며 메모리상에 생성되어있는 객체이므로 새로운 3객체가 생성되지 b,c는 이미 생성된 3객체를 가리킵니다.\n참고링크1 : https://guebin.github.io/DL2022/posts/Appendix/2022-12-14-A1.html 참고링크2 : http://pythonstudy.xyz/python/article/512-%ED%8C%8C%EC%9D%B4%EC%8D%AC-Object-Interning"
  },
  {
    "objectID": "posts/python/python class,instance,self,namespace.html",
    "href": "posts/python/python class,instance,self,namespace.html",
    "title": "HIHO",
    "section": "",
    "text": "파이썬에서 어떤 부분(스코프)에서 코드(할당문)가 작성되느냐에 따라 객체를 가리키는 식별자(변수명)와 객체는 다른 공간에 속하게 된다.\n이와같은 식별자와 객체가 속하는 서로다른 공간을 Namespace(이름공간)이라고 한다.\n모듈에서 코드가 작성되면 모듈이 가지는 네임스페이스인 전역네임스페이스에 식별자와 객체가 생성되며 매핑되는 관계를 가지게 된다.\n함수안에서 코드가 작성되면 함수의 네임스페이스인 지역네임스페이스에서 식별자와 객체가 생성되며 서로 매핑되는 관계를 가지며 클래스,인스턴스에서도 네임스페이스라는 개념이 존재한다.\n이 매핑관계는 파이썬에서 딕셔너리로 구현된다.\n\n\n\n\n\n네임스페이스는 단순히 식별자와 객체가 소속되는 공간이라는 의미를 넘어서서 나아가 코드의 실행이 이루어지는 공간을 의미하기도 한다.\n\n\n\n객체에 대해 뭔가 실행 -> 네임스페이스에서 객체를 조회 -> 객체를 가져옴(없거나 다른 네임스페이스에도 없으면 에러) 지역네임스페이스에 없으면 -> 전역네임스페이스 조회,여기에도 없으면 -> 빌트인 네임스페이스 조회,여기에도 없으면 오류\n스코프 : 코드상의 어떤 영역을 말한다. 하나 또는 여러개의 네임스페이스와 네임스페이스와 연결되어 있다.\n참고링크 https://post.naver.com/viewer/postView.nhn?volumeNo=27785662&memberNo=21815&navigationType=push https://schoolofweb.net/blog/posts/%ED%8C%8C%EC%9D%B4%EC%8D%AC-oop-part-3-%ED%81%B4%EB%9E%98%EC%8A%A4-%EB%B3%80%EC%88%98class-variable/ https://wikidocs.net/1743 https://hcnoh.github.io/2021-04-17-dkvmn"
  },
  {
    "objectID": "posts/python/python class,instance,self,namespace.html#class",
    "href": "posts/python/python class,instance,self,namespace.html#class",
    "title": "HIHO",
    "section": "CLASS",
    "text": "CLASS\n\nclass Resnet(): #클래스 함수를 정의하는 경우 def대신 class를 입력한다.\n  temp = 25\n  #print(locals())\n# 이 코드를 실행하면 c는 Resnet이라는 클래스 함수의 호출을 통해 생성된 객체의 식별자가 된다.\n# 클래스 함수는 그 반환값이 사용자가 정의한 객체가 되며 이 객체를 변수c가 가리키게 된다.\n\n\nResnet.__dict__ #클래스의 네임스페이스 확인\n#클래스에 print(locals())로도 확인할 수 있음.\n\nmappingproxy({'__module__': '__main__',\n              'temp': 25,\n              '__dict__': <attribute '__dict__' of 'Resnet' objects>,\n              '__weakref__': <attribute '__weakref__' of 'Resnet' objects>,\n              '__doc__': None})\n\n\n\nglobals() #전역네임스페이스에 클래스 Resnet이 추가된 것도 확인\n\n{'__name__': '__main__',\n '__doc__': 'Automatically created module for IPython interactive environment',\n '__package__': None,\n '__loader__': None,\n '__spec__': None,\n '__builtin__': <module 'builtins' (built-in)>,\n '__builtins__': <module 'builtins' (built-in)>,\n '_ih': ['',\n  'a=10;b=20',\n  '#전역네임스페이스를 보여주는 명령어\\n#전역네임스페이스에 존재하는 식별자와 객체들을 볼 수 있다.\\nglobals()',\n  '#지역네임스페이스를 보여주는 명령어\\n#지역네임스페이스는 어떤 스코프에서의 네임스페이스를 보여준다.\\n#모듈의 지역네임스페이스는 전역네임스페이스와 같으므로 두 결과는 서로 같다.\\nlocals()',\n  'def printhi():\\n  text = \"hi\"\\n  text2 = \"hello\"\\n  print(\"hi\")\\n  print(__name__)\\n  print(locals())\\nprinthi()',\n  'globals()',\n  'x = 1\\ndef no_local():\\n    print(locals())\\n    print(x)\\nno_local()',\n  'class Resnet(): #클래스 함수를 정의하는 경우 def대신 class를 입력한다.\\n  temp = 25\\n  #print(locals())\\n# 이 코드를 실행하면 c는 Resnet이라는 클래스 함수의 호출을 통해 생성된 객체의 식별자가 된다.\\n# 클래스 함수는 그 반환값이 사용자가 정의한 객체가 되며 이 객체를 변수c가 가리키게 된다.',\n  'Resnet.__dict__ #클래스의 네임스페이스 확인\\n#클래스에 print(locals())로도 확인할 수 있음.',\n  'globals() #전역네임스페이스에 클래스 Resnet이 추가된 것도 확인'],\n '_oh': {2: {...},\n  3: {...},\n  5: {...},\n  8: mappingproxy({'__module__': '__main__',\n                'temp': 25,\n                '__dict__': <attribute '__dict__' of 'Resnet' objects>,\n                '__weakref__': <attribute '__weakref__' of 'Resnet' objects>,\n                '__doc__': None})},\n '_dh': ['/content'],\n 'In': ['',\n  'a=10;b=20',\n  '#전역네임스페이스를 보여주는 명령어\\n#전역네임스페이스에 존재하는 식별자와 객체들을 볼 수 있다.\\nglobals()',\n  '#지역네임스페이스를 보여주는 명령어\\n#지역네임스페이스는 어떤 스코프에서의 네임스페이스를 보여준다.\\n#모듈의 지역네임스페이스는 전역네임스페이스와 같으므로 두 결과는 서로 같다.\\nlocals()',\n  'def printhi():\\n  text = \"hi\"\\n  text2 = \"hello\"\\n  print(\"hi\")\\n  print(__name__)\\n  print(locals())\\nprinthi()',\n  'globals()',\n  'x = 1\\ndef no_local():\\n    print(locals())\\n    print(x)\\nno_local()',\n  'class Resnet(): #클래스 함수를 정의하는 경우 def대신 class를 입력한다.\\n  temp = 25\\n  #print(locals())\\n# 이 코드를 실행하면 c는 Resnet이라는 클래스 함수의 호출을 통해 생성된 객체의 식별자가 된다.\\n# 클래스 함수는 그 반환값이 사용자가 정의한 객체가 되며 이 객체를 변수c가 가리키게 된다.',\n  'Resnet.__dict__ #클래스의 네임스페이스 확인\\n#클래스에 print(locals())로도 확인할 수 있음.',\n  'globals() #전역네임스페이스에 클래스 Resnet이 추가된 것도 확인'],\n 'Out': {2: {...},\n  3: {...},\n  5: {...},\n  8: mappingproxy({'__module__': '__main__',\n                'temp': 25,\n                '__dict__': <attribute '__dict__' of 'Resnet' objects>,\n                '__weakref__': <attribute '__weakref__' of 'Resnet' objects>,\n                '__doc__': None})},\n 'get_ipython': <bound method InteractiveShell.get_ipython of <google.colab._shell.Shell object at 0x7fe22c68e490>>,\n 'exit': <IPython.core.autocall.ZMQExitAutocall at 0x7fe229652590>,\n 'quit': <IPython.core.autocall.ZMQExitAutocall at 0x7fe229652590>,\n '_': mappingproxy({'__module__': '__main__',\n               'temp': 25,\n               '__dict__': <attribute '__dict__' of 'Resnet' objects>,\n               '__weakref__': <attribute '__weakref__' of 'Resnet' objects>,\n               '__doc__': None}),\n '__': {...},\n '___': {...},\n '_i': 'Resnet.__dict__ #클래스의 네임스페이스 확인\\n#클래스에 print(locals())로도 확인할 수 있음.',\n '_ii': 'class Resnet(): #클래스 함수를 정의하는 경우 def대신 class를 입력한다.\\n  temp = 25\\n  #print(locals())\\n# 이 코드를 실행하면 c는 Resnet이라는 클래스 함수의 호출을 통해 생성된 객체의 식별자가 된다.\\n# 클래스 함수는 그 반환값이 사용자가 정의한 객체가 되며 이 객체를 변수c가 가리키게 된다.',\n '_iii': 'x = 1\\ndef no_local():\\n    print(locals())\\n    print(x)\\nno_local()',\n '_i1': 'a=10;b=20',\n 'a': 10,\n 'b': 20,\n '_i2': '#전역네임스페이스를 보여주는 명령어\\n#전역네임스페이스에 존재하는 식별자와 객체들을 볼 수 있다.\\nglobals()',\n '_2': {...},\n '_i3': '#지역네임스페이스를 보여주는 명령어\\n#지역네임스페이스는 어떤 스코프에서의 네임스페이스를 보여준다.\\n#모듈의 지역네임스페이스는 전역네임스페이스와 같으므로 두 결과는 서로 같다.\\nlocals()',\n '_3': {...},\n '_i4': 'def printhi():\\n  text = \"hi\"\\n  text2 = \"hello\"\\n  print(\"hi\")\\n  print(__name__)\\n  print(locals())\\nprinthi()',\n 'printhi': <function __main__.printhi()>,\n '_i5': 'globals()',\n '_5': {...},\n '_i6': 'x = 1\\ndef no_local():\\n    print(locals())\\n    print(x)\\nno_local()',\n 'x': 1,\n 'no_local': <function __main__.no_local()>,\n '_i7': 'class Resnet(): #클래스 함수를 정의하는 경우 def대신 class를 입력한다.\\n  temp = 25\\n  #print(locals())\\n# 이 코드를 실행하면 c는 Resnet이라는 클래스 함수의 호출을 통해 생성된 객체의 식별자가 된다.\\n# 클래스 함수는 그 반환값이 사용자가 정의한 객체가 되며 이 객체를 변수c가 가리키게 된다.',\n 'Resnet': __main__.Resnet,\n '_i8': 'Resnet.__dict__ #클래스의 네임스페이스 확인\\n#클래스에 print(locals())로도 확인할 수 있음.',\n '_8': mappingproxy({'__module__': '__main__',\n               'temp': 25,\n               '__dict__': <attribute '__dict__' of 'Resnet' objects>,\n               '__weakref__': <attribute '__weakref__' of 'Resnet' objects>,\n               '__doc__': None}),\n '_i9': 'globals() #전역네임스페이스에 클래스 Resnet이 추가된 것도 확인'}\n\n\n\ndef로 정의하는 함수객체와는 달리 class로 정의한 클래스 함수 객체와 이 클래스의 네임스페이스에 속한 객체들을 “.”이라는 기호를 통해 접근할 수 있음.\n위와 같이 클래스안에서 25라는 객체와 바인딩된 temp를 클래스변수라하며 식별자와 객체는 별도의 클래스(라는 또하나의 다른 함수)네임스페이스에 속하게 된다."
  },
  {
    "objectID": "posts/python/python class,instance,self,namespace.html#instance",
    "href": "posts/python/python class,instance,self,namespace.html#instance",
    "title": "HIHO",
    "section": "instance",
    "text": "instance\n\n클래스라는 함수를 호출하면 객체를 반환함.\n이 반환된 객체를 인스턴스라 하며 인스턴스는 클래스가 별도로 클래스의 네임스페이스를 가진것처럼 인스턴스도 별도로 인스턴스의 네임스페이스를 가짐.\n\n\nr1 = Resnet() #클래스 호출하여 인스턴스 생성\nr2 = Resnet()\nprint(id(r1),id(r2))\n#메모리상에 다른 주소에서 저장됨을 확인\n\n140609160422160 140609160422416\n\n\n\ndir() #전역네임스페이스에 r1,r2가 추가적으로 있음을 확인\n\n['In',\n 'Out',\n 'Resnet',\n '_',\n '_2',\n '_3',\n '_5',\n '_8',\n '_9',\n '__',\n '___',\n '__builtin__',\n '__builtins__',\n '__doc__',\n '__loader__',\n '__name__',\n '__package__',\n '__spec__',\n '_dh',\n '_i',\n '_i1',\n '_i10',\n '_i11',\n '_i2',\n '_i3',\n '_i4',\n '_i5',\n '_i6',\n '_i7',\n '_i8',\n '_i9',\n '_ih',\n '_ii',\n '_iii',\n '_oh',\n 'a',\n 'b',\n 'exit',\n 'get_ipython',\n 'no_local',\n 'printhi',\n 'quit',\n 'r1',\n 'r2',\n 'x']\n\n\n\nr1.__dict__ #인스턴스의 네임스페이스 확인\n#아무것도 없음을 확인할 수 있음\n\n{}\n\n\n\nr2.__dict__\n\n{}\n\n\n클래스 함수를 호출하여 인스턴스(객체)의 네임스페이스에 새로운 식별자와 객체가 속하게 하려면 init(self)와 self.변수명 을 작성하면 됨.\n\nclass Resnet():\n  temp = 25\n  def __init__(self):\n    self.v1 = [1,2,3,4]\n    self.s1 = \"hi\"\n  #print(locals())\nr1 = Resnet()\nr2 = Resnet()\n\n\nr1.__dict__ #객체(인스턴스)의 네임스페이스에 새로운 식별자와 변수들의 매핑관계가 있음을 확인할 수 있음.\n\n{'v1': [1, 2, 3, 4], 's1': 'hi'}\n\n\n\nr2.__dict__\n\n{'v1': [1, 2, 3, 4], 's1': 'hi'}\n\n\n\n#클래스의 네임스페이스에 \".\"이라는 기호로 접근할 수 있음\nprint(Resnet.temp)\nprint(id(Resnet.temp))\n\n#인스턴스의 네임스페이스에 \".\"이라는 기호로 접근가능\nc = Resnet()\nprint(c.temp)\nprint(id(c.temp))\n\n#메모리상에서 같은 주소에 저장되어 있음도 확인\n\n25\n11127456\n25\n11127456"
  },
  {
    "objectID": "posts/python/python class,instance,self,namespace.html#lcgb-rule",
    "href": "posts/python/python class,instance,self,namespace.html#lcgb-rule",
    "title": "HIHO",
    "section": "LCGB RULE",
    "text": "LCGB RULE\n\n새로 생성된 객체의 내부 지역네임스페이스 즉 인스턴스의네임스페이스에 없는 객체를 가져오라 명령=> 클래스의 네임스페이스를 조회하고 그래도 없으면 상위 네임스페이스를 조회하게 된다.\n\n\nclass Resnet_2():\n  z=2\n  def __init__(self):\n    temp = 2\n    temp = \"hi\"\n    #print(locals())\n\n\nresnet = Resnet_2()\nprint(resnet.__dict__,resnet.z)\n#resnet이라는 인스턴스(객체)의 네임스페이스에 z라는 변수가 없음\n#근데 z라는 변수가 가리키는 객체를 가져오라고 명령 -> 클래스의 네임스페이스를 조회하고 \n#그래도 없으면 더 상위의네임스페이스를 조회\n\n{} 2\n\n\n\nclass Resnet_2():\n  z=2\n  def __init__(self):\n    temp = 2\n    temp = \"hi\"\n    self.z = 32\nresnet = Resnet_2()\nprint(resnet.__dict__,resnet.z)\n#이 경우는 인스턴스의 네임스페이스에 식별자와 객체가 있으므로 클래스네임스페이스를 조회하지 않음.\n\n{'z': 32} 32\n\n\n#self에 대한 고찰 결론 1. self는 태몽? 같은 느낌이다. 즉,클래스에 대해서 코드를 작성할때에는 만들지 않았지만 … 미래에 클래스를 호출하여 객체를 만들면 어떤 객체가 메모리상의 어떤 공간에 있을텐데 그 객체를 가리키며 객체와 바인딩되는 변수라고 생각하면 된다. 예를들어서, cookie = Cookie()이렇게 하면 cookie라는 변수는 Cookie객체와 바인딩된다. self도 이와같이 Cookie객체와 바인딩되는데 이제 클래스를 코드로 작성할때에는 미리만들어질 객체에 지칭을 해주기 위해서 이런 이름이 붙었다고 생각하면 될 것 같다. 2. “.”을 사용하여 객체와 변수사이에 .을 붙이면 즉,객체.변수 하면 객체의 네임스페이스안에 있는 또다른 객체들에 접근할 수 있다..(정확히는 객체(인스턴스)와바인딩된변수.객체(인스턴스)의 네임스페이스에 속한 변수명(식별자)라고 할 수 있다.)클래스를 작성할때 메소드안에 이 self를 적는 이유는 클래스로부터 만들어진 인스턴스의 네임스페이스안에 있는 객체에 대해서 접근하여 뭔가를 바꾸거나 조작하기 위해서이다. 3. 인스턴스.메서드()를 하면 함수에 아무인자를 넣어주지않았지만 파이썬스스로 객체(인스턴스)그 자체를 첫번째 인자로 전달해준다.이는 클래스.메소드(인스턴스)라는 코드와 완전히 동일하다. 4. 만약에 인스턴스(객체)의 메소드를 실행시키지 않고 클래스의 자체에서 메소드를 실행시키는 경우,self는 필요가 없다.\n핵심요약 1. self는 객체 또는 인스턴스 그 자체를 가리키는 변수. 2. 객체(인스턴스)의 네임스페이스에 속한 또다른 객체들은 객체(인스턴스)와 바인딩된 변수.객체와 바인딩된변수라는 방식으로 접근 가능.간단히 인스턴스.멤버변수로 접근가능 3. 1,2에 의하여, self라는 객체와 바인딩된 변수를 메소드의 인자로 받으면 self.멤버변수 이런식으로 인스턴스의 변수에 접근 가능함.\n인스턴스의 가용범위 : https://andamiro25.tistory.com/38\n\nclass cookie():\n  given = \"cookie\" #클래스변수 cookie선언 \n  maked_num = 0 #클래스변수 선언,모든 인스턴스에 대해서 공통된 연산이 되어야 할 때 할당.\n  def __init__(self,flavor,size):\n    self.flavor = flavor\n    self.size = size\n    print(\"메모리 주소 :\",id(self))\n  def sizeup(self): #self를 왜쓸까? => self는 객체(인스턴스)그 자체이므로 self.~~로 네임스페이스에 있는 객체에 접근 가능함.인스턴스의 네임스페이스에 있는 객체에 뭔가를 추가하거나 조작하거나 해야하는 메서드는 self를 사용함\n    self.size +=5\ncookie1 = cookie(\"banana\",25)\ncookie2 = cookie(\"melon\",10)\nprint(\"쿠키1\",id(cookie1))\nprint(\"쿠키2\",id(cookie2))\n#self라는 객체의 메모리주소를 가리켜보니 변수cookie1,cookie2가 가리키는 객체의 메모리주소와 같다\n#즉,self는 객체(인스턴스) 그 자체를 가리킨다!\n\n메모리 주소 : 140609086555024\n메모리 주소 : 140609086554896\n쿠키1 140609086555024\n쿠키2 140609086554896\n\n\n\nprint(cookie1.maked_num,cookie1.flavor,cookie1.size)\nprint(cookie2.maked_num,cookie2.flavor,cookie2.size)\n\n0 banana 25\n0 melon 10\n\n\n\ncookie1.__dict__,cookie2.__dict__ #cookie1,cookie2라는 객체(인스턴스)의 네임스페이스 확인\n#서로다른 독립적인 네임스페이스를 가지는 것을 확인\n\n({'flavor': 'banana', 'size': 25}, {'flavor': 'melon', 'size': 10})\n\n\n\ncookie.sizeup(cookie1);print(cookie1.size)\n#==cookie1.sizeup()\n\n30\n\n\n\nclass cookie_inform():\n  #클래스 변수 선언\n  making_time = \"1345\"\n  price =2000\n  maked_num = 0\n  def making_up(maked_num):\n    maked_num+=1\ncookie_inform.making_up(cookie_inform.maked_num)\ncookie_inform.maked_num\n\n0\n\n\n\ncookie1.__dict__\n\n{'flavor': 'banana', 'size': 25, 'd': 'hi'}"
  },
  {
    "objectID": "posts/python data structure and algorithm/CP1/CP1-1 Programming and Execution Environment.html",
    "href": "posts/python data structure and algorithm/CP1/CP1-1 Programming and Execution Environment.html",
    "title": "[Algorithm & Datastructure]1-1.Programming and Execution Environment",
    "section": "",
    "text": "Programming and DS&A\n\n프로그레밍,Data structure(DS), Algorithm(A)은 서로다른 계층에 위치하지만 상당한 연관성을 가진다.\n건축가는 건물을 짓기 위해 구조,동작을 감안한 설계도를 만들고 도구,장비를 사용하여 구현,현실화 한다.\n개발자의 입장에서 구현은 프로그래밍이며 설계는 UML이라는 표준적인 형식을 사용하지만 그 안에서 자료구조와 알고리즘이 특히 중요하다.\n설계,구현 둘 다 중요하다.\n\n\n\nPython\n\n인터프리터 언어\n객체지향\nDynamic type of variables\n\n파이썬은 Dynamic Type Language로 런타임에서 변수의 type이 결정된다. 쉽게 말하면 변수의 type을 미리 정해주지 않고 실행될때마다 그때그때 타입이 dynamic하게 변한다.\n자바는 Static Type Language로 컴파일타임에서 변수의 type이 결정된다. 쉽게 변수의 타입을 미리 지정해줘야 한다.\n인터프리터 언어의 특징 1\n\n빠른 개발 속도,느린 실행 속도(인터프리터언어의 특징 2)\n독특한 코드 구조를 지닌다.(좋은 방향으로)\n데이터 분석에 특화되어 있다."
  },
  {
    "objectID": "posts/python data structure and algorithm/CP1/CP1-2 Procedure-oriented program.html",
    "href": "posts/python data structure and algorithm/CP1/CP1-2 Procedure-oriented program.html",
    "title": "[Algorithm & Datastructure]1-2.Procedure-oriented program vs Object-oriented program",
    "section": "",
    "text": "Procedure-oriented program\n\n절차지향프로그램이라 하며 함수를 기반으로 많은 부분이 구현된다.\n\n메인함수가 존재한다.\n\n크게 두 가지 부분으로 나누어 진다.\n\nDefinition part(정의,선언)\nExecution part(실행)\n\n\n\n# Definition part(정의해봐)\ndef main(): \n    print(\"Hello, world\")\n    print(\"This program computes the average of two number\")\n\n    n1 = int(input(\"Input first number\"))\n    n2 = int(input(\"Second number\"))\n    average = (n1+n2) / 2\n    print(\"The average of the scores is :\",average)\n\n# Execution part(실행해봐)\nmain()\n\nHello, world\nThis program computes the average of two number\nThe average of the scores is : 3.0\n\n\n\ndef이라는 keyword를 함수를 정의할때 사용한다.\n함수내부의 block은 :로 시작하며 indentation(들여쓰기)을 사용하여 다른 block과 구분한다.\n\nprint(“hello, world”) ~ print(“The average of ….까지가 main에 속하는 하나의 블록이된다.\nmain()부터는 다른 블록이다.\n따라서 위의 코드에서는 총2개의 블록이 존재한다.\n\ndefinition은 이런게 있다~라고 컴퓨터에게 알려주는 역할이다.\nexecution을 위해 main()을 입력해줘야 한다. 입력해주면 함수가 실행되면서 프로그램이 돌아간다.\n\n\n\nObject-oriented program\n\n객체지향프로그램이라 하며 함수외에도 클래스,인스턴스(객체)기반으로 많은 부분이 구현된다.\n\n메인함수가 존재한다.\n\n크게 두 가지 부분으로 나누어 진다.\n\nDefinition part(정의,선언)\nExecution part(실행)\n\n\n\nclass Helloworld:\n    def __init__(self):\n        print(\"Hello World! Just one more time\")\n    def __del__(self):\n        print(\"Good bye!\")\n    def performAverage(self,val1,val2):\n        average = (val1 + val2) / 2.0\n        print(\"The average of the score is :\",average)\ndef main():\n    world = Helloworld()\n    score1 = int(input(\"Input first score\"))\n    score2 = int(input(\"Input second score\"))\n    world.performAverage(score1,score2)\nmain()\n\nHello World! Just one more time\nThe average of the score is : 6.0\nGood bye!\n\n\n\n객체지향에서는 기본적으로 클래스를 사용한다.\nclass라는 keyword는 클래스를 정의하기 위해서 사용한다.\n함수와 마찬가지로 클래스내부의 블록은 :로 시작하며 들여쓰기를 통해서 클래스의 멤버변수(attribute),멤버함수(method)를 정의한다.(함수에서는 실행할 구문을 block에 적었다.)\nHellworld()를 호출하여 instane를 생성하고 world에 저장한다.\n\nHelloworld()는 클래스를 호출하여 instance를 생성하는 것이다.이는 빵틀을 사용하여 실제로 빵을 만들어내는 것이라 할 수 있다.(실체가 있는 뭔가를 만든다.)\ninstance는 메모리상의 어떤 위치에 저장된다. world는 instance를 저장하고,가리키고 있는 variable이다.(포인터 느낌이다.)\n\n마찬가지로 main()을 통해서 실행하는 Execution part가 존재한다.\n이외 클래스에 대한 self,__init__,__del__에 대한 설명은 차후 강의에서 들어보자~\n\n\n\n정리\n\nProcedure-oriented program은 function위주로 구현되며 Object-oriented program은 이와 더불어 class,instance와 함께 사용한다.\nclass는 instance를 실체화 할 수 있는 템플릿이다. 즉, 객체를 생성한다.\n\n표현이 헷갈리는데 class는 instance와 짝으로 객체는 단독으로 올 때가 많다.\nex) class의 instance를 생성한다. 객체를 생성한다.\n근데 그냥 막 혼용해서 쓴다.\nex) 인스턴스가 생성되었다.\n\nObject oriented program은 객체를 먼저 생성하고 이를 기반으로 구현한다."
  },
  {
    "objectID": "posts/python data structure and algorithm/CP1/CP1-3 Naming, Styling and Comments.html",
    "href": "posts/python data structure and algorithm/CP1/CP1-3 Naming, Styling and Comments.html",
    "title": "[Algorithm & Datastructure]1-2. Naming, Styling and Comments",
    "section": "",
    "text": "Naming and Styling\n\nNaming: 의미를 확실하게 전달해야 함\n\ncamel casing을 사용함.\n\nclass name : 클래스로 표현되는 개념에 대한 명사로\n\n각 단어의 첫 글자는 대문자로\ne.g class MyFirstClass\n\nvariable name : 저장되는 개념에 대한 명사로\n\n소문자로 시작\ne.g numberOfStudents = 100\nintCount와 같은 이름은 사용하지 않음(파이썬은 동적 변수 자료형을 지원하기 때문)\n\nmethod name : 메소드가 하는 행동에 대한 동사로\n\n소문자로 시작\ne.g def performAverage(self,val1,val2)\n\n\n\nIndentation(들여쓰기)\n\n4칸의 공백을 각 level에 사용\n\n\n\n\nComments\n\n코드에 대한 설명을 달기\n여러줄은 ``` or “““을 사용\n\n'''\ncreated on 2023-04-24\n\nauthor : hoyeon\n'''\n\n\"\"\"\ncreated on 2023-04-24\n\nauthor : hoyeon\n\"\"\"\n한 줄은 #을 사용\n# print문 출력하기\nprint(\"hello\")"
  },
  {
    "objectID": "posts/python data structure and algorithm/CP2/CP2.html",
    "href": "posts/python data structure and algorithm/CP2/CP2.html",
    "title": "Untitled",
    "section": "",
    "text": "일반적으로 abstraction(추상화)라는 용어는 대상의 느낌,형태와 같은 내재되어 있는 모습에 대한 간단한 표현.\nAbstract Data type(ADT)이란?\n\nData structure가 어떻게 돌아가는지에 대해 간단히,추상적으로 표현한 것.\nADT는 다음과 같은 것들을 기술함\n\n어떤 데이터가 저장되는가?(어떤 데이터가 있어?)\n저장된 데이터는 어떤 연산이 가능한 것인가?(데이터로 뭘 계산할 수 있어?)\n연산에서 나올 수 있는 에러조건은 어떻게 되는가?(계산에서 어떤 오류가 나올 수 있어?)\n\nStock trading system\n\nbuy,sell에 대한 order가 저장된다.\n지원되는 연산들\n\norder buy(stock,shares,price)\norder sell(storck,shares,price)\nvoid cancel(order)\n\nError conditions:\n\n존재하지 않는 stock에 대한 buy,sell\n존재하지 않는 order를 취소"
  },
  {
    "objectID": "posts/python data structure and algorithm/CP2/CP2.html#search",
    "href": "posts/python data structure and algorithm/CP2/CP2.html#search",
    "title": "Untitled",
    "section": "Search",
    "text": "Search\nArray x에서 d와 c를 찾는다고 해보자.\n\n대략적인 과정은 다음과 같을 것이다.\n\n처음부터 끝에 있는 모든 요소를 하나하나씩 접근한다.\nd or c를 만나면 종료한다.\n\nd or c가 포함되어있지 않다면? \\(\\to\\) N(리스트의 길이)번의 retrievals가 필요\nd or c가 포함되어 있다면? \\(\\to\\) 최대 N번의 retrievals가 필요\n\n\n\nretrieval은 연산(operation)으로 일단 이해하자.(자세하게 뭔지는 모르겠다.)\n대략 몇 번정도의 연산이 일어나는지를 의미한다.\n\n\n# c\nx = ['a','b','d','e','f']\n\nfor itr in range(0,len(x)):\n    if \"c\" == x[itr]:\n        print(True)\n        break\n    else:\n        pass\n\n\n# d\nx = ['a','b','d','e','f']\n\nfor itr in range(0,len(x)):\n    if \"d\" == x[itr]:\n        print(True)\n        break\n    else:\n        pass\n\nTrue"
  },
  {
    "objectID": "posts/python data structure and algorithm/CP2/CP2.html#insert",
    "href": "posts/python data structure and algorithm/CP2/CP2.html#insert",
    "title": "Untitled",
    "section": "Insert",
    "text": "Insert\nArray x의 원소인 b와 d사이에 c를 넣으려면 어떻게 해야할까?\n\n대략적인 과정은?\n\n새로운 리스트 y를 만든다.\n\n기존리스트보다 길이가 1만큼 더 길다\n\nx[0:a-1]까지 y[0:a-1]에 있는 reference를 복사한다. (retrieval cnt : a)\n\na는 b와 d사이의 index를 의미한다.\n0:a-1까지 y가 x의 reference를 복사했으므로 같은 object를 가리키고 있다.\n리스트는 객체에 대한 reference를 가리키고 있는 구조임을 유의하자.\n\nc의 레퍼런스를 y[a]에 넣는다. (retrieval cnt : 1)\n나머지 원소 d,e,f에 대한 래퍼런스를 복사한다. 즉 x[a:]를 y[a+1:]에 복사한다.\n마지막에 x의 reference를 y의 reference로 바꾼다.(우리는 x에 넣는 것을 원했다.)\n\n\n총 n번의 retrievals가 필요하다."
  },
  {
    "objectID": "posts/RL/A simple bandit algorithm.html",
    "href": "posts/RL/A simple bandit algorithm.html",
    "title": "[강화학습] Simple Bandit Algorithm Implementation",
    "section": "",
    "text": "Import\n\nimport numpy as np\nfrom scipy.stats import bernoulli,norm\nimport random\n\n\n\n Implementation\n\ndef argmax(q_func):\n    q_max = max(q_func.values())\n    max_actions = []\n    for action in q_func.keys():\n        if q_func[action] == q_max:\n            max_actions.append(action)\n    argmax_q = random.sample(max_actions,1)\n    return argmax_q[0]\n\n\ndef bandit(q_star):\n    #각 bandit에 해당하는 정규분포에서 샘플하나 가져오기\n    sampled_reward = norm.rvs(loc=q_star,scale=1,size=1)\n    return sampled_reward[0]\n\n\ndef simple_bandit(num_actions, #action의 갯수\n                  epsilon,     #greedy하게 안 움직일 확률\n                  q_stars, #각 action(k개의 bandit)에 대한 reward의 (정규)분포의 mean\n                  terminate_cond\n                  ):\n    if num_actions != len(q_stars):\n        print(\"action의 갯수와 bandit_mean의 갯수는 같아야 함\")\n        return \n    Q = {}\n    n = {}\n\n    #Initialize,for a=1 to k\n    for i in range(1,num_actions+1):\n        Q[i] = 0\n        n[i] = 0\n    \n    #set probability,reward distribution(bandit)\n    max_prob = 1-epsilon\n\n    #epsilon-greedy action\n    while True:\n\n        #epsilon-greedy action\n        #1.greedy action? or random action?\n        greedy_or_random = bernoulli.rvs(p=max_prob,size=1)\n        #2.select action\n        if greedy_or_random == 1: \n            action = argmax(Q)\n        else:\n            action = random.sample(Q.keys(),1)[0]\n\n        #sampling reward from gaussian(mean = q_star,variance = 1)\n        q_star = q_stars[action-1]\n        reward = bandit(q_star) #sampling\n\n        #incremental Q-update\n        n[action] +=1\n        Q[action] = Q[action] + (1/n[action]) * (reward - Q[action])\n\n        #Terminating\n        if np.max(q_stars - np.array(list(Q.values()))) <= terminate_cond:\n            break\n    return n,Q\n\n\nnum_actions=10\nq_stars = norm.rvs(loc=0,scale=1,size=num_actions)\nnum_action,Q_estimated = simple_bandit(num_actions=num_actions,epsilon=0.1,q_stars=q_stars,terminate_cond=0.01)\nfor i in range(1,num_actions+1):\n    print(\"action :\",i)    \n    print(\"taken_num\",num_action[i])\n    print(\"q_star :\",q_stars[i-1])\n    print(\"Q_estimated\",Q_estimated[i]) \n    print(\"=============================================\")\n\nC:\\Users\\22668\\AppData\\Local\\Temp\\ipykernel_5296\\2459308436.py:30: DeprecationWarning: Sampling from a set deprecated\nsince Python 3.9 and will be removed in a subsequent version.\n  action = random.sample(Q.keys(),1)[0]\n\n\naction : 1\ntaken_num 7710\nq_star : 1.0622402967553395\nQ_estimated 1.0780006731009761\n=============================================\naction : 2\ntaken_num 7750\nq_star : -0.04393405775378968\nQ_estimated -0.0464016778867334\n=============================================\naction : 3\ntaken_num 7688\nq_star : -0.2977471187595547\nQ_estimated -0.28941229886239545\n=============================================\naction : 4\ntaken_num 7766\nq_star : -0.6942140347822109\nQ_estimated -0.6957038264269383\n=============================================\naction : 5\ntaken_num 702767\nq_star : 1.114864011478195\nQ_estimated 1.1163009140188438\n=============================================\naction : 6\ntaken_num 7695\nq_star : -2.0791842661153535\nQ_estimated -2.0891168451782134\n=============================================\naction : 7\ntaken_num 7704\nq_star : -1.204190113360393\nQ_estimated -1.2086679112146015\n=============================================\naction : 8\ntaken_num 7754\nq_star : 0.9093721477182354\nQ_estimated 0.908951189305117\n=============================================\naction : 9\ntaken_num 7658\nq_star : 0.0790709385366291\nQ_estimated 0.08704136534792037\n=============================================\naction : 10\ntaken_num 7736\nq_star : -0.06424396205536466\nQ_estimated -0.05355515687875377\n============================================="
  },
  {
    "objectID": "posts/RL/Bellman Equation(번외).html",
    "href": "posts/RL/Bellman Equation(번외).html",
    "title": "Bellman Equation",
    "section": "",
    "text": "\\[\\begin{aligned}\nv_{\\pi}(s) &= \\mathbb{E}[G_t|S_t = s]\\\\\n&= \\sum_{g_t}p(g_t|s)g_t \\\\\n&= \\sum_{g_t}\\sum_{a}p(g_t,a|s)g_t\\\\\n&= \\sum_{g_t}\\sum_{a}p(g_t|s,a)p(a|s)g_t\\\\\n&= \\sum_{a}p(a|s)\\sum_{g_t}p(g_t|s,a)g_t \\\\\n&= \\sum_{a}Pr(A_t=a|S_t=s)\\mathbb{E}[G_t|S_t=s,A_t=a] \\\\\n&= \\sum_{a}\\pi(a|s)q_{\\pi}(s,a)\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/RL/Bellman Equation(번외).html#optimal-state-value-function",
    "href": "posts/RL/Bellman Equation(번외).html#optimal-state-value-function",
    "title": "Bellman Equation",
    "section": "Optimal state value function",
    "text": "Optimal state value function\n\\[\\begin{aligned}\nv_{*}(s) = \\underset{\\pi}{\\text{max}}\\,v_{\\pi}(s) , \\forall s \\in S\n\\end{aligned}\\]\n각각의 모든 state에서 모든 policy를 고려했을때 state-value function의 max값을 함숫값으로 가지는 함수이다. 이때 함숫값(max값)은 optimal policy에 의한 값이다."
  },
  {
    "objectID": "posts/RL/Bellman Equation(번외).html#optimal-state-value-function-1",
    "href": "posts/RL/Bellman Equation(번외).html#optimal-state-value-function-1",
    "title": "Bellman Equation",
    "section": "Optimal state value function",
    "text": "Optimal state value function\n\\[\\begin{aligned}\nq_{*}(s,a) = \\underset{\\pi}{\\text{max}}\\,q_{\\pi}(s,a),\\, \\forall s \\in S,\\forall a \\in A(s)\n\\end{aligned}\\]\n각각의 모든 state-action pair에서 모든 policy를 고려했을때 action-value function의 max값을 함숫값으로 가지는 함수. 이때 함숫값(max값)은 마찬가지로 optimal policy에 의한 값이다."
  },
  {
    "objectID": "posts/RL/Bellman Equation(번외).html#optimal-policy",
    "href": "posts/RL/Bellman Equation(번외).html#optimal-policy",
    "title": "Bellman Equation",
    "section": "Optimal policy",
    "text": "Optimal policy\n\nIf \\(v_{\\pi}(s) \\geq v_{\\pi'}(s)\\) for all \\(s \\in S\\) then we say \\(\\pi\\) is better than or equal to \\(\\pi'\\).\nThere is always at least one policy that is better than or equal to all othere policies => 다른 모든 정책들과 비교했을때 모든 state에서 value 같거나 더 나은 policy는 최소 한 개 이상 존재한다. 이러한 policy들을 모두 optimal policy라고 하며 \\(\\pi_*\\) 로 표기한다.\noptimal policy는 여러개일 수 있다.\noptimal policy들은 모두 동일한 optimal state value function과 optimal action value function을 공유한다.\n\n\\[\\begin{aligned}\np(a|s) =\n\\begin{cases}\n1,\\quad a = \\text{argmax}_{a\\in A(s)}q_*(s,a) \\\\\n0,\\quad\\text{else}\n\\end{cases}\n\\end{aligned}\\]\noptimal policy \\(\\pi_*\\)는 state-value function이 모든 s에서 다른 모든 policy들 보다 높거나 같은 값을 가지는 함수이다. 즉,다음과 같다.\nstate-value function은 action-value function으로 표현하는 Bellman equation에 의해 아래처럼 표현할 수 있었다.\n\\[\\begin{aligned}\nv_{\\pi}(s) &= \\sum_{a}\\pi(a|s)q_{\\pi}(s,a)\\\\\n&= \\sum_{a}p(a|s)q_{\\pi}(s,a)\n\\end{aligned}\\]\n\\(v_{\\pi}(s)\\)를 maximize하는 policy는 어떻게 구해야 할까? 먼저 optimal action value function을 구했다고 가정해보자.(실제로 나중에 이를 추정하는 방법이 나온다.)\noptimal action value function을 구했다는 것은 뭘까? optimal action value function은 agent가 어떤 state,action pair를 선택했을때 여러가지 policy를 다 고려해봐서 return(미래에 받을 reward의 총합)의 최댓값을 돌려준다. 즉, agent가 state에서 action을 선택했을 때 가장 많이 받을 수 있는 return이다.\n예시를 들어보자. 임의의 state \\(s\\)에서 가능한 action의 집합을 \\(A(s)= \\{a_1,a_2,a_3,a_4\\}\\)이고 optimal action value function의 값이 다음과 같다고 하자.\n\\[\\begin{aligned}\n&q_{*}(s,a_1) = 4 \\\\\n&q_{*}(s,a_2) = -2 \\\\\n&q_{*}(s,a_3) = 2 \\\\\n&q_{*}(s,a_4) = 9 \\\\\n\\end{aligned}\\]\na1이라는 행동을 하면 가장 많이 return을 받아봤자 4이고 a2행동을 하면 가장 많이 받아봤자 return이 -2로 더 작을것이다. a4를 선택했을때에는 가장많이 받으면 return이 9이므로 a4를 s4에서의 action으로 취하면 가장 합리적일 것이다.\n\\(v_{\\pi}(s)\\)를 maximize하는 policy는 어떻게 구해야 할까? 먼저 state value function자체가 action value function부터 maximize가 되어 있어야 먼저 optimal action value function을 구했다고 가정해보자.(실제로 나중에 이를 추정하는 방법이 나온다.)\n모든 \\(\\pi\\)를 고려했을 때 위의 state-value function을 maximize하는 policy가 optimal policy였다. 다시쓰면 아래와 같다.\n\\[\\pi_* = \\underset{\\pi}{\\text{argmax}}\\,v_{\\pi}(s)\\,,\\forall s\\]"
  },
  {
    "objectID": "posts/RL/Chatper1 - Introduction/Introduction.html",
    "href": "posts/RL/Chatper1 - Introduction/Introduction.html",
    "title": "[강화학습] Introduction",
    "section": "",
    "text": "우리는 환경과 상호작용(interacting)하면서 배운다는 것은 우리가 학습의 본질에 대해 생각할때 가장 먼저 떠올릴 수 있는 생각이다.\n이 책에서는 상호작용으로부터 배우기 위한 computational approach를 배운다\n우리가 배울 접근방식 즉, reinforcement learning은 기계학습의 다른 접근법들보다 훨씬 목표지향적 학습에 중점을 둔다."
  },
  {
    "objectID": "posts/RL/Chatper1 - Introduction/Introduction.html#강화학습이란",
    "href": "posts/RL/Chatper1 - Introduction/Introduction.html#강화학습이란",
    "title": "[강화학습] Introduction",
    "section": " 강화학습이란?",
    "text": "강화학습이란?\n\n강화학습은 어떤 situation에서 어떻게 action을 대응시킬지를 배우는 학습이다.\nlearner에게 어떤 행동을 취해야할지 알려주지 않으며 따라서 learner는 어떤 action이 가장 많은 reward를 주는지 여러번 시도하면서 발견해야 합니다.\n대부분의 문제에서 action은 즉각적으로 받는 reward 뿐만 아니라 다음 situation과 뒤따르는 모든 후속 reward에 영향을 미칠 수 있습니다.\n이와 같은 강화학습의 두 가지 특성 trial and error search 그리고 delayed reward는 강화학습의 가장 중요하면서 차별적인 두 가지 특징입니다."
  },
  {
    "objectID": "posts/RL/Chatper1 - Introduction/Introduction.html#강화학습-vs-지도학습",
    "href": "posts/RL/Chatper1 - Introduction/Introduction.html#강화학습-vs-지도학습",
    "title": "[강화학습] Introduction",
    "section": " 강화학습 vs 지도학습",
    "text": "강화학습 vs 지도학습\n\n지도학습은 labeling이 되어 있는 example(또는 observation)또는 training set으로부터 학습합니다.\n각 example은 situation과 그때의 situation에 대해 시스템이 취해야 하는 올바른 행동(label)에 대한 label입니다.\n지도학습의 목적은 training set에 없었던 상황에서 올바르게 동작하도록 extrapolate(추론)하거나 generalization(일반화)하는 것입니다.\n상호작용하는 문제에서 agent에게 바라거나,해야하는 행동을 정확히 대표하는 예시를 얻는 다는 것은 비현실적입니다. (쉽게 생각해보자면 말하자면 정확히 어떤 상황에서 어떤 행동을 하라고 정확하게 알려주는 것은 불가능하다. 아마도 상황이 수없이 많을 뿐더러 어떤 행동이 최적의 선택인지도 모르기 때문인 것 같다.)\n미지의 영역(학습이 가장 도움이 될 것으로 예상되는)에서 에이전트는 자신의 경험으로 부터 스스로 배울 수 있어야 합니다."
  },
  {
    "objectID": "posts/RL/Chatper1 - Introduction/Introduction.html#강화학습-vs-비지도학습",
    "href": "posts/RL/Chatper1 - Introduction/Introduction.html#강화학습-vs-비지도학습",
    "title": "[강화학습] Introduction",
    "section": " 강화학습 vs 비지도학습",
    "text": "강화학습 vs 비지도학습\n\n비지도학습은 unlabeled data에 대한 hidden structure를 찾는 것이 목적입니다.\n강화학습이 correct behavior에 대한 labeled data를 가지고 있지 않기 때문에 누군가는 비지도 학습으로 착각하기도 하지만 강화학습의 목적은 reward signal을 최대화 하는 것이 목적이며 비지도학습 처럼 hidden structure를 찾는 것이 목적이 아닙니다.\n물론 hidden structure를 찾는 것이 강화학습에 유용할 수는 있지만 reward signal을 최대화하는 것이 목적이라는 강화학습의 문제를 설명하지 못합니다."
  },
  {
    "objectID": "posts/RL/Chatper1 - Introduction/Introduction.html#exploration-vs-exploitation",
    "href": "posts/RL/Chatper1 - Introduction/Introduction.html#exploration-vs-exploitation",
    "title": "[강화학습] Introduction",
    "section": " exploration vs exploitation",
    "text": "exploration vs exploitation\n\n강화학습에만 존재하는 한가지 문제는 exploration과 exploitation사이의 trade-off입니다.\nexploration,exploitation은 의사결정문제에서 가능한 두가지 행동방식으로 서로간에 장단점을 가집니다.\nexploitation\n\n더 많은 reward를 얻기 위해 지금까지의 데이터를 통해 optimal하다고 여겨지는 decision을 선택하는 것입니다.(알려진 것을 계속 연구하는 것)\n리워드를 최대화하기위해 알려진 정보를 사용하여 행동하는 것입니다.\n\nexploration\n\n지금까지 데이터를 통해 optimal하다고 여겨지는 decision을 선택하지 않는 것 입니다.\nenvironment에 대해서 더 많은 정보를 찾기위해 랜덤하게 행동하는 것입니다.\n이는 관측된 데이터가 best option을 결정하기에 충분하지 않다는 사실을 가정합니다.(알려진 것을 거부하고 새롭게 탐험하는 것)."
  },
  {
    "objectID": "posts/RL/RL 1.html",
    "href": "posts/RL/RL 1.html",
    "title": "[강화학습] 1 - 강화학습 용어정리",
    "section": "",
    "text": "강화학습 독학하면서 익숙하지 않거나 모르는 용어들을 정리해놓은 페이지입니다.개선할 부분이 있다면 댓글로 알려주세요..!\n\n 용어정리\n\nagent : 강화학습에서 학습하는 대상(object)를 말한다.\naction : agent가 하는 실제 행동을 말한다. action을 취하면 state가 변화한다.\nstate : agent가 인식하고 있는 자신의 상태이며 action을 취하기 위한 구체적 정보이다.어떠한 action을 취하면 (environment에 의해)변화한다.\nobservation : 실제문제에서 agent는 모든 state를 알기는 불가능하다. agent는 state중 일부정보만을 받는데 이것을 observation이라고 한다.\nreward : agent가 action을 취했을때 받는 보수,보상,값이다. 어떠한 action을 유도하기 위해서는 + 보상을 action을 방지하기 위해서는 -보상을 준다.\nEnvironment  – 1)agent가 놓여있는 world(세계)이다. – 2)agent가 어떤 action을 취하면 state가 변화하는데 이 때 새로운 state를 return해주는 존재를 말한다. – 3)agent가 어떤 행동을 했을때 어떤 보상을 줘야하는지에 대한 setting이다. – 뭔가 직관적으로 이해가 가지 않는다.예시를 들어보자면 말은 안되지만 아주더운 나가기면 하면 땀나는 아프리카 사막에서 1시간 달리는 것과 시원한 헬스장에서 1시간 달리는 것을 비교해보자. 사막에서 달리는게 차원이 다른 힘듦과 체중감량 효과를 줄 것이다. 이는 같은 action을 취해도 서로다른 envirionment가 다른state를 return(제공)해주기에 그렇다. 각각의 환경은 또한 감량에 대하여 서로다른 setting에 의해 reward를 줄것이다.\npolicy  – agent가 어떻게 행동을 해야할지 알려주는 방향,가이드,지표의 느낌이다. 정책라는 의미가 너무 와닿지 않아서 네이버에 검색해보니 개인의 앞으로 나아갈 노선이나 취해야 할 방침라고 한다. (지표의 그냥 어려운 말) – policy는 크게 deterministic policy와 stochastic policy로 나뉜다고 한다. 내가 이해하는대로 쉽게 말해보자면 상황에 따라서 가능한 행동이 단 한가지라면 deterministic(정해져 있는,결정론적인) policy가 있는 것이고 하고 가능한 행동이 여러가지라면 stochastic(안정해져 있는,확률론적인) policy가 있는 것이다.\nreturn  agent는 당장에 받는 reward뿐만 아니라 미래에 받는 reward도 고려해야 한다. return은 현재의 state로부터 미래의 action,state를 전부 고려했을때 내가 미래에 받게 될 discounted reward의 총합,누적합이다. 여기서 discounted가 붙는 이유는 미래에 받게되는 reward에 대해서는 어느정도 discount가 들어가기 때문이다. discount의 양은 discount factor(\\(\\gamma\\))에 의해 결정되는데 이값을 1에 가깝게 할수록 미래에 받는 action 대한 reward의 discount가 커지고 0에 가깝게 할수록 미래의 action에 대한 reward의 discount가 작아진다. 강화학습의 최종 목적은 평균적으로 받는 return이 가장 커지도록(return도 랜덤이기 때문에 평균적으로 커져야 한다.)policy를 학습하는 것이다.\nexploration 현재 agent가 가지고 있는 사전지식,경험을 버리고 틀을깨어 새로운 방식으로 action하여 보는 것을 의미한다. 랜덤한 확률(\\(\\epsilon\\))로 policy를 거슬러서 다른 모든 액션중에 랜덤한 하나의 action을 취하게 된다. locacl optima에서 벗어나 최적의 policy를 탐색할 더 많은 기회를 제공한다.\nexploitation 현재 agent가 가지고 있는 사전지식,경험을 최대한 활용하여 action을 취하는 것을 말한다.\nexploration exploitation trade off 너무 exploitation만 할 경우 사전지식만을 가지고 action하여 더 좋은 policy를 찾지 못하고 너무 exploration 위주인 경우 사전지식은 활용하지 않게되고 학습이 더디게 진행된다.\n\n비유적으로 생각해보자.만약에 내가 엄청 살이찐 상태(state)에서 나 자신을 강화학습?한다고 하자. 2달동안 살을 빼는 것을 목적으로 하고 BMI수치가 좋아지면 reward를 받는다고 하자.나(agent)는 여러가지 운동(action)을 해볼 수 있다. 처음에는 조금 힘드니까 쉬운 운동(걷기?숨쉬기..)정도 하지만 이렇게 하면 나는 절대로 몸무게가 빠지지 않거나 오히려 더 찌는 상황(state)이 발생할 수 있다.(reward가 0이거나 오히려 -reward)\n이렇게 될 경우 해야할 운동의 방향(policy)에 대해 다시 생각해본다. 생각해봤더니 걷기,달리기,근력운동,굶기 같은 더 좋은 방법,운동들이 있었고 이번에는 그러한 방법들을 실천해본다. 이러한 경우들 모두 BMI수치가 개선되어 더 좋아졌는데 그런데 굶기의 경우 당장에 가장 많이 BMI수치가 개선되어 앞으로도 이 방법만 선택한다고 해보자. BMI수치는 근육량도 고려하기 때문에 장기적으로 볼때 굶기만 한다면 몸무게가 빠지긴 하는데 결국에는 근육만 다 빠져서 결국에는 몸이 안좋아질 것이다.(reward의 합인 return을 고려하는 이유!) 그러므로 굶는 방법은 어느정도 자제하고 걷기,뛰기,근력운동하기 위주로 내가 운동해야할 방향이 결정된다.\n\n\nexploration exploitation tradeoff\n살을 빼고 있는데 어느날 갑자기 친구가 내가 정한 운동시간 말고 같이 운동하자고 했다. 근데 나는 여태까지 밤에 산책하는게 좋았기에 (밤에 산책하기가) 거절한다고 하면 이건 exploitation 위주로 학습을 하는 것이다. 이와는 다르게 낮에 친구의 제안을 받아들여 같이 헬스장으로 운동하러 가는거면 이는 exploration위주로 학습을 해봤다고 할 수 있다. 중요한 점은 이 둘 사이에는 tradeoff가 있다는 것이다. 너무 exploitation만 하면 더 좋은 방법(친구가 잘 맞으면 최종적으로는 더 운동을 많이하게 되어 BMI가 낮아질 것이다.)을 놓치게 되고 너무 exploration만 할 경우는(여러명의 친구에게 제안이 왔다면 그 친구중에는 운동은 별로안하는데 끝나고 같이치킨먹는 좋은?친구도 있다.)학습을 더디게 한다. 이러한 tradeoff가 있어서 결국에 2달동안 최대한 살을 많이 빼려는 궁극적인 목적을 이루려면 이 둘 사이의 tradeoff를 잘 조절하는 것이 중요한 요소이다.\n롤로 비유하는 것도 가능하다. 최종적으로 새롭게 시작되는 2023시즌에 챌린저를 가는게 목표라고 하자. 하던챔만 계속하는 것이 exploitation이고 다른챔도 해보는게 exploitation이다. 하던챔만 하면 나는 모르지만 내가 재능을 가진 챔피언을 영원히 모를수도 있고 재능을 가진 챔피언을 찾고자 새로운 챔피언만 계속하다보면 경험치가 잘 쌓이지 않고 학습이 더디게 된다. 여기서도 마찬가지로 2023 - 챌린저라는 목표를 이루기 위해서는 둘 사이의 tradeoff를 적절히 조절하는게 중요하다.\n\n\n 참고자료\n나무위키 - 강화학습용어정리\nFundamental of Reinforcement Learning - 이웅원님 깃북\nMLI\n내가 보려는 기술 블로그"
  },
  {
    "objectID": "posts/RL/RL 2 - MDP.html",
    "href": "posts/RL/RL 2 - MDP.html",
    "title": "[강화학습] 2-1 Markov Decision process and property",
    "section": "",
    "text": "Markov Decision Process\n자기전에 스타를 했더니 상대방에게 핵을 맞았다고 하자…\n\n나는 왜 핵을 맞았을까? 게임안에서 주어지는 여러가지 state에서 action을 취했을 때 가능한 여러가지 결과 중 하나가 나에게 돌아온 것이다.\n조건부 확률과 용어를 빌리면 action들과 state들에 대한 조건이 주어져 있을때 state에 대한 조건부확률분포(conditional distribution)에서 하나의 event가 sample로 뽑힌 것이다.(여기서는 핵을 맞는 사건이 뽑힌것이다.)\n\\[p_{S_t|S_0,A_0\\dots,S_{t-1}A_{t-1}}(s_t|s_0,a_0,\\dots,s_{t-1},a_{t-1})\\]\n이와 비슷하게 내가 핵을 맞기전에 취할 수 있는 판단도 여러가지며 또한 취하는 판단의 근거도 마찬가지로 이전의 상황과 내가 취한 판단에 의해 결정되었을 것이다. action과 state들이 주어졌을때 취할 수 있는 action에 대한 조건부 확률분포는 다음과 같다.\n\\[p_{A_t|S_0,A_0,\\dots,S_{t-1},A_{t-1}}(a_t|s_0,a_0,\\dots,s_t)\\]\n위의 내용은 일반적으로 우리가 생각하는 직관과 일치하는 경우이다. 하지만 (지금까지 연구된)강화학습의 경우 이와는 다르게 Markov Decision Process를 가정한다. 그렇다면 Markov decision process란 뭔가? 위키피디아의 정의에 의하면 “각 사건에 대한 확률이 사건으로부터 얻은 상태에만 의존하는 일련의 가능한 이벤트를 설명하는 확률적 모델”이라고 적혀있다. 강화학습이 MDP를 따르므로 다시 말하자면 현재 내가 놓인 상황(의 확률분포)이나 현재 내가 하는 액션(의 확률분포)은 바로 이전의 상태나 행동의 영향만을 받는다는 것이다. MDP가 따르는 이러한 특성을 Markov property라고 한다.\n\\[\\begin{aligned}\n&\\text{Markov property}\\\\\n&\\forall{t,}\\,\\,p_{\\small{S_t|S_0,A_0\\dots,S_{t-1}A_{t-1}}}(s_t|s_0,a_0,\\dots,s_{t-1},a_{t-1}) = p_{\\small{S_t|S_{t-1}},A_{t-1}}(s_t|s_{t-1},a_{t-1})\\\\\n&\\forall{t,}\\,\\,p_{\\small{A_t|S_0,A_0,\\dots,S_{t-1},A_{t-1}}}(a_t|s_0,a_0,\\dots,s_t)=p_{\\small{A_t|S_{t}}}(a_t|s_t)\n\\end{aligned}\\]\n첫번째 확률은 상태\\(s_{t-1}\\)에서 (\\(a_{t-1}\\)을 취하여) \\(s_{t}\\)로 transition(변할때)에 대한 확률분포이므로 transition probability라고 부른다. 두번째 확률은 policy(정책,지표)로 어떤 상황에서 어떤 액션을 취할지에 대한 기준이 되는 확률분포이다.(후에 optimal policy에 대해 자세히 다룬다.)\n\n\ntransition probability\n\\[\\begin{aligned}\n&\\text{definition of transition probability}\\\\\n&p_{\\small{S_t|S_{t-1}},A_{t-1}}(s_t|s_{t-1},a_{t-1})\\\\\n\\end{aligned}\\]\n\n위와 같은 그림을 살펴보자.왼쪽로봇의 경우 Deterministic Grid World(=Envirion ment)에 놓여있고 앞으로 가는 action을 취할 경우 반드시 앞으로 가므로 state가 결정적(deterministic)이라고 할 수 있다.반면에 오른쪽로봇의 경우 Stochastic Grid World에 놓여있다.이러한 경우에는 action을 취해도 3가지 상황에 취해질 수 있으며 이 경우 state는 Stochastic하다고(바람의 영향이나,로봇이 오작동하거나)할 수 있다.다시말하면 state는 확률분포에 따라 임의적(randomly)이다.\n\n\nPolicy\n정책\\(\\pi(a_t|s_t)\\)는 어떤 상태가 주어질때 어떤 행동을 취할 것인지 명시한 (조건부)확률분포를 말한다.\n\\[\\begin{aligned}\n&\\text{definition of poilcy} \\\\\n&{\\pi}({a_t,s_t}) \\overset{\\Delta}{=}p_{A_t|S_{t}}(a_t|s_t)\n\\end{aligned}\\]\n아래와 같은 그림을 보자 초기 state는 파랑색 위치이며 agent는 왼쪽위나 오른쪽아래의 종료지점까지 가야한다.\n\n\n왼쪽위의 그림에서 모든 policy는 다음과 같다.\n\\[\\begin{aligned}\n\\forall{t},\\pi(a_t,s_t) = p_{\\small{A_t|S_t}}(a_t|s_t) =\n\n\\begin{cases}\np_{\\small{A_t|S_t}}(\\text{right}|s_t) = \\frac{1}{4}\\\\\np_{\\small{A_t|S_t}}(\\text{left}|s_t) = \\frac{1}{4}\\\\\np_{\\small{A_t|S_t}}(\\text{up}|s_t) = \\frac{1}{4}\\\\\np_{\\small{A_t|S_t}}(\\text{down}|s_t) = \\frac{1}{4}\\\\\n\n\\end{cases}\n\n\\end{aligned}\\]\n각각의 state에서 action은 위와 같은 policy를 따르므로 아래와 같은 경로가 예제해로 나올 수 있다.\n왼쪽에서 두번째 있는 그림의 모든 policy는 다음과 같다.\n\\[\\begin{aligned}\np_{\\small{A_t|S_t}}(a_t|s_t)=\n\\begin{cases}\n\\frac{1}{4} \\text{  if } a_t = \\text{down} \\\\\n0\\text{ otherwise}\n\\end{cases}\n\\end{aligned}\\]\n위와 같은 policy를 따르므로 모든 state에 대해서 남쪽방향으로만 나온다.\n오른쪽에서 두번째 그림의 policy는 다음과 같다.\n\n가장 오른쪽 그림의 policy는 다음과 같다.\n\n생각해보면 가장오른쪽 위 사각형같은 policy가 정해지면 가장 빠르게 목표에 도달할 수 있다. 이는 최고의 reward를 받도록 학습된 결과이다.\n\n\n정리\n\n강화학습은 MDP를 가정한다. 이는 이전 state나 액션에 의해서만 확률분포가 영향을 받는다는 것이다.\npolicy는 임의의 state에 취한 action의 확률분포함수로 어떤 action을 할지는 이것에 의해 결정된다.\n\n\n\n참고자료\n위키피디아 - markov chain(=markov process) 혁펜하임-[강화학습] 2-1강. Markov Decision Process (MDP) 쉬운 설명 Fundamental of Reinforcement Learning wordbe"
  },
  {
    "objectID": "posts/RL/RL2-2 value function.html",
    "href": "posts/RL/RL2-2 value function.html",
    "title": "[강화학습] 2-2 Reward & Return & State Value f & Action Value f",
    "section": "",
    "text": "리워드의 정의는 다음과 같다.\n\\[\\mathcal{R}_{t+1} \\overset{\\Delta}{=} \\mathbb{E}({R}_{t+1}|S_t=s,A_t=a)\\]\n이는 \\(s_t\\)에서 \\(a_t\\)를 했을때 t+1에서 얻는 값을 나타내는 확률변수\\(R_{t+1}\\)의 기댓값이다.이다.\n알파고를 예를 들어서 생각해보자. 알파고가 바둑판에(\\(s_t\\))에 검은돌을 놓으면(\\(a_{t}\\)) 상대하는 사람(또는기계)도 어떤 위치에 흰돌을 놓을것이다. 이 흰돌의 위치는 random이기 때문에 따라서 알파고가 확률변수 \\(R_{t+1}\\)이 존재하며 그것의 평균값을 리워드\\(\\mathcal{R}_{t+1}\\)로 정의한다.\n생각해보면 리워드는 뭔가 \\(s_t\\)에서 \\(a_t\\)를 해서 변하는 상황 \\(s_{t+1}\\)에 부여되는게 맞을 것 같다.찾아보니 위키피디아에는 reward를 \\(R_a(s,s')\\)로 쓴다. 변하는 상황에 따라 부여되는것도 맞고 어떤 액션을 취하면 그것에 상응한다고 봐도 무방할 것 같다.(개인적인 의견입니다.)\n그냥 \\(R_{t+1}\\)을 리워드라 하는 경우도 많은 것 같다\n\n\n\n\n\n\nNote\n\n\n\n위에서 인식하는 상황이라고 썼는데 사실 state는 agent가 인식하는 것이 아니라 사실은 environment에가 반환(return)하는 것입니다. agent는 state중 일부를 받는데 이것을 observation이라고 합니다. 그러나 실제 논문에서는 딱히 state와 observation을 구별하지않고 쓰는 경우가 많다고 합니다."
  },
  {
    "objectID": "posts/RL/RL2-2 value function.html#state-value-function",
    "href": "posts/RL/RL2-2 value function.html#state-value-function",
    "title": "[강화학습] 2-2 Reward & Return & State Value f & Action Value f",
    "section": "state value function",
    "text": "state value function\n\nThe value function of a state \\(s\\) under a policy \\(\\pi\\) is the expected return when starting \\(s\\) and following \\(\\pi\\) thereafter. \\(\\leftrightarrow\\) policy \\(\\pi\\)를 따를때 state \\(s\\)의 value function은 state \\(s\\)에서 시작하여 그 후 \\(\\pi\\)를 따를때 return의 기댓값이다.\n각각의 상태가 얼마나 좋은지 그 가치를 expected return으로 계산한 함수이므로 state value function이라는 이름이 붙었다.\n단,\\(s\\)에서 시작하다는 조건하에서의 기댓값이므로 \\(s\\)가 given인 conditional expectation을 구하면 된다.\n\n\\[v_{\\pi}\\overset{\\Delta}{=} \\mathbb{E}_{\\pi}[G_t|S_t=s] = \\mathbb{E_{\\pi}}\\left[{\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}|S_t=s}\\right],\\text{for all s } \\in S\\]"
  },
  {
    "objectID": "posts/RL/RL2-2 value function.html#action-value-function",
    "href": "posts/RL/RL2-2 value function.html#action-value-function",
    "title": "[강화학습] 2-2 Reward & Return & State Value f & Action Value f",
    "section": "action value function",
    "text": "action value function\n\nSimilary, we define the value of taking action \\(a\\) in state \\(s\\) under a policy \\(\\pi\\) as the expected return starting from s,taking action a,and thereafter following policy \\(\\pi\\) \\(\\leftrightarrow\\) 유사하게 policy \\(\\pi\\)를 따르고 state \\(s\\)에서 action \\(a\\)를 취하는 것의 가치는 state \\(s\\)에서 시작하여 action \\(a\\)를 취하고 그 후 \\(\\pi\\)를 따를때의 기댓값으로 정의할 수 있다.\n각각의 state에서 action을 취했을때 그 가치를 expected return으로 측정하므로 action value function이라는 이름이 붙었다.\n여기서는 state s에서 action a를 취한것에 대한 기댓값을 계산하므로 given s,a일때의 conditional expectation of return을 구하면 된다.\n\n\\[q_{\\pi}(s,a) \\overset{\\Delta}{=} \\mathbb{E}[G_t|S_t = s,A_t = a] = \\mathbb{E}_{\\pi}\\left[\\sum_{k=0}^{\\infty}\\gamma^kR_{t+k+1}|S_t=s,A_t=a\\right]\\]"
  },
  {
    "objectID": "posts/RL/RL2-2 value function.html#recursive-relationships",
    "href": "posts/RL/RL2-2 value function.html#recursive-relationships",
    "title": "[강화학습] 2-2 Reward & Return & State Value f & Action Value f",
    "section": "recursive relationships",
    "text": "recursive relationships\n\nv를 next v로 표현하기\n\nFor any policy \\(\\pi\\) and any state \\(s\\), the following consistency condition holds between the value of \\(s\\) and the value of its possible succesor states \\(\\leftrightarrow\\) 상태 \\(s\\)의 state value function은 다음 상태 \\(s'\\)에 대한 state value function이 포함된 식으로 표현할 수 있다.즉,현재상태의 가치는 다음상태의 가치와 연관과 관련이 있다.\n\n\\[\\begin{align}\nv_{\\pi}(s) &\\overset{\\Delta}{=} \\mathbb{E}_{\\pi}[G_t|S_t=s] \\\\\n&=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_t=s] \\\\\n&=\\sum_a \\pi(a|s)\\sum_{s'}\\sum_{r}p(s',r|s,a)\\left[r + \\gamma E_{\\pi}[G_{t+1}|S_{t+1}=s'\\right]]\\\\\n&=\\sum_a \\pi(a|s)\\sum_{s',r}p(s',r|s,a)\\left[r + \\gamma  v_{\\pi}(s')\\right] , \\text{for all } s \\in S\\\\\n\\end{align}\\]\n\n혹시 2에서 3번식으로 넘어가는게 이해가 잘 안된다면 링크를 참고. 핵심은 law of total expectation을 이해하는 것이다.\n이 식이 기억하기가 상당히 어렵다. 따라서 \\(s \\rightarrow s'\\)을 나타낸 \\(v_{\\pi}\\)의 backup diagram을 통해서 생각해볼 수 있다.\n\n\n\n\nbackup diagram of \\(v_{\\pi}\\)\n\n\n\n각각의 흰색원은 state를 검은색원은 state,action pair를 나타낸다.\n상태s에서 시작하며 policy \\(\\pi\\)에 의하여 action을 취한다.\naction을 취하면 리워드\\(r\\)을 얻고 \\(s \\rightarrow s'\\)으로 상태가 바뀌며 이는 transition \\(p(s',r|s,a)\\)에 의해 결정된다.\n\nThe Bellman equation (3.14) averages over all the possibilities, weighting each by its probability of occurring. It states that the value of the start state must equal the (discounted) value of the expected next state, plus the reward expected along the way. \\(\\leftrightarrow\\)즉, 초기상태 \\(s\\)에서 시작하여 나올 수 있는 모든 \\(r,v_{\\pi'}\\)의 경우에 대해 각각이 나올 확률을 곱하여 averaging(expectation)을 구하면 \\(v_{\\pi}(s)\\)라는 것이다.\n\\[\\begin{align}\nv_{\\pi}(s) &= \\sum_{a,s',r}\\pi(a|s)p(s',r|s,a)\\left[r + \\gamma  v_{\\pi}(s')\\right] , \\\\\n&= \\sum_{a}\\pi(a|s) \\sum_{s',r}(s',r|s,a)\\left[r+\\gamma v_{\\pi}(s')\\right] \\,\\text{for all } s \\in S\\\\\n\\end{align}\\]\n개인적으로 좀 더 자세히 기억하려고 정리해봤다.diagram은 수식자체는 아니기 때문에 그림을 보고 나름대로 기억할 수 있는 방법을 찾으면 된다고 생각한다. 따라서 아래와 같이 정리해봤지만 헷갈리면 pass해도 무방하다.\n\n\\(s \\rightarrow s'\\)는 여러가지 경우가 존재하며 그러므로 상태 \\(s\\)의 가치 \\(v_{\\pi}(s)\\)는 다음상태 \\(s'\\)의 가치 \\(v_{\\pi}(s')\\)에 영향을 받는다.\n그런데 \\(s'\\)은 \\(r\\)이 항상 같이 따라오므로 \\(v_{\\pi}(s)\\)는 \\(r+v_{\\pi}(s')\\)에 영향을 받는다.\n상태s에서 a를 취하며 \\(r\\)과 \\(s'\\)이 나올 확률은 policy와 transition의 곱이다. 즉 \\(\\pi(a|s)p(s',r|s,a)\\)이다.\n모든 경우에 대해 고려해야 하므로 모든 \\(a,r,s'\\)에 곱해준다.\n\n\n\nQ를 next Q로 표현하기\n\n마찬가지로 \\(s,a\\)에서의 action value function도 next \\(s',a'\\)에서의 action value function으로 표현할 수 있다.\n\n\\[\\begin{align}\nq_{\\pi}(s,a) &\\overset{\\Delta}{=} \\mathbb{E}_{\\pi}[G_t|S_t=s,A_t=a] \\\\\n&=\\mathbb{E}_{\\pi}[R_{t+1}+\\gamma G_{t+1}|S_t=s,A_t=a] \\\\\n&=\\sum_{s',r}p(s',r|s,a)\\big[r+\\sum_{a'}p(s',r|s,a)\\pi(a'|s')q_{\\pi}(a',s')]\n\\end{align}\\]\n\nbackup diagram은 다음과 같다.\n\n\n\n\nbackup diagram of \\(q_{\\pi}\\)\n\n\n\nbackup diagram을 보고 나올 수 있는 \\(r,q_{\\pi}(s',a')\\)의 모든 경우에 대하여 계산해보면 다음과 같다.(state value function의 \\(r,s'\\)은 같이 따라오므로 같이 계산해줬지만 action value function은 \\(r,a\\)는 따로이므로 따로 계산하여 더해준다.)\n\n\\[\\begin{aligned}\nq_{\\pi}(s,a)&=\\sum_{s',r}p(s',r|s,a)r + \\sum_{s',r,a'}p(s',r|s,a)\\pi(a'|s')q_{\\pi}(a',s')\\\\\n&=\\sum_{s',r}p(s',r|s,a)\\big[r+\\sum_{a'}p(s',r|s,a)\\pi(a'|s')q_{\\pi}(a',s')]\\,\\text{for all } s \\in S,a \\in A(s)\\\\\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/RL/RL2-2 value function.html#optimal-policy",
    "href": "posts/RL/RL2-2 value function.html#optimal-policy",
    "title": "[강화학습] 2-2 Reward & Return & State Value f & Action Value f",
    "section": "optimal policy",
    "text": "optimal policy\noptimal policy는 value function인 \\(V(s_t)\\)를 가장 크게 하는 policy들(\\(p(a_t|s_t),p(a_{t+1}|s_{t+1}),\\dots,p(a_{\\infty}|s_{\\infty})\\))이다.나중에 더 자세히 공부해야할 것 같다.\naction value function은 state,action의 함수의 입력으로 주어지는 어떤 state에서 (마찬가지로 주어지는)action을 취했을때 특정 policy가 좋은지 나쁜지(가치)를 평가한다. 평가는 action을 취한뒤의 다음 state부터 마지막 시점까지 Agent가 가능한 모든 action과 놓여질 수 있는 모든 state를 고려하여 기대되는 보수의 총합을 계산하는방식이다. 이것도 마찬가지로 결국은 \\(G_t\\)에 대한 조건부 함수이다.\n\\[\\begin{aligned}\nQ^{\\pi}(s_t,a_t) \\underset{=}{\\Delta} \\mathbb{E}[G_t|A_t=a_t,S_t=s_t,\\pi] = \\int_{s_{t+1}:a_{\\infty}}G_tp(s_{t+1},\\dots,a_{\\infty})ds_{t+1}:da_{\\infty}\n\\end{aligned}\\]\n\\[\\begin{aligned}\n\\end{aligned}\\]\n\\[\\begin{aligned}\nv_{\\pi}\n\\end{aligned}\\]"
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": "",
    "text": "앞선 포스팅에서 르벡메져를 도입하여 무리수 집합의 길이는 \\(2\\pi\\) 유리수 집합의 길이는 0이였었습니다.\n그러나 왜 이렇게 되는지에 대한 직관은 전혀 얻을 수 없고 받아들여야 했습니다.\n이번포스팅에서는 이는 집합간의 전사,단사,전단사 함수가 존재유무를 통해서 무한집합들간의 cardinality(유한집합에서의 크기 개념)를 비교하고자 합니다.\n이를 통해 유리수집합의 길이가 무리수집합의 길이보다 왜 작은지 직관을 얻을 수 있습니다.(그렇다고 모순이 완전히 사라지지는 않습니다. 르벡메져를 도입해도 여전히 확률을 모순없이 정의하기에는 부족합니다.)"
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#injective-function",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#injective-function",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": " Injective function",
    "text": "Injective function\n\n\n\n\n\n\ndefinition of injective function\n\n\n\n\\[\\begin{aligned}\n&\\text{The function is injective, or one-to-one}\\\\\n&\\Longleftrightarrow \\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\n\\end{aligned}\\]\n\n\n\ndomain에 속하는 서로다른 모든 두 원소가 codomain에 속하는 서로다른 두 원소에 mapping될 때 Injection function입니다.\n암기(입력이 다르면 출력이 다르다)\n느낌(화살표가 쫙 펴지는 느낌이다.)"
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#surjective-function",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#surjective-function",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": " surjective function",
    "text": "surjective function\n\n\n\n\n\n\ndefinition of surjective function\n\n\n\n\\[\\begin{aligned}\n&\\text{The function is surjective, or onto} \\\\\n&\\Longleftrightarrow \\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\n\\end{aligned}\\]\n\n\n\ncodomain에 속하는 모든 원소가 domain에 속하는 원소에 적어도 하나의 mapping을 받을 때 surjective function입니다.\n암기1:domian의 image(상)인 치역이 공역과 같습니다.\n암기2:공역의 모든 원소가 역상(inverseimage)이 존재합니다.\n느낌 : 화살표가 쫙 모이는 느낌"
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#bijective-function",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#bijective-function",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": " bijective function",
    "text": "bijective function\n\n\n\n\n\n\ndefinition of bijective function\n\n\n\n\\[\\begin{aligned}\n&\\text{The function is bijective, or one-to-one and onto}\\\\\n&\\Longleftrightarrow \\forall y \\in Y,\\exists !x \\in X \\text{ such that } y = f(x) \\\\\n&\\text{where } \\exists!x \\text{ means \"there exists exactly one x\".}\n\\end{aligned}\\]\n\n\n\ncodomain에 속하는 임의의,모든 원소가 domain에 속하는 정확히 하나의 원소에만 mapping되는 경우 bijective function이라고 한다.\ninjective이며 동시에 surjective인 function이다."
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#용어-정리헷갈림",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#용어-정리헷갈림",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": " 용어 정리(헷갈림)",
    "text": "용어 정리(헷갈림)\n\ninjective = 단사 = one-to-one, injective function = injection = 단사함수\nsurjective = 전사 = onto, surjective function = surjection = 전사함수\nbijecitve = 전단사 = one-to-one and onto , bijective function = bijection = 전단사함수"
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#함수의-종류와-cardinality와의-관계",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#함수의-종류와-cardinality와의-관계",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": " 함수의 종류와 Cardinality와의 관계",
    "text": "함수의 종류와 Cardinality와의 관계\n\n\n\n\n\n\nproperty1 : 전사함수이면 단사함수이면 전단사함수이다\n\n\n\n\n어떤 함수가 전사함수 & 단사함수 \\(\\Longleftrightarrow\\) 전단사함수\n\n\n\n\n\n\n\n\n\nproperty2 : 함수의 종류와 두 집합간의 크기비교\n\n\n\n두 집합사이에 적절한 함수를 정의하여 집합간의 크기비교가 가능하다.\n\n단사함수와 cardinality\n\n집합 \\(X\\)에서 집합 \\(Y\\)로 가는 단사함수가 존재한다. \\(\\Longleftrightarrow |X| \\leq |Y|\\)\n집합 \\(X\\)에서 집합 \\(Y\\)로 가는 단사함수가 존재하지 않는다. \\(\\Longleftrightarrow |X| > |Y|\\)\n주의! 두 명제가 “이”의 관계에 있어서 자명하게 참이되는 것이 아니라 대응관계를 봤을때 참이 되는 것입니다.\n\n\n\n전사함수와 cardinality\n\n집합 \\(X\\)에서 집합 \\(Y\\)로 가는 전사함수가 존재한다. \\(\\Longleftrightarrow |X| \\geq |Y|\\)\n집합 \\(X\\)에서 집합 \\(Y\\)로 가는 전사함수가 존재하지 않는다. \\(\\Longleftrightarrow |X| < |Y|\\)\n\n\n\n전단사함수와 cardinality\n\n집합 \\(X\\)에서 집합 \\(Y\\)로 가는 전단사함수가 존재한다. \\(\\Longleftrightarrow |X| = |Y|\\)\n주의! 전단사 함수는 존재하지 않아도 두 집합의 크기는 같을 수 있습니다.(위의 그림의 단사도 전사도 아닌 경우에서 Y의 a를 빼고 생각해봅시다.)\n\n\n\n\n\n\n\n\n\n\nproperty3 : 부분집합과 크기비교\n\n\n\n\\(X \\subset Y \\Rightarrow |X| \\leq |Y|\\)  \\(X \\supset Y \\Leftrightarrow |X| \\geq |Y|\\)  \\(X = Y \\Rightarrow |X| = |Y|\\)\n\n\n\n\n\n\n\n\n동치명제 정리\n\n\n\n\n\\(X \\subset Y \\Longleftrightarrow |X| \\leq |y|\\) \n\\(X \\supset Y \\Longleftrightarrow |X| \\geq |y|\\) \n\\(X \\subset = \\Longleftrightarrow |X| = |y|\\)\n\n\n\n\n위의 사실들은 유한집합에 대해서 생각해보면 자명한 사실들입니다.\n우리는 이를 무한집합에 확장하여 적용함으로서 무한집합간의 cardinality를 비교할 수 있습니다."
  },
  {
    "objectID": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#정리",
    "href": "posts/stochastic process/SP-caardinality_typeoffunction/sp_2023_03_17.html#정리",
    "title": "[Stochestic Process] 2.Cardinality",
    "section": " 정리",
    "text": "정리\n\n유한집합에서 두 집합간의 크기의 비교는 두 집합간에 존재할 수 있는 함수가 어떤 함수인지를 통해서 가능합니다.\n이를 무한집합에서 적용하여 두 집합간의 크기비교를 해보고자 합니다.(다음 포스팅!)"
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html",
    "title": "[Stochestic Process] 3.무한집합간의 크기 비교",
    "section": "",
    "text": "이전포스팅에서 유한집합간의 크기비교는 두 집합간의 어떤 함수가 존재하는지를 통해서 가능하다고 정리했습니다.\n여기서는 이를 무한집합에서 확장하여 실제로 여러가지 서로다른 무한집합간의 크기(엄밀히,cardinality입니다. 포스팅에서는 크기와 cardinality를 혼용합니다.)를 비교합니다.\n즉, 두 무한집합간의 전사,단사,전단사 함수가 존재여부를 확인합니다.\n결과적으로 이전의 포스팅에서 무리수 집합의 길이는 \\(2\\pi\\) 유리수 집합의 길이는 0이였는데 이에 대한 직관을 얻을 수 있습니다."
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#자연수집합의-vs-홀수집합짝수집합",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#자연수집합의-vs-홀수집합짝수집합",
    "title": "[Stochestic Process] 3.무한집합간의 크기 비교",
    "section": " 자연수집합의 vs 홀수집합,짝수집합",
    "text": "자연수집합의 vs 홀수집합,짝수집합\n\n홀수집합 vs 짝수집합의 경우\n\n먼저 홀수집합 vs 짝수집합의 크기를 비교합니다.\n즉,\\(X = \\{1,3,5,\\dots\\},Y = \\{2,4,6,\\dots\\}\\)의 크기를 비교합니다.\n두 무한집합사이에 크기를 비교하려면 어떤함수가 정의될 수 있는지 확인한 후 함수의 종류와 Cardinality와의 관계을 통해서 결론을 내리면 됩니다.\n\n\n\\(f\\)정의하기\n\\[\\begin{aligned}\nf(1) &= 2 \\\\\nf(3) &= 4 \\\\\nf(5) &= 6 \\\\\n&\\vdots\n\\end{aligned}\\]\n단사함수인가?(정의를 통해 확인)\n\ndefinition : \\(\\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\\)\n정의역에 속한 임의의,모든 원소가 공역에 속하는 서로다른 원소에 mapping되므로 단사함수이다.\n\n전사함수인가?(정의를 통해 확인)\n\ndefinition : \\(\\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\\)\n공역에 속한 임의의,모든 원소가 적어도 하나의 정의역에 속하는 원소에 mapping되므로 전사함수이다.\n\n\n\n\n\n\n\n\n홀수집합 vs 짝수집합\n\n\n\n\n무한집합인 홀수집합과 짝수집합사이에는 전단사함수가 존재합니다.\n따라서 홀수집합과 짝수집합의 크기는 같습니다.\n즉,\\(|X| = |Y|\\)\n\n\n\n\n\n자연수(양의정수)집합 vs 홀수,짝수집합의 경우\n\n\\(f\\)정의하기\n\\[\\begin{aligned}\nf(1) &= 2 \\\\\nf(2) &= 4 \\\\\nf(3) &= 6 \\\\\n&\\vdots\n\\end{aligned}\\]\n단사함수인가?\n\ndefinition : \\(\\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\\)\n정의역에 속한 임의의,모든 원소가 공역에 속하는 서로다른 원소에 mapping되므로 단사함수이다.\n\n전사함수인가?\n\ndefinition : \\(\\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\\)\n공역에 속한 임의의,모든 원소가 적어도 하나의 정의역에 속하는 원소에 mapping되므로 전사함수이다.\n\n\n\n\n\n\n\n\n자연수집합 vs 홀수,짝수집합\n\n\n\n\n자연수집합과 짝수집합사이에 전단사함수가 존재합니다.\n따라서 자연수집합의 크기와 짝수집합의 크기는 같습니다.\n이전에 짝수집합의 크기와 홀수집합의 크기는 같았다고 결론내렸으므로 세 집합의 크기는 같습니다.\n즉, \\(|\\mathbb{N}| = |Y| = \\aleph_0\\)(여기서 y는 짝수집합 or 홀수집합의 크기를 의미합니다.)\n\n\n\n\n결론 Q-궁금한거 : 정수집합에 대해서는 어떻게 cardinality가 같음을 유도할 수 있는가?\n\n또한 이를 확장하여,정수집합(양의정수 + 음의정수 + 0)도 자연수집합과 cardinality가 같음을 보일 수 있다.(???)\n즉, \\(|\\mathbb{N}| = |\\mathbb{N}| = \\aleph_0\\)"
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#자연수집합-vs-자연수집합-cup-0",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#자연수집합-vs-자연수집합-cup-0",
    "title": "[Stochestic Process] 3.무한집합간의 크기 비교",
    "section": "자연수집합 vs 자연수집합 \\(\\cup \\{0\\}\\)",
    "text": "자연수집합 vs 자연수집합 \\(\\cup \\{0\\}\\)\n\n자연수집합과 자연수집합 \\(\\cup \\{0\\}\\)의 크기를 비교해보겠습니다.\n즉, \\(\\mathbb{N} = \\{1,2,3,\\dots\\}, \\mathbb{N}\\cup \\{0\\} = \\{0,1,2,3,\\dots\\}\\)의 크기를 비교합니다.\n\n\n\\(f\\)정의하기\n\\[\\begin{aligned}\nf(1) &= 0 \\\\\nf(2) &= 1 \\\\\nf(3) &= 2 \\\\\n&\\vdots\n\\end{aligned}\\]\n단사함수인가?(정의를 통해 확인)\n\ndefinition : \\(\\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\\)\n정의역에 속한 임의의,모든 원소가 공역에 속하는 서로다른 원소에 mapping되므로 단사함수입니다.\n\n전사함수인가?(정의를 통해 확인)\n\ndefinition : \\(\\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\\)\n공역에 속한 임의의,모든 원소가 적어도 하나의 정의역에 속하는 원소에 mapping되므로 전사함수입니다.\n\n\n\n\n\n\n\n\n자연수집합 vs 자연수집합 \\(\\cup \\{0\\}\\)\n\n\n\n\n자연수집합과 자연수집합 \\(\\cup \\{0\\}\\)사이에 전단사함수가 존재합니다.\n따라서 두 집합의 크기는 같습니다.\n즉, |\\(\\mathbb{N}| = |\\mathbb{N} \\cup \\{0\\}|\\)입니다.\n조금 과감하게 \\(\\aleph_0 + 1 = \\aleph_0\\)으로 기억하면 좋습니다.(엄밀하진 않습니다.)"
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#자연수집합-vs-자연수집합-cup-음의정수집합",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#자연수집합-vs-자연수집합-cup-음의정수집합",
    "title": "[Stochestic Process] 3.무한집합간의 크기 비교",
    "section": "자연수집합 vs 자연수집합 \\(\\cup\\) 음의정수집합",
    "text": "자연수집합 vs 자연수집합 \\(\\cup\\) 음의정수집합\n\n자연수집합과 양의정수집합\\(\\cup\\)음의정수집합을 비교합니다.\n즉, \\(\\mathbb{N} = \\{1,2,3,\\dots\\}, \\mathbb{N}\\cup \\mathbb{N}^- = \\{-1,1,-2,2,\\dots\\}\\)의 크기를 비교합니다.\n\n\n\\(f\\)정의하기\n\\[\\begin{aligned}\nf(1) &= -1 \\\\\nf(2) &= 1 \\\\\nf(3) &= -2 \\\\\nf(4) &= 2 \\\\\n&\\vdots\n\\end{aligned}\\]\n\n\n두 집합간의 mapping을 -1,1,-2,2 이런식으로 mapping하는게 포인트입니다.(전 못했습니다…)\n\n\n단사함수인가?(정의를 통해 확인)\n\ndefinition : \\(\\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\\)\n정의역에 속한 임의의,모든 원소가 공역에 속하는 서로다른 원소에 mapping되므로 단사함수입니다.\n\n전사함수인가?(정의를 통해 확인)\n\ndefinition : \\(\\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\\)\n공역에 속한 임의의,모든 원소가 적어도 하나의 정의역에 속하는 원소에 mapping되므로 전사함수입니다.\n\n\n\n\n\n\n\n\n자연수집합 vs 자연수집합 \\(\\cup\\) 음의정수집합\n\n\n\n\n자연수집합과 자연수집합 \\(\\cup\\) 음의정수집합사이에 전단사함수가 존재합니다.\n따라서 두 집합의 크기는 같습니다.\n즉, |\\(\\mathbb{N}| = |\\mathbb{N} \\cup \\mathbb{N}^-| = \\aleph_0\\)입니다.\n이것도 조금 과감하게 \\(\\aleph_0 \\times 2 = \\aleph_0\\)로 기억하면 좋습니다.\n자연수집합크기\\(\\aleph_0\\) = 자연수집합크기\\(\\aleph_0\\) + 음의정수집합크기\\(\\aleph_0\\)이기 때문입니다.(음의정수집합의 크기는 증명생략,해보면 자연수집합과 같습니다.)"
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#자연수집합-vs-정수집합",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#자연수집합-vs-정수집합",
    "title": "[Stochestic Process] 3.무한집합간의 크기 비교",
    "section": "자연수집합 vs 정수집합",
    "text": "자연수집합 vs 정수집합\n\n자연수집합과 양의정수집합\\(\\cup\\)음의정수집합을 비교합니다.\n즉, \\(\\mathbb{N} = \\{1,2,3,\\dots\\}, \\mathbb{Z} = \\{0,-1,1,-2,2,\\dots\\}\\)의 크기를 비교합니다.\n\n\n\\(f\\)정의하기\n\\[\\begin{aligned}\nf(1) &= 0 \\\\\nf(2) &= -1 \\\\\nf(3) &= -1 \\\\\nf(4) &= -2 \\\\\nf(5) &= -2 \\\\\n&\\vdots\n\\end{aligned}\\]\n\n\n이전 챕터와 비슷하게 두 집합간의 mapping을 0,-1,1,-2,2 이런식으로 교차하면서 mapping합니다.\n\n\n단사함수인가?(정의를 통해 확인)\n\ndefinition : \\(\\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\\)\n정의역에 속한 임의의,모든 원소가 공역에 속하는 서로다른 원소에 mapping되므로 단사함수입니다.\n\n전사함수인가?(정의를 통해 확인)\n\ndefinition : \\(\\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\\)\n공역에 속한 임의의,모든 원소가 적어도 하나의 정의역에 속하는 원소에 mapping되므로 전사함수입니다.\n\n\n\n\n\n\n\n\n자연수집합 vs 정수집합\n\n\n\n\n자연수집합과 정수집합사이에 전단사함수가 존재합니다.\n따라서 두 집합의 크기는 같습니다.\n즉, |\\(\\mathbb{N}| = |\\mathbb{Z}| = \\aleph_0\\)입니다.\n이것에 생각했던 과감한 논리들을 합쳐서 구해볼 수도 있습니다.(\\(\\aleph_0 \\times 2 = \\aleph_0\\),\\(\\aleph_0 + 1 = \\aleph_0\\))"
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#자연수집합-vs-유리수집합",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#자연수집합-vs-유리수집합",
    "title": "[Stochestic Process] 3.무한집합간의 크기 비교",
    "section": "자연수집합 vs 유리수집합",
    "text": "자연수집합 vs 유리수집합\n\n자연수집합과 유리수집합의 크기를 비교합니다.\n유리수 이전의 비교처럼 뭔가 한번에 두 집합사이를 비교하기가 어렵습니다.\n따라서 먼저 양의 유리수집합 (\\(\\mathbb{Q}^+\\)로 정의하겠습니다.)을 포함하는 집합 \\(Y\\)을 가정하고 이를 먼저 자연수 집합과 비교합니다.\n즉,\\(\\mathbb{N} = \\{1,2,\\dots\\},Y = \\{1,\\frac{2}{1}\\,\\frac{1}{2},\\frac{1}{3},\\frac{2}{2},\\frac{3}{1}\\dots\\} \\supset \\mathbb{Q}^+\\)입니다.\n그렇다면, 집합\\(Y\\)의 모든 원소는 다음과 같은 격자의 가로축을 분모로 세로축을 분자로 생각할때 격자위에 모두 놓일 수 있습니다.\n\n\n\n\\(f\\)정의하기\n\\[\\begin{aligned}\nf(1) &= 1 \\\\\nf(2) &= \\frac{2}{1} \\\\\nf(3) &= \\frac{1}{2} \\\\\nf(4) &= \\frac{1}{3} \\\\\nf(5) &= \\frac{2}{2} \\\\\nf(6) &= \\frac{3}{1} \\\\\n&\\vdots\n\\end{aligned}\\]\n\n\n자연수1부터 차례대로 격자위의 화살표의 순서를 따라서 집합\\(Y\\)의 원소에 매핑합니다.\n대각선 어떤 방향이던 일단 방향이 정해지면 정해진 방향의 원소는 모두 스치면서 지나갑니다.\n\n\n단사함수인가?\n\ndefinition : \\(\\forall x_1,x_2\\in X,x_1\\neq x_2\\implies f(x_1)\\neq f(x_2)\\)\n정의역에 속한 임의의,모든 원소가 공역에 속하는 서로다른 원소에 mapping되므로 단사함수이다.\n\n전사함수인가?\n\ndefinition : \\(\\forall y \\in Y,\\exists x \\in X \\text{ such that } y = f(x)\\)\n공역(양의 유리수집합)에 속한 임의의,모든 원소가 적어도 하나의 정의역에 속하는 원소에 mapping되므로 전사함수이다.\n\n유도\n\n\n우리는 \\(\\mathbb{Q}^+ \\subset Y\\) 이므로 \\(|\\mathbb{Q}^+| \\leq |Y|\\)와 \\(\\mathbb{N} \\subset \\mathbb{Q}^+\\)이므로 \\(|\\mathbb{N}| = \\aleph_0 \\leq |\\mathbb{Q}^+|\\)을 이미 알고있습니다.(양의 유리수집합은 자연수를 포함하며, 명제는 함수의 종류와 cardinality와의 관계에서 증명된 사실입니다.)\n또한 3번까지의 유도과정에 의하여 상기식에 의하여 \\(|\\mathbb{N}| = |Y| = \\aleph_0\\) 입니다.\n그렇다면 \\(|\\mathbb{N}| \\leq|\\mathbb{Q}^+| \\leq |Y|\\) 이고 \\(|\\mathbb{N}| = |Y| = \\aleph_0\\)이므로 \\(|\\mathbb{Q}|^+ = \\aleph_0\\)임을 알 수 있습니다.\n또한 음의 유리수집합 \\(|\\mathbb{Q}|^-\\)도 자연수집합의 크기와 같음을 알 수 있습니다.(증명생략,위와 같이 격자에 대고 하면 됩니다.)\n유리수집합은 양의 유리수집합 + 음의 유리수집합 + \\(\\{0\\}\\)입니다.\n여기서 과감하게 사용한 앞선 정리들을 사용합니다. (\\(\\aleph_0 \\times 2 = \\aleph_0\\),\\(\\aleph_0 + 1 = \\aleph_0\\))\n따라서 \\(|\\mathbb{Q}| = |\\mathbb{Q}^+| + |\\mathbb{Q}^-| + |{0}| = \\aleph_0 \\times 2 + 1 = \\aleph_0 + 1 = \\aleph_0\\) 입니다.\n\n\n\n\n\n\n\n자연수집합 vs 유리수집합\n\n\n\n\n유리수집합보다 더 큰 집합을 상정하고 부분집합과 관련된 명제를 사용하여 유리수집합의 크기를 유도했습니다.\n결과적으로, 유리수집합과 자연수집합의 크기는 같습니다.\n즉, \\(|\\mathbb{Q}| = |\\mathbb{N}| = \\aleph_0\\)입니다."
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#직관적인-정리",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#직관적인-정리",
    "title": "[Stochestic Process] 3.무한집합간의 크기 비교",
    "section": "직관적인 정리",
    "text": "직관적인 정리\n\n위에서 나왔던 (엄밀하진 않은..)사실들을 정리합니다.\n\\(\\aleph_0 + 1 = \\aleph_0\\,\\,,|\\mathbb{N}| = |\\mathbb{N}| \\cup \\{0\\}\\)\n\\(\\aleph_0 \\times 2 = \\aleph_0\\,\\,,\\)자연수집합과 음의정수집합을 더한 집합의 카디널리티는 자연수집합의 카디널리티와 같습니다.\n\\(\\aleph_0\\times \\aleph_0 = \\aleph_0 ^2 = \\aleph_0\\,\\,,\\)자연수집합과 자연수집합의 곱집합으로 유리수집합(보다 큰)을 만들 수 있는데 이 집합도 자연수집합의 카디널리티와 같습니다."
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#귀류법",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#귀류법",
    "title": "[Stochestic Process] 3.무한집합간의 크기 비교",
    "section": "귀류법",
    "text": "귀류법\n\n귀류법은 명제의 참 or 거짓을 증명하는 하나의 방법입니다.\n대략 이런 느낌으로 증명을 진행합니다.\n\n어떤 명제가 참임을 증명하고자 한다고 가정해봅시다. ex)“A는 치킨먹을때 물은 절대 안먹는다.”라는 명제가 참임을 증명해봅시다.\n그렇다면 이에 정확히 반대가 되는 명제를 가정하고 식을 전개했을때 어떤 이상함,맞지않음(모순)을 찾습니다. ex)여기서는 “A가 물을 먹을때도 있다.”라는 명제가 참이라고 가정하고 하고 모순을 찾습니다.\n만약 모순을 찾는다면 반대가 되는 명제는 이상함이 있는 것이므로 참인명제가 아닙니다. 따라서 이에 반대되는 명제는 사실입니다. ex)치킨을 같이 먹어본 결과 A가 물을 전혀 안 먹었습니다. 그러면 “A가 물을 먹을때도 있다.”라는 명제는 거짓이므로 “A는 치킨먹을때 물은 절대 안먹는다.”라는 명제가 참입니다.\n\n느낌 : 틀리거나 정확하거나,맞거나 아니거나 뭐던지 하나가 아니라고 나오면 다른건 무조건 맞는 말인 이분법느낌?"
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#유리수집합-vs-실수집합",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#유리수집합-vs-실수집합",
    "title": "[Stochestic Process] 3.무한집합간의 크기 비교",
    "section": "유리수집합 vs 실수집합",
    "text": "유리수집합 vs 실수집합\n\n문제 쉽게만들기\n\n우리의 목표는 유리수집합 vs 무리수집합의 크기를 비교하여 길이가 다름에 대한 직관을 얻는 것이였습니다.\n그러기 위해서 먼저 유리수집합과 실수집합의 크기를 비교합니다.\n증명하고자 명제는 다음과 같습니다.\n\n\\[|\\mathbb{Q}| < |\\mathbb{R}|\\]\n\n그런데 여기서 문제를 살짝 바꿔서 \\(|\\mathbb{Q}| = |\\mathbb{N}|\\)을 이용하여 다음과 같은 명제를 증명하는 문제로 바꿔서 생각합니다.(아마 바로 풀기 어려워서 그런 것 같습니다.)\n\n\\[ |\\mathbb{N}| < |\\mathbb{R}|\\]\n\n여기서 또 한번 문제를 바꿔서 생각합니다.\n\\([0,1] \\subset \\mathbb{R}\\)이므로 \\(|[0,1]| \\leq |\\mathbb{R}|\\)입니다.\n만약 자연수집합의 크기가 [0,1]집합의 크기보다 작다는 명제가 참이라고 증명된다면 [0,1]집합은 반드시 실수집합의 크기보다 작거나 같으므로 위의 명제도 반드시 참입니다.(작은 집합보다 작은데 그보다 큰(적어도 같은)집합보다 클 수 가 없어요,충분조건)\n즉 이 문제는 다음의 명제를 증명하는 문제로 바뀌게 됩니다.\n\n\\[ |\\mathbb{N}| < |[0,1]|\\]\n\n\n귀류법 사용하기\n\n위의 명제가 참임을 증명하기 위해서 귀류법을 사용합니다.\n먼저 정확히 반대가 되는 명제는 다음과 같습니다.\n\n\\[|\\mathbb{N}| \\geq |[0,1]|\\]\n\n이 명제가 참이라는 것은 \\(f : \\mathbb{N} \\rightarrow [0,1]\\)인 전사함수가 존재한다는 것과 동치입니다.\n따라서 아래와 같은 전사함수가 존재한다고 가정합니다.\n\n\\[\\begin{aligned}\n&f(1) = 0.7302 \\\\\n&f(2) = 0.4823 \\\\\n&f(3) = 0.2325 \\\\\n&\\quad\\quad\\quad\\vdots\n\\end{aligned}\\]\n\n전사함수라 가정했으므로 정의에 의해 \\([f(1),f(2),f(3),\\dots] = [0,1]\\)이 됩니다.(전사함수는 치역과 공역이 같은 함수입니다. 공역의 모든 원소는 정의역의 원소에 대응됩니다.)\n이제 \\(0.x_1x_2x_3\\dots\\)라는 숫자를 가정해봅시다.\n\\(x_i\\)는 소수점 아래 \\(i\\)번째 숫자를 의미하며 이는 \\(f(i)\\)의 소수점 첫번째 숫자와는 다른 숫자입니다.\n위의 예시에서는 \\(x_1 \\not=7,x_2\\not=4,x_3\\not=2\\)입니다.\n소수점 한자리씩 살펴보면 다음과 같은 사실이 나타납니다. \\[0.x_1 \\not\\in [f(1)],0.x_1x_2 \\not\\in [f(1),f(2)],0.x_1x_2x_3 \\not\\in [f(1),f(2),f(3)]\\]\n이를 계속해서 무한히 살펴보면 다음과 같습니다.(집합기호 오른쪽은 전사함수 정의에 의해 공역 = 치역이므로 같습니다.) \\[0.x_1x_2x_3x_4\\dots \\not\\in [f(1),f(2),f(3),\\dots] = [0,1]\\]\n분명히 \\(0.x_1x_2x_3x_4\\dots\\)는 \\([0,1]\\)에 속하는 숫자인데 전개가 된 위의 식에 의하여 \\([0,1]\\)에 속하지 않는다는 모순이 발생합니다.\n따라서 위의 명제 \\(|\\mathbb{N}| \\geq |[0,1]|\\) 거짓이며 \\(|\\mathbb{N}| < |[0,1]|\\)가 참인 명제입니다.\n이는 궁극적으로 우리가 궁금했던 명제\\(|\\mathbb{Q}| < |\\mathbb{R}|\\)가 참임을 증명하기 위한 충분조건이였습니다.\n따라서 \\(|\\mathbb{Q}| < |\\mathbb{R}|\\)는 참입니다.\n\n\n\n\n\n\n\n유리수집합 vs 실수집합\n\n\n\n\n문제를 조금 쉽게 바꾸고 귀류법을 사용하여 반대되는 명제가 거짓임을 밝혔습니다.\n결과적으로 \\(|\\mathbb{Q}| < |\\mathbb{R}|\\)입니다."
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#유리수집합-vs-무리수집합",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#유리수집합-vs-무리수집합",
    "title": "[Stochestic Process] 3.무한집합간의 크기 비교",
    "section": "유리수집합 vs 무리수집합",
    "text": "유리수집합 vs 무리수집합\n\n여기서는 귀류법을 사용하여 무리수집합의 카디널리티가 무리수집합의 카디널리티와 다름을 증명합니다.\n즉 다음을 증명하고자 하는 명제는 다음과 같습니다. \\[|\\mathbb{Q}| \\not= |\\mathbb{Q}^c|\\]\n먼저 이와 반대되는 명제를 가정합니다. \\[|\\mathbb{Q}| = |\\mathbb{Q}^c| = \\aleph_0\\]\n윗식에서 유리수집합의 크기는 \\(\\aleph_0\\)로 이미 증명된 사실입니다.\n따라서 반대되는 명제에서 유리수집합과 무리수집합의 크기는 \\(\\aleph_0\\)로 같습니다.\n음의정수 집합의 크기 \\(|\\mathbb{N}^-| = \\aleph_0\\)이므로 반대되는 명제와 함께 다음의 명제를 유도할 수 있습니다.\n\n\\(|\\mathbb{Q}| = \\aleph_0\\)이므로 \\(f : \\mathbb{Q} \\rightarrow \\mathbb{N}\\)인 전단사함수 \\(f\\)가 존재한다.\n\\(|\\mathbb{Q^c}| = \\aleph_0\\)이므로 \\(g : \\mathbb{Q^c} \\rightarrow \\mathbb{N^-}\\)인 전단사함수 \\(g\\)가 존재한다.\n\n따라서 \\(h : \\mathbb{Q}\\cup\\mathbb{Q^c} \\rightarrow \\mathbb{N} \\cup \\mathbb{N}^-\\)인 전단사함수가 \\(h\\)가 존재함을 알 수 있으며 \\(|\\mathbb{Q}\\cup\\mathbb{Q^c}| = |\\mathbb{N} \\cup \\mathbb{N}^-| = \\aleph_0\\)임을 유도할 수 있습니다.(정수집합에서 0을 뺀 집합인 \\(\\mathbb{N} \\cup \\mathbb{N}^-\\)도 자연수집합과 크기가 같음을 위해서 증명했었습니다.)\n그러나 여기서 \\(|\\mathbb{Q} \\cup \\mathbb{Q^c}|\\)는 우리는 실수집합 \\(\\mathbb{R}\\)이며 \\(|\\mathbb{Q}| < |\\mathbb{R}|\\)이미 참인 명제임을 이전에 증명했었습니다.\n따라서 모순이 생겼으므로 \\(|\\mathbb{Q}| = |\\mathbb{Q}^c|\\)는 거짓인 명제이며 따라서 증명하고자 했던 명제\\(|\\mathbb{Q}| \\not= |\\mathbb{Q^c}|\\)는 참임이 밝혀졌습니다.\n\n\n\n\n\n\n\n유리수집합 vs 무리수집합\n\n\n\n\n귀류법을 사용하여 반대되는 명제가 거짓임을 증명했습니다.\n결과적으로 \\(|\\mathbb{Q}| \\not= |\\mathbb{Q^c}|\\)입니다."
  },
  {
    "objectID": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#해결해야-할-것",
    "href": "posts/stochastic process/SP-cardinality_comparison/SP_cardinality_comparison.html#해결해야-할-것",
    "title": "[Stochestic Process] 3.무한집합간의 크기 비교",
    "section": "해결해야 할 것",
    "text": "해결해야 할 것\n\n\\(|\\mathbb{Q}| < |\\mathbb{Q^c}|\\) 증명하는 방법?"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": "",
    "text": "지난 포스팅에서 유리수집합과 무리수집합의 길이가 다름을 증명했고 따라서 르벡메져를 통한 두 집합의 길이가 왜 다른지 어느정도의 직관을 얻을 수 있었습니다.\n이로서 “표본공간의 모든 부분집합에 대해서 모순없이 확률을 정의할 수 있는건가?” 싶었지만 여전히 모순이 발생하게 하는 집합(비탈리집합)이 존재함이 밝혀졌습니다.\n그렇다면 다른 measure를 도입하거나 아예 모든 부분집합에 대해서 확률을 모순없이 정의하는 것을 포기하는 방법이 있습니다. 여기서 다른 measure를 도입해도 여전히 모순이 발생함이 이미 밝혀져 있습니다.\n따라서 표본공간의 모든 부분집합이 아니라 가능한 모든 부분집합들 중 일부이자 확률을 모순없이 정의할 수 있는 집합들의 모음인 sigma field를 약속하고 이에 대하여 확률을 정의합니다.\n이번 포스팅에서는 확률을 모순없이 정의할 수 있는 집합들의 모음인 simga field에는 어떠한 조건이 들어갈지에 대해 대략적으로 파악하고 또한 sigma field를 정의하기거 얼마나 어려운지를 정리합니다.\n이어지는 포스팅에서는 sigma field를 보다 엄격하게 + 최대한 간단히 정의합니다."
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#예시1-omega-ht-mathcalf-emptysethtomega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#예시1-omega-ht-mathcalf-emptysethtomega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " 예시1 \\(\\Omega = \\{H,T\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\{H\\},\\{T\\},\\Omega\\}\\)",
    "text": "예시1 \\(\\Omega = \\{H,T\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\{H\\},\\{T\\},\\Omega\\}\\)\n\ncollection \\(\\mathcal{F}\\) 즉, 확률을 모순없이 정의할 수 있는 집합들의 모음인 sigma field를 위와 같이 정하는 것은 합리적일까요?\n여기서 모순이 없다는 확률에 대해 알고있던 사실들과 공리를 벗어나지 않음을 의미합니다.(간단히)\n하나의 예시를 들어보자면 확률을 \\(P(\\emptyset) = 0,P(\\{H\\}) = \\frac{1}{3},P(\\{T\\}) = \\frac{2}{3},P(\\Omega) = 1\\)처럼 정의할 수 있습니다.\n이렇게 확률을 정의한다면 어떠한 모순도 보이지 않습니다."
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#예시2-omega-ht-mathcalf-emptysethomega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#예시2-omega-ht-mathcalf-emptysethomega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " 예시2 \\(\\Omega = \\{H,T\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\{H\\},\\Omega\\}\\)",
    "text": "예시2 \\(\\Omega = \\{H,T\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\{H\\},\\Omega\\}\\)\n\nsigma field를 위와 같이 정하는 것은 합리적일까요?\n먼저 sigma field를 다시 생각해봅시다.\nsigma field는 모순없이 확률을 정의할 수 있는 집합들의 묶음이였습니다.\n여기서 정의와 어긋남이 발생합니다.(합리적이지 못해요)동전던지기를 예시로 들어봅사디\n\n앞면(H)가 나올 확률 = \\(P(\\{H\\}) = \\frac{1}{2}\\)라고 합시다.\n그렇다면 동전의 뒷면이 나올 확률은 자연스럽게 \\(P(\\{H\\}^c) = P(\\{T\\}) = 1- \\frac{1}{2} = \\frac{1}{2}\\)임을 알 수 있습니다.\n즉 어떤 원소의 여사건을 포함시켜도 모순없이 확률이 정의가 되지만 위에서 \\(\\mathcal{F}\\)는 여사건을 포함시키지 않고 있습니다.\n말로 설명하자면 \\(\\mathcal{F}\\)는 “동전의 앞면이 나올 확률만 알 수 있어.”입니다. 이는 합리적이지 못하며 “동전의 앞면이 나올 확률을 알면 뒷면이 나올 확률도 알 수 있어”가 합리적인 주장입니다.\n\n위의 예시로부터 sigma field를 합리적으로 정의하기 위한 조건 중 하나는 “표본공간에 속하는 모든 사건에 대해서 그 사건이 sigma field에 포함되면 여사건도 포함되어야 한다.”를 도출해낼 수 있습니다.\n즉, 다음의 명제는 참입니다.\n\n\n\n\n\n\n\n조건1\n\n\n\n\n\\(\\forall A \\subset \\Omega : A \\in \\mathcal{F} \\Rightarrow A^c \\in \\mathcal{F}\\)"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#예시3-omega-ht-mathcalf-htomega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#예시3-omega-ht-mathcalf-htomega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " 예시3 \\(\\Omega = \\{H,T\\}\\), \\(\\mathcal{F} = \\{\\{H\\},\\{T\\},\\Omega\\}\\)",
    "text": "예시3 \\(\\Omega = \\{H,T\\}\\), \\(\\mathcal{F} = \\{\\{H\\},\\{T\\},\\Omega\\}\\)\n\nsigma field를 위와 같이 정하는 것은 합리적일 까요?\n조건1에 의하면 sigma field에 속한 임의의 원소에 대한 여사건도 반드시 sigma field에 포함되어야 있어야 합니다.\n그러나 \\(\\Omega^c = \\emptyset \\not\\in \\mathcal{F}\\)이므로 위와 같은 sigma field는 합리적이지 못합니다.\n\n\n\n\n\n\n\n조건2\n\n\n\n\n\\(\\Omega,\\emptyset \\in \\mathcal{F}\\)"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#예시5-omega-123456-mathcalf-emptyset12dots12dots123456omega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#예시5-omega-123456-mathcalf-emptyset12dots12dots123456omega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " 예시5\\(\\Omega = \\{1,2,3,4,5,6\\}\\)\\(\\mathcal{F} = \\{\\emptyset,\\{1\\},\\{2\\},\\dots,\\{1,2\\},\\dots,\\{1,2,3,4,5,6\\},\\Omega\\}\\)",
    "text": "예시5\\(\\Omega = \\{1,2,3,4,5,6\\}\\)\\(\\mathcal{F} = \\{\\emptyset,\\{1\\},\\{2\\},\\dots,\\{1,2\\},\\dots,\\{1,2,3,4,5,6\\},\\Omega\\}\\)\n\n예시5는 sigma field가 sample space에서 모든 부분집합들의 모음인 멱집합(powerset,\\(2^\\Omega\\)로 표기)입니다.\n위와 같은 sigma field는 합리적인데 위의 예시를 주사위를 던지는 실험이라고 생각하면 모든 경우에 대해 확률을 모순없이 정의할 수 있기 때문입니다.\n주목할 점은 이 경우에는 powerset 즉, 표본공간에서 가능한 모든 부분집합들의 모음을 sigma field로 정했지만 이는 항상 합리적이진 않으며 sample space마다 다르다는 점입니다.\n주목할 점은 이 경우에는 powerset 즉, 표본공간에서 가능한 모든 부분집합들의 모음을 sigma field로 정해도 합리적이라는 사실입니다.\n우리는 sample space의 모든 부분집합들의 모임(powerset)에 대해서 확률을 모순없이 정의하는 것이 불가능 한 경우가 있기에 합리적인 sigma field의 조건을 찾는 중이었습니다.\n위의 예시는 sample space의 모든 subset에 대해서 확률을 정의하는게 가능한 경우도 있다는 것을 보여줍니다.(항상 일부가 아닙니다.)"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#예시6-omega-123456-mathcalf-emptyset612345omega-2omega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#예시6-omega-123456-mathcalf-emptyset612345omega-2omega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " 예시6 \\(\\Omega = \\{1,2,3,4,5,6\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\{6\\},\\{1,2,3,4,5\\},\\Omega\\} = 2^{\\Omega}\\)",
    "text": "예시6 \\(\\Omega = \\{1,2,3,4,5,6\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\{6\\},\\{1,2,3,4,5\\},\\Omega\\} = 2^{\\Omega}\\)\n\n예시6은 sample space가 예시5와 거의 동일하지만 sigma field \\(\\mathcal{F}\\)만 다른 경우입니다.\n위와 같은 sigama field도 합리적입니다.\n다만 예시5에서는 표본공간의 모든 부분집합에 대해서 관심이 있었다면 이 경우는 6이거나 6이아닌 경우의 확률만 관심이 있기 때문에 sigma field를 더 작게 정의했다고 할 수 있습니다.\n이렇게 sigma field를 정의하는 또다른 예시들은 아래와 같은 것들이 있습니다.\n\n롤에서 미드 vs 다른라인 고르는 경우(탑or원딜…)에 대한 확률 (저는 이게 제일 와닿네요 ㅎㅎ)\n카페에서 아메리카노를 고르거나 고르지 않는 경우(카페라떼,카푸치노 등등등..)에 대한 확률\n\n위와 같은 예시로부터 하나의 sample space에 대해서 sigma field는 유일하지 않으며 다양하게 존재함을 알 수 있습니다.\n\n\n\n\n\n\n\n알 수 있는 성질\n\n\n\n\n하나의 표본공간에 대하여 시그마 필드는 유일하지 않다.\n관심있는 사건이 다른 경우 시그마 필드를 다르게 정의할 수 있다."
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#예시7-omega-123456-mathcalf-emptysetomega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#예시7-omega-123456-mathcalf-emptysetomega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " 예시7 \\(\\Omega = \\{1,2,3,4,5,6\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\Omega\\}\\)",
    "text": "예시7 \\(\\Omega = \\{1,2,3,4,5,6\\}\\), \\(\\mathcal{F} = \\{\\emptyset,\\Omega\\}\\)\n\n위와 같은 sigma field도 합리적이긴 합니다.\n하지만 \\(P(\\Omega) = 1,P(\\emptyset) = 0\\)인 것은 자명합니다.(쓸모가 없습니다.)\n이러한 sigma field를 trivial sigma field라고 합니다."
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field.html#예시8-omega-1234mathcalfemptyset-1-2-234-134-omega",
    "href": "posts/stochastic process/SP-sigmafield/sigma field.html#예시8-omega-1234mathcalfemptyset-1-2-234-134-omega",
    "title": "[Stochestic Process] 4.sigma field(1)",
    "section": " 예시8 \\(\\Omega = \\{1,2,3,4\\},\\mathcal{F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega \\}\\)",
    "text": "예시8 \\(\\Omega = \\{1,2,3,4\\},\\mathcal{F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega \\}\\)\n\n 풀이1\n\n위와 같은 sigma field는 합리적이지 못합니다.\n예시를 들어봅시다.\n\n주사위를 던져서 1이 나올 확률과 2가 나올 확률을 알고있습니다.\n그렇다면 3또는4가 나오는 사건 \\(\\{1,2\\}\\)에 대한 확률은 각각의 확률을 더함으로서 자명하게 구할 수 있으며 이를 sigma field에 포함시켜도 확률에 모순이 없습니다.\n즉, 표본공간에 속하는 교집합이 없는 임의의 두 사건이 sigma field에 속할 때 임의의 두 사건에 대한 합집합이 sigma field포함되어도 모순이 없지만 위에서 \\(\\mathcal{F}\\)는 합집합을 포함하지 않습니다.\n말로 설명하자면 위의 \\(\\mathcal{F}\\)는 “3이 나올 확률은 알고 4가 나올 확률은 아는데 3또는4가 나올 확률은 몰라”라고 말하고 있습니다.\n이는 합리적이지 못하며 “3이 나올 확률은 알고 4가 나올 확률은 아니까 3또는4가 나올 확률도 알아”가 합리적입니다.\n\n위의 예시로부터 sigma field를 합리적으로 정의하기 위한 조건 중 하나는 “표본공간에 속하는 임의의 두 사건이 교집합이 존재하지 않을 때 두 사건이 모두 sigma field에 속한다면 두 사건의 합집합도 포함되어야 한다.”를 도출해낼 수 있습니다.\n즉, 다음의 명제는 참입니다.\n\n\n\n\n\n\n\n조건3\n\n\n\n\\[\\forall A,B \\subset \\Omega \\text{ such that } A \\cap B = \\emptyset : A,B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}\\]\n\n\n\n이와 같은 논리로 disjoint한 두 사건의 합집합을 sigma field에 포함시키고 또한 여집합도 고려했을때 수정된 시그마필드는 다음과 같습니다\n\n\\[\\Omega = \\{1,2,3,4\\},\\mathcal{F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega,\\{1,2\\},\\{3,4\\}\\}\\]\n\n\n 풀이2\n\n예시를 들어봅시다.\n\n주사위를 던져서 1이거나 3이거나 4가 나올 확률과 1이 나올 확률을 알고 있습니다.\n그렇다면 \\(\\{3,4\\}\\)이 나올 확률은 \\(P(\\{1,3,4\\}) - P(\\{1\\})\\)로 자명하게 구할 수 있습니다.\n즉, 표본공간에 속하며 포함관계에 있는 임의의 두 사건이 sigma field에 속할 때 두 사건에 대한 차집합도 sigma field에 포함시켜도 확률을 정의할 때 모순이 없지만 위에서 \\(\\mathcal{F}\\)는 차집합을 포함하지 않습니다.\n말로 설명하자면 위의 \\(\\mathcal{F}\\)는 “1아니면 3아니면 4가 나올 확률은 알고 1이 나올 확률은 아는데 3또는4가 나올 확률은 모른다”라고 말하고 있는 것입니다.\n이는 합리적이지 못하며 “1아니면 3아니면 4가 나올 확률을 알고 1이 나올 확률도 알기 때문에 3또는4가 나올 확률도 알아”가 합리적입니다.\n\n\n\n\n\n\n\n\n조건4\n\n\n\n\\[\\forall A,B \\subset \\Omega \\text{ such that } A \\subset B: A,B \\in \\mathcal{F} \\Rightarrow B - A \\in \\mathcal{F}\\]\n\n\n\n이와 같은 논리로 포함관계에 있는 두 사건의 차집합을 sigma field에 포함시키고 또한 여집합도 고려했을때 수정된 시그마필드는 다음과 같습니다.\n\n\\[\\Omega = \\{1,2,3,4\\},\\mathcal{F}=\\{\\emptyset, \\{1\\}, \\{2\\}, \\{2,3,4\\}, \\{1,3,4\\}, \\Omega,\\{3,4\\},\\{1,2\\}\\}\\]"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field2.html",
    "href": "posts/stochastic process/SP-sigmafield/sigma field2.html",
    "title": "[Stochestic Process] 5.sigma field(2)",
    "section": "",
    "text": "지난 포스팅에서는 확률을 모순없이 정의할 수 있는 집합들의 모음인 simga field에는 어떠한 조건이 들어갈지에 대해 대략적으로 파악하고 또한 sigma field를 정의하기거 얼마나 어려운지를 정리했었습니다.\n이번 포스팅에서는 sigma field에 보다 엄격하게 정의하기 위한 추가적인 조건들을 정리합니다.\n또한 조건들을 여러가지 조건들을 최대한 간단히하여 sigma field를 정의합니다."
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field2.html#용어정리",
    "href": "posts/stochastic process/SP-sigmafield/sigma field2.html#용어정리",
    "title": "[Stochestic Process] 5.sigma field(2)",
    "section": " 용어정리",
    "text": "용어정리\n\n측정할 수 있는 집합 = 잴 수 있는 집합 = measurable set\n측정할 수 있는 집합들의 모임 = measure에서 sigma field"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field2.html#measure",
    "href": "posts/stochastic process/SP-sigmafield/sigma field2.html#measure",
    "title": "[Stochestic Process] 5.sigma field(2)",
    "section": " measure",
    "text": "measure\n\n지금까지 우리는 sigma field를 확률을 모순없이 정의할 수 있는 sample space의 부분집합의 모음이라고 생각하며 확률에 대해서만 생각해왔습니다.\n하지만 사실 sigma field는 확률을 포함하는 좀 더 넓고 일반적인 개념인 measure와 함께 사용합니다.\nmeasure는 기하학적인 측정(길이,면적,부피)과 더불어 질량,확률과 같은 다른 흔한 개념을 일반화 또는 형식화한 것입니다.\n분명히 이러한 개념은 서로다른 구별되는 개념이지만 유사한 점이 많고 하나의 수학적인 맥락으로 자주 취급할때가 많다고 합니다.\n따라서 이러한 개념들을 조금 더 일반화한 개념인 measure를 사용합니다.\n정의는 다음과 같습니다.\n\n\n\n\n\n\n\n\nDefinition\n\n\n\n\nmeasure는 어떤 집합 \\(X\\)가 있을 때, \\(X\\)에 대한 sigma field가 정의역이고 공역은 \\(\\mathbb{R}\\)인 집합에 대한 함수이며 다음과 같은 property를 가집니다.\n\nmeasure의 값은 항상 0보다 크거나 같습니다.\n공집합의 경우 measure는 0입니다.\n교집합이 없는 disjoint sets들에 대하여 countable union의 measure는 각각의 집합에서 구한 measure의 countable sum과 같습니다.\n\n\n\n\n\nmeasure에서 생각해봤을때 sigma field는 모순없이 잴 수 있는 전체집합의 부분집합(잴 수 있는 집합)의 모음입니다.(collection of measurable sets)\n\n\n\n\nmeasure\n\n\n\nmeasure는 함수로 본다면 monotone function으로 생각할 수 있습니다.\n예를 들어 \\(A \\subseteq B\\)라면 \\(\\mu(A)\\leq\\mu(B)\\)임이 일반적입니다."
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field2.html#구해놨던-조건들",
    "href": "posts/stochastic process/SP-sigmafield/sigma field2.html#구해놨던-조건들",
    "title": "[Stochestic Process] 5.sigma field(2)",
    "section": " 구해놨던 조건들",
    "text": "구해놨던 조건들\n\nsigma field는 확률을 모순없이 정의할 수 있는 표본공간의 부분집합들의 모음이었습니다.\n지금까지 구해본 sigma field가 만족하는 조건은 대략적으로 다음과 같았습니다.\n\n\n\n\n\n\n\nsigma field가 갖춰야할 조건들\n\n\n\n\n\\(\\Omega,\\emptyset \\in \\mathcal{F}\\)\n\\(\\forall A \\subset \\Omega : A \\in \\mathcal{F} \\Rightarrow A^c \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega : A,B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega : A,B \\in \\mathcal{F} \\Rightarrow A - B \\in \\mathcal{F}\\)"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field2.html#추가적인-조건들-구하기",
    "href": "posts/stochastic process/SP-sigmafield/sigma field2.html#추가적인-조건들-구하기",
    "title": "[Stochestic Process] 5.sigma field(2)",
    "section": " 추가적인 조건들 구하기",
    "text": "추가적인 조건들 구하기\n(엄밀하지 않은 설명입니다.)\n\n확률은 위와 같은 measure에 한 종류입니다.\n여기서 measure 중 하나인 길이에 대하여 다음을 생각해봅시다.\n\n[1,3]과 [2,4]의 길이가 어느정도인지 잴 수 있는(=측정할 수 있는) tape가 있어\n[2,3]도 그러면 잴 수 있어.(tape의 길이가 [2,3]보다는 길테니까?!)\n\n위의 예시는 두 구간이 각각 잴 수 있다면 겹치는 구간도 잴 수 있다는 것을 의미합니다.\n이렇게 또다른 measure인 길이에서 sigma field를 생각해봤을때 겹치는 구간에 대해서 잴 수 있다는 것은 합리적입니다.\n확률의 입장에서 교집합을 잴 수 있다는 것은 합리적이지 않아 보일 수 있습니다.\n하지만 여기서는 일반화된 measure의 개념에서 생각하기 때문에 교집합도 가능하다고 가정합니다.\n따라서 두 집합이 각각 잴 수 있다면 두 집합의 교집합도 잴 수 있다는 다음의 명제는 sigma field의 조건입니다.\n\n\n\n\n\n\n\n조건5\n\n\n\n\n\\(\\forall A,B \\subset \\Omega : A,B \\in \\mathcal{F} \\Rightarrow A \\cap B \\in \\mathcal{F}\\)\n\n\n\n\n교집합을 잴 수 있다면 합집합도 잴 수 있음은 당연한 사실입니다.\n따라서 다음의 명제도 참입니다.\n\n\n\n\n\n\n\n조건6\n\n\n\n\n\\(\\forall A,B \\subset \\Omega : A,B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}\\)\n\n\n\n(엄밀하진 않음)\n\n확률의 공리를 보면 \\(\\sigma\\)-additivity라 해서 교집합이 없는(disjoint) 각각의 집합들이 셀 수 있는 집합(countable set)이 \\(\\mathcal{F}\\)에 포함되면 그것들의 합집합도 잴 수 있다고 정의하고 있습니다\n공리에 포함됐다는 것은 어느정도 자명한 사실이라고 합의한 것입니다.(납득이 잘 되지 않지만 일단 이렇게 하기로 했구나~ 이런 느낌으로 넘어가야 할 것 같습니다.)\n\n\n\n\n\n\n\n조건7\n\n\n\n\n\\(\\forall B_1,B_2,... \\subset \\Omega \\text{ such that} B_1,B_2,... \\text{ are disjoint} : B_1,B_2,... \\in \\mathcal{F} \\Rightarrow \\cup_{i=1}^{\\infty}B_i \\in \\mathcal{F}\\)\n\n\n\n\n조건 5와 7을 결합하면 다음과 같은 새로운 조건을 이끌어낼 수 있습니다.\n이는 disjoint일 뿐만 아니라 disjoint가 아닌 집합들에도 적용되는 더 넓은 조건입니다.\n\n\n\n\n\n\n\n조건8\n\n\n\n\\(\\forall A_1,A_2,... \\subset \\Omega : A_1,A_2,...\\in \\mathcal{F} \\Rightarrow \\cup_{i=1}^{i=\\infty}A_i \\in \\mathcal{F}\\)\n\n\n\n이외에도 얼마든지 다른 조건을 만들어낼 수 있습니다.\n예를들면 다음과 같은 조건이 있습니다.\n\n\n\n\n\n\n\n조건 9,10,11\n\n\n\n\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} \\Rightarrow A-B \\in {\\cal F}\\) (by 2,5)\n\\(\\forall A,B,C \\subset \\Omega: A,B,C \\in {\\cal F} \\Rightarrow A\\cup B \\cup C \\in {\\cal F}\\) (by 6)\n\\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\cap_{i=1}^{\\infty}A_i \\in {\\cal F}\\) (by 2,7)"
  },
  {
    "objectID": "posts/stochastic process/SP-sigmafield/sigma field2.html#최대한-간단히-정리하기",
    "href": "posts/stochastic process/SP-sigmafield/sigma field2.html#최대한-간단히-정리하기",
    "title": "[Stochestic Process] 5.sigma field(2)",
    "section": " 최대한 간단히 정리하기",
    "text": "최대한 간단히 정리하기\n\n지금까지 나온 모든 조건들은 이렇습니다.\n\n\n\n\n\n\n\nsigma field가 갖춰야할 조건들\n\n\n\n\n\\(\\Omega,\\emptyset \\in \\mathcal{F}\\)\n\\(\\forall A \\subset \\Omega : A \\in \\mathcal{F} \\Rightarrow A^c \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega \\text{ such that } A \\cap B = \\emptyset : A,B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega \\text{ such that } A \\subset B: A,B \\in \\mathcal{F} \\Rightarrow B - A \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega : A,B \\in \\mathcal{F} \\Rightarrow A \\cap B \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega : A,B \\in \\mathcal{F} \\Rightarrow A \\cup B \\in \\mathcal{F}\\)\n\\(\\forall B_1,B_2,... \\subset \\Omega \\text{ such that} B_1,B_2,... \\text{ are disjoint} : B_1,B_2,... \\in \\mathcal{F} \\Rightarrow \\cup_{i=1}^{\\infty}B_i \\in \\mathcal{F}\\)\n\\(\\forall A_1,A_2,... \\subset \\Omega : A_1,A_2,...\\in \\mathcal{F} \\Rightarrow \\cup_{i=1}^{i=\\infty}A_i \\in \\mathcal{F}\\)\n\\(\\forall A,B \\subset \\Omega:~ A,B \\in {\\cal F} \\Rightarrow A-B \\in {\\cal F}\\) (by 2,5)\n\\(\\forall A,B,C \\subset \\Omega: A,B,C \\in {\\cal F} \\Rightarrow A\\cup B \\cup C \\in {\\cal F}\\) (by 6)\n\\(A_1,A_2,\\dots \\in {\\cal F} \\Rightarrow \\cap_{i=1}^{\\infty}A_i \\in {\\cal F}\\) (by 2,7)\n\n\n\n\n여기서부터는 위의 모든 조건들을 전부다 포함하도록 가장 간단히 몇개의 조건만 남겨서 sigma field를 정의해보겠습니다.\n\n\n조건9,10,11은 조건 2,5,6,7에 의하여 도출할 수 있습니다. 따라서 9,10,11은 삭제합니다.\n조건1은 2에 의해서 공집합까지 표기할 필요가 없도록 수정할 수 있습니다.\n조건4는 2,5에 의해서 도출할 수 있는 조건입니다. 따라서 4는 삭제합니다.\n조건5는 2,6에 의해서 도출할 수 있는 조건입니다. 따라서 5는 삭제합니다.\n조건6은 조건3의 disjoint집합을 포함합니다.. 따라서 3을 삭제합니다.\n조건8은 조건6을 포함하는 더 넓은 countable union입니다. 따라서 6을 삭제합니다.\n조건8은 조건7의 disjoin집합을 포함하는 countable union입니다. 따라서 7을 삭제합니다. \n\n\n삭제조건 : 3,4,5,6,7,9,10,11\n변경될 조건 : 1\n남아있는 조건 : 1,2,8\n이렇게 반영한 조건들이 바로 sigma field의 조건입니다."
  },
  {
    "objectID": "posts/stochastic process/sp_appendix_set.html",
    "href": "posts/stochastic process/sp_appendix_set.html",
    "title": "[Stochestic Process] 부록 - 자꾸 걸리는 집합개념 정리",
    "section": "",
    "text": "집합은 원소들의 모임이며 중복된 원소를 포함하지 않습니다.\n크게 중괄호 안에 원소를 나열하는 원소나열법과 특정한 조건을 만족하는 원소들로 이루어진 집합을 정의하는 조건제시법으로 집합을 정의할 수 있다.\n\n\n\n\n집합론에서 집합 A의 원소가 집합 B의 모든 원소에 속하면 집합 A는 집합 B의 부분집합(subset)이며 집합 B는 집합 A의 상위집합(superset)입니다.\n즉, 부분집합과 상위집합은 두 집합사이에 정의되는 개념으로서 기호로 \\(A \\subseteq B\\)라 표기합니다.\n쉽게 생각해보자면 부분집합은 어떤 집합에 존재하는 각각의 모든 원소에서 판단(선택할지 말지)을 통해 만든 또다른 집합으로 볼 수 있습니다.\n모든 집합은 반드시 자기 자신의 부분집합입니다. 이는 각각의 판단에서 모든 원소를 선택했다고 볼 수 있습니다.\n공집합은 모든 집합의 부분집합입니다. 이는 각각의 판단에서 모든 원소를 선택하지 않음으로 볼 수 있습니다.(엄밀하진 않습니다.공집합이 왜 모든 집합의 부분집합인지를 쉽게 받아들여지지 않아서 이를 기억하기 위함입니다.)\n\n\n\n\n\n집합 A의 원소가 집합 B의 모든 원소에 속하면서 집합 A와 집합 B가 같지않다면(not equal) 집합 A는 집합 B의 진부분집합(proper or strict subset)입니다.\n기호로는 \\(A \\subsetneq B\\)로 표기합니다.\n\n\n\n\n\n위에서 부분집합 기호는 단순히 \\(A\\)가 \\(B\\)의 부분집합임을 나타내었습니다.\n이외에도 부분집합 기호는 참 거짓을 판단해야하는 수학적 명제에서 사용할 수 있습니다.(제일 헷갈렸던 부분 …)\n\\(A \\subseteq B\\)는 `\\(A\\)는 \\(B\\)의 부분집합이다`라는 명제를 의미하거나\n\\(A \\subsetneq B\\)는 `\\(A\\)는 \\(B\\)의 진부분집합이다`라는 명제를 의미합니다.\n\n\n\n\n\n\n\n부분집합을 나타내기 위해서 \\(\\subset\\)을 사용하며 이는 위에서의 \\(\\subseteq\\)와 같은 개념입니다.\n이러한 경우 \\(A \\subset A\\)는 참입니다. 부분집합은 항상 자기자신을 부분집합으로 갖기 때문입니다.\n또한 이 경우 \\(\\subseteq\\)는 단지 두 집합이 같을 수 있음을 강조하는 표현이라 할 수 있습니다.\n이어지는 포스팅에서는 case1의 notation을 활용합니다. 즉, 다음의 경우는 모두 동치입니다.\n\n\\(A \\subset B\\) \\(\\Longleftrightarrow A \\subseteq B\\) \\(\\Longleftrightarrow\\) A가 B의 진부분집합이거나 같다. \\(\\Longleftrightarrow\\) A는 B의 부분집합이다.\n\n\n\n\n\n\n진부분집합(strict or proper subset)을 나타내기 위해서 \\(\\subsetneq\\)대신에 \\(\\subset\\)을 사용할 수 있습니다. 이 경우 \\(\\leq\\),\\(<\\)와 같은 느낌입니다. \n이러한 경우 위와 다르게 \\(A \\subset A\\)는 거짓입니다. \\(\\subset\\)는 자기자신을 뺀 부분집합인 진부분집합을 의미하는 기호이기 때문입니다.\n반대로 \\(A \\subseteq A\\)는 참입니다. \\(\\subseteq\\)는 진부분집합이거나 같음(자기자신을 포함하여 부분집합)을 의미하기 때문입니다.\nstochastic process 포스팅에서 이 notation은 사용하지 않습니다."
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html",
    "title": "plotly 시각화 모음",
    "section": "",
    "text": "Code\n# %pip install plotly (jupyter notebook)\nfrom plotly.offline import iplot\nimport plotly.graph_objs as go\nimport plotly.io as pio\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\npio.renderers.default = \"plotly_mimetype+notebook\"\n\n\n\n\nCode\nimport pandas as pd\ntimesData = pd.read_csv(\"./timesData.csv\")\ntimesData.head(5)\n\n\n\n\n\n\n  \n    \n      \n      world_rank\n      university_name\n      country\n      teaching\n      international\n      research\n      citations\n      income\n      total_score\n      num_students\n      student_staff_ratio\n      international_students\n      female_male_ratio\n      year\n    \n  \n  \n    \n      0\n      1\n      Harvard University\n      United States of America\n      99.7\n      72.4\n      98.7\n      98.8\n      34.5\n      96.1\n      20,152\n      8.9\n      25%\n      NaN\n      2011\n    \n    \n      1\n      2\n      California Institute of Technology\n      United States of America\n      97.7\n      54.6\n      98.0\n      99.9\n      83.7\n      96.0\n      2,243\n      6.9\n      27%\n      33 : 67\n      2011\n    \n    \n      2\n      3\n      Massachusetts Institute of Technology\n      United States of America\n      97.8\n      82.3\n      91.4\n      99.9\n      87.5\n      95.6\n      11,074\n      9.0\n      33%\n      37 : 63\n      2011\n    \n    \n      3\n      4\n      Stanford University\n      United States of America\n      98.3\n      29.5\n      98.1\n      99.2\n      64.3\n      94.3\n      15,596\n      7.8\n      22%\n      42 : 58\n      2011\n    \n    \n      4\n      5\n      Princeton University\n      United States of America\n      90.9\n      70.3\n      95.4\n      99.9\n      -\n      94.2\n      7,929\n      8.4\n      27%\n      45 : 55\n      2011\n    \n  \n\n\n\n\n\n\nCode\ntimesData.info()\n\n\n<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 2603 entries, 0 to 2602\nData columns (total 14 columns):\n #   Column                  Non-Null Count  Dtype  \n---  ------                  --------------  -----  \n 0   world_rank              2603 non-null   object \n 1   university_name         2603 non-null   object \n 2   country                 2603 non-null   object \n 3   teaching                2603 non-null   float64\n 4   international           2603 non-null   object \n 5   research                2603 non-null   float64\n 6   citations               2603 non-null   float64\n 7   income                  2603 non-null   object \n 8   total_score             2603 non-null   object \n 9   num_students            2544 non-null   object \n 10  student_staff_ratio     2544 non-null   float64\n 11  international_students  2536 non-null   object \n 12  female_male_ratio       2370 non-null   object \n 13  year                    2603 non-null   int64  \ndtypes: float64(4), int64(1), object(9)\nmemory usage: 284.8+ KB"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#add-markers-and-text",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#add-markers-and-text",
    "title": "plotly 시각화 모음",
    "section": "add markers and text",
    "text": "add markers and text\n\n\nCode\n#1 data frame\ndf = timesData.iloc[:100]\n\n#2 trace and data\ntrace = go.Scatter(\n    x = df.world_rank,\n    y = df.citations,\n    mode = \"lines+markers\", #add marker,\n    marker = dict(color = \"rgba(16,112,2,0.8)\"),\n    text = df.university_name #add text\n)\ndata = [trace]\n#3 layout and data\nlayout = go.Layout(\n    title = \"citation\",\n    xaxis = dict(title = \"World Rank\",ticklen = 5)\n)\n\n#4 create figure\nfig = go.Figure(data = data,layout = layout)\n\n#5 plot figure\nfig.show()\n\n\n\n                                                \n\n\nversion2가 뭔가 더 좋을듯?"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#add-markers-and-text-1",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#add-markers-and-text-1",
    "title": "plotly 시각화 모음",
    "section": "add markers and text",
    "text": "add markers and text\n\n\nCode\n#1 data frame\ndf2014 = timesData[timesData.year == 2014].iloc[:100,:]\n\n#2 trace,data\ntrace = go.Scatter(\n    x = df2014.world_rank,\n    y = df2014.citations,\n    mode = \"markers\",\n    #marker = dict(color = \"green\",opacity=0.8), #alpha(불투명도) 조절 vs1\n    marker = dict(color = \"rgba(255,128,2,0.8)\"), #alpha(불투명도) 조절 vs2\n    text = df2014.university_name,\n\n)\ndata = [trace]\n\n#3 layout\nlayout = go.Layout(xaxis = dict(title = \"World Rank\"),yaxis = dict(title = \"Citation\"))\n\n#4 create figure\nfig = go.Figure(data=data,layout=layout)\n\n#5 plot\nfig.show()"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#여러개의-차트-겹처-그리기",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#여러개의-차트-겹처-그리기",
    "title": "plotly 시각화 모음",
    "section": "여러개의 차트 겹처 그리기",
    "text": "여러개의 차트 겹처 그리기\n\n여기서는 histogram으로 했으나 다른차트들도 가능\n\n\n\nCode\n#1.dataframe\nx2011 = timesData.student_staff_ratio[timesData.year == 2011]\nx2012 = timesData.student_staff_ratio[timesData.year == 2012]\n#2.trace&data\ntrace1 = go.Histogram(\n    x=x2011,\n    #opacity=0.7, #불투명도 조절\n    name=\"2011\", #범례(legend)를 설정하기 위한 이름 설정\n    marker=dict(color=\"rgb(171,50,96)\",opacity=0.7)\n)\n\ntrace2 = go.Histogram(\n    x=x2012,\n    name=\"2012\",\n    marker=dict(color=\"blue\",opacity=0.7)\n)\ndata=[trace1,trace2]\n#3.layout\nlayout = go.Layout(\n    barmode = \"overlay\", #trace 겹쳐 그리기\n    xaxis=dict(title=\"students-staff ratio\"),\n    yaxis=dict(title=\"count\"),\n    title = dict(text = \"histogram\",x = 0.5)\n)\n#4 figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()\n\n\n\n                                                \n\n\n참고자료 - Opacity와 alpha? : Opacity는 marker안팎에서 모두 쓰일 수 있으며 alpha는 rgba와 쓸때만 입력,같은 역할을 함. 단,Opacity를 marker의 밖에서 입력하면 trace안에서 밀도를 표현 하지 못함. 다른 trace끼리 겹칠때에는 밀도표현됨.(같은 trace에서만 안됨.)\n\n\nCode\n# 1.data frame\ndataframe = timesData[timesData.year == 2015]\n\n#2.trace and data\ndata = []\nfor col in [\"world_rank\",\"citations\",\"income\",\"total_score\"]:\n    _trace = go.Scatter(\n        x = dataframe[\"world_rank\"],\n        y = dataframe[col],\n        mode = \"lines\"\n    )\n    data.append(_trace)\n\n#3. layout\nlayout = go.Layout(\n    xaxis=dict(\n        domain=[0, 0.45]\n    ),\n    yaxis=dict(\n        domain=[0, 0.45]\n    ),\n    xaxis2=dict(\n        domain=[0.55, 1]\n    ),\n    xaxis3=dict(\n        domain=[0, 0.45],\n        anchor='y3'\n    ),\n    xaxis4=dict(\n        domain=[0.55, 1],\n        anchor='y4'\n    ),\n    yaxis2=dict(\n        domain=[0, 0.45],\n        anchor='x2'\n    ),\n    yaxis3=dict(\n        domain=[0.55, 1]\n    ),\n    yaxis4=dict(\n        domain=[0.55, 1],\n        anchor='x4'\n    ),\n    title = 'Research, citation, income and total score VS World Rank of Universities'\n)\n\n#4. fig\nfig = make_subplots(rows=2,cols=2)\n#5. plot\nrow = 1\ncol = 1\nfor trace in data:\n    fig.append_trace(trace,row=row,col=col)\n    col+=1\n    if col > 2:\n        col = 1\n        row+=1\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nfig = make_subplots(\n    rows=2, cols=2,\n    specs=[[{\"type\": \"xy\"}, {\"type\": \"polar\"}],\n           [{\"type\": \"domain\"}, {\"type\": \"scene\"}]],\n)\n\nfig.add_trace(go.Bar(y=[2, 3, 1]),\n              row=1, col=1)\n\nfig.add_trace(go.Barpolar(theta=[0, 45, 90], r=[2, 3, 1]),\n              row=1, col=2)\n\nfig.add_trace(go.Pie(values=[2, 3, 1]),\n              row=2, col=1)\n\nfig.add_trace(go.Scatter3d(x=[2, 3, 1], y=[0, 0, 0],\n                           z=[0.5, 1, 2], mode=\"lines\"),\n              row=2, col=2)\n\nfig.update_layout(height=700, showlegend=False)\n\nfig.show()"
  },
  {
    "objectID": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#vector-fieldquiver-plot",
    "href": "posts/visualization/plotly_visualization/ploty 시각화 모음.html#vector-fieldquiver-plot",
    "title": "plotly 시각화 모음",
    "section": "Vector field(quiver plot)",
    "text": "Vector field(quiver plot)\n\n사전준비\n\nnp.meshgrid : x좌표,y좌표를 가지는 벡터를 입력했을때, 두 벡터로 만들 수 있는 격자의 좌표(x,y)를 출력\n\n\n\nCode\nimport numpy as np\nx_coord = np.arange(0,2,.2)\ny_coord = np.arange(0,2,.2)\nx,y = np.meshgrid(np.arange(0,2,.2),np.arange(0,2,.2))\nprint(x_coord.shape,y_coord.shape)\nprint(x.shape,y.shape)\n\n\n(10,) (10,)\n(10, 10) (10, 10)\n\n\n\n격자(grid,matrix)에 함수 적용하면? => matrix(x,y 각각의 좌표)의 모든 요소에 함수가 적용됨\n\n\n\nCode\nprint(np.cos(x).shape,np.sin(x).shape)\n\n\n(10, 10) (10, 10)\n\n\n\n배열의 요소 값 차례대로 읽어보기 …\n\n(0,0),(0.2,0),(0.4,0) … (1.8,0) => (0,0.2),(0.2,0.2),(0.4,0.2)… x좌표 다 읽고 y좌표증가 그 다음 x좌표 다 읽고 y좌표 증가 …\n\n\nCode\nx,y\n\n\n(array([[0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8],\n        [0. , 0.2, 0.4, 0.6, 0.8, 1. , 1.2, 1.4, 1.6, 1.8]]),\n array([[0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. , 0. ],\n        [0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2],\n        [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4],\n        [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6],\n        [0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8],\n        [1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. , 1. ],\n        [1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2, 1.2],\n        [1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4, 1.4],\n        [1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6, 1.6],\n        [1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8, 1.8]]))\n\n\n\n\nGradient Vector Field\n\n\\(\\nabla f = xe^{-x^2-y^2}\\)\n\n\nCode\n#1.prepare data\nx,y = np.meshgrid(np.arange(-2,2,0.2),np.arange(-2,2,.25)) #좌표\nz = x*np.exp(-x**2-y**2) #함수\n\ndx=0.2;dy=0.25 #dx,dy\nv,u = np.gradient(z,dx,dy) #함수의 그레디언트(각좌표에서의 미분계수)\n\n\n\n\nCode\n#2.trace and data => 생략\n#3.fig\nfig = ff.create_quiver(x,y,u,v,scale=.25,arrow_scale=.4,name=\"quiver\",line_width=1)\nfig.add_trace(go.Scatter(x=[-.7,.75],y=[0,0],\n                         mode=\"markers\",\n                         marker_size=12,\n                         name=\"points\"))\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nx = np.linspace(-1,1,100)\ny = np.linspace(-1,1,100)\nxx,yy = np.meshgrid(x,y)\nfor i in range()\n\n\n(array([[-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        ...,\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ],\n        [-1.        , -0.97979798, -0.95959596, ...,  0.95959596,\n          0.97979798,  1.        ]]),\n array([[-1.        , -1.        , -1.        , ..., -1.        ,\n         -1.        , -1.        ],\n        [-0.97979798, -0.97979798, -0.97979798, ..., -0.97979798,\n         -0.97979798, -0.97979798],\n        [-0.95959596, -0.95959596, -0.95959596, ..., -0.95959596,\n         -0.95959596, -0.95959596],\n        ...,\n        [ 0.95959596,  0.95959596,  0.95959596, ...,  0.95959596,\n          0.95959596,  0.95959596],\n        [ 0.97979798,  0.97979798,  0.97979798, ...,  0.97979798,\n          0.97979798,  0.97979798],\n        [ 1.        ,  1.        ,  1.        , ...,  1.        ,\n          1.        ,  1.        ]]))\n\n\n\n\n\n1. 시점\n\n종점은 화살표로 표시해야 하므로 시점만 만들기\n\n\n\nCode\nimport plotly.graph_objs as go\n\n\n\n\nCode\n#1. prepare data\n\n#첫번째 벡터의 시점 x[0],y[0],z[0] 종점 x[1],y[1],z[1]\n#두번째 벡터의 시점 x[2],y[2],z[2] 종점 x[2],y[2],z[2]\n#두 개씩 묶임\nx = [10.1219, 10.42579, 15.21396, 15.42468, 20.29639,20.46268, 25.36298, 25.49156]\ny = [5.0545,  5.180104, 5.0545,   5.20337,  5.0545,  5.194271, 5.0545,   5.231627]\nz = [5.2713,  5.231409, 5.2713,   5.231409, 5.2713 ,  5.235852,  5.2713, 5.231627]\n#pairs = [(0,1),(2,3),(4,5),(6,7)]\n[coord for coord in range(0,len(x),2)]\n\n\n[0, 2, 4, 6]\n\n\n\n\nCode\n#2. trace,data(trace set)\ntrace1 = go.Scatter3d(\n    x=[x[coord] for coord in range(0,len(x),2)],\n    y=[y[coord] for coord in range(0,len(y),2)],\n    z=[z[coord] for coord in range(0,len(z),2)],\n    mode = \"markers\",\n    line=dict(color=\"red\")\n)\ndata = [trace1]\n\n#3. Layout\nlayout = go.Layout(title=dict(text = \"vectors\"))\n\n#4. figure\nfig = go.Figure(data=data,layout=layout)\nfig.show()\n\n\n\n                                                \n\n\n\n\n2. 선 만들기\n\n\nCode\n#1.prepare data\nx_lines = list()\ny_lines = list()\nz_lines = list()\n\nfor i in range(len(x)):\n    x_lines.append(x[i])\n    y_lines.append(y[i])\n    z_lines.append(z[i])\n    #plotly에서 Scatter의 line mode는 점과 점 사이에 선을 만듦\n    #0,1번째 자리의 좌표에는 시점,종점을 넣고 3번째 자리에 None을 추가하여 점을 만들지 않음 \n    #따라서, 선이 생기지 않음\n    if i % 2 == 1:    \n        x_lines.append(None)\n        y_lines.append(None)\n        z_lines.append(None)\n\n#2.trace and tr_set(=data)\ntrace2 = go.Scatter3d(\n    x=x_lines,\n    y=y_lines,\n    z=z_lines,\n    mode = \"lines\",\n    line = dict(width = 2, color = 'rgb(255, 0,0)')\n)\ndata = [trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\n#중간체크\ndata = [trace1,trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\n3.종점 만들기\n\n\nCode\ndata = [trace1,trace2]\n\n#3.layout\nlayout = go.Layout(title = \"lines\")\n#4.figure\nfig = go.Figure(data=data,layout=layout)\n#5.plotting\nfig.show()\n\n\n\n                                                \n\n\n\n\nCode\nimport plotly.graph_objs as go\n# plotly.offline.init_notebook_mode()\n\nx = [10.1219, 10.42579, 15.21396, 15.42468, 20.29639,20.46268, 25.36298, 25.49156]\ny = [5.0545,  5.180104, 5.0545,   5.20337,  5.0545,  5.194271, 5.0545,   5.231627]\nz = [5.2713,  5.231409, 5.2713,   5.231409, 5.2713 ,  5.235852,  5.2713, 5.231627]\n\npairs = [(0,1), (2,3),(4,5), (6,7)]\n\n## plot ONLY the first ball in each pair of balls\ntrace1 = go.Scatter3d(\n    x=[x[p[0]] for p in pairs],\n    y=[y[p[0]] for p in pairs],\n    z=[z[p[0]] for p in pairs],\n    mode='markers',\n    name='markers',\n    line=dict(color='red')\n)\n\ngo.Figure(data=trace1)"
  },
  {
    "objectID": "posts/temp/jbig.html",
    "href": "posts/temp/jbig.html",
    "title": "Untitled",
    "section": "",
    "text": "\\[\\begin{split}\n\\begin{aligned}\n&\\text{Let } A \\in R^{200 \\times 20},\\text{rank}(A) = 20\\\\\n&A = U\\Sigma V^T\\\\\n&\\Longleftrightarrow\nA =\n\n\\begin{bmatrix}\n\\boxed{\\begin{matrix} \\\\ \\\\ \\\\ \\\\ \\\\ \\, u_1 \\, \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\end{matrix}} \\!\\!\\!\\!&\n\\boxed{\\begin{matrix} \\\\ \\\\ \\\\ \\\\ \\\\ \\, u_2 \\, \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\end{matrix}} \\!\\!\\!\\!&\n\\boxed{\\begin{matrix} \\\\ \\\\ \\\\ \\\\ \\\\ \\, u_3 \\, \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\end{matrix}} \\!\\!\\!\\!&\n\\cdots \\!\\!\\!\\!&\n\\boxed{\\begin{matrix} \\\\ \\\\ \\\\ \\\\ \\\\  u_M  \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\end{matrix}} \\!\\!\\!\\!&\n\\cdots \\!\\!\\!\\!&\n\\boxed{\\begin{matrix} \\\\ \\\\ \\\\ \\\\ \\\\ u_{200} \\\\ \\\\ \\\\ \\\\ \\\\ \\\\ \\end{matrix}}\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boxed{\\sigma_1 \\phantom{\\dfrac{}{}} \\!\\!} & 0 & 0 & \\cdots & 0 \\\\\n0 & \\boxed{\\sigma_2 \\phantom{\\dfrac{}{}} \\!\\!} & 0 & \\cdots & 0 \\\\\n0 & 0 & \\boxed{\\sigma_3 \\phantom{\\dfrac{}{}} \\!\\!} & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots & \\ddots \\\\\n0 & 0 & 0 & \\cdots & \\boxed{\\sigma_{20} \\phantom{\\dfrac{}{}} \\!\\!} \\\\\n0 & 0 & 0 & \\cdots & 0 \\\\\n\\vdots & \\vdots & \\vdots &  & \\vdots \\\\\n0 & 0 & 0 & \\cdots & 0 \\\\\n\\end{bmatrix}\n\\begin{bmatrix}\n\\boxed{\\begin{matrix} & & & v_1^T & & & \\end{matrix}} \\\\\n\\boxed{\\begin{matrix} & & & v_2^T & & & \\end{matrix}} \\\\\n\\vdots \\\\\n\\boxed{\\begin{matrix} & & & v_{20}^T & & & \\end{matrix}} \\\\\n\\end{bmatrix}\n\\tag{3.4.5}\\\\\n&\\Longleftrightarrow\nA = \\begin{bmatrix}\\sigma_1u_1\\ & \\sigma_2u_2,\\dots,\\sigma_{20}u_{20}\\end{bmatrix}\n\\begin{bmatrix}\n\\boxed{\\begin{matrix} & & & v_1^T & & & \\end{matrix}} \\\\\n\\boxed{\\begin{matrix} & & & v_2^T & & & \\end{matrix}} \\\\\n\\vdots \\\\\n\\boxed{\\begin{matrix} & & & v_{20}^T & & & \\end{matrix}} \\\\\n\\end{bmatrix}\n\\\\\n&\\Longleftrightarrow A = \\sigma_1u_1v_1^T + \\sigma_2u_2v_2^T + \\sigma_3u_3v_3^T + \\dots\n\\end{aligned}\n\\end{split}\\]"
  }
]